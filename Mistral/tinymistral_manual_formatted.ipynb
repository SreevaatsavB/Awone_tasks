{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.trainer_utils import set_seed\n",
    "import torch\n",
    "SEED = 6\n",
    "set_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MistralConfig {\n",
       "  \"_name_or_path\": \"./tiny-mistral\",\n",
       "  \"architectures\": [\n",
       "    \"MistralForCausalLM\"\n",
       "  ],\n",
       "  \"attention_dropout\": 0.0,\n",
       "  \"bos_token_id\": 1,\n",
       "  \"dropout_p\": 0,\n",
       "  \"eos_token_id\": 2,\n",
       "  \"hidden_act\": \"silu\",\n",
       "  \"hidden_size\": 8,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 8,\n",
       "  \"max_position_embeddings\": 32768,\n",
       "  \"model_type\": \"mistral\",\n",
       "  \"num_attention_heads\": 4,\n",
       "  \"num_hidden_layers\": 1,\n",
       "  \"num_key_value_heads\": 2,\n",
       "  \"output_hidden_states\": true,\n",
       "  \"rms_norm_eps\": 1e-05,\n",
       "  \"rope_theta\": 10000.0,\n",
       "  \"sliding_window\": 3,\n",
       "  \"tie_word_embeddings\": false,\n",
       "  \"torch_dtype\": \"bfloat16\",\n",
       "  \"transformers_version\": \"4.38.0.dev0\",\n",
       "  \"use_cache\": false,\n",
       "  \"vocab_size\": 32000\n",
       "}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoConfig, MistralConfig\n",
    "\n",
    "\n",
    "mistral_config = MistralConfig.from_pretrained(\"openaccess-ai-collective/tiny-mistral\", num_hidden_layers = 1, use_cache = False, hidden_size = 8, num_attention_heads = 4, \n",
    "                                           output_hidden_states=True,  num_key_value_heads = 2, past_key_values = True, intermediate_size = 8, sliding_window = 3, dropout_p = 0)\n",
    "\n",
    "\n",
    "mistral_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel\n",
    "\n",
    "tinymistral = AutoModel.from_config(mistral_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "src_sent = \"hi how are you doing\"\n",
    "\n",
    "mistal_tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-v0.1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    1, 12014,   910,   460,   368,  2548]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_src_dict = mistal_tokenizer.encode_plus(src_sent, return_tensors='pt')\n",
    "tokenized_src_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    1, 12014,   910,   460,   368,  2548]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_src_dict = mistal_tokenizer.encode_plus(src_sent, return_tensors='pt')\n",
    "tokenized_src_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    1, 12014,   910,   460,   368,  2548]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_tokenized = tokenized_src_dict[\"input_ids\"]\n",
    "src_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s> hi how are you doing'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mistal_tokenizer.decode(*src_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6,)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_tokenized_np = src_tokenized.numpy()[0]\n",
    "src_tokenized_np.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mistral "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **SLIDING WINDOW ATTENTION** and **GROUPED-QUERY ATTENTION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mistral_config.num_key_value_heads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_rep =  2\n"
     ]
    }
   ],
   "source": [
    "embed_dim = d_model = 8\n",
    "\n",
    "num_heads = 4\n",
    "\n",
    "n_heads_q = num_heads\n",
    "\n",
    "n_kv_heads = 2\n",
    "\n",
    "seq_len = 6\n",
    "\n",
    "head_dim = embed_dim // num_heads\n",
    "\n",
    "n_rep = n_heads_q//n_kv_heads\n",
    "print(\"n_rep = \", n_rep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict = tinymistral.state_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Source token embeddings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word index: 1, Embedding: [-0.00737046 -0.02541412  0.00674542  0.01122305  0.0314588   0.0374115\n",
      "  0.01647354 -0.0181274 ]\n",
      "Word index: 12014, Embedding: [-0.01305792  0.02176759  0.02252432  0.05248258  0.0101565   0.03629604\n",
      "  0.00324979 -0.01367457]\n",
      "Word index: 910, Embedding: [-0.03094564  0.02090054 -0.02141277  0.02651504 -0.01471478 -0.02430361\n",
      "  0.03498438  0.02504197]\n",
      "Word index: 460, Embedding: [-0.01378937 -0.01297163  0.01389042  0.00852889 -0.00494728  0.00086873\n",
      " -0.00707719  0.00835401]\n",
      "Word index: 368, Embedding: [ 0.03763684 -0.01263603  0.00670438 -0.00092739  0.01852957 -0.03717487\n",
      " -0.01282768  0.02035378]\n",
      "Word index: 2548, Embedding: [-0.01604516  0.01489605  0.00696284 -0.00985821 -0.0117752  -0.01232624\n",
      "  0.01767547  0.03663577]\n",
      "\n",
      "(6, 8)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "src_vocab_embeds = state_dict[\"embed_tokens.weight\"].numpy()\n",
    "\n",
    "src_embedding = np.zeros((src_tokenized_np.shape[0], d_model))\n",
    "\n",
    "for i in range(src_tokenized_np.shape[0]):\n",
    "        word_index = src_tokenized_np[i]\n",
    "        if word_index < 0 or word_index >= src_vocab_embeds.shape[0]:\n",
    "            raise ValueError(f\"Invalid word index: {word_index}\")\n",
    "        src_embedding[i, :] = src_vocab_embeds[word_index, :]\n",
    "\n",
    "        print(f\"Word index: {word_index}, Embedding: {src_vocab_embeds[word_index, :]}\")\n",
    "print()\n",
    "print(src_embedding.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "residual = src_embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Pre-normalization (RMSNorm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "variance_epsilon = 1e-05\n",
    "\n",
    "wt = state_dict[\"layers.0.input_layernorm.weight\"].numpy()\n",
    "\n",
    "dtype = src_embedding.dtype\n",
    "src_embedding = src_embedding.astype(np.float32)\n",
    "variance = np.mean(src_embedding**2, axis=-1, keepdims=True)\n",
    "src_embedding = src_embedding * (1/np.sqrt(variance + variance_epsilon))\n",
    "\n",
    "hidden_state = wt * src_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[-0.33207855, -1.1450424 ,  0.30391717,  0.5056585 ,  1.4173874 ,\n",
       "          1.6855887 ,  0.7422213 , -0.8167365 ],\n",
       "        [-0.49335742,  0.8224283 ,  0.85101897,  1.982909  ,  0.3837353 ,\n",
       "          1.3713453 ,  0.12278415, -0.51665574],\n",
       "        [-1.2029388 ,  0.81245935, -0.83237123,  1.0307097 , -0.57200253,\n",
       "         -0.94474554,  1.3599354 ,  0.9734475 ],\n",
       "        [-1.3383939 , -1.2590235 ,  1.3482019 ,  0.82781243, -0.48018178,\n",
       "          0.08431846, -0.68691105,  0.810839  ],\n",
       "        [ 1.6818779 , -0.5646665 ,  0.2995986 , -0.0414422 ,  0.8280312 ,\n",
       "         -1.661234  , -0.5732305 ,  0.9095496 ],\n",
       "        [-0.881556  ,  0.8184212 ,  0.3825538 , -0.5416313 , -0.6469551 ,\n",
       "         -0.6772302 ,  0.9711286 ,  2.0128489 ]], dtype=float32),\n",
       " (6, 8))"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_state, hidden_state.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Getting the Q,K and V matrices for attention calcualtion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "Wq = state_dict[\"layers.0.self_attn.q_proj.weight\"].numpy()\n",
    "Wk = state_dict[\"layers.0.self_attn.k_proj.weight\"].numpy()\n",
    "Wv = state_dict[\"layers.0.self_attn.v_proj.weight\"].numpy()\n",
    "\n",
    "\n",
    "query = np.matmul(hidden_state, Wq.T)\n",
    "key = np.matmul(hidden_state, Wk.T)\n",
    "value = np.matmul(hidden_state, Wv.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[-0.01127685, -0.02173477, -0.03691161, -0.02306341,  0.00182827,\n",
       "          0.00958379, -0.01198711,  0.04022793],\n",
       "        [-0.05099525, -0.05224977, -0.05096808, -0.03565389, -0.05645479,\n",
       "          0.04469625,  0.0819565 , -0.01108029],\n",
       "        [-0.01349053,  0.00335787,  0.03539798, -0.09065476, -0.03000753,\n",
       "          0.02455559,  0.0032218 , -0.06504469],\n",
       "        [ 0.00718971,  0.02755949, -0.07008822,  0.03163479,  0.06265695,\n",
       "          0.10716037, -0.01247996,  0.08862031],\n",
       "        [ 0.01310077,  0.04581054,  0.05383541,  0.06036859, -0.02822899,\n",
       "          0.02486397, -0.04735787, -0.03042608],\n",
       "        [-0.04263002,  0.05932804,  0.01338688, -0.05481394,  0.04012501,\n",
       "          0.08863796,  0.00213639, -0.00484931]], dtype=float32),\n",
       " array([[-0.068159  ,  0.09239922, -0.01655504,  0.0246703 ],\n",
       "        [-0.09634116, -0.00719259, -0.02887959,  0.01964808],\n",
       "        [-0.02263346, -0.11099781, -0.04456816,  0.00532937],\n",
       "        [ 0.06082123, -0.0749862 , -0.01108772, -0.011057  ],\n",
       "        [ 0.01568168,  0.06713226,  0.07962654, -0.05882697],\n",
       "        [ 0.06085258, -0.1769565 , -0.03754871,  0.05678561]],\n",
       "       dtype=float32),\n",
       " array([[-0.03718761, -0.07516474, -0.04240099,  0.01042155],\n",
       "        [-0.03341069, -0.10257128,  0.00500367, -0.01255119],\n",
       "        [-0.03865023,  0.01835186,  0.04970702, -0.07333571],\n",
       "        [-0.01121775, -0.06711451, -0.01892604, -0.01721327],\n",
       "        [-0.01313276,  0.09430597, -0.09991245, -0.01003497],\n",
       "        [ 0.11535889,  0.03899225,  0.03449281, -0.03830849]],\n",
       "       dtype=float32))"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query, key, value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query_shape =  (6, 8)\n",
      "key_shape =  (6, 4)\n",
      "value_shape =  (6, 4)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"query_shape = \", query.shape)\n",
    "print(\"key_shape = \", key.shape)\n",
    "print(\"value_shape = \", value.shape)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After reshaping... \n",
      "\n",
      "query_shape =  (4, 6, 2)\n",
      "key_shape =  (2, 6, 2)\n",
      "value_shape =  (2, 6, 2)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "q_len, _ = query.shape\n",
    "\n",
    "\n",
    "print(\"After reshaping... \\n\")\n",
    "\n",
    "query1 = np.transpose(np.reshape(query, (q_len, num_heads, head_dim)), (1, 0, 2))\n",
    "key1 = np.transpose(np.reshape(key, (q_len, n_kv_heads, head_dim)), (1, 0, 2))\n",
    "value1 = np.transpose(np.reshape(value, (q_len, n_kv_heads, head_dim)), (1, 0, 2))\n",
    "\n",
    "print(\"query_shape = \", query1.shape)\n",
    "print(\"key_shape = \", key1.shape)\n",
    "print(\"value_shape = \", value1.shape)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Obtaining the rotary embeddings \n",
    "\n",
    "#### 4.1 Pre-computing the sin and cos values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "base = 10000\n",
    "max_seq_len = 2048\n",
    "dim = head_dim\n",
    "\n",
    "\n",
    "inv_freq = 1.0 / (base ** (np.arange(0, dim, 2, dtype=np.float32) / dim))\n",
    "t = np.arange(max_seq_len, dtype=np.float32)\n",
    "freqs = np.outer(t, inv_freq)\n",
    "# emb = np.concatenate((np.cos(freqs), np.sin(freqs)), axis=-1)\n",
    "emb = np.concatenate((freqs,freqs), axis=-1)\n",
    "\n",
    "cos, sin =  np.cos(emb[:seq_len]), np.sin(emb[:seq_len])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 1.        ,  1.        ],\n",
       "        [ 0.5403023 ,  0.5403023 ],\n",
       "        [-0.4161468 , -0.4161468 ],\n",
       "        [-0.9899925 , -0.9899925 ],\n",
       "        [-0.6536436 , -0.6536436 ],\n",
       "        [ 0.28366217,  0.28366217]], dtype=float32),\n",
       " array([[ 0.       ,  0.       ],\n",
       "        [ 0.841471 ,  0.841471 ],\n",
       "        [ 0.9092974,  0.9092974],\n",
       "        [ 0.14112  ,  0.14112  ],\n",
       "        [-0.7568025, -0.7568025],\n",
       "        [-0.9589243, -0.9589243]], dtype=float32))"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cos, sin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 Applying the rotations on the Q and K matrices "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q matrix rotation\n",
    "\n",
    "unsqueeze_dim = 0\n",
    "\n",
    "cos_exp = np.expand_dims(cos, axis=unsqueeze_dim)\n",
    "sin_exp = np.expand_dims(sin, axis=unsqueeze_dim)\n",
    "\n",
    "\n",
    "# Half rotation \n",
    "q1 = query1[..., :query1.shape[-1] // 2]\n",
    "q2 = query1[..., query1.shape[-1] // 2:]\n",
    "q_half_rot = np.concatenate((-q2, q1), axis=-1)\n",
    "\n",
    "\n",
    "query_rotated = query1*cos_exp + q_half_rot*sin_exp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K matrix rotation\n",
    "\n",
    "unskueeze_dim = 0\n",
    "\n",
    "cos_exp = np.expand_dims(cos, axis=unskueeze_dim)\n",
    "sin_exp = np.expand_dims(sin, axis=unskueeze_dim)\n",
    "\n",
    "\n",
    "# Half rotation \n",
    "k1 = key1[..., :key1.shape[-1] // 2]\n",
    "k2 = key1[..., key1.shape[-1] // 2:]\n",
    "key_half_rot = np.concatenate((-k2, k1), axis=-1)\n",
    "\n",
    "\n",
    "key_rotated = key1*cos_exp + key_half_rot*sin_exp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[[-0.01127685, -0.02173477],\n",
       "         [ 0.01641382, -0.07114169],\n",
       "         [ 0.00256074, -0.01366427],\n",
       "         [-0.01100695, -0.02626907],\n",
       "         [ 0.0261063 , -0.03985846],\n",
       "         [ 0.04479858,  0.05770808]],\n",
       " \n",
       "        [[-0.03691161, -0.02306341],\n",
       "         [ 0.00246354, -0.06215204],\n",
       "         [ 0.06770138,  0.06991298],\n",
       "         [ 0.06492251, -0.04120906],\n",
       "         [ 0.01049793, -0.08020231],\n",
       "         [-0.04876507, -0.02838565]],\n",
       " \n",
       "        [[ 0.00182827,  0.00958379],\n",
       "         [-0.06811325, -0.02335558],\n",
       "         [-0.00984079, -0.0375045 ],\n",
       "         [-0.07715238, -0.09724581],\n",
       "         [ 0.03726882,  0.00511159],\n",
       "         [ 0.09637903, -0.01333361]],\n",
       " \n",
       "        [[-0.01198711,  0.04022793],\n",
       "         [ 0.05360502,  0.06297731],\n",
       "         [ 0.05780423,  0.02999771],\n",
       "         [-0.00015103, -0.08949462],\n",
       "         [ 0.00792864,  0.05572837],\n",
       "         [-0.00404411, -0.0034242 ]]], dtype=float32),\n",
       " array([[[-0.068159  ,  0.09239922],\n",
       "         [-0.04600099, -0.08495446],\n",
       "         [ 0.11034887,  0.02561084],\n",
       "         [-0.04963051,  0.08281887],\n",
       "         [ 0.04055563, -0.0557485 ],\n",
       "         [-0.15242632, -0.10854888]],\n",
       " \n",
       "        [[-0.01655504,  0.0246703 ],\n",
       "         [-0.032137  , -0.01368544],\n",
       "         [ 0.01370091, -0.04274352],\n",
       "         [ 0.01253712,  0.00938164],\n",
       "         [-0.09656778, -0.02180969],\n",
       "         [ 0.04380195,  0.0521143 ]]], dtype=float32))"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_rotated, key_rotated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[-0.03718761, -0.07516474],\n",
       "        [-0.03341069, -0.10257128],\n",
       "        [-0.03865023,  0.01835186],\n",
       "        [-0.01121775, -0.06711451],\n",
       "        [-0.01313276,  0.09430597],\n",
       "        [ 0.11535889,  0.03899225]],\n",
       "\n",
       "       [[-0.04240099,  0.01042155],\n",
       "        [ 0.00500367, -0.01255119],\n",
       "        [ 0.04970702, -0.07333571],\n",
       "        [-0.01892604, -0.01721327],\n",
       "        [-0.09991245, -0.01003497],\n",
       "        [ 0.03449281, -0.03830849]]], dtype=float32)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Sliding window attention\n",
    "### (Grouped Query Attention)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1 Repating the K and V values for the GQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repeating the Value vector \n",
    "\n",
    "num_value_value_heads, seq_len, head_dim = value1.shape[0], value1.shape[1], value1.shape[2]\n",
    "\n",
    "if n_rep > 1:\n",
    "\n",
    "    value1 = np.broadcast_to(value1[:, np.newaxis, :, :], (num_value_value_heads, n_rep, seq_len, head_dim))\n",
    "    value1 =  value1.reshape(num_value_value_heads * n_rep, seq_len, head_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[-0.03718761, -0.07516474],\n",
       "        [-0.03341069, -0.10257128],\n",
       "        [-0.03865023,  0.01835186],\n",
       "        [-0.01121775, -0.06711451],\n",
       "        [-0.01313276,  0.09430597],\n",
       "        [ 0.11535889,  0.03899225]],\n",
       "\n",
       "       [[-0.03718761, -0.07516474],\n",
       "        [-0.03341069, -0.10257128],\n",
       "        [-0.03865023,  0.01835186],\n",
       "        [-0.01121775, -0.06711451],\n",
       "        [-0.01313276,  0.09430597],\n",
       "        [ 0.11535889,  0.03899225]],\n",
       "\n",
       "       [[-0.04240099,  0.01042155],\n",
       "        [ 0.00500367, -0.01255119],\n",
       "        [ 0.04970702, -0.07333571],\n",
       "        [-0.01892604, -0.01721327],\n",
       "        [-0.09991245, -0.01003497],\n",
       "        [ 0.03449281, -0.03830849]],\n",
       "\n",
       "       [[-0.04240099,  0.01042155],\n",
       "        [ 0.00500367, -0.01255119],\n",
       "        [ 0.04970702, -0.07333571],\n",
       "        [-0.01892604, -0.01721327],\n",
       "        [-0.09991245, -0.01003497],\n",
       "        [ 0.03449281, -0.03830849]]], dtype=float32)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repeating the Key vector \n",
    "\n",
    "num_key_value_heads, seq_len, head_dim = key_rotated.shape[0], key_rotated.shape[1], key_rotated.shape[2]\n",
    "\n",
    "if n_rep > 1:\n",
    "\n",
    "    key_rotated = np.broadcast_to(key_rotated[:, np.newaxis, :, :], (num_key_value_heads, n_rep, seq_len, head_dim))\n",
    "    key_rotated =  key_rotated.reshape(num_key_value_heads * n_rep, seq_len, head_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[-0.068159  ,  0.09239922],\n",
       "        [-0.04600099, -0.08495446],\n",
       "        [ 0.11034887,  0.02561084],\n",
       "        [-0.04963051,  0.08281887],\n",
       "        [ 0.04055563, -0.0557485 ],\n",
       "        [-0.15242632, -0.10854888]],\n",
       "\n",
       "       [[-0.068159  ,  0.09239922],\n",
       "        [-0.04600099, -0.08495446],\n",
       "        [ 0.11034887,  0.02561084],\n",
       "        [-0.04963051,  0.08281887],\n",
       "        [ 0.04055563, -0.0557485 ],\n",
       "        [-0.15242632, -0.10854888]],\n",
       "\n",
       "       [[-0.01655504,  0.0246703 ],\n",
       "        [-0.032137  , -0.01368544],\n",
       "        [ 0.01370091, -0.04274352],\n",
       "        [ 0.01253712,  0.00938164],\n",
       "        [-0.09656778, -0.02180969],\n",
       "        [ 0.04380195,  0.0521143 ]],\n",
       "\n",
       "       [[-0.01655504,  0.0246703 ],\n",
       "        [-0.032137  , -0.01368544],\n",
       "        [ 0.01370091, -0.04274352],\n",
       "        [ 0.01253712,  0.00938164],\n",
       "        [-0.09656778, -0.02180969],\n",
       "        [ 0.04380195,  0.0521143 ]]], dtype=float32)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "key_rotated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4, 6, 2), (4, 6, 2), (4, 6, 2))"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_rotated.shape, key_rotated.shape, value1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sliding window attention mask "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[False,  True,  True,  True,  True,  True],\n",
       "       [False, False,  True,  True,  True,  True],\n",
       "       [False, False, False,  True,  True,  True],\n",
       "       [ True, False, False, False,  True,  True],\n",
       "       [ True,  True, False, False, False,  True],\n",
       "       [ True,  True,  True, False, False, False]])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_length = src_tokenized_np.shape[0]\n",
    "sliding_window_len = 3\n",
    "\n",
    "sliding_window_mask = np.triu(np.ones((seq_length, seq_length)), k=1).astype('bool')\n",
    "\n",
    "# print(sliding_window_mask)\n",
    "\n",
    "for i in range(sliding_window_mask.shape[0]-1, -1, -1):\n",
    "\n",
    "    li = i - sliding_window_len + 1\n",
    "\n",
    "    if li > 0:\n",
    "\n",
    "        sliding_window_mask[i][0:li] = True\n",
    "\n",
    "sliding_window_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_mask = sliding_window_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention bias = \n",
      "[[  0. -inf -inf -inf -inf -inf]\n",
      " [  0.   0. -inf -inf -inf -inf]\n",
      " [  0.   0.   0. -inf -inf -inf]\n",
      " [-inf   0.   0.   0. -inf -inf]\n",
      " [-inf -inf   0.   0.   0. -inf]\n",
      " [-inf -inf -inf   0.   0.   0.]]\n"
     ]
    }
   ],
   "source": [
    "L, S = query.shape[-2], key.shape[-2]\n",
    "\n",
    "scale_factor = 1 / np.sqrt(query.shape[-1])\n",
    "attn_bias = np.zeros((L, S), dtype=query.dtype)\n",
    "\n",
    "if attn_mask is not None:\n",
    "    if attn_mask.dtype == np.bool_:\n",
    "        attn_bias[attn_mask] = -np.inf\n",
    "    else:\n",
    "        attn_bias += attn_mask\n",
    "\n",
    "    print(\"Attention bias = \")\n",
    "    print(attn_bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SoftMax (Scaled Dot Product of Q and K) = \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[1.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        ],\n",
       "        [0.49885264, 0.50114733, 0.        , 0.        , 0.        ,\n",
       "         0.        ],\n",
       "        [0.33318213, 0.3334744 , 0.3333435 , 0.        , 0.        ,\n",
       "         0.        ],\n",
       "        [0.        , 0.3336867 , 0.33314148, 0.33317187, 0.        ,\n",
       "         0.        ],\n",
       "        [0.        , 0.        , 0.33353096, 0.33277047, 0.33369857,\n",
       "         0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.33410344, 0.33363643,\n",
       "         0.3322601 ]],\n",
       "\n",
       "       [[1.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        ],\n",
       "        [0.49902087, 0.5009791 , 0.        , 0.        , 0.        ,\n",
       "         0.        ],\n",
       "        [0.33346903, 0.3321865 , 0.33434448, 0.        , 0.        ,\n",
       "         0.        ],\n",
       "        [0.        , 0.33339387, 0.33405393, 0.3325522 , 0.        ,\n",
       "         0.        ],\n",
       "        [0.        , 0.        , 0.3333515 , 0.33261362, 0.3340349 ,\n",
       "         0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.33294097, 0.33288628,\n",
       "         0.33417276]],\n",
       "\n",
       "       [[1.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        ],\n",
       "        [0.499827  , 0.5001729 , 0.        , 0.        , 0.        ,\n",
       "         0.        ],\n",
       "        [0.3331832 , 0.33337077, 0.33344606, 0.        , 0.        ,\n",
       "         0.        ],\n",
       "        [0.        , 0.33358473, 0.33350092, 0.33291435, 0.        ,\n",
       "         0.        ],\n",
       "        [0.        , 0.        , 0.33348176, 0.33350807, 0.3330102 ,\n",
       "         0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.33363363, 0.33244446,\n",
       "         0.33392197]],\n",
       "\n",
       "       [[1.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        ],\n",
       "        [0.50028735, 0.49971268, 0.        , 0.        , 0.        ,\n",
       "         0.        ],\n",
       "        [0.33342466, 0.33318293, 0.33339244, 0.        , 0.        ,\n",
       "         0.        ],\n",
       "        [0.        , 0.33331272, 0.33361852, 0.33306873, 0.        ,\n",
       "         0.        ],\n",
       "        [0.        , 0.        , 0.33320808, 0.3335494 , 0.33324254,\n",
       "         0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.33332253, 0.3333871 ,\n",
       "         0.33329037]]], dtype=float32)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "key_rotated_T = np.transpose(key_rotated, axes=(0, 2, 1))\n",
    "\n",
    "attn_weight = query_rotated @ key_rotated_T * scale_factor\n",
    "attn_weight += attn_bias\n",
    "\n",
    "exp_attn_weight = np.exp(attn_weight)\n",
    "sum_exp_attn_weight = np.sum(exp_attn_weight, axis=-1, keepdims=True)\n",
    "softmax_attn_weight = exp_attn_weight / sum_exp_attn_weight\n",
    "\n",
    "print(\"SoftMax (Scaled Dot Product of Q and K) = \")\n",
    "softmax_attn_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_output = softmax_attn_weight @ value1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[-0.03718761, -0.07516474],\n",
       "        [-0.03529482, -0.08889945],\n",
       "        [-0.03641566, -0.05313097],\n",
       "        [-0.02776214, -0.05047357],\n",
       "        [-0.02100636,  0.01525696],\n",
       "        [ 0.0301997 ,  0.02199629]],\n",
       "\n",
       "       [[-0.03718761, -0.07516474],\n",
       "        [-0.03529545, -0.08889484],\n",
       "        [-0.03642198, -0.05300206],\n",
       "        [-0.02778067, -0.0503852 ],\n",
       "        [-0.02100209,  0.01529591],\n",
       "        [ 0.03044323,  0.02207814]],\n",
       "\n",
       "       [[-0.04240099,  0.01042155],\n",
       "        [-0.01869046, -0.00106879],\n",
       "        [ 0.00411539, -0.02516541],\n",
       "        [ 0.01194573, -0.03437496],\n",
       "        [-0.02300747, -0.03353863],\n",
       "        [-0.0280118 , -0.02187104]],\n",
       "\n",
       "       [[-0.04240099,  0.01042155],\n",
       "        [-0.01871228, -0.00105822],\n",
       "        [ 0.00410155, -0.02515661],\n",
       "        [ 0.0119473 , -0.03438282],\n",
       "        [-0.02304507, -0.0335216 ],\n",
       "        [-0.02812188, -0.02185095]]], dtype=float32)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Post attention "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "Wo = state_dict[\"layers.0.self_attn.o_proj.weight\"].numpy()\n",
    "\n",
    "self_attn_op = np.transpose(attn_output, (1, 0, 2)).copy()\n",
    "self_attn_op = self_attn_op.reshape(q_len, embed_dim)\n",
    "\n",
    "sa_output = np.matmul(self_attn_op, Wo.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[-5.4631353e-04, -4.2850545e-04, -2.7580101e-03,  3.4945257e-04,\n",
       "          1.8132386e-03, -9.3881667e-05, -1.9176917e-03, -1.4552541e-05],\n",
       "        [ 4.6211353e-04, -6.6357432e-04, -3.0626440e-03,  4.6732419e-04,\n",
       "          3.0408024e-03, -2.9950534e-04,  3.0283217e-04, -2.4522718e-03],\n",
       "        [ 1.6187821e-03, -2.3624374e-04, -1.4780506e-03,  5.6520052e-04,\n",
       "          2.9295764e-03, -7.5796153e-04,  2.8198441e-03, -3.0192956e-03],\n",
       "        [ 2.0560285e-03, -1.8307311e-04, -1.3614881e-03, -7.2920178e-05,\n",
       "          3.3871541e-03, -1.0801419e-03,  3.4962103e-03, -3.6736529e-03],\n",
       "        [ 8.1324280e-04,  8.0571848e-04,  7.3775818e-04, -1.1942638e-03,\n",
       "          6.8257906e-04, -1.2898125e-03,  3.6089559e-04,  1.4434158e-03],\n",
       "        [-1.1867824e-04,  7.3713245e-04,  4.5464179e-04, -3.7712425e-03,\n",
       "          2.9719065e-04, -1.3826506e-03, -2.1593508e-03,  2.1981997e-03]],\n",
       "       dtype=float32),\n",
       " (6, 8))"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sa_output, sa_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.00791677, -0.02584263,  0.00398741,  0.0115725 ,  0.03327203,\n",
       "         0.03731762,  0.01455585, -0.01814195],\n",
       "       [-0.01259581,  0.02110402,  0.01946167,  0.0529499 ,  0.0131973 ,\n",
       "         0.03599653,  0.00355262, -0.01612684],\n",
       "       [-0.02932685,  0.0206643 , -0.02289083,  0.02708024, -0.01178521,\n",
       "        -0.02506157,  0.03780422,  0.02202267],\n",
       "       [-0.01173335, -0.0131547 ,  0.01252894,  0.00845597, -0.00156012,\n",
       "        -0.00021141, -0.00358098,  0.00468036],\n",
       "       [ 0.03845008, -0.01183031,  0.00744214, -0.00212165,  0.01921215,\n",
       "        -0.03846469, -0.01246678,  0.02179719],\n",
       "       [-0.01616384,  0.01563318,  0.00741749, -0.01362945, -0.01147801,\n",
       "        -0.01370889,  0.01551612,  0.03883397]])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_states = residual + sa_output\n",
    "\n",
    "hidden_states\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "residual = hidden_states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. LayerNorm 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "variance_epsilon = 1e-05\n",
    "\n",
    "wt = state_dict[\"layers.0.post_attention_layernorm.weight\"].numpy()\n",
    "\n",
    "dtype = hidden_states.dtype\n",
    "hidden_states = hidden_states.astype(np.float32)\n",
    "variance = np.mean(hidden_states**2, axis=-1, keepdims=True)\n",
    "hidden_states = hidden_states * (1/np.sqrt(variance + variance_epsilon))\n",
    "\n",
    "hidden_state = wt * hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.35400555, -1.1555766 ,  0.17830051,  0.51747495,  1.4877894 ,\n",
       "         1.6686914 ,  0.650878  , -0.8112339 ],\n",
       "       [-0.47569054,  0.79700977,  0.73498535,  1.9996945 ,  0.49840653,\n",
       "         1.3594372 ,  0.13416739, -0.60904276],\n",
       "       [-1.1387956 ,  0.8024186 , -0.88887715,  1.0515571 , -0.45763317,\n",
       "        -0.9731697 ,  1.4679816 ,  0.8551657 ],\n",
       "       [-1.2953333 , -1.4522474 ,  1.3831645 ,  0.933519  , -0.17223391,\n",
       "        -0.0233397 , -0.39533207,  0.51670074],\n",
       "       [ 1.6706715 , -0.5140318 ,  0.3233638 , -0.09218662,  0.8347757 ,\n",
       "        -1.671306  , -0.5416866 ,  0.9470969 ],\n",
       "       [-0.84970623,  0.8218103 ,  0.38992497, -0.7164776 , -0.60337996,\n",
       "        -0.7206535 ,  0.81565654,  2.0414376 ]], dtype=float32)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. MLP layer of mistral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_down_proj = state_dict[\"layers.0.mlp.down_proj.weight\"].numpy()\n",
    "\n",
    "W_up_proj = state_dict[\"layers.0.mlp.up_proj.weight\"].numpy()\n",
    "\n",
    "W_gate_proj = state_dict[\"layers.0.mlp.gate_proj.weight\"].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UP PROJ =  [[-0.02633362 -0.03738315  0.03618297  0.02019078  0.04695692  0.01560353\n",
      "  -0.02826723  0.03186499]\n",
      " [-0.10240708  0.01969982 -0.03214018 -0.00515962  0.0641224   0.05243002\n",
      "  -0.00151322  0.00098214]\n",
      " [-0.06928502 -0.02650194 -0.0288421   0.09493146  0.05052106  0.09632429\n",
      "  -0.03608317  0.01299343]\n",
      " [-0.03276506  0.07164638 -0.02559902 -0.01827805  0.00236751  0.00413582\n",
      "  -0.0561991  -0.02200339]\n",
      " [ 0.0753521   0.02986558  0.00396085 -0.083946   -0.06246215 -0.06265807\n",
      "  -0.11088322 -0.03623838]\n",
      " [-0.00975075  0.02046935 -0.00375318 -0.02138426  0.00363632  0.04210124\n",
      "  -0.03814134 -0.06613762]]\n",
      "\n",
      "GATE PROJ =  [[-3.32227238e-02  1.18128814e-01 -3.43855433e-02 -1.72991175e-02\n",
      "  -2.88696140e-02 -3.13251726e-02 -9.33740381e-03 -3.74927334e-02]\n",
      " [-3.84216122e-02  7.73807988e-02 -7.75982216e-02 -1.13112681e-01\n",
      "  -3.36851692e-03 -5.13751172e-02  1.05518986e-04 -1.00727126e-01]\n",
      " [ 7.45595098e-02 -1.85438283e-02  7.86799844e-03  4.11682576e-02\n",
      "  -1.64301712e-02 -3.12208980e-02 -3.12646292e-02  1.40749114e-02]\n",
      " [ 9.36163403e-03 -3.44763175e-02 -3.74469459e-02 -2.46329755e-02\n",
      "   5.75545132e-02  7.81132141e-03 -4.36380655e-02 -7.79552609e-02]\n",
      " [ 6.72771782e-02  5.01717106e-02 -1.74350012e-02 -3.36041744e-03\n",
      "   4.00055572e-02  4.59935293e-02 -1.81184951e-02  4.79376642e-03]\n",
      " [ 1.37177214e-01 -9.30148885e-02  2.98477076e-02  1.30720690e-01\n",
      "  -6.33350685e-02  6.83527905e-03 -3.03768907e-02 -1.67954601e-02]]\n",
      "\n",
      "ACT =  [[ 4.37628594e-04 -2.20313855e-03 -6.21698564e-04 -1.74610846e-04\n",
      "  -6.77354576e-04 -2.44331924e-04  1.31988709e-04 -5.96996048e-04]\n",
      " [ 1.97119289e-03  7.62774784e-04  1.24856550e-03  2.91894568e-04\n",
      "  -1.07987034e-04 -1.34498533e-03 -7.98367239e-08 -4.94614069e-05]\n",
      " [-2.57625710e-03  2.45784060e-04 -1.13451912e-04  1.95789989e-03\n",
      "  -4.14862559e-04 -1.50140456e-03  5.64381538e-04  9.14490374e-05]\n",
      " [-1.53343732e-04 -1.23352639e-03  4.79532289e-04  2.25172058e-04\n",
      "   6.81351521e-05  1.61533790e-05  1.22771366e-03  8.58375628e-04]\n",
      " [ 2.54116347e-03  7.49764789e-04 -3.45275221e-05  1.41066703e-04\n",
      "  -1.24785560e-03 -1.43885671e-03  1.00552768e-03 -8.68516290e-05]\n",
      " [-6.68343215e-04 -9.51070746e-04 -5.60087465e-05 -1.39572914e-03\n",
      "  -1.15139963e-04  1.43907557e-04  5.79643238e-04  5.55714360e-04]]\n"
     ]
    }
   ],
   "source": [
    "up_proj = np.matmul(hidden_state, W_up_proj.T)\n",
    "gate_proj = np.matmul(hidden_state, W_gate_proj.T)\n",
    "\n",
    "print(\"UP PROJ = \", up_proj)\n",
    "print()\n",
    "\n",
    "print(\"GATE PROJ = \", gate_proj)\n",
    "print()\n",
    "\n",
    "temp_proj = up_proj * gate_proj\n",
    "\n",
    "\n",
    "# SilU \n",
    "temp_proj = temp_proj / (1 + np.exp(-temp_proj))\n",
    "\n",
    "print(\"ACT = \", temp_proj)\n",
    "\n",
    "down_proj = np.matmul(temp_proj, W_down_proj.T)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.1603549e-04,  1.0476179e-04,  5.0112540e-05,  8.0561222e-06,\n",
       "         3.7039474e-05,  6.7446716e-05,  4.9148895e-07, -2.5759321e-06],\n",
       "       [ 4.9017112e-06,  1.3649859e-05,  2.6713100e-05, -9.1412177e-05,\n",
       "         2.5664525e-05, -4.4378277e-05,  9.8380751e-06, -1.4420082e-05],\n",
       "       [-5.1639217e-05, -1.4465520e-05, -7.2297342e-05,  1.1697599e-05,\n",
       "        -6.9711867e-05,  5.1093048e-05, -5.3775126e-05, -4.8950795e-05],\n",
       "       [-1.6044300e-05,  1.7578410e-05,  9.4723309e-06, -4.8006841e-05,\n",
       "        -1.9136964e-05, -2.3289011e-05, -2.9307079e-05, -2.1518310e-05],\n",
       "       [ 1.5083115e-05,  1.4094624e-05,  1.0383761e-04, -2.3425488e-05,\n",
       "        -7.3550050e-06,  6.7495741e-05,  7.6142584e-05, -1.7058817e-06],\n",
       "       [-1.2485162e-05,  2.0857897e-05,  8.2886154e-06, -4.5903553e-06,\n",
       "        -2.7969083e-05, -5.5890367e-05,  2.3006802e-05, -1.7650222e-05]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_state = down_proj\n",
    "hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Residual connection\n",
    "hidden_state = hidden_state + residual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.0080328 , -0.02573786,  0.00403752,  0.01158056,  0.03330907,\n",
       "         0.03738506,  0.01455634, -0.01814453],\n",
       "       [-0.01259091,  0.02111767,  0.01948838,  0.05285849,  0.01322297,\n",
       "         0.03595215,  0.00356246, -0.01614126],\n",
       "       [-0.02937849,  0.02064983, -0.02296312,  0.02709194, -0.01185492,\n",
       "        -0.02501048,  0.03775045,  0.02197372],\n",
       "       [-0.01174939, -0.01313712,  0.01253841,  0.00840796, -0.00157926,\n",
       "        -0.0002347 , -0.00361029,  0.00465884],\n",
       "       [ 0.03846516, -0.01181622,  0.00754597, -0.00214508,  0.0192048 ,\n",
       "        -0.03839719, -0.01239064,  0.02179549],\n",
       "       [-0.01617632,  0.01565404,  0.00742577, -0.01363404, -0.01150598,\n",
       "        -0.01376478,  0.01553912,  0.03881632]])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6, 8)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_state.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "my_project_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
