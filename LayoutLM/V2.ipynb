{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **LayoutLM V2**\n",
    "\n",
    "\n",
    "### Objectives for pre-training :-\n",
    "\n",
    "2 new training objectives compared to V1:- (Other than Maksed Visual Language Modelling (MVLM))\n",
    "\n",
    "1. Text-image alignment \n",
    "    - Some tokens lines are randomly selected, and their image regions are covered on the document image.\n",
    "\n",
    "2. Text-image matching\n",
    "    - Feed the output representation at [CLS] into a classifier to predict whether the image and text are from the same document page.\n",
    "    - A classification layer is built above the encoder outputs. This layer predicts a label for each text token depending on whether it is covered or not\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Components of V2:- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Text embedding:- \n",
    "\n",
    "        ti = TokEmb(wi) + PosEmb1D(i) + SegEmb(si)\n",
    "\n",
    "\n",
    "2. Visual embedding:-\n",
    "\n",
    "        vi = Proj(VisTokEmb(I)i) + PosEmb1D(i) + SegEmb([C])\n",
    "        \n",
    "    Here, the document page image is resized to 224x244 and then the feature maps are avg pooled with width W and height H.\n",
    "    Layer these are flattened into W*H shape and then passed through a projection layer.\n",
    "\n",
    "    Positional encodings are added as the CNN based vision encoder is not able to capture the positional information.\n",
    "\n",
    "    All vision tokens segement = C\n",
    "\n",
    "\n",
    "3. Layout Embedding:- \n",
    "\n",
    "        li = Concat(PosEmb2Dx(xmin, xmax, width), PosEmb2Dy(ymin, ymax, height))\n",
    "\n",
    "   2 embedding layers to encode x and y features.\n",
    "   Given the normalized bounding box of the i-th (0 ≤ i < WH + L) \n",
    "   \n",
    "   text/visual token boxi = (xmin, xmax, ymin, ymax, width, height)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spatial-Aware Self attention :- \n",
    "\n",
    "#### Relative positonal embeddings \n",
    "**Modelled the semantic relative position and spatial relative position as bias terms to prevent adding too many parameters**\n",
    "\n",
    "    α′ij = αij + bj-i(1D) + bxj-xi(2Dx) + byj-yi(2Dy)\n",
    "\n",
    "These bias terms are learnable "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Flow :-**\n",
    "\n",
    "\n",
    "#### Initialising the image embeddings and other layers:- \n",
    "\n",
    "    self.has_visual_segment_embedding = config.has_visual_segment_embedding\n",
    "    self.embeddings = LayoutLMv2Embeddings(config)\n",
    "\n",
    "    self.visual = LayoutLMv2VisualBackbone(config)\n",
    "\n",
    "    self.visual_proj = nn.Linear(config.image_feature_pool_shape[-1], config.hidden_size)\n",
    "\n",
    "    if self.has_visual_segment_embedding:\n",
    "        self.visual_segment_embedding = nn.Parameter(nn.Embedding(1, config.hidden_size).weight[0])\n",
    "    \n",
    "    self.visual_LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
    "    self.visual_dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "\n",
    "    self.encoder = LayoutLMv2Encoder(config)\n",
    "    self.pooler = LayoutLMv2Pooler(config)\n",
    "\n",
    "\n",
    "## 1. Image processing:- \n",
    "Tesseract OCR is applied on the on the document image.\n",
    "\n",
    "- Words and normalised bounding boxes are obtianed from this step  \n",
    "- If needed the image will be resized and the color channels will be flipped from RGB to BGR\n",
    "\n",
    "\n",
    "###  **Here, the LayoutLMv2VisualBackbone is 'detectron2' model for extracting the visual features from the document image**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image features and embeddings "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Obtaining the image embeddings:- \n",
    "\n",
    "    def _calc_img_embeddings(self, image, bbox, position_ids):\n",
    "\n",
    "        visual_embeddings = self.visual_proj(self.visual(image))\n",
    "\n",
    "        position_embeddings = self.embeddings.position_embeddings(position_ids)\n",
    "\n",
    "        spatial_position_embeddings = self.embeddings._calc_spatial_position_embeddings(bbox)\n",
    "\n",
    "        embeddings = visual_embeddings + position_embeddings + spatial_position_embeddings\n",
    "        \n",
    "        if self.has_visual_segment_embedding:\n",
    "            embeddings += self.visual_segment_embedding\n",
    "\n",
    "        embeddings = self.visual_LayerNorm(embeddings)\n",
    "        embeddings = self.visual_dropout(embeddings)\n",
    "        \n",
    "        return embeddings\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.  Textual processing:- \n",
    "**LayoutLMv2TokenizerFast**, which turns the words and bounding boxes into token-level **input_ids**,\n",
    "**attention_mask**, **token_type_ids**, **bbox**. \n",
    "\n",
    "### Text embeddings and features\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Obtaining text embeddings :- \n",
    "        def _calc_text_embeddings(self, input_ids, bbox, position_ids, token_type_ids, inputs_embeds=None):\n",
    "\n",
    "           position_embeddings = self.embeddings.position_embeddings(position_ids)\n",
    "\n",
    "           spatial_position_embeddings = self.embeddings._calc_spatial_position_embeddings(bbox)\n",
    "\n",
    "           token_type_embeddings = self.embeddings.token_type_embeddings(token_type_ids)\n",
    "\n",
    "           embeddings = inputs_embeds + position_embeddings + spatial_position_embeddings + token_type_embeddings\n",
    "\n",
    "           embeddings = self.embeddings.LayerNorm(embeddings)\n",
    "           embeddings = self.embeddings.dropout(embeddings)\n",
    "\n",
    "           return embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Prepearing for the encoder output:-\n",
    "\n",
    "    visual_bbox = self._calc_visual_bbox(self.config.image_feature_pool_shape, bbox, device, final_shape)\n",
    "\n",
    "\n",
    "Where, the visual boxes are calculated in this manner:- \n",
    "\n",
    "    def _calc_visual_bbox(self, image_feature_pool_shape, bbox, device, final_shape):\n",
    "        visual_bbox_x = torch.div(\n",
    "            torch.arange(\n",
    "                0,\n",
    "                1000 * (image_feature_pool_shape[1] + 1),\n",
    "                1000,\n",
    "                device=device,\n",
    "                dtype=bbox.dtype,\n",
    "            ),\n",
    "            self.config.image_feature_pool_shape[1],\n",
    "            rounding_mode=\"floor\",\n",
    "        )\n",
    "        visual_bbox_y = torch.div(\n",
    "            torch.arange(\n",
    "                0,\n",
    "                1000 * (self.config.image_feature_pool_shape[0] + 1),\n",
    "                1000,\n",
    "                device=device,\n",
    "                dtype=bbox.dtype,\n",
    "            ),\n",
    "            self.config.image_feature_pool_shape[0],\n",
    "            rounding_mode=\"floor\",\n",
    "        )\n",
    "        visual_bbox = torch.stack(\n",
    "            [\n",
    "                visual_bbox_x[:-1].repeat(image_feature_pool_shape[0], 1),\n",
    "                visual_bbox_y[:-1].repeat(image_feature_pool_shape[1], 1).transpose(0, 1),\n",
    "                visual_bbox_x[1:].repeat(image_feature_pool_shape[0], 1),\n",
    "                visual_bbox_y[1:].repeat(image_feature_pool_shape[1], 1).transpose(0, 1),\n",
    "            ],\n",
    "            dim=-1,\n",
    "        ).view(-1, bbox.size(-1))\n",
    "\n",
    "        visual_bbox = visual_bbox.repeat(final_shape[0], 1, 1)\n",
    "        return visual_bbox\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    final_bbox = torch.cat([bbox, visual_bbox], dim=1)\n",
    "\n",
    "Here, bbox is the input bounding box passed to the model for inference. (Optional input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    if bbox is None:\n",
    "        bbox = torch.zeros(tuple(list(input_shape) + [4]), dtype=torch.long, device=device)\n",
    "\n",
    "    text_layout_emb = self._calc_text_embeddings(\n",
    "        input_ids=input_ids,\n",
    "        bbox=bbox,\n",
    "        token_type_ids=token_type_ids,\n",
    "        position_ids=position_ids,\n",
    "        inputs_embeds=inputs_embeds,\n",
    "    )\n",
    "\n",
    "    visual_emb = self._calc_img_embeddings(\n",
    "        image=image,\n",
    "        bbox=visual_bbox,\n",
    "        position_ids=visual_position_ids,\n",
    "    )\n",
    "    \n",
    "    final_emb = torch.cat([text_layout_emb, visual_emb], dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.  Passing the prepared inputs to the encoder :- \n",
    "\n",
    "    encoder_outputs = self.encoder(\n",
    "        final_emb,\n",
    "        extended_attention_mask,\n",
    "        bbox=final_bbox,\n",
    "        position_ids=final_position_ids,\n",
    "        head_mask=head_mask,\n",
    "        output_attentions=output_attentions,\n",
    "        output_hidden_states=output_hidden_states,\n",
    "        return_dict=return_dict,\n",
    "    )\n",
    "\n",
    "    sequence_output = encoder_outputs[0]\n",
    "    pooled_output = self.pooler(sequence_output)\n",
    "\n",
    "NOTE:-\n",
    "**Here, the bbox is used to calculate the Relative 2d position embeddings in the modified self-attention mechanism**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2D positional embeddings (Relative)\n",
    "\n",
    "    def _calculate_2d_position_embeddings(self, bbox):\n",
    "        position_coord_x = bbox[:, :, 0]\n",
    "        position_coord_y = bbox[:, :, 3]\n",
    "        rel_pos_x_2d_mat = position_coord_x.unsqueeze(-2) - position_coord_x.unsqueeze(-1)\n",
    "        rel_pos_y_2d_mat = position_coord_y.unsqueeze(-2) - position_coord_y.unsqueeze(-1)\n",
    "        rel_pos_x = relative_position_bucket(\n",
    "            rel_pos_x_2d_mat,\n",
    "            num_buckets=self.rel_2d_pos_bins,\n",
    "            max_distance=self.max_rel_2d_pos,\n",
    "        )\n",
    "        rel_pos_y = relative_position_bucket(\n",
    "            rel_pos_y_2d_mat,\n",
    "            num_buckets=self.rel_2d_pos_bins,\n",
    "            max_distance=self.max_rel_2d_pos,\n",
    "        )\n",
    "        rel_pos_x = self.rel_pos_x_bias.weight.t()[rel_pos_x].permute(0, 3, 1, 2)\n",
    "        rel_pos_y = self.rel_pos_y_bias.weight.t()[rel_pos_y].permute(0, 3, 1, 2)\n",
    "        rel_pos_x = rel_pos_x.contiguous()\n",
    "        rel_pos_y = rel_pos_y.contiguous()\n",
    "        rel_2d_pos = rel_pos_x + rel_pos_y\n",
    "        return rel_2d_pos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1D postional embeddings  (Relative)\n",
    "\n",
    "    def _calculate_1d_position_embeddings(self, position_ids):\n",
    "        rel_pos_mat = position_ids.unsqueeze(-2) - position_ids.unsqueeze(-1)\n",
    "        rel_pos = relative_position_bucket(\n",
    "            rel_pos_mat,\n",
    "            num_buckets=self.rel_pos_bins,\n",
    "            max_distance=self.max_rel_pos,\n",
    "        )\n",
    "        rel_pos = self.rel_pos_bias.weight.t()[rel_pos].permute(0, 3, 1, 2)\n",
    "        rel_pos = rel_pos.contiguous()\n",
    "        return rel_pos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention modification \n",
    "\n",
    "**Spatially self aware attention**\n",
    "\n",
    "    attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
    "        \n",
    "        if self.has_relative_attention_bias:\n",
    "            attention_scores += rel_pos\n",
    "\n",
    "        if self.has_spatial_attention_bias:\n",
    "            attention_scores += rel_2d_pos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
