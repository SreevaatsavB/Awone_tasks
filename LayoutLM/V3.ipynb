{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **LayoutLM V3**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objectives for pre-training :-\n",
    "\n",
    "1. Maksed Language Modelling \n",
    "    - Predicting the text in a patch of the document given 2d positonal embedding (Similar to Maksed Visual Language Modelling (MVLM))\n",
    "\n",
    "2. Masked Image Modelling\n",
    "    - Masking the image tokens and trying to predict them with the other context \n",
    "\n",
    "3. Word-Patch Alignement \n",
    "    - Predict whether the corresponding image patches of a text word are masked.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- #### Same attention mechanism as V2\n",
    "- #### No need of a complete CNN encoder here for image embeddings, rather uses Vision-Transformer by dividing the image into patches\n",
    "- #### Image patches are also positionally encoded and used here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Patch Embeddings**\n",
    "\n",
    "\n",
    "    class LayoutLMv3PatchEmbeddings(nn.Module):\n",
    "\n",
    "        def __init__(self, config):\n",
    "            super().__init__()\n",
    "\n",
    "            image_size = (\n",
    "                config.input_size\n",
    "                if isinstance(config.input_size, collections.abc.Iterable)\n",
    "                else (config.input_size, config.input_size)\n",
    "            )\n",
    "            \n",
    "            patch_size = (\n",
    "                config.patch_size\n",
    "                if isinstance(config.patch_size, collections.abc.Iterable)\n",
    "                else (config.patch_size, config.patch_size)\n",
    "            )\n",
    "\n",
    "            self.patch_shape = (image_size[0] // patch_size[0], image_size[1] // patch_size[1])\n",
    "\n",
    "            self.proj = nn.Conv2d(config.num_channels, config.hidden_size, kernel_size=patch_size, stride=patch_size)\n",
    "\n",
    "        def forward(self, pixel_values, position_embedding=None):\n",
    "\n",
    "            embeddings = self.proj(pixel_values)\n",
    "\n",
    "            if position_embedding is not None:\n",
    "\n",
    "                # interpolate the position embedding to the corresponding size\n",
    "                position_embedding = position_embedding.view(1, self.patch_shape[0], self.patch_shape[1], -1)\n",
    "                position_embedding = position_embedding.permute(0, 3, 1, 2)\n",
    "\n",
    "                patch_height, patch_width = embeddings.shape[2], embeddings.shape[3]\n",
    "                position_embedding = F.interpolate(position_embedding, size=(patch_height, patch_width), mode=\"bicubic\")\n",
    "                \n",
    "                embeddings = embeddings + position_embedding\n",
    "\n",
    "            embeddings = embeddings.flatten(2).transpose(1, 2)\n",
    "            return embeddings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Text Embeddings**\n",
    "\n",
    "#### 2D positional (spatial) embeddings for the text batches in the document image\n",
    "\n",
    "    def calculate_spatial_position_embeddings(self, bbox):\n",
    "\n",
    "        left_position_embeddings = self.x_position_embeddings(bbox[:, :, 0])\n",
    "        upper_position_embeddings = self.y_position_embeddings(bbox[:, :, 1])\n",
    "        right_position_embeddings = self.x_position_embeddings(bbox[:, :, 2])\n",
    "        lower_position_embeddings = self.y_position_embeddings(bbox[:, :, 3])\n",
    "\n",
    "\n",
    "        h_position_embeddings = self.h_position_embeddings(torch.clip(bbox[:, :, 3] - bbox[:, :, 1], 0, 1023))\n",
    "        w_position_embeddings = self.w_position_embeddings(torch.clip(bbox[:, :, 2] - bbox[:, :, 0], 0, 1023))\n",
    "\n",
    "\n",
    "        spatial_position_embeddings = torch.cat(\n",
    "                    [\n",
    "                        left_position_embeddings,\n",
    "                        upper_position_embeddings,\n",
    "                        right_position_embeddings,\n",
    "                        lower_position_embeddings,\n",
    "                        h_position_embeddings,\n",
    "                        w_position_embeddings,\n",
    "                    ],\n",
    "                    dim=-1\n",
    "        )\n",
    "\n",
    "        return spatial_position_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "############################################################################################################################\n",
    "\n",
    "### Token type embeddings for texts \n",
    "\n",
    "\n",
    "    self.word_embeddings = nn.Embedding(config.vocab_size, config.hidden_size, padding_idx=config.pad_token_id) # (OR input_embeds)\n",
    "    self.token_type_embeddings = nn.Embedding(config.type_vocab_size, config.hidden_size)\n",
    "\n",
    "    token_type_embeddings = self.token_type_embeddings(token_type_ids)\n",
    "\n",
    "############################################################################################################################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "    embeddings = inputs_embeds + token_type_embeddings\n",
    "\n",
    "    # 1D textual positional embeddings \n",
    "    position_embeddings = self.position_embeddings(position_ids)\n",
    "    embeddings += position_embeddings\n",
    "\n",
    "    # 2D-spatial positional embeddings \n",
    "    spatial_position_embeddings = self.calculate_spatial_position_embeddings(bbox)\n",
    "\n",
    "    embeddings = embeddings + spatial_position_embeddings\n",
    "\n",
    "    embeddings = self.LayerNorm(embeddings)\n",
    "    embeddings = self.dropout(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Attention mechanism**\n",
    "- CogView Attention \n",
    "\n",
    "        def cogview_attention(self, attention_scores, alpha=32):\n",
    " \n",
    "                scaled_attention_scores = attention_scores / alpha\n",
    "                max_value = scaled_attention_scores.amax(dim=(-1)).unsqueeze(-1)\n",
    "                new_attention_scores = (scaled_attention_scores - max_value) * alpha\n",
    "                return nn.Softmax(dim=-1)(new_attention_scores)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    attention_scores += (rel_pos + rel_2d_pos) / math.sqrt(self.attention_head_size)\n",
    "\n",
    "\n",
    "Here, relative positional biases are added to the attetntion scores  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "############################################################################################################################\n",
    "\n",
    "\n",
    "#### **Relative positional biases for attention mechanism**\n",
    "\n",
    "\n",
    "    rel_pos = self._cal_1d_pos_emb(position_ids) if self.has_relative_attention_bias else None\n",
    "    rel_2d_pos = self._cal_2d_pos_emb(bbox) if self.has_spatial_attention_bias else None\n",
    "\n",
    "\n",
    "    def relative_position_bucket(self, relative_position, bidirectional=True, num_buckets=32, max_distance=128):\n",
    "        ret = 0\n",
    "        if bidirectional:\n",
    "            num_buckets //= 2\n",
    "            ret += (relative_position > 0).long() * num_buckets\n",
    "            n = torch.abs(relative_position)\n",
    "        else:\n",
    "            n = torch.max(-relative_position, torch.zeros_like(relative_position))\n",
    "            # now n is in the range [0, inf)\n",
    "\n",
    "        # half of the buckets are for exact increments in positions\n",
    "        max_exact = num_buckets // 2\n",
    "        is_small = n < max_exact\n",
    "\n",
    "        # The other half of the buckets are for logarithmically bigger bins in positions up to max_distance\n",
    "        val_if_large = max_exact + (\n",
    "            torch.log(n.float() / max_exact) / math.log(max_distance / max_exact) * (num_buckets - max_exact)\n",
    "        ).to(torch.long)\n",
    "        val_if_large = torch.min(val_if_large, torch.full_like(val_if_large, num_buckets - 1))\n",
    "\n",
    "        ret += torch.where(is_small, n, val_if_large)\n",
    "        return ret\n",
    "\n",
    "\n",
    "\n",
    "    def _cal_1d_pos_emb(self, position_ids):\n",
    "        rel_pos_mat = position_ids.unsqueeze(-2) - position_ids.unsqueeze(-1)\n",
    "\n",
    "        rel_pos = self.relative_position_bucket(\n",
    "            rel_pos_mat,\n",
    "            num_buckets=self.rel_pos_bins,\n",
    "            max_distance=self.max_rel_pos,\n",
    "        )\n",
    "        rel_pos = self.rel_pos_bias.weight.t()[rel_pos].permute(0, 3, 1, 2)\n",
    "        rel_pos = rel_pos.contiguous()\n",
    "        return rel_pos\n",
    "\n",
    "\n",
    "\n",
    "    def _cal_2d_pos_emb(self, bbox):\n",
    "        position_coord_x = bbox[:, :, 0]\n",
    "        position_coord_y = bbox[:, :, 3]\n",
    "        rel_pos_x_2d_mat = position_coord_x.unsqueeze(-2) - position_coord_x.unsqueeze(-1)\n",
    "        rel_pos_y_2d_mat = position_coord_y.unsqueeze(-2) - position_coord_y.unsqueeze(-1)\n",
    "        rel_pos_x = self.relative_position_bucket(\n",
    "            rel_pos_x_2d_mat,\n",
    "            num_buckets=self.rel_2d_pos_bins,\n",
    "            max_distance=self.max_rel_2d_pos,\n",
    "        )\n",
    "        rel_pos_y = self.relative_position_bucket(\n",
    "            rel_pos_y_2d_mat,\n",
    "            num_buckets=self.rel_2d_pos_bins,\n",
    "            max_distance=self.max_rel_2d_pos,\n",
    "        )\n",
    "        rel_pos_x = self.rel_pos_x_bias.weight.t()[rel_pos_x].permute(0, 3, 1, 2)\n",
    "        rel_pos_y = self.rel_pos_y_bias.weight.t()[rel_pos_y].permute(0, 3, 1, 2)\n",
    "        rel_pos_x = rel_pos_x.contiguous()\n",
    "        rel_pos_y = rel_pos_y.contiguous()\n",
    "        rel_2d_pos = rel_pos_x + rel_pos_y\n",
    "        return rel_2d_pos\n",
    "\n",
    "############################################################################################################################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Forward pass** \n",
    "\n",
    "### 1. Textual + 2D postional embedddings \n",
    "\n",
    "        embedding_output = self.embeddings(\n",
    "                    input_ids=input_ids,\n",
    "                    bbox=bbox,\n",
    "                    position_ids=position_ids,\n",
    "                    token_type_ids=token_type_ids,\n",
    "                    inputs_embeds=inputs_embeds,\n",
    "                )\n",
    "\n",
    "    \n",
    "### 2. Image patches embeddings \n",
    "\n",
    "        def forward_image(self, pixel_values):\n",
    "            embeddings = self.patch_embed(pixel_values)\n",
    "\n",
    "        where, self.patch_embed = LayoutLMv3PatchEmbeddings(config)\n",
    "\n",
    "        *\n",
    "        visual_embeddings = self.forward_image(pixel_values)\n",
    "\n",
    "\n",
    "### 3. Concatenating visual and textual embeddings \n",
    "\n",
    "        embedding_output = torch.cat([embedding_output, visual_embeddings], dim=1)\n",
    "        embedding_output = self.LayerNorm(embedding_output)\n",
    "        embedding_output = self.dropout(embedding_output)\n",
    "\n",
    "### 4. Passing through encoder\n",
    "\n",
    "        encoder_outputs = self.encoder(\n",
    "            embedding_output,\n",
    "            bbox=final_bbox,\n",
    "            position_ids=final_position_ids,\n",
    "            attention_mask=extended_attention_mask,\n",
    "            head_mask=head_mask,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "            patch_height=patch_height,\n",
    "            patch_width=patch_width,\n",
    "        )\n",
    "\n",
    "        sequence_output = encoder_outputs[0]\n",
    "\n",
    "        return sequence_output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "my_project_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
