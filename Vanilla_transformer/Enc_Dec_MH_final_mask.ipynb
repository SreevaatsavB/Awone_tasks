{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-head attention transformer\n",
    "### Encoder and Decoder\n",
    "### (With masking)\n",
    "\n",
    "Pytorch's implementation (in built)\n",
    "\n",
    "NOTE :- A new exmple must be used for testing the masked attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source embeddings :- \n",
      "\n",
      "tensor([[[-0.5432,  1.5555,  0.3620,  0.0788],\n",
      "         [-0.1357,  1.8274,  0.5430,  1.2430],\n",
      "         [ 0.2074, -0.5689, -0.5513, -0.1528],\n",
      "         [-1.2523,  0.0173,  1.2028, -0.4656]]], grad_fn=<EmbeddingBackward0>)\n",
      "\n",
      "Target embeddings :- \n",
      "\n",
      "tensor([[[-0.3095, -0.8334,  0.5895, -0.5896],\n",
      "         [-1.5573, -0.1136,  0.9670, -0.5579],\n",
      "         [-0.4276,  2.0494,  1.2553,  0.2608],\n",
      "         [-0.4276,  2.0494,  1.2553,  0.2608]]], grad_fn=<EmbeddingBackward0>)\n",
      "\n",
      "Positional encoded source embeddings:\n",
      "tensor([[[-0.5432,  2.5555,  0.3620,  1.0788],\n",
      "         [ 0.7058,  2.3677,  0.5530,  2.2430],\n",
      "         [ 1.1167, -0.9851, -0.5314,  0.8470],\n",
      "         [-1.1112, -0.9727,  1.2328,  0.5340]]], grad_fn=<AddBackward0>)\n",
      "\n",
      "Positional encoded target embeddings:\n",
      "tensor([[[-0.3095,  0.1666,  0.5895,  0.4104],\n",
      "         [-0.7159,  0.4267,  0.9770,  0.4421],\n",
      "         [ 0.4817,  1.6332,  1.2753,  1.2606],\n",
      "         [-0.2865,  1.0594,  1.2853,  1.2604]]], grad_fn=<AddBackward0>)\n",
      "\n",
      "query =  tensor([[[-0.5432,  2.5555,  0.3620,  1.0788],\n",
      "         [ 0.7058,  2.3677,  0.5530,  2.2430],\n",
      "         [ 1.1167, -0.9851, -0.5314,  0.8470],\n",
      "         [-1.1112, -0.9727,  1.2328,  0.5340]]], grad_fn=<AddBackward0>)\n",
      "query shape =  torch.Size([1, 4, 4])\n",
      "\n",
      " _in_projection_packed \n",
      "\n",
      "Q :-  tensor([[[-0.6798,  1.4007, -0.2742, -1.0386],\n",
      "         [-1.9563,  0.6655, -0.6882, -0.6257],\n",
      "         [-0.6674, -0.6622, -0.4141,  0.8993],\n",
      "         [-0.2284, -0.3723, -0.8246, -0.2570]]], grad_fn=<SelectBackward0>)\n",
      "\n",
      "K :-  tensor([[[ 1.3458,  1.3452,  0.7641, -1.2760],\n",
      "         [ 0.8736,  1.8444,  1.0950, -0.9651],\n",
      "         [-0.5440, -0.1505, -0.0823,  0.2863],\n",
      "         [-0.6199,  0.2640, -0.8359,  1.0204]]], grad_fn=<SelectBackward0>)\n",
      "\n",
      "V :-  tensor([[[ 1.5610,  0.2397,  1.2207,  0.6767],\n",
      "         [ 1.8320,  0.3493,  1.1892,  0.1905],\n",
      "         [-0.4484, -0.4026, -0.0178, -0.5572],\n",
      "         [ 0.4017,  0.0647, -0.5237, -0.4228]]], grad_fn=<SelectBackward0>)\n",
      "\n",
      "tgt_len =  1\n",
      "bsz =  4\n",
      "num_heads =  2\n",
      "head_dim =  2\n",
      "\n",
      "tensor([[[-0.6798,  1.4007, -0.2742, -1.0386],\n",
      "         [-1.9563,  0.6655, -0.6882, -0.6257],\n",
      "         [-0.6674, -0.6622, -0.4141,  0.8993],\n",
      "         [-0.2284, -0.3723, -0.8246, -0.2570]]], grad_fn=<SelectBackward0>) torch.Size([1, 4, 4])\n",
      "Q reshaped =  tensor([[[-0.6798,  1.4007]],\n",
      "\n",
      "        [[-0.2742, -1.0386]],\n",
      "\n",
      "        [[-1.9563,  0.6655]],\n",
      "\n",
      "        [[-0.6882, -0.6257]],\n",
      "\n",
      "        [[-0.6674, -0.6622]],\n",
      "\n",
      "        [[-0.4141,  0.8993]],\n",
      "\n",
      "        [[-0.2284, -0.3723]],\n",
      "\n",
      "        [[-0.8246, -0.2570]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "K reshaped =  tensor([[[ 1.3458,  1.3452]],\n",
      "\n",
      "        [[ 0.7641, -1.2760]],\n",
      "\n",
      "        [[ 0.8736,  1.8444]],\n",
      "\n",
      "        [[ 1.0950, -0.9651]],\n",
      "\n",
      "        [[-0.5440, -0.1505]],\n",
      "\n",
      "        [[-0.0823,  0.2863]],\n",
      "\n",
      "        [[-0.6199,  0.2640]],\n",
      "\n",
      "        [[-0.8359,  1.0204]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "V reshaped =  tensor([[[ 1.5610,  0.2397]],\n",
      "\n",
      "        [[ 1.2207,  0.6767]],\n",
      "\n",
      "        [[ 1.8320,  0.3493]],\n",
      "\n",
      "        [[ 1.1892,  0.1905]],\n",
      "\n",
      "        [[-0.4484, -0.4026]],\n",
      "\n",
      "        [[-0.0178, -0.5572]],\n",
      "\n",
      "        [[ 0.4017,  0.0647]],\n",
      "\n",
      "        [[-0.5237, -0.4228]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "torch.Size([4, 2, 1, 2]) torch.Size([4, 2, 1, 2]) torch.Size([4, 2, 1, 2])\n",
      "\n",
      "scaled_dot_product_attention\n",
      "attn_output = \n",
      "tensor([[ 1.5610,  0.2397,  1.2207,  0.6767],\n",
      "        [ 1.8320,  0.3493,  1.1892,  0.1905],\n",
      "        [-0.4484, -0.4026, -0.0178, -0.5572],\n",
      "        [ 0.4017,  0.0647, -0.5237, -0.4228]], grad_fn=<ViewBackward0>)\n",
      "\n",
      "x + self_atten(x) = \n",
      "tensor([[[ 1.3527,  3.0306,  0.4087,  0.6277],\n",
      "         [ 2.6174,  2.7587,  0.9248,  1.9399],\n",
      "         [ 0.2619, -0.9797, -0.0907,  1.1440],\n",
      "         [-1.2071, -1.4466,  1.5910,  0.4359]]], grad_fn=<AddBackward0>)\n",
      "\n",
      "norm1(x + self_atten(x)) = \n",
      "tensor([[[-0.0022,  1.6291, -0.9199, -0.7070],\n",
      "         [ 0.7687,  0.9635, -1.5663, -0.1659],\n",
      "         [ 0.2339, -1.3974, -0.2294,  1.3928],\n",
      "         [-0.8456, -1.0383,  1.4069,  0.4770]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "\n",
      "feed_fwd_op =  tensor([[[ 0.3988,  0.3053, -0.2622, -0.2372],\n",
      "         [ 0.3977,  0.3675, -0.2304, -0.2142],\n",
      "         [ 0.1692,  0.3715, -0.1308, -0.1429],\n",
      "         [ 0.0955,  0.2936, -0.1547, -0.1615]]], grad_fn=<ViewBackward0>)\n",
      "\n",
      "norm2(norm1(x + self_atten(x)) + feed_fwd_op) = \n",
      "tensor([[[ 0.2780,  1.5154, -0.9923, -0.8010],\n",
      "         [ 0.8535,  0.9828, -1.4747, -0.3617],\n",
      "         [ 0.3958, -1.2856, -0.5023,  1.3922],\n",
      "         [-0.9211, -0.9146,  1.4792,  0.3564]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "\n",
      "##################################\n",
      "\n",
      "################### Transformer Decoder start #####################\n",
      "\n",
      "query =  tensor([[[-0.3095,  0.1666,  0.5895,  0.4104],\n",
      "         [-0.7159,  0.4267,  0.9770,  0.4421],\n",
      "         [ 0.4817,  1.6332,  1.2753,  1.2606],\n",
      "         [-0.2865,  1.0594,  1.2853,  1.2604]]], grad_fn=<AddBackward0>)\n",
      "query shape =  torch.Size([1, 4, 4])\n",
      "\n",
      " _in_projection_packed \n",
      "\n",
      "Q :-  tensor([[[ 0.0632,  0.2018,  0.5668, -0.7167],\n",
      "         [-0.0096,  0.2866,  0.8592, -1.2781],\n",
      "         [ 0.5190,  0.4592,  2.0303, -1.8184],\n",
      "         [ 0.3563,  0.6196,  1.7640, -1.9240]]], grad_fn=<SelectBackward0>)\n",
      "\n",
      "K :-  tensor([[[-0.2527, -0.2785, -0.2614, -0.5521],\n",
      "         [-0.2099, -0.4170, -0.6074, -0.8707],\n",
      "         [ 0.0705, -1.5230, -0.9272, -0.7401],\n",
      "         [-0.3300, -1.0468, -0.8275, -1.0830]]], grad_fn=<SelectBackward0>)\n",
      "\n",
      "V :-  tensor([[[ 0.0145, -0.0974, -0.1493,  0.3044],\n",
      "         [ 0.0765, -0.2509, -0.4107,  0.5028],\n",
      "         [-0.1104,  1.0977,  1.0221,  0.1571],\n",
      "         [ 0.0020,  0.4013,  0.3516,  0.5678]]], grad_fn=<SelectBackward0>)\n",
      "\n",
      "tgt_len =  1\n",
      "bsz =  4\n",
      "num_heads =  2\n",
      "head_dim =  2\n",
      "\n",
      "tensor([[[ 0.0632,  0.2018,  0.5668, -0.7167],\n",
      "         [-0.0096,  0.2866,  0.8592, -1.2781],\n",
      "         [ 0.5190,  0.4592,  2.0303, -1.8184],\n",
      "         [ 0.3563,  0.6196,  1.7640, -1.9240]]], grad_fn=<SelectBackward0>) torch.Size([1, 4, 4])\n",
      "Q reshaped =  tensor([[[ 0.0632,  0.2018]],\n",
      "\n",
      "        [[ 0.5668, -0.7167]],\n",
      "\n",
      "        [[-0.0096,  0.2866]],\n",
      "\n",
      "        [[ 0.8592, -1.2781]],\n",
      "\n",
      "        [[ 0.5190,  0.4592]],\n",
      "\n",
      "        [[ 2.0303, -1.8184]],\n",
      "\n",
      "        [[ 0.3563,  0.6196]],\n",
      "\n",
      "        [[ 1.7640, -1.9240]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "K reshaped =  tensor([[[-0.2527, -0.2785]],\n",
      "\n",
      "        [[-0.2614, -0.5521]],\n",
      "\n",
      "        [[-0.2099, -0.4170]],\n",
      "\n",
      "        [[-0.6074, -0.8707]],\n",
      "\n",
      "        [[ 0.0705, -1.5230]],\n",
      "\n",
      "        [[-0.9272, -0.7401]],\n",
      "\n",
      "        [[-0.3300, -1.0468]],\n",
      "\n",
      "        [[-0.8275, -1.0830]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "V reshaped =  tensor([[[ 0.0145, -0.0974]],\n",
      "\n",
      "        [[-0.1493,  0.3044]],\n",
      "\n",
      "        [[ 0.0765, -0.2509]],\n",
      "\n",
      "        [[-0.4107,  0.5028]],\n",
      "\n",
      "        [[-0.1104,  1.0977]],\n",
      "\n",
      "        [[ 1.0221,  0.1571]],\n",
      "\n",
      "        [[ 0.0020,  0.4013]],\n",
      "\n",
      "        [[ 0.3516,  0.5678]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "torch.Size([4, 2, 1, 2]) torch.Size([4, 2, 1, 2]) torch.Size([4, 2, 1, 2])\n",
      "\n",
      "scaled_dot_product_attention\n",
      "attn_output = \n",
      "tensor([[ 0.0145, -0.0974, -0.1493,  0.3044],\n",
      "        [ 0.0765, -0.2509, -0.4107,  0.5028],\n",
      "        [-0.1104,  1.0977,  1.0221,  0.1571],\n",
      "        [ 0.0020,  0.4013,  0.3516,  0.5678]], grad_fn=<ViewBackward0>)\n",
      "\n",
      "x =  tensor([[[-0.3095,  0.1666,  0.5895,  0.4104],\n",
      "         [-0.7159,  0.4267,  0.9770,  0.4421],\n",
      "         [ 0.4817,  1.6332,  1.2753,  1.2606],\n",
      "         [-0.2865,  1.0594,  1.2853,  1.2604]]], grad_fn=<AddBackward0>)\n",
      "\n",
      "sa_op =  tensor([[[ 0.3354,  0.2949,  0.3642, -0.1185],\n",
      "         [ 0.7282,  0.5757,  0.7341, -0.0761],\n",
      "         [-1.3969, -0.5911, -1.1591, -0.9625],\n",
      "         [-0.2074,  0.1613, -0.0765, -0.7489]]], grad_fn=<ViewBackward0>)\n",
      "\n",
      "temop =  tensor([[[ 0.0258,  0.4616,  0.9537,  0.2919],\n",
      "         [ 0.0123,  1.0024,  1.7110,  0.3660],\n",
      "         [-0.9153,  1.0421,  0.1162,  0.2981],\n",
      "         [-0.4940,  1.2207,  1.2088,  0.5115]]], grad_fn=<AddBackward0>)\n",
      "\n",
      "Decoder norm1(x + sa(x)) = \n",
      "tensor([[[-1.2044,  0.0837,  1.5386, -0.4179],\n",
      "         [-1.1748,  0.3544,  1.4489, -0.6285],\n",
      "         [-1.5035,  1.2978, -0.0273,  0.2330],\n",
      "         [-1.5796,  0.8699,  0.8529, -0.1432]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "\n",
      "Memory = \n",
      "tensor([[[ 0.2780,  1.5154, -0.9923, -0.8010],\n",
      "         [ 0.8535,  0.9828, -1.4747, -0.3617],\n",
      "         [ 0.3958, -1.2856, -0.5023,  1.3922],\n",
      "         [-0.9211, -0.9146,  1.4792,  0.3564]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "\n",
      "query =  tensor([[[-1.2044,  0.0837,  1.5386, -0.4179],\n",
      "         [-1.1748,  0.3544,  1.4489, -0.6285],\n",
      "         [-1.5035,  1.2978, -0.0273,  0.2330],\n",
      "         [-1.5796,  0.8699,  0.8529, -0.1432]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "query shape =  torch.Size([1, 4, 4])\n",
      "\n",
      " _in_projection_packed \n",
      "\n",
      "Q :-  tensor([[[ 0.2437, -0.8084, -0.2721,  0.8057],\n",
      "         [ 0.5289, -0.7797, -0.1821,  0.9150],\n",
      "         [ 1.1448, -0.3859,  0.2385, -0.1027],\n",
      "         [ 0.8741, -0.6890,  0.0020,  0.4243]]], grad_fn=<ViewBackward0>)\n",
      "\n",
      "K :-  tensor([[[ 1.1384,  0.6546,  0.8390, -0.5678],\n",
      "         [ 1.1360,  1.1477,  0.4548,  0.0592],\n",
      "         [-0.5776,  0.2464, -1.0737,  1.1551],\n",
      "         [-1.1307, -1.1934, -0.4308, -0.1117]]], grad_fn=<SelectBackward0>)\n",
      "\n",
      "V :-  tensor([[[ 0.2457,  0.0424,  0.5397,  0.8359],\n",
      "         [-0.1711, -0.0743,  0.2930,  1.1949],\n",
      "         [-0.7913, -0.3454, -0.6070,  0.2404],\n",
      "         [ 0.1975,  0.0695, -0.2703, -1.2053]]], grad_fn=<SelectBackward0>)\n",
      "\n",
      "tgt_len =  1\n",
      "bsz =  4\n",
      "num_heads =  2\n",
      "head_dim =  2\n",
      "\n",
      "tensor([[[ 0.2437, -0.8084, -0.2721,  0.8057],\n",
      "         [ 0.5289, -0.7797, -0.1821,  0.9150],\n",
      "         [ 1.1448, -0.3859,  0.2385, -0.1027],\n",
      "         [ 0.8741, -0.6890,  0.0020,  0.4243]]], grad_fn=<ViewBackward0>) torch.Size([1, 4, 4])\n",
      "Q reshaped =  tensor([[[ 0.2437, -0.8084]],\n",
      "\n",
      "        [[-0.2721,  0.8057]],\n",
      "\n",
      "        [[ 0.5289, -0.7797]],\n",
      "\n",
      "        [[-0.1821,  0.9150]],\n",
      "\n",
      "        [[ 1.1448, -0.3859]],\n",
      "\n",
      "        [[ 0.2385, -0.1027]],\n",
      "\n",
      "        [[ 0.8741, -0.6890]],\n",
      "\n",
      "        [[ 0.0020,  0.4243]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "K reshaped =  tensor([[[ 1.1384,  0.6546]],\n",
      "\n",
      "        [[ 0.8390, -0.5678]],\n",
      "\n",
      "        [[ 1.1360,  1.1477]],\n",
      "\n",
      "        [[ 0.4548,  0.0592]],\n",
      "\n",
      "        [[-0.5776,  0.2464]],\n",
      "\n",
      "        [[-1.0737,  1.1551]],\n",
      "\n",
      "        [[-1.1307, -1.1934]],\n",
      "\n",
      "        [[-0.4308, -0.1117]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "V reshaped =  tensor([[[ 0.2457,  0.0424]],\n",
      "\n",
      "        [[ 0.5397,  0.8359]],\n",
      "\n",
      "        [[-0.1711, -0.0743]],\n",
      "\n",
      "        [[ 0.2930,  1.1949]],\n",
      "\n",
      "        [[-0.7913, -0.3454]],\n",
      "\n",
      "        [[-0.6070,  0.2404]],\n",
      "\n",
      "        [[ 0.1975,  0.0695]],\n",
      "\n",
      "        [[-0.2703, -1.2053]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "torch.Size([4, 2, 1, 2]) torch.Size([4, 2, 1, 2]) torch.Size([4, 2, 1, 2])\n",
      "\n",
      "scaled_dot_product_attention\n",
      "attn_output = \n",
      "tensor([[ 0.2457,  0.0424,  0.5397,  0.8359],\n",
      "        [-0.1711, -0.0743,  0.2930,  1.1949],\n",
      "        [-0.7913, -0.3454, -0.6070,  0.2404],\n",
      "        [ 0.1975,  0.0695, -0.2703, -1.2053]], grad_fn=<ViewBackward0>)\n",
      "\n",
      "Decoder norm2(x + mha(x, mem)) = \n",
      "tensor([[[-0.6313, -1.0292,  1.5930,  0.0675],\n",
      "         [-1.1430, -0.4263,  1.5847, -0.0154],\n",
      "         [-1.4210,  1.3967, -0.1094,  0.1336],\n",
      "         [-1.3878,  0.9595,  0.9427, -0.5144]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "\n",
      "Decoder norm3(x + ff(x)) = \n",
      "tensor([[[-0.8488, -0.8435,  1.5998,  0.0924],\n",
      "         [-1.2543, -0.3016,  1.5280,  0.0280],\n",
      "         [-1.5712,  1.2070,  0.1186,  0.2456],\n",
      "         [-1.4815,  0.8364,  0.9924, -0.3473]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "\n",
      "tensor([[[ 1.4197, -0.4107, -0.1168,  0.9968, -0.2262,  0.8008,  0.1751,\n",
      "          -0.5905,  1.7112, -0.8088],\n",
      "         [ 1.5076, -0.4805, -0.0610,  1.3007, -0.1737,  0.7942,  0.3358,\n",
      "          -0.8888,  1.4928, -0.8529],\n",
      "         [ 1.0689, -0.2571, -0.0720,  1.4597,  0.3436,  0.2593,  0.7902,\n",
      "          -0.8153,  0.1954, -0.6916],\n",
      "         [ 1.2454, -0.4862, -0.1692,  1.4866, -0.2175,  0.4440,  0.5660,\n",
      "          -1.1615,  0.6706, -0.7323]]], grad_fn=<ViewBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sreevaatsav/.pyenv/versions/project_env/lib/python3.10/site-packages/torch/nn/modules/transformer.py:282: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=512, dropout=0):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.encoding = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1).float()\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(torch.log(torch.tensor(10000.0)) / d_model))\n",
    "        self.encoding[:, 0::2] = torch.sin(position * div_term)\n",
    "        self.encoding[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.encoding = self.encoding.unsqueeze(0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.encoding[:, :x.size(1)].detach()\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, src_vocab_size, tgt_vocab_size, d_model=4, nhead=2, num_encoder_layers=1, num_decoder_layers=1):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.src_embedding = nn.Embedding(src_vocab_size, d_model)\n",
    "        self.tgt_embedding = nn.Embedding(tgt_vocab_size, d_model)\n",
    "        self.positional_encoding = PositionalEncoding(d_model, dropout=0)\n",
    "        self.transformer = nn.Transformer(\n",
    "            d_model=d_model,\n",
    "            nhead=nhead,\n",
    "            num_encoder_layers=num_encoder_layers,\n",
    "            num_decoder_layers=num_decoder_layers,\n",
    "            dropout=0\n",
    "        )\n",
    "        self.fc = nn.Linear(d_model, tgt_vocab_size)\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "\n",
    "        print(\"Source embeddings :- \\n\")\n",
    "        print(self.src_embedding(src))\n",
    "        print()\n",
    "\n",
    "        print(\"Target embeddings :- \\n\")\n",
    "        print(self.tgt_embedding(tgt))\n",
    "        print()\n",
    "\n",
    "\n",
    "        src = self.src_embedding(src) + self.positional_encoding(src)\n",
    "        tgt = self.tgt_embedding(tgt) + self.positional_encoding(tgt)\n",
    "\n",
    "        print(\"Positional encoded source embeddings:\")\n",
    "        print(src)\n",
    "        print()\n",
    "\n",
    "        print(\"Positional encoded target embeddings:\")\n",
    "        print(tgt)\n",
    "        print()\n",
    "\n",
    "        output = self.transformer(src, tgt)\n",
    "        output = self.fc(output)\n",
    "        return output\n",
    "\n",
    "    \n",
    "\n",
    "src_vocab_size = 10  # Source language vocabulary size\n",
    "tgt_vocab_size = 10  # Target language vocabulary size\n",
    "d_model = 4  # Dimension of the model\n",
    "num_heads = 2\n",
    "model = TransformerModel(src_vocab_size, tgt_vocab_size, d_model=d_model, nhead=num_heads)\n",
    "# Source sentence in the source language\n",
    "src_sentence = torch.tensor([[0, 1, 2, 3]])  # Source sequence\n",
    "\n",
    "# Target sentence in the target language (translation of the source sentence)\n",
    "tgt_sentence = torch.tensor([[1, 0, 3, 3]])  # Target sequence\n",
    "\n",
    "# Forward pass\n",
    "output = model(src_sentence, tgt_sentence)\n",
    "print(output)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions to get the manual calulations for each component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import functional as F\n",
    "\n",
    "# Using the state dictionary to get the intermediate outputs\n",
    "state_dict = model.state_dict()\n",
    "\n",
    "need_weights = False\n",
    "\n",
    "src_mask = None\n",
    "\n",
    "tgt_mask = None\n",
    "\n",
    "memory_mask = None\n",
    "\n",
    "embed_dim = 4\n",
    "\n",
    "num_heads = 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def look_up_table(sentence, vocab_embeds, embedding):\n",
    "\n",
    "    for i in range(sentence.size(0)):\n",
    "        for j in range(sentence.size(1)):\n",
    "            # Get the index for the current word in the sequence\n",
    "            word_index = sentence[i, j].item()\n",
    "\n",
    "            # Check if the index is within valid range\n",
    "            if word_index < 0 or word_index >= vocab_embeds.size(0):\n",
    "                raise ValueError(f\"Invalid word index: {word_index}\")\n",
    "\n",
    "            # Lookup the corresponding embedding vector for the word\n",
    "            embedding[i, j, :] = vocab_embeds[word_index, :]\n",
    "\n",
    "            # Print intermediate results for debugging\n",
    "            print(f\"Word index: {word_index}, Embedding: {vocab_embeds[word_index, :]}\")\n",
    "    print()\n",
    "    \n",
    "\n",
    "    return embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embeddings and Positional encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding_outputs(src_sentence,  tgt_sentence, state_dict, d_model):\n",
    "\n",
    "    src_vocab_embeds = state_dict[\"src_embedding.weight\"]\n",
    "\n",
    "    src_embedding = torch.zeros(src_sentence.size(0), src_sentence.size(1), d_model)\n",
    "    print(\"Source sentence embedding\")\n",
    "    src_embedding =  look_up_table(src_sentence, src_vocab_embeds, src_embedding)\n",
    "\n",
    "    tgt_vocab_embeds = state_dict[\"tgt_embedding.weight\"]\n",
    "\n",
    "    tgt_embedding = torch.zeros(tgt_sentence.size(0), tgt_sentence.size(1), d_model)\n",
    "\n",
    "\n",
    "    print(\"Target sentence embedding\")\n",
    "    tgt_embedding =  look_up_table(tgt_sentence, tgt_vocab_embeds, tgt_embedding)\n",
    "\n",
    "\n",
    "    pe = PositionalEncoding(d_model = d_model, dropout=0)\n",
    "\n",
    "    pe_src_embeds = src_embedding + pe(src_sentence)\n",
    "\n",
    "    pe_tgt_embeds = tgt_embedding + pe(tgt_sentence)\n",
    "\n",
    "    print(\"PE source embeddings : \\n\")\n",
    "    print(pe_src_embeds)\n",
    "    print()\n",
    "\n",
    "    print(\"PE target embeddings : \\n\")\n",
    "    print(pe_tgt_embeds)\n",
    "    print()\n",
    "\n",
    "    return pe_src_embeds, pe_tgt_embeds\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source sentence embedding\n",
      "Word index: 0, Embedding: tensor([-0.5432,  1.5555,  0.3620,  0.0788])\n",
      "Word index: 1, Embedding: tensor([-0.1357,  1.8274,  0.5430,  1.2430])\n",
      "Word index: 2, Embedding: tensor([ 0.2074, -0.5689, -0.5513, -0.1528])\n",
      "Word index: 3, Embedding: tensor([-1.2523,  0.0173,  1.2028, -0.4656])\n",
      "\n",
      "Target sentence embedding\n",
      "Word index: 1, Embedding: tensor([-0.3095, -0.8334,  0.5895, -0.5896])\n",
      "Word index: 0, Embedding: tensor([-1.5573, -0.1136,  0.9670, -0.5579])\n",
      "Word index: 3, Embedding: tensor([-0.4276,  2.0494,  1.2553,  0.2608])\n",
      "Word index: 3, Embedding: tensor([-0.4276,  2.0494,  1.2553,  0.2608])\n",
      "\n",
      "PE source embeddings : \n",
      "\n",
      "tensor([[[-0.5432,  2.5555,  0.3620,  1.0788],\n",
      "         [ 0.7058,  2.3677,  0.5530,  2.2430],\n",
      "         [ 1.1167, -0.9851, -0.5314,  0.8470],\n",
      "         [-1.1112, -0.9727,  1.2328,  0.5340]]])\n",
      "\n",
      "PE target embeddings : \n",
      "\n",
      "tensor([[[-0.3095,  0.1666,  0.5895,  0.4104],\n",
      "         [-0.7159,  0.4267,  0.9770,  0.4421],\n",
      "         [ 0.4817,  1.6332,  1.2753,  1.2606],\n",
      "         [-0.2865,  1.0594,  1.2853,  1.2604]]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pe_src_embeds, pe_tgt_embeds = get_embedding_outputs(src_sentence,  tgt_sentence, state_dict, d_model = d_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder function to display the intermediate outputs and get the final outputs from the encoder\n",
    "\n",
    "### Self attention "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def atten_product_needs_wts_false(Q, V, K, bsz, head_dim, src_len, tgt_len, attn_mask):\n",
    "    Q1 = Q.view(bsz, num_heads, tgt_len, head_dim)\n",
    "    K1 = K.view(bsz, num_heads, src_len, head_dim)\n",
    "    V1 = V.view(bsz, num_heads, src_len, head_dim)\n",
    "\n",
    "\n",
    "    L, S = Q1.size(-2), K1.size(-2)\n",
    "\n",
    "    scale_factor = 1 / math.sqrt(Q1.size(-1)) \n",
    "    # scale_factor = 1\n",
    "    attn_bias = torch.zeros(L, S, dtype=Q1.dtype)\n",
    "\n",
    "    if attn_mask is not None:\n",
    "        if attn_mask.dtype == torch.bool:\n",
    "            attn_mask.masked_fill_(attn_mask.logical_not(), float(\"-inf\"))\n",
    "        else:\n",
    "            attn_bias += attn_mask\n",
    "\n",
    "    print(Q1.shape, K1.shape, K1.transpose(-2, -1).shape)\n",
    "    attn_weight = Q1 @ K1.transpose(-2, -1) * scale_factor\n",
    "    attn_weight += attn_bias\n",
    "\n",
    "    print(\"Attention weights = \")\n",
    "    print(attn_weight.shape, attn_weight)\n",
    "\n",
    "    attn_weight = torch.softmax(attn_weight, dim=-1)\n",
    "    \n",
    "    attn_output = attn_weight @ V1\n",
    "\n",
    "    attn_output = attn_output.permute(2, 0, 1, 3).contiguous().view(bsz * tgt_len, embed_dim)\n",
    "\n",
    "    return attn_output\n",
    "\n",
    "\n",
    "\n",
    "def atten_product_needs_wts_true(Q, K, V, bsz, tgt_len, attn_mask):\n",
    "\n",
    "    B, Nt, E = Q.shape\n",
    "\n",
    "    Q_scaled = Q / math.sqrt(E)\n",
    "\n",
    "    if attn_mask is not None:\n",
    "        temp_pdt_matrix = torch.baddbmm(attn_mask, Q_scaled, K.transpose(-2, -1))\n",
    "    else:\n",
    "        temp_pdt_matrix = torch.bmm(Q_scaled, K.transpose(-2, -1))\n",
    "\n",
    "\n",
    "    # Obtaining the diagonal entries of the un-normalised product (K.Q_scaled)\n",
    "    pdt_matrix = torch.diagonal(temp_pdt_matrix, dim1=-2, dim2=-1).unsqueeze(-1)\n",
    "\n",
    "    attn_wt_matrix = torch.nn.functional.softmax(pdt_matrix, dim=-1)\n",
    "\n",
    "    attn_output = torch.bmm(attn_wt_matrix, V)\n",
    "\n",
    "    attn_output = attn_output.transpose(0, 1).contiguous().view(tgt_len * bsz, embed_dim)\n",
    "\n",
    "    print(\"Encoder Attention output = \")\n",
    "    print(attn_output)\n",
    "    print()\n",
    "\n",
    "    return attn_output, attn_wt_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder_block_attn_output(x, state_dict, layer_num, embed_dim, num_heads, need_weights = False, src_mask = None):\n",
    "\n",
    "    \n",
    "    query_enc = key_enc = value_enc = x\n",
    "\n",
    "    tgt_len, bsz, embed_dim = x.shape\n",
    "\n",
    "\n",
    "    W_enc = state_dict[\"transformer.encoder.layers.{}.self_attn.in_proj_weight\".format(layer_num)]\n",
    "    b_enc = state_dict[\"transformer.encoder.layers.{}.self_attn.in_proj_bias\".format(layer_num)]\n",
    "\n",
    "\n",
    "    head_dim = embed_dim//num_heads\n",
    "\n",
    "    tempop1 = query_enc[0]@W_enc.T\n",
    "    tempop1 = tempop1.T\n",
    "\n",
    "    Q_enc,K_enc,V_enc = tempop1.T.chunk(3, dim= -1)\n",
    "\n",
    "\n",
    "    Q_enc = Q_enc.unsqueeze(0)\n",
    "    K_enc = K_enc.unsqueeze(0)\n",
    "    V_enc = V_enc.unsqueeze(0)\n",
    "\n",
    "\n",
    "    Q_enc = Q_enc.reshape(tgt_len, bsz * num_heads, head_dim).transpose(0, 1)\n",
    "    K_enc = K_enc.reshape(tgt_len, bsz * num_heads, head_dim).transpose(0, 1)\n",
    "    V_enc = V_enc.reshape(tgt_len, bsz * num_heads, head_dim).transpose(0, 1)\n",
    "\n",
    "    print(\"Q_enc_{} = \".format(layer_num))\n",
    "    print(Q_enc)\n",
    "    print()\n",
    "\n",
    "    print(\"K_enc_{} = \".format(layer_num))\n",
    "    print(K_enc)\n",
    "    print()\n",
    "\n",
    "    print(\"V_enc_{} = \".format(layer_num))\n",
    "    print(V_enc)\n",
    "    print()\n",
    "\n",
    "    src_len = K_enc.size(1)\n",
    "\n",
    "\n",
    "    attn_mask = src_mask\n",
    "    if attn_mask is not None:\n",
    "\n",
    "        # Ensuring attn_mask's dim is 3\n",
    "        if attn_mask.dim() == 2:\n",
    "            correct_2d_size = (tgt_len, src_len)\n",
    "            if attn_mask.shape != correct_2d_size:\n",
    "                raise RuntimeError(f\"The shape of the 2D attn_mask is {attn_mask.shape}, but should be {correct_2d_size}.\")\n",
    "            attn_mask = attn_mask.unsqueeze(0)\n",
    "        elif attn_mask.dim() == 3:\n",
    "            correct_3d_size = (bsz * num_heads, tgt_len, src_len)\n",
    "            if attn_mask.shape != correct_3d_size:\n",
    "                raise RuntimeError(f\"The shape of the 3D attn_mask is {attn_mask.shape}, but should be {correct_3d_size}.\")\n",
    "        else:\n",
    "            raise RuntimeError(f\"attn_mask's dimension {attn_mask.dim()} is not supported\")\n",
    "\n",
    "    # attn_mask can be either (L,S) or (N*num_heads, L, S)\n",
    "    # if attn_mask's shape is (1, L, S) we need to unsqueeze to (1, 1, L, S)\n",
    "    # in order to match the input for SDPA of (N, num_heads, L, S)\n",
    "    if attn_mask is not None:\n",
    "        if attn_mask.size(0) == 1 and attn_mask.dim() == 3:\n",
    "            attn_mask = attn_mask.unsqueeze(0)\n",
    "        else:\n",
    "            attn_mask = attn_mask.view(bsz, num_heads, -1, src_len)\n",
    "\n",
    "\n",
    "    if need_weights is False:\n",
    "\n",
    "        attn_output = atten_product_needs_wts_false(Q_enc, V_enc, K_enc, bsz, head_dim, src_len, tgt_len, attn_mask)\n",
    "\n",
    "        return attn_output, src_len, head_dim, None\n",
    "    \n",
    "    else:\n",
    "\n",
    "        attn_enc_output,attn_wt_matrix_enc = atten_product_needs_wts_true(Q_enc, K_enc, V_enc, bsz, tgt_len, attn_mask)\n",
    "\n",
    "        return attn_enc_output, src_len, head_dim, attn_wt_matrix_enc\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q_enc_0 = \n",
      "tensor([[[-0.6798,  1.4007]],\n",
      "\n",
      "        [[-0.2742, -1.0386]],\n",
      "\n",
      "        [[-1.9563,  0.6655]],\n",
      "\n",
      "        [[-0.6882, -0.6257]],\n",
      "\n",
      "        [[-0.6674, -0.6622]],\n",
      "\n",
      "        [[-0.4141,  0.8993]],\n",
      "\n",
      "        [[-0.2284, -0.3723]],\n",
      "\n",
      "        [[-0.8246, -0.2570]]])\n",
      "\n",
      "K_enc_0 = \n",
      "tensor([[[ 1.3458,  1.3452]],\n",
      "\n",
      "        [[ 0.7641, -1.2760]],\n",
      "\n",
      "        [[ 0.8736,  1.8444]],\n",
      "\n",
      "        [[ 1.0950, -0.9651]],\n",
      "\n",
      "        [[-0.5440, -0.1505]],\n",
      "\n",
      "        [[-0.0823,  0.2863]],\n",
      "\n",
      "        [[-0.6199,  0.2640]],\n",
      "\n",
      "        [[-0.8359,  1.0204]]])\n",
      "\n",
      "V_enc_0 = \n",
      "tensor([[[ 1.5610,  0.2397]],\n",
      "\n",
      "        [[ 1.2207,  0.6767]],\n",
      "\n",
      "        [[ 1.8320,  0.3493]],\n",
      "\n",
      "        [[ 1.1892,  0.1905]],\n",
      "\n",
      "        [[-0.4484, -0.4026]],\n",
      "\n",
      "        [[-0.0178, -0.5572]],\n",
      "\n",
      "        [[ 0.4017,  0.0647]],\n",
      "\n",
      "        [[-0.5237, -0.4228]]])\n",
      "\n",
      "Encoder Attention output = \n",
      "tensor([[ 1.5610,  0.2397,  1.2207,  0.6767],\n",
      "        [ 1.8320,  0.3493,  1.1892,  0.1905],\n",
      "        [-0.4484, -0.4026, -0.0178, -0.5572],\n",
      "        [ 0.4017,  0.0647, -0.5237, -0.4228]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# need_weights = True\n",
    "\n",
    "attn_enc_output, src_len, head_dim, attn_weights = encoder_block_attn_output(pe_src_embeds, state_dict, layer_num = 0, need_weights = need_weights, embed_dim=embed_dim, num_heads=num_heads)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Post self attention in encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder_block_post_attn_output(x, attn_enc_output, state_dict, layer_num, bsz, tgt_len):\n",
    "\n",
    "    op_enc_1 = torch.matmul(attn_enc_output, state_dict[\"transformer.encoder.layers.{}.self_attn.out_proj.weight\".format(layer_num)].t()) + state_dict[\"transformer.encoder.layers.{}.self_attn.out_proj.bias\".format(layer_num)]\n",
    "\n",
    "    attn_enc_output = op_enc_1.view(tgt_len, bsz, attn_enc_output.size(1))\n",
    "\n",
    "\n",
    "    # Here src is the original passed inputs to the 1st transformer encoder layer which \n",
    "    # are pe_src_embeds \n",
    "    output_enc_1 = attn_enc_output + x\n",
    "    output_enc_1\n",
    "\n",
    "    linear_result_enc_1 = output_enc_1*state_dict[\"transformer.encoder.layers.{}.norm1.weight\".format(layer_num)] + state_dict[\"transformer.encoder.layers.{}.norm1.bias\".format(layer_num)]\n",
    "\n",
    "    # Layer normalization from Torch's implementation\n",
    "    layernorm_enc_1 = torch.nn.LayerNorm(normalized_shape=linear_result_enc_1.shape[2:])\n",
    "    linear_op_enc_1 = layernorm_enc_1(linear_result_enc_1)\n",
    "\n",
    "\n",
    "    # Manual Layer Normalization\n",
    "    x = linear_result_enc_1\n",
    "\n",
    "    # Obtained layer norm weights and biases (learnable)\n",
    "    w = layernorm_enc_1.weight\n",
    "    b = layernorm_enc_1.bias\n",
    "\n",
    "    linear_result_enc_1f = w*x + b\n",
    "\n",
    "    epsilon = 1e-5  \n",
    "    mean = linear_result_enc_1f.mean(dim=-1, keepdim=True)\n",
    "    std = linear_result_enc_1f.std(dim=-1, unbiased=False, keepdim=True)\n",
    "    normalized_result_enc_1 = (linear_result_enc_1f - mean) / (std + epsilon) * w + b\n",
    "\n",
    "\n",
    "    op_enc_1 = torch.matmul(normalized_result_enc_1, state_dict[\"transformer.encoder.layers.{}.linear1.weight\".format(layer_num)].t()) + state_dict[\"transformer.encoder.layers.{}.linear1.bias\".format(layer_num)]\n",
    "    op_enc_1_relu = torch.nn.functional.relu(op_enc_1)\n",
    "    op_enc_2 = torch.matmul(op_enc_1_relu, state_dict[\"transformer.encoder.layers.{}.linear2.weight\".format(layer_num)].t()) + state_dict[\"transformer.encoder.layers.{}.linear2.bias\".format(layer_num)]\n",
    "\n",
    "\n",
    "    output_enc_2 = op_enc_2 + linear_op_enc_1\n",
    "    output_enc_2_norm = output_enc_2*state_dict[\"transformer.encoder.layers.{}.norm2.weight\".format(layer_num)] + state_dict[\"transformer.encoder.layers.{}.norm2.bias\".format(layer_num)]\n",
    "\n",
    "    # Layer normalization from Torch's implementation\n",
    "    layernorm_enc_final = torch.nn.LayerNorm(normalized_shape=output_enc_2_norm.shape[2:])\n",
    "    output_enc_final = layernorm_enc_final(output_enc_2_norm)\n",
    "\n",
    "\n",
    "    # Manual Layer Normalization \n",
    "    x = output_enc_2_norm\n",
    "\n",
    "    # Obtained layer norm weights and biases (learnable)\n",
    "    w = layernorm_enc_final.weight\n",
    "    b = layernorm_enc_final.bias\n",
    "\n",
    "    linear_result_enc_2 = w*x + b\n",
    "\n",
    "    epsilon = 1e-5  \n",
    "    mean = linear_result_enc_2.mean(dim=-1, keepdim=True)\n",
    "    std = linear_result_enc_2.std(dim=-1, unbiased=False, keepdim=True)\n",
    "    output_enc_final = (linear_result_enc_2 - mean) / (std + epsilon) * w + b\n",
    "\n",
    "    print(\"Final Encoder {} Output :\".format(layer_num))\n",
    "    print(\"norm2(norm1(x + self_atten(x)) + feed_fwd_op)\\n\")\n",
    "    print(output_enc_final)\n",
    "    print()\n",
    "\n",
    "    return output_enc_final\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Encoder 0 Output :\n",
      "norm2(norm1(x + self_atten(x)) + feed_fwd_op)\n",
      "\n",
      "tensor([[[ 0.2780,  1.5154, -0.9923, -0.8010],\n",
      "         [ 0.8535,  0.9828, -1.4747, -0.3617],\n",
      "         [ 0.3958, -1.2856, -0.5023,  1.3922],\n",
      "         [-0.9210, -0.9146,  1.4792,  0.3564]]], grad_fn=<AddBackward0>)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tgt_len, bsz, embed_dim = pe_src_embeds.shape\n",
    "output_enc_final = encoder_block_post_attn_output(pe_src_embeds, attn_enc_output, state_dict, layer_num = 0 , bsz = bsz, tgt_len = tgt_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder functions to display the intermediate outputs and get the final outputs from the decoder\n",
    "\n",
    "\n",
    "### Self attention outputs from a decoder block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoder_block_self_attn_output(x, state_dict, layer_num, tgt_mask = None,need_weights = False):\n",
    "\n",
    "    query_dec = key_dec = value_dec = x\n",
    "\n",
    "    tgt_len, bsz, embed_dim = x.shape\n",
    "\n",
    "    W_dec = state_dict[\"transformer.decoder.layers.{}.self_attn.in_proj_weight\".format(layer_num)]\n",
    "    b_dec = state_dict[\"transformer.decoder.layers.{}.self_attn.in_proj_bias\".format(layer_num)]\n",
    "\n",
    "\n",
    "    head_dim = embed_dim//num_heads\n",
    "\n",
    "    tempop1 = query_dec[0]@W_dec.T\n",
    "    tempop1 = tempop1.T\n",
    "\n",
    "    Q_dec,K_dec,V_dec = tempop1.T.chunk(3, dim= -1)\n",
    "\n",
    "\n",
    "    Q_dec = Q_dec.unsqueeze(0)\n",
    "    K_dec = K_dec.unsqueeze(0)\n",
    "    V_dec = V_dec.unsqueeze(0)\n",
    "\n",
    "\n",
    "    Q_dec = Q_dec.reshape(tgt_len, bsz * num_heads, head_dim).transpose(0, 1)\n",
    "    K_dec = K_dec.reshape(tgt_len, bsz * num_heads, head_dim).transpose(0, 1)\n",
    "    V_dec = V_dec.reshape(tgt_len, bsz * num_heads, head_dim).transpose(0, 1)\n",
    "\n",
    "    print(\"Q_dec_{} = \".format(layer_num))\n",
    "    print(Q_dec)\n",
    "    print()\n",
    "\n",
    "    print(\"K_dec_{} = \".format(layer_num))\n",
    "    print(K_dec)\n",
    "    print()\n",
    "\n",
    "    print(\"V_dec_{} = \".format(layer_num))\n",
    "    print(V_dec)\n",
    "    print()\n",
    "\n",
    "    src_len = K_dec.size(1)\n",
    "\n",
    "\n",
    "    attn_mask = tgt_mask\n",
    "    if attn_mask is not None:\n",
    "\n",
    "        # Ensuring attn_mask's dim is 3\n",
    "        if attn_mask.dim() == 2:\n",
    "            correct_2d_size = (tgt_len, src_len)\n",
    "            if attn_mask.shape != correct_2d_size:\n",
    "                raise RuntimeError(f\"The shape of the 2D attn_mask is {attn_mask.shape}, but should be {correct_2d_size}.\")\n",
    "            attn_mask = attn_mask.unsqueeze(0)\n",
    "        elif attn_mask.dim() == 3:\n",
    "            correct_3d_size = (bsz * num_heads, tgt_len, src_len)\n",
    "            if attn_mask.shape != correct_3d_size:\n",
    "                raise RuntimeError(f\"The shape of the 3D attn_mask is {attn_mask.shape}, but should be {correct_3d_size}.\")\n",
    "        else:\n",
    "            raise RuntimeError(f\"attn_mask's dimension {attn_mask.dim()} is not supported\")\n",
    "\n",
    "    # attn_mask can be either (L,S) or (N*num_heads, L, S)\n",
    "    # if attn_mask's shape is (1, L, S) we need to unsqueeze to (1, 1, L, S)\n",
    "    # in order to match the input for SDPA of (N, num_heads, L, S)\n",
    "    if attn_mask is not None:\n",
    "        if attn_mask.size(0) == 1 and attn_mask.dim() == 3:\n",
    "            attn_mask = attn_mask.unsqueeze(0)\n",
    "        else:\n",
    "            attn_mask = attn_mask.view(bsz, num_heads, -1, src_len)\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    if need_weights is False:\n",
    "        attn_output = atten_product_needs_wts_false(Q_dec, V_dec, K_dec, bsz, head_dim, src_len, tgt_len, attn_mask)\n",
    "\n",
    "        print(\"Decoder Self Attention = \")\n",
    "        print(attn_output)\n",
    "        print()\n",
    "\n",
    "        op_dec_1 = torch.matmul(attn_output, state_dict[\"transformer.decoder.layers.{}.self_attn.out_proj.weight\".format(layer_num)].t()) + state_dict[\"transformer.decoder.layers.{}.self_attn.out_proj.bias\".format(layer_num)]\n",
    "        attn_dec_output = op_dec_1.view(tgt_len, bsz, attn_output.size(1))\n",
    "\n",
    "        return attn_dec_output, None\n",
    "    \n",
    "    else:\n",
    "\n",
    "        attn_dec_output,attn_wt_matrix_dec = atten_product_needs_wts_true(Q_dec, K_dec, V_dec, bsz, tgt_len, attn_mask)\n",
    "\n",
    "        print(\"Decoder Attention output = \")\n",
    "        print(attn_wt_matrix_dec)\n",
    "        print()\n",
    "\n",
    "        op_dec_1 = torch.matmul(attn_dec_output, state_dict[\"transformer.decoder.layers.{}.self_attn.out_proj.weight\".format(layer_num)].t()) + state_dict[\"transformer.decoder.layers.{}.self_attn.out_proj.bias\".format(layer_num)]\n",
    "        attn_dec_output = op_dec_1.view(tgt_len, bsz, attn_dec_output.size(1))\n",
    "    \n",
    "\n",
    "        return attn_dec_output, attn_wt_matrix_dec\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# query_dec = key_dec = value_dec = pe_tgt_embeds \n",
    "\n",
    "# tgt_len, bsz, embed_dim = pe_src_embeds.shape\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "need_weights "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q_dec_0 = \n",
      "tensor([[[ 0.0632,  0.2018]],\n",
      "\n",
      "        [[ 0.5668, -0.7167]],\n",
      "\n",
      "        [[-0.0096,  0.2866]],\n",
      "\n",
      "        [[ 0.8592, -1.2781]],\n",
      "\n",
      "        [[ 0.5190,  0.4592]],\n",
      "\n",
      "        [[ 2.0303, -1.8184]],\n",
      "\n",
      "        [[ 0.3563,  0.6196]],\n",
      "\n",
      "        [[ 1.7640, -1.9240]]])\n",
      "\n",
      "K_dec_0 = \n",
      "tensor([[[-0.2527, -0.2785]],\n",
      "\n",
      "        [[-0.2614, -0.5521]],\n",
      "\n",
      "        [[-0.2099, -0.4170]],\n",
      "\n",
      "        [[-0.6074, -0.8707]],\n",
      "\n",
      "        [[ 0.0705, -1.5230]],\n",
      "\n",
      "        [[-0.9272, -0.7401]],\n",
      "\n",
      "        [[-0.3300, -1.0468]],\n",
      "\n",
      "        [[-0.8275, -1.0830]]])\n",
      "\n",
      "V_dec_0 = \n",
      "tensor([[[ 0.0145, -0.0974]],\n",
      "\n",
      "        [[-0.1493,  0.3044]],\n",
      "\n",
      "        [[ 0.0765, -0.2509]],\n",
      "\n",
      "        [[-0.4107,  0.5028]],\n",
      "\n",
      "        [[-0.1104,  1.0977]],\n",
      "\n",
      "        [[ 1.0221,  0.1571]],\n",
      "\n",
      "        [[ 0.0020,  0.4013]],\n",
      "\n",
      "        [[ 0.3516,  0.5678]]])\n",
      "\n",
      "torch.Size([4, 2, 1, 2]) torch.Size([4, 2, 1, 2]) torch.Size([4, 2, 2, 1])\n",
      "Attention weights = \n",
      "torch.Size([4, 2, 1, 1]) tensor([[[[-0.0510]],\n",
      "\n",
      "         [[ 0.1750]]],\n",
      "\n",
      "\n",
      "        [[[-0.0831]],\n",
      "\n",
      "         [[ 0.4179]]],\n",
      "\n",
      "\n",
      "        [[[-0.4686]],\n",
      "\n",
      "         [[-0.3795]]],\n",
      "\n",
      "\n",
      "        [[[-0.5418]],\n",
      "\n",
      "         [[ 0.4412]]]])\n",
      "Decoder Self Attention = \n",
      "tensor([[ 0.0145, -0.0974, -0.1493,  0.3044],\n",
      "        [ 0.0765, -0.2509, -0.4107,  0.5028],\n",
      "        [-0.1104,  1.0977,  1.0221,  0.1571],\n",
      "        [ 0.0020,  0.4013,  0.3516,  0.5678]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# need_weights = True\n",
    "\n",
    "self_attn_dec, dec_sa_wts = decoder_block_self_attn_output(pe_tgt_embeds, state_dict, layer_num = 0, need_weights = need_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.3354,  0.2949,  0.3642, -0.1185],\n",
       "         [ 0.7282,  0.5757,  0.7341, -0.0761],\n",
       "         [-1.3969, -0.5911, -1.1591, -0.9625],\n",
       "         [-0.2074,  0.1613, -0.0765, -0.7489]]])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "self_attn_dec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dec_post_self_attn(self_attn_dec, x, state_dict, layer_num):\n",
    "    output_dec_1 = self_attn_dec + x\n",
    "\n",
    "    linear_result_dec_1 = output_dec_1*state_dict[\"transformer.decoder.layers.{}.norm1.weight\".format(layer_num)] + state_dict[\"transformer.decoder.layers.{}.norm1.bias\".format(layer_num)]\n",
    "\n",
    "    layernorm_dec_1 = torch.nn.LayerNorm(normalized_shape=linear_result_dec_1.shape[2:])\n",
    "    linear_op_dec_1 = layernorm_dec_1(linear_result_dec_1)\n",
    "\n",
    "    # From torch's implementation\n",
    "    # print(linear_op_dec_1)\n",
    "\n",
    "    x = linear_result_dec_1\n",
    "\n",
    "    # Obtained layer norm weights and biases (learnable)\n",
    "    w = layernorm_dec_1.weight\n",
    "    b = layernorm_dec_1.bias\n",
    "\n",
    "    linear_result_dec_1f = w*x + b\n",
    "\n",
    "    epsilon = 1e-5  \n",
    "    mean = linear_result_dec_1f.mean(dim=-1, keepdim=True)\n",
    "    std = linear_result_dec_1f.std(dim=-1, unbiased=False, keepdim=True)\n",
    "    normalized_result_dec_1 = (linear_result_dec_1f - mean) / (std + epsilon) * w + b\n",
    "\n",
    "\n",
    "    print(\"Decoder_{} norm1(x + sa(x))\".format(layer_num))\n",
    "    print(normalized_result_dec_1)\n",
    "    print()\n",
    "\n",
    "    return normalized_result_dec_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoder_0 norm1(x + sa(x))\n",
      "tensor([[[-1.2045,  0.0837,  1.5386, -0.4179],\n",
      "         [-1.1748,  0.3544,  1.4489, -0.6285],\n",
      "         [-1.5035,  1.2978, -0.0273,  0.2330],\n",
      "         [-1.5796,  0.8699,  0.8529, -0.1432]]], grad_fn=<AddBackward0>)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x_dec = dec_post_self_attn(self_attn_dec, pe_tgt_embeds, state_dict, layer_num = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross attention in decoder \n",
    "### query, key, value =  x, mem, mem \n",
    "\n",
    "### Cross attention between the encoder's final output and the decoder layer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.2045,  0.0837,  1.5386, -0.4179],\n",
       "         [-1.1748,  0.3544,  1.4489, -0.6285],\n",
       "         [-1.5035,  1.2978, -0.0273,  0.2330],\n",
       "         [-1.5796,  0.8699,  0.8529, -0.1432]]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "memory = output_enc_final\n",
    "x_dec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoder_block_cross_attn_output(x_dec, memory, state_dict, layer_num, tgt_len, src_len, head_dim, memory_mask = None,need_weights = False):\n",
    "\n",
    "    query_dec_mha = x_dec\n",
    "    key_dec_mha, value_dec_mha = memory, memory\n",
    "\n",
    "\n",
    "    W_dec_mha = state_dict[\"transformer.decoder.layers.{}.multihead_attn.in_proj_weight\".format(layer_num)]\n",
    "    b_dec_mha = state_dict[\"transformer.decoder.layers.{}.multihead_attn.in_proj_bias\".format(layer_num)]\n",
    "\n",
    "\n",
    "    ########## CHANGE FROM SELF ATTENTION ############\n",
    "\n",
    "    W_q, W_k, W_v = W_dec_mha.chunk(3)\n",
    "\n",
    "    Q_dec_mha = query_dec_mha[0]@W_q.T\n",
    "\n",
    "    K_dec_mha = key_dec_mha[0]@W_k.T\n",
    "\n",
    "    V_dec_mha = value_dec_mha[0]@W_v.T\n",
    "\n",
    "    #################################################\n",
    "\n",
    "    K_dec_mha = K_dec_mha.unsqueeze(0)\n",
    "    V_dec_mha = V_dec_mha.unsqueeze(0)\n",
    "    Q_dec_mha = Q_dec_mha.unsqueeze(0)\n",
    "\n",
    "    Q_dec_mha = Q_dec_mha.reshape(tgt_len, bsz * num_heads, head_dim).transpose(0, 1)\n",
    "    K_dec_mha = K_dec_mha.reshape(tgt_len, bsz * num_heads, head_dim).transpose(0, 1)\n",
    "    V_dec_mha = V_dec_mha.reshape(tgt_len, bsz * num_heads, head_dim).transpose(0, 1)\n",
    "\n",
    "    print(\"Q_dec_{} = \".format(layer_num))\n",
    "    print(Q_dec_mha)\n",
    "    print()\n",
    "\n",
    "    print(\"K_dec_{} = \".format(layer_num))\n",
    "    print(K_dec_mha)\n",
    "    print()\n",
    "\n",
    "    print(\"V_dec_{} = \".format(layer_num))\n",
    "    print(V_dec_mha)\n",
    "    print()\n",
    "\n",
    "\n",
    "    attn_mask = memory_mask\n",
    "    if attn_mask is not None:\n",
    "\n",
    "        # Ensuring attn_mask's dim is 3\n",
    "        if attn_mask.dim() == 2:\n",
    "            correct_2d_size = (tgt_len, src_len)\n",
    "            if attn_mask.shape != correct_2d_size:\n",
    "                raise RuntimeError(f\"The shape of the 2D attn_mask is {attn_mask.shape}, but should be {correct_2d_size}.\")\n",
    "            attn_mask = attn_mask.unsqueeze(0)\n",
    "        elif attn_mask.dim() == 3:\n",
    "            correct_3d_size = (bsz * num_heads, tgt_len, src_len)\n",
    "            if attn_mask.shape != correct_3d_size:\n",
    "                raise RuntimeError(f\"The shape of the 3D attn_mask is {attn_mask.shape}, but should be {correct_3d_size}.\")\n",
    "        else:\n",
    "            raise RuntimeError(f\"attn_mask's dimension {attn_mask.dim()} is not supported\")\n",
    "\n",
    "    # attn_mask can be either (L,S) or (N*num_heads, L, S)\n",
    "    # if attn_mask's shape is (1, L, S) we need to unsqueeze to (1, 1, L, S)\n",
    "    # in order to match the input for SDPA of (N, num_heads, L, S)\n",
    "    if attn_mask is not None:\n",
    "        if attn_mask.size(0) == 1 and attn_mask.dim() == 3:\n",
    "            attn_mask = attn_mask.unsqueeze(0)\n",
    "        else:\n",
    "            attn_mask = attn_mask.view(bsz, num_heads, -1, src_len)\n",
    "\n",
    "\n",
    "\n",
    "    if need_weights is False:\n",
    "\n",
    "        attn_output_dec_mha = atten_product_needs_wts_false(Q_dec_mha, V_dec_mha, K_dec_mha, bsz, head_dim, src_len, tgt_len, attn_mask)\n",
    "        \n",
    "        print(\"Cross attention in decoder_{}\".format(layer_num))\n",
    "        print(attn_output_dec_mha)\n",
    "        print()\n",
    "\n",
    "        op_dec_mha_1 = torch.matmul(attn_output_dec_mha, state_dict[\"transformer.decoder.layers.{}.multihead_attn.out_proj.weight\".format(layer_num)].t()) + state_dict[\"transformer.decoder.layers.{}.multihead_attn.out_proj.bias\".format(layer_num)]\n",
    "\n",
    "        attn_dec_mha_output = op_dec_mha_1.view(tgt_len, bsz, attn_output_dec_mha.size(1))\n",
    "\n",
    "        return attn_dec_mha_output, None\n",
    "    \n",
    "    else: \n",
    "\n",
    "    \n",
    "        attn_dec_mha_output ,attn_wt_matrix_dec_mha = atten_product_needs_wts_true(Q_dec_mha, K_dec_mha, V_dec_mha, bsz, tgt_len, attn_mask)\n",
    "\n",
    "        print(\"Decoder mha Attention output = \")\n",
    "        print(attn_dec_mha_output)\n",
    "        print()\n",
    "\n",
    "\n",
    "        op_dec_mha = torch.matmul(attn_dec_mha_output, state_dict[\"transformer.decoder.layers.{}.multihead_attn.out_proj.weight\".format(layer_num)].t()) + state_dict[\"transformer.decoder.layers.{}.multihead_attn.out_proj.bias\".format(layer_num)]\n",
    "        attn_dec_output = op_dec_mha.view(tgt_len, bsz, attn_dec_mha_output.size(1))\n",
    "\n",
    "\n",
    "        return attn_dec_output , attn_wt_matrix_dec_mha\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q_dec_0 = \n",
      "tensor([[[ 0.2437, -0.8084]],\n",
      "\n",
      "        [[-0.2721,  0.8057]],\n",
      "\n",
      "        [[ 0.5289, -0.7797]],\n",
      "\n",
      "        [[-0.1821,  0.9150]],\n",
      "\n",
      "        [[ 1.1448, -0.3859]],\n",
      "\n",
      "        [[ 0.2385, -0.1027]],\n",
      "\n",
      "        [[ 0.8741, -0.6890]],\n",
      "\n",
      "        [[ 0.0020,  0.4243]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "K_dec_0 = \n",
      "tensor([[[ 1.1384,  0.6546]],\n",
      "\n",
      "        [[ 0.8390, -0.5678]],\n",
      "\n",
      "        [[ 1.1360,  1.1477]],\n",
      "\n",
      "        [[ 0.4548,  0.0592]],\n",
      "\n",
      "        [[-0.5776,  0.2464]],\n",
      "\n",
      "        [[-1.0737,  1.1551]],\n",
      "\n",
      "        [[-1.1307, -1.1934]],\n",
      "\n",
      "        [[-0.4308, -0.1117]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "V_dec_0 = \n",
      "tensor([[[ 0.2457,  0.0424]],\n",
      "\n",
      "        [[ 0.5397,  0.8359]],\n",
      "\n",
      "        [[-0.1711, -0.0743]],\n",
      "\n",
      "        [[ 0.2930,  1.1949]],\n",
      "\n",
      "        [[-0.7912, -0.3454]],\n",
      "\n",
      "        [[-0.6070,  0.2404]],\n",
      "\n",
      "        [[ 0.1975,  0.0695]],\n",
      "\n",
      "        [[-0.2703, -1.2053]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "torch.Size([4, 2, 1, 2]) torch.Size([4, 2, 1, 2]) torch.Size([4, 2, 2, 1])\n",
      "Attention weights = \n",
      "torch.Size([4, 2, 1, 1]) tensor([[[[-0.1780]],\n",
      "\n",
      "         [[-0.4849]]],\n",
      "\n",
      "\n",
      "        [[[-0.2079]],\n",
      "\n",
      "         [[-0.0202]]],\n",
      "\n",
      "\n",
      "        [[[-0.5348]],\n",
      "\n",
      "         [[-0.2650]]],\n",
      "\n",
      "\n",
      "        [[[-0.1175]],\n",
      "\n",
      "         [[-0.0341]]]], grad_fn=<AddBackward0>)\n",
      "Cross attention in decoder_0\n",
      "tensor([[ 0.2457,  0.0424,  0.5397,  0.8359],\n",
      "        [-0.1711, -0.0743,  0.2930,  1.1949],\n",
      "        [-0.7912, -0.3454, -0.6070,  0.2404],\n",
      "        [ 0.1975,  0.0695, -0.2703, -1.2053]], grad_fn=<ViewBackward0>)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# need_weights = True\n",
    "\n",
    "attn_dec_mha_output, attn_dec_mha_wts = decoder_block_cross_attn_output(x_dec, memory, state_dict, layer_num = 0, tgt_len = tgt_len, src_len = src_len, head_dim = head_dim, need_weights = need_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoder_block_post_attn_output(x_dec, attn_dec_mha_output, state_dict, layer_num):\n",
    "\n",
    "    output_dec_2 = attn_dec_mha_output + x_dec\n",
    "\n",
    "    linear_result_dec_2 = output_dec_2*state_dict[\"transformer.decoder.layers.{}.norm2.weight\".format(layer_num)] + state_dict[\"transformer.decoder.layers.{}.norm2.bias\".format(layer_num)]\n",
    "\n",
    "    # Layer normalization from Torch's implementation \n",
    "    layernorm_dec_2 = torch.nn.LayerNorm(normalized_shape=linear_result_dec_2.shape[2:])\n",
    "    linear_op_dec_2 = layernorm_dec_2(linear_result_dec_2)\n",
    "\n",
    "    x = linear_result_dec_2\n",
    "    w = layernorm_dec_2.weight\n",
    "    b = layernorm_dec_2.bias\n",
    "\n",
    "    linear_result_dec_2f = w*x + b\n",
    "\n",
    "    epsilon = 1e-5  \n",
    "    mean = linear_result_dec_2f.mean(dim=-1, keepdim=True)\n",
    "    std = linear_result_dec_2f.std(dim=-1, unbiased=False, keepdim=True)\n",
    "    normalized_result_dec_2 = (linear_result_dec_2f - mean) / (std + epsilon) * w + b\n",
    "\n",
    "    print(\"norm2(x' + mha(x', mem)), \\n where x' = Decoder_curr_layer norm1(x + sa(x))\")\n",
    "    print(normalized_result_dec_2)\n",
    "    print(\"\\n\\n\")\n",
    "\n",
    "    x_dec2_norm = normalized_result_dec_2\n",
    "\n",
    "    op_dec_1 = torch.matmul(x_dec2_norm, state_dict[\"transformer.decoder.layers.{}.linear1.weight\".format(layer_num)].t()) + state_dict[\"transformer.decoder.layers.{}.linear1.bias\".format(layer_num)]\n",
    "    op_dec_1_relu = torch.nn.functional.relu(op_dec_1)\n",
    "    op_dec_2 = torch.matmul(op_dec_1_relu, state_dict[\"transformer.decoder.layers.{}.linear2.weight\".format(layer_num)].t()) + state_dict[\"transformer.decoder.layers.{}.linear2.bias\".format(layer_num)]\n",
    "\n",
    "    ff_dec = op_dec_2\n",
    "\n",
    "    x_dec3_unorm = x_dec2_norm + ff_dec\n",
    "\n",
    "    linear_result_dec_3 = x_dec3_unorm*state_dict[\"transformer.decoder.layers.0.norm3.weight\"] + state_dict[\"transformer.decoder.layers.0.norm3.bias\"]\n",
    "\n",
    "    # Layer normalization from Torch's implementation \n",
    "    layernorm_dec_3 = torch.nn.LayerNorm(normalized_shape=linear_result_dec_3.shape[2:])\n",
    "    linear_op_dec_3 = layernorm_dec_3(linear_result_dec_3)\n",
    "\n",
    "    # From torch's implementation\n",
    "    x = linear_result_dec_3\n",
    "\n",
    "    # Obtained layer norm weights and biases (learnable)\n",
    "    w = layernorm_dec_3.weight\n",
    "    b = layernorm_dec_3.bias\n",
    "\n",
    "    linear_result_dec_3f = w*x + b\n",
    "\n",
    "    epsilon = 1e-5  \n",
    "    mean = linear_result_dec_3f.mean(dim=-1, keepdim=True)\n",
    "    std = linear_result_dec_3f.std(dim=-1, unbiased=False, keepdim=True)\n",
    "    normalized_result_dec_3 = (linear_result_dec_3f - mean) / (std + epsilon) * w + b\n",
    "\n",
    "    print(\"norm3(x'' + ff(x'')) \\n where, x'' = Decoder_curr_layer norm2(x' + mha(x'))\")\n",
    "    print(normalized_result_dec_3)\n",
    "    print()\n",
    "\n",
    "    return normalized_result_dec_3\n",
    "\n",
    "def feef_fwd_transformer(dec_output_final, state_dict):\n",
    "\n",
    "    final_op = dec_output_final@state_dict[\"fc.weight\"].T + state_dict[\"fc.bias\"]\n",
    "\n",
    "    return final_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "norm2(x' + mha(x', mem)), \n",
      " where x' = Decoder_curr_layer norm1(x + sa(x))\n",
      "tensor([[[-0.6314, -1.0291,  1.5930,  0.0675],\n",
      "         [-1.1430, -0.4263,  1.5847, -0.0154],\n",
      "         [-1.4210,  1.3967, -0.1094,  0.1336],\n",
      "         [-1.3878,  0.9595,  0.9427, -0.5144]]], grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "\n",
      "norm3(x'' + ff(x'')) \n",
      " where, x'' = Decoder_curr_layer norm2(x' + mha(x'))\n",
      "tensor([[[-0.8488, -0.8435,  1.5998,  0.0924],\n",
      "         [-1.2543, -0.3016,  1.5280,  0.0280],\n",
      "         [-1.5712,  1.2070,  0.1186,  0.2456],\n",
      "         [-1.4815,  0.8364,  0.9924, -0.3473]]], grad_fn=<AddBackward0>)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final_attn_op_decoder = decoder_block_post_attn_output(x_dec, attn_dec_mha_output, state_dict, layer_num = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_op = feef_fwd_transformer(final_attn_op_decoder, state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.4197, -0.4107, -0.1168,  0.9968, -0.2262,  0.8008,  0.1751,\n",
       "          -0.5905,  1.7112, -0.8088],\n",
       "         [ 1.5076, -0.4805, -0.0610,  1.3007, -0.1737,  0.7942,  0.3358,\n",
       "          -0.8888,  1.4928, -0.8529],\n",
       "         [ 1.0689, -0.2571, -0.0720,  1.4597,  0.3436,  0.2593,  0.7902,\n",
       "          -0.8153,  0.1954, -0.6916],\n",
       "         [ 1.2454, -0.4862, -0.1692,  1.4866, -0.2175,  0.4440,  0.5660,\n",
       "          -1.1615,  0.6706, -0.7323]]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_op"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "########################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examaple :- \n",
    "\n",
    "Transformer with 3 encoders and 3 decoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source embeddings :- \n",
      "\n",
      "tensor([[[-0.2205,  1.7253, -0.2514,  0.9972],\n",
      "         [ 0.1335, -0.5650, -0.7537,  0.0105],\n",
      "         [-0.8226,  0.9250,  1.7916, -0.4149],\n",
      "         [ 0.2400, -2.0239, -0.6187, -0.7110]]], grad_fn=<EmbeddingBackward0>)\n",
      "\n",
      "Target embeddings :- \n",
      "\n",
      "tensor([[[ 0.0958, -0.3552,  1.2360, -0.8826],\n",
      "         [ 0.0204,  1.1688, -0.0480, -1.3756],\n",
      "         [-0.1728,  0.3034, -1.1816, -1.1548],\n",
      "         [-0.1728,  0.3034, -1.1816, -1.1548]]], grad_fn=<EmbeddingBackward0>)\n",
      "\n",
      "Positional encoded source embeddings:\n",
      "tensor([[[-0.2205,  3.7253,  1.7486,  4.9972],\n",
      "         [ 0.9750,  0.9753,  1.2563,  4.0105],\n",
      "         [ 0.0867,  1.5088,  3.8116,  3.5849],\n",
      "         [ 0.3811, -2.0139,  1.4113,  3.2885]]], grad_fn=<AddBackward0>)\n",
      "\n",
      "Positional encoded target embeddings:\n",
      "tensor([[[ 1.0958,  0.6448,  4.2360,  3.1174],\n",
      "         [ 1.8619,  1.7091,  2.9620,  2.6244],\n",
      "         [ 1.7365, -0.1128,  1.8384,  2.8450],\n",
      "         [ 0.9683, -0.6866,  1.8484,  2.8448]]], grad_fn=<AddBackward0>)\n",
      "\n",
      "query =  tensor([[[-0.2205,  3.7253,  1.7486,  4.9972],\n",
      "         [ 0.9750,  0.9753,  1.2563,  4.0105],\n",
      "         [ 0.0867,  1.5088,  3.8116,  3.5849],\n",
      "         [ 0.3811, -2.0139,  1.4113,  3.2885]]], grad_fn=<AddBackward0>)\n",
      "query shape =  torch.Size([1, 4, 4])\n",
      "\n",
      " _in_projection_packed \n",
      "\n",
      "Q :-  tensor([[[ 0.6200, -0.9966, -0.1113,  1.0026],\n",
      "         [-0.1846, -0.6229,  0.4113, -0.8594],\n",
      "         [-1.4594, -0.9644,  0.6233, -0.4554],\n",
      "         [-1.3783, -0.2546,  1.3028, -2.3128]]], grad_fn=<SelectBackward0>)\n",
      "\n",
      "K :-  tensor([[[ 1.9172, -1.6883,  0.3543,  3.3778],\n",
      "         [ 1.4162, -1.2422,  0.8147,  1.4920],\n",
      "         [ 0.0826, -1.0527, -0.6941,  2.5530],\n",
      "         [ 0.7099, -1.9596,  0.4214,  0.1708]]], grad_fn=<SelectBackward0>)\n",
      "\n",
      "V :-  tensor([[[ 3.0345, -4.3447,  4.4862, -0.7491],\n",
      "         [ 2.3662, -3.3168,  2.7636, -0.3210],\n",
      "         [ 3.1573, -4.3181,  2.9714,  0.0482],\n",
      "         [ 2.5920, -2.3741,  0.7878,  0.8075]]], grad_fn=<SelectBackward0>)\n",
      "\n",
      "tgt_len =  1\n",
      "bsz =  4\n",
      "num_heads =  2\n",
      "head_dim =  2\n",
      "\n",
      "tensor([[[ 0.6200, -0.9966, -0.1113,  1.0026],\n",
      "         [-0.1846, -0.6229,  0.4113, -0.8594],\n",
      "         [-1.4594, -0.9644,  0.6233, -0.4554],\n",
      "         [-1.3783, -0.2546,  1.3028, -2.3128]]], grad_fn=<SelectBackward0>) torch.Size([1, 4, 4])\n",
      "Q reshaped =  tensor([[[ 0.6200, -0.9966]],\n",
      "\n",
      "        [[-0.1113,  1.0026]],\n",
      "\n",
      "        [[-0.1846, -0.6229]],\n",
      "\n",
      "        [[ 0.4113, -0.8594]],\n",
      "\n",
      "        [[-1.4594, -0.9644]],\n",
      "\n",
      "        [[ 0.6233, -0.4554]],\n",
      "\n",
      "        [[-1.3783, -0.2546]],\n",
      "\n",
      "        [[ 1.3028, -2.3128]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "K reshaped =  tensor([[[ 1.9172, -1.6883]],\n",
      "\n",
      "        [[ 0.3543,  3.3778]],\n",
      "\n",
      "        [[ 1.4162, -1.2422]],\n",
      "\n",
      "        [[ 0.8147,  1.4920]],\n",
      "\n",
      "        [[ 0.0826, -1.0527]],\n",
      "\n",
      "        [[-0.6941,  2.5530]],\n",
      "\n",
      "        [[ 0.7099, -1.9596]],\n",
      "\n",
      "        [[ 0.4214,  0.1708]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "V reshaped =  tensor([[[ 3.0345, -4.3447]],\n",
      "\n",
      "        [[ 4.4862, -0.7491]],\n",
      "\n",
      "        [[ 2.3662, -3.3168]],\n",
      "\n",
      "        [[ 2.7636, -0.3210]],\n",
      "\n",
      "        [[ 3.1573, -4.3181]],\n",
      "\n",
      "        [[ 2.9714,  0.0482]],\n",
      "\n",
      "        [[ 2.5920, -2.3741]],\n",
      "\n",
      "        [[ 0.7878,  0.8075]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "torch.Size([4, 2, 1, 2]) torch.Size([4, 2, 1, 2]) torch.Size([4, 2, 1, 2])\n",
      "\n",
      "scaled_dot_product_attention\n",
      "attn_output = \n",
      "tensor([[ 3.0345, -4.3447,  4.4862, -0.7491],\n",
      "        [ 2.3662, -3.3168,  2.7636, -0.3210],\n",
      "        [ 3.1573, -4.3181,  2.9714,  0.0482],\n",
      "        [ 2.5920, -2.3741,  0.7878,  0.8075]], grad_fn=<ViewBackward0>)\n",
      "\n",
      "x + self_atten(x) = \n",
      "tensor([[[ 0.3759, 10.1658, -0.2625,  9.7144],\n",
      "         [ 1.3622,  5.3116, -0.1058,  7.1487],\n",
      "         [ 0.5270,  6.4245,  2.3854,  7.1627],\n",
      "         [ 0.7453, -0.1984,  1.3652,  5.1093]]], grad_fn=<AddBackward0>)\n",
      "\n",
      "norm1(x + self_atten(x)) = \n",
      "tensor([[[-0.9339,  1.0440, -1.0629,  0.9528],\n",
      "         [-0.7074,  0.6443, -1.2099,  1.2730],\n",
      "         [-1.3032,  0.8330, -0.6301,  1.1004],\n",
      "         [-0.5013, -0.9697, -0.1936,  1.6646]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "\n",
      "feed_fwd_op =  tensor([[[-0.1663,  0.2646, -0.1747,  0.0518],\n",
      "         [-0.1308,  0.2821, -0.1690,  0.0694],\n",
      "         [-0.1481,  0.2724, -0.1941,  0.0482],\n",
      "         [-0.0589,  0.3677, -0.1883,  0.0552]]], grad_fn=<ViewBackward0>)\n",
      "\n",
      "norm2(norm1(x + self_atten(x)) + feed_fwd_op) = \n",
      "tensor([[[-0.9361,  1.1250, -1.0537,  0.8648],\n",
      "         [-0.7420,  0.7963, -1.2133,  1.1590],\n",
      "         [-1.2530,  0.9626, -0.7096,  1.0000],\n",
      "         [-0.6221, -0.6651, -0.4386,  1.7258]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "\n",
      "##################################\n",
      "query =  tensor([[[-0.9361,  1.1250, -1.0537,  0.8648],\n",
      "         [-0.7420,  0.7963, -1.2133,  1.1590],\n",
      "         [-1.2530,  0.9626, -0.7096,  1.0000],\n",
      "         [-0.6221, -0.6651, -0.4386,  1.7258]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "query shape =  torch.Size([1, 4, 4])\n",
      "\n",
      " _in_projection_packed \n",
      "\n",
      "Q :-  tensor([[[ 0.3627,  0.2891,  0.1520, -1.9940],\n",
      "         [ 0.3101, -0.0514, -0.1626, -1.9222],\n",
      "         [ 0.6701,  0.3549,  0.3880, -1.8755],\n",
      "         [ 0.6906, -0.7477, -0.4020, -0.7598]]], grad_fn=<SelectBackward0>)\n",
      "\n",
      "K :-  tensor([[[ 0.4935, -0.7928, -0.7312,  1.2475],\n",
      "         [ 0.4188, -0.6940, -0.7778,  1.1314],\n",
      "         [ 0.5690, -1.0316, -0.6252,  1.4893],\n",
      "         [ 0.2258, -0.6703, -0.3773,  0.9025]]], grad_fn=<SelectBackward0>)\n",
      "\n",
      "V :-  tensor([[[ 0.4030, -0.8576, -1.5052, -0.5695],\n",
      "         [ 0.4024, -0.5434, -1.3514, -0.2802],\n",
      "         [ 0.4675, -0.8433, -1.5064, -0.5631],\n",
      "         [ 0.3788,  0.4803, -0.4493,  0.5641]]], grad_fn=<SelectBackward0>)\n",
      "\n",
      "tgt_len =  1\n",
      "bsz =  4\n",
      "num_heads =  2\n",
      "head_dim =  2\n",
      "\n",
      "tensor([[[ 0.3627,  0.2891,  0.1520, -1.9940],\n",
      "         [ 0.3101, -0.0514, -0.1626, -1.9222],\n",
      "         [ 0.6701,  0.3549,  0.3880, -1.8755],\n",
      "         [ 0.6906, -0.7477, -0.4020, -0.7598]]], grad_fn=<SelectBackward0>) torch.Size([1, 4, 4])\n",
      "Q reshaped =  tensor([[[ 0.3627,  0.2891]],\n",
      "\n",
      "        [[ 0.1520, -1.9940]],\n",
      "\n",
      "        [[ 0.3101, -0.0514]],\n",
      "\n",
      "        [[-0.1626, -1.9222]],\n",
      "\n",
      "        [[ 0.6701,  0.3549]],\n",
      "\n",
      "        [[ 0.3880, -1.8755]],\n",
      "\n",
      "        [[ 0.6906, -0.7477]],\n",
      "\n",
      "        [[-0.4020, -0.7598]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "K reshaped =  tensor([[[ 0.4935, -0.7928]],\n",
      "\n",
      "        [[-0.7312,  1.2475]],\n",
      "\n",
      "        [[ 0.4188, -0.6940]],\n",
      "\n",
      "        [[-0.7778,  1.1314]],\n",
      "\n",
      "        [[ 0.5690, -1.0316]],\n",
      "\n",
      "        [[-0.6252,  1.4893]],\n",
      "\n",
      "        [[ 0.2258, -0.6703]],\n",
      "\n",
      "        [[-0.3773,  0.9025]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "V reshaped =  tensor([[[ 0.4030, -0.8576]],\n",
      "\n",
      "        [[-1.5052, -0.5695]],\n",
      "\n",
      "        [[ 0.4024, -0.5434]],\n",
      "\n",
      "        [[-1.3514, -0.2802]],\n",
      "\n",
      "        [[ 0.4675, -0.8433]],\n",
      "\n",
      "        [[-1.5064, -0.5631]],\n",
      "\n",
      "        [[ 0.3788,  0.4803]],\n",
      "\n",
      "        [[-0.4493,  0.5641]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "torch.Size([4, 2, 1, 2]) torch.Size([4, 2, 1, 2]) torch.Size([4, 2, 1, 2])\n",
      "\n",
      "scaled_dot_product_attention\n",
      "attn_output = \n",
      "tensor([[ 0.4030, -0.8576, -1.5052, -0.5695],\n",
      "        [ 0.4024, -0.5434, -1.3514, -0.2802],\n",
      "        [ 0.4675, -0.8433, -1.5064, -0.5631],\n",
      "        [ 0.3788,  0.4803, -0.4493,  0.5641]], grad_fn=<ViewBackward0>)\n",
      "\n",
      "x + self_atten(x) = \n",
      "tensor([[[-1.6024, -0.0855, -0.8318,  2.0456],\n",
      "         [-1.1145, -0.1943, -0.8227,  2.1095],\n",
      "         [-1.8601, -0.1886, -0.4596,  2.1959],\n",
      "         [ 0.0045, -0.6032,  0.2809,  1.7661]]], grad_fn=<AddBackward0>)\n",
      "\n",
      "norm1(x + self_atten(x)) = \n",
      "tensor([[[-1.0913,  0.0243, -0.5246,  1.5916],\n",
      "         [-0.8763, -0.1492, -0.6457,  1.6712],\n",
      "         [-1.2221, -0.0757, -0.2616,  1.5595],\n",
      "         [-0.4103, -1.1077, -0.0931,  1.6112]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "\n",
      "feed_fwd_op =  tensor([[[ 0.4470, -0.5749, -0.1584, -0.0712],\n",
      "         [ 0.4435, -0.5669, -0.1590, -0.0966],\n",
      "         [ 0.4216, -0.5813, -0.1474, -0.0723],\n",
      "         [ 0.3456, -0.5575, -0.0970, -0.1832]]], grad_fn=<ViewBackward0>)\n",
      "\n",
      "norm2(norm1(x + self_atten(x)) + feed_fwd_op) = \n",
      "tensor([[[-0.5963, -0.4956, -0.6379,  1.7297],\n",
      "         [-0.3472, -0.6382, -0.7293,  1.7147],\n",
      "         [-0.7637, -0.6084, -0.3400,  1.7120],\n",
      "         [ 0.0533, -1.4090, -0.0613,  1.4170]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "\n",
      "##################################\n",
      "query =  tensor([[[-0.5963, -0.4956, -0.6379,  1.7297],\n",
      "         [-0.3472, -0.6382, -0.7293,  1.7147],\n",
      "         [-0.7637, -0.6084, -0.3400,  1.7120],\n",
      "         [ 0.0533, -1.4090, -0.0613,  1.4170]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "query shape =  torch.Size([1, 4, 4])\n",
      "\n",
      " _in_projection_packed \n",
      "\n",
      "Q :-  tensor([[[ 0.0405,  0.5774, -0.2596,  0.5393],\n",
      "         [ 0.1239,  0.6862, -0.3060,  0.4950],\n",
      "         [ 0.1620,  0.5662, -0.1612,  0.4365],\n",
      "         [ 0.7716,  0.9559, -0.1349, -0.0280]]], grad_fn=<SelectBackward0>)\n",
      "\n",
      "K :-  tensor([[[ 0.0710,  0.5148, -1.2144,  0.3167],\n",
      "         [ 0.1238,  0.3417, -1.1581,  0.3155],\n",
      "         [ 0.0116,  0.5938, -1.1497,  0.1432],\n",
      "         [ 0.1141, -0.0613, -0.6354, -0.2518]]], grad_fn=<SelectBackward0>)\n",
      "\n",
      "V :-  tensor([[[0.8298, 0.5688, 0.2635, 0.6075],\n",
      "         [0.8487, 0.6739, 0.1504, 0.7465],\n",
      "         [0.9210, 0.4544, 0.4513, 0.4206],\n",
      "         [1.0978, 0.6180, 0.3093, 0.5803]]], grad_fn=<SelectBackward0>)\n",
      "\n",
      "tgt_len =  1\n",
      "bsz =  4\n",
      "num_heads =  2\n",
      "head_dim =  2\n",
      "\n",
      "tensor([[[ 0.0405,  0.5774, -0.2596,  0.5393],\n",
      "         [ 0.1239,  0.6862, -0.3060,  0.4950],\n",
      "         [ 0.1620,  0.5662, -0.1612,  0.4365],\n",
      "         [ 0.7716,  0.9559, -0.1349, -0.0280]]], grad_fn=<SelectBackward0>) torch.Size([1, 4, 4])\n",
      "Q reshaped =  tensor([[[ 0.0405,  0.5774]],\n",
      "\n",
      "        [[-0.2596,  0.5393]],\n",
      "\n",
      "        [[ 0.1239,  0.6862]],\n",
      "\n",
      "        [[-0.3060,  0.4950]],\n",
      "\n",
      "        [[ 0.1620,  0.5662]],\n",
      "\n",
      "        [[-0.1612,  0.4365]],\n",
      "\n",
      "        [[ 0.7716,  0.9559]],\n",
      "\n",
      "        [[-0.1349, -0.0280]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "K reshaped =  tensor([[[ 0.0710,  0.5148]],\n",
      "\n",
      "        [[-1.2144,  0.3167]],\n",
      "\n",
      "        [[ 0.1238,  0.3417]],\n",
      "\n",
      "        [[-1.1581,  0.3155]],\n",
      "\n",
      "        [[ 0.0116,  0.5938]],\n",
      "\n",
      "        [[-1.1497,  0.1432]],\n",
      "\n",
      "        [[ 0.1141, -0.0613]],\n",
      "\n",
      "        [[-0.6354, -0.2518]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "V reshaped =  tensor([[[0.8298, 0.5688]],\n",
      "\n",
      "        [[0.2635, 0.6075]],\n",
      "\n",
      "        [[0.8487, 0.6739]],\n",
      "\n",
      "        [[0.1504, 0.7465]],\n",
      "\n",
      "        [[0.9210, 0.4544]],\n",
      "\n",
      "        [[0.4513, 0.4206]],\n",
      "\n",
      "        [[1.0978, 0.6180]],\n",
      "\n",
      "        [[0.3093, 0.5803]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "torch.Size([4, 2, 1, 2]) torch.Size([4, 2, 1, 2]) torch.Size([4, 2, 1, 2])\n",
      "\n",
      "scaled_dot_product_attention\n",
      "attn_output = \n",
      "tensor([[0.8298, 0.5688, 0.2635, 0.6075],\n",
      "        [0.8487, 0.6739, 0.1504, 0.7465],\n",
      "        [0.9210, 0.4544, 0.4513, 0.4206],\n",
      "        [1.0978, 0.6180, 0.3093, 0.5803]], grad_fn=<ViewBackward0>)\n",
      "\n",
      "x + self_atten(x) = \n",
      "tensor([[[-0.6962, -0.3005, -0.8981,  1.6969],\n",
      "         [-0.5437, -0.5735, -1.1419,  1.7736],\n",
      "         [-0.7530, -0.1603, -0.3758,  1.5299],\n",
      "         [-0.1229, -1.0374, -0.2774,  1.3355]]], grad_fn=<AddBackward0>)\n",
      "\n",
      "norm1(x + self_atten(x)) = \n",
      "tensor([[[-0.6273, -0.2435, -0.8232,  1.6940],\n",
      "         [-0.3771, -0.4038, -0.9114,  1.6923],\n",
      "         [-0.9298, -0.2521, -0.4984,  1.6803],\n",
      "         [-0.1133, -1.1784, -0.2933,  1.5851]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "\n",
      "feed_fwd_op =  tensor([[[-0.0765, -0.2176, -0.0241, -0.0379],\n",
      "         [-0.0726, -0.2200, -0.0284, -0.0344],\n",
      "         [-0.0856, -0.2137, -0.0139, -0.0377],\n",
      "         [-0.0755, -0.2121, -0.0265, -0.0180]]], grad_fn=<ViewBackward0>)\n",
      "\n",
      "norm2(norm1(x + self_atten(x)) + feed_fwd_op) = \n",
      "tensor([[[-0.6045, -0.3659, -0.7456,  1.7160],\n",
      "         [-0.3525, -0.5226, -0.8313,  1.7063],\n",
      "         [-0.9077, -0.3700, -0.4154,  1.6931],\n",
      "         [-0.0998, -1.2328, -0.2232,  1.5558]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "\n",
      "##################################\n",
      "\n",
      "################### Transformer Decoder start #####################\n",
      "\n",
      "query =  tensor([[[ 1.0958,  0.6448,  4.2360,  3.1174],\n",
      "         [ 1.8619,  1.7091,  2.9620,  2.6244],\n",
      "         [ 1.7365, -0.1128,  1.8384,  2.8450],\n",
      "         [ 0.9683, -0.6866,  1.8484,  2.8448]]], grad_fn=<AddBackward0>)\n",
      "query shape =  torch.Size([1, 4, 4])\n",
      "\n",
      " _in_projection_packed \n",
      "\n",
      "Q :-  tensor([[[-3.1998,  2.1538, -1.0128,  0.7383],\n",
      "         [-2.6388,  1.2949, -0.3135,  0.9396],\n",
      "         [-2.5799,  0.7808, -0.3913, -0.4072],\n",
      "         [-2.3275,  1.0272, -0.7647, -0.5439]]], grad_fn=<SelectBackward0>)\n",
      "\n",
      "K :-  tensor([[[ 1.0097,  1.4010,  0.8216, -3.9416],\n",
      "         [ 1.1088,  2.1519,  0.0169, -3.9404],\n",
      "         [ 0.6041,  1.1918, -0.2197, -2.3607],\n",
      "         [ 0.5968,  0.5446,  0.1456, -1.7790]]], grad_fn=<SelectBackward0>)\n",
      "\n",
      "V :-  tensor([[[-2.4218,  1.2453,  0.6700,  1.9331],\n",
      "         [-1.5839,  0.7873,  0.8833,  1.9419],\n",
      "         [-2.1300,  0.6044,  0.7146,  0.7506],\n",
      "         [-2.1505,  0.7811,  0.6343,  0.5329]]], grad_fn=<SelectBackward0>)\n",
      "\n",
      "tgt_len =  1\n",
      "bsz =  4\n",
      "num_heads =  2\n",
      "head_dim =  2\n",
      "\n",
      "tensor([[[-3.1998,  2.1538, -1.0128,  0.7383],\n",
      "         [-2.6388,  1.2949, -0.3135,  0.9396],\n",
      "         [-2.5799,  0.7808, -0.3913, -0.4072],\n",
      "         [-2.3275,  1.0272, -0.7647, -0.5439]]], grad_fn=<SelectBackward0>) torch.Size([1, 4, 4])\n",
      "Q reshaped =  tensor([[[-3.1998,  2.1538]],\n",
      "\n",
      "        [[-1.0128,  0.7383]],\n",
      "\n",
      "        [[-2.6388,  1.2949]],\n",
      "\n",
      "        [[-0.3135,  0.9396]],\n",
      "\n",
      "        [[-2.5799,  0.7808]],\n",
      "\n",
      "        [[-0.3913, -0.4072]],\n",
      "\n",
      "        [[-2.3275,  1.0272]],\n",
      "\n",
      "        [[-0.7647, -0.5439]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "K reshaped =  tensor([[[ 1.0097,  1.4010]],\n",
      "\n",
      "        [[ 0.8216, -3.9416]],\n",
      "\n",
      "        [[ 1.1088,  2.1519]],\n",
      "\n",
      "        [[ 0.0169, -3.9404]],\n",
      "\n",
      "        [[ 0.6041,  1.1918]],\n",
      "\n",
      "        [[-0.2197, -2.3607]],\n",
      "\n",
      "        [[ 0.5968,  0.5446]],\n",
      "\n",
      "        [[ 0.1456, -1.7790]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "V reshaped =  tensor([[[-2.4218,  1.2453]],\n",
      "\n",
      "        [[ 0.6700,  1.9331]],\n",
      "\n",
      "        [[-1.5839,  0.7873]],\n",
      "\n",
      "        [[ 0.8833,  1.9419]],\n",
      "\n",
      "        [[-2.1300,  0.6044]],\n",
      "\n",
      "        [[ 0.7146,  0.7506]],\n",
      "\n",
      "        [[-2.1505,  0.7811]],\n",
      "\n",
      "        [[ 0.6343,  0.5329]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "torch.Size([4, 2, 1, 2]) torch.Size([4, 2, 1, 2]) torch.Size([4, 2, 1, 2])\n",
      "\n",
      "scaled_dot_product_attention\n",
      "attn_output = \n",
      "tensor([[-2.4218,  1.2453,  0.6700,  1.9331],\n",
      "        [-1.5839,  0.7873,  0.8833,  1.9419],\n",
      "        [-2.1300,  0.6044,  0.7146,  0.7506],\n",
      "        [-2.1505,  0.7811,  0.6343,  0.5329]], grad_fn=<ViewBackward0>)\n",
      "\n",
      "x =  tensor([[[ 1.0958,  0.6448,  4.2360,  3.1174],\n",
      "         [ 1.8619,  1.7091,  2.9620,  2.6244],\n",
      "         [ 1.7365, -0.1128,  1.8384,  2.8450],\n",
      "         [ 0.9683, -0.6866,  1.8484,  2.8448]]], grad_fn=<AddBackward0>)\n",
      "\n",
      "sa_op =  tensor([[[-2.5407, -0.7950,  3.1156,  3.2390],\n",
      "         [-1.9186, -0.9082,  2.6121,  2.2215],\n",
      "         [-1.5350, -0.6587,  1.9988,  2.4555],\n",
      "         [-1.5773, -0.6308,  1.9817,  2.5988]]], grad_fn=<ViewBackward0>)\n",
      "\n",
      "temop =  tensor([[[-1.4449, -0.1502,  7.3516,  6.3564],\n",
      "         [-0.0567,  0.8009,  5.5741,  4.8459],\n",
      "         [ 0.2016, -0.7714,  3.8372,  5.3005],\n",
      "         [-0.6090, -1.3174,  3.8301,  5.4436]]], grad_fn=<AddBackward0>)\n",
      "\n",
      "Decoder norm1(x + sa(x)) = \n",
      "tensor([[[-1.1561, -0.8215,  1.1174,  0.8602],\n",
      "         [-1.1617, -0.8118,  1.1353,  0.8382],\n",
      "         [-0.7746, -1.1630,  0.6767,  1.2608],\n",
      "         [-0.8526, -1.0996,  0.6949,  1.2574]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "\n",
      "Memory = \n",
      "tensor([[[-0.6045, -0.3659, -0.7456,  1.7160],\n",
      "         [-0.3525, -0.5226, -0.8313,  1.7063],\n",
      "         [-0.9077, -0.3700, -0.4154,  1.6931],\n",
      "         [-0.0998, -1.2328, -0.2232,  1.5558]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "\n",
      "query =  tensor([[[-1.1561, -0.8215,  1.1174,  0.8602],\n",
      "         [-1.1617, -0.8118,  1.1353,  0.8382],\n",
      "         [-0.7746, -1.1630,  0.6767,  1.2608],\n",
      "         [-0.8526, -1.0996,  0.6949,  1.2574]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "query shape =  torch.Size([1, 4, 4])\n",
      "\n",
      " _in_projection_packed \n",
      "\n",
      "Q :-  tensor([[[-0.5945, -0.7982,  0.2541, -1.1856],\n",
      "         [-0.5907, -0.8145,  0.2590, -1.1908],\n",
      "         [-0.5886, -0.3429,  0.0768, -0.9543],\n",
      "         [-0.6079, -0.3806,  0.1015, -0.9830]]], grad_fn=<ViewBackward0>)\n",
      "\n",
      "K :-  tensor([[[-0.7237, -0.3619,  1.0531, -0.5075],\n",
      "         [-0.6827, -0.4790,  1.2487, -0.4996],\n",
      "         [-0.5655, -0.3325,  0.8726, -0.2977],\n",
      "         [-0.0128, -0.9119,  1.5721,  0.1687]]], grad_fn=<SelectBackward0>)\n",
      "\n",
      "V :-  tensor([[[-0.9121,  1.2251, -0.1781,  1.1367],\n",
      "         [-1.1346,  1.1204, -0.0801,  1.2016],\n",
      "         [-0.6177,  1.2510, -0.4478,  0.9994],\n",
      "         [-1.2460,  0.6969, -0.4321,  1.0623]]], grad_fn=<SelectBackward0>)\n",
      "\n",
      "tgt_len =  1\n",
      "bsz =  4\n",
      "num_heads =  2\n",
      "head_dim =  2\n",
      "\n",
      "tensor([[[-0.5945, -0.7982,  0.2541, -1.1856],\n",
      "         [-0.5907, -0.8145,  0.2590, -1.1908],\n",
      "         [-0.5886, -0.3429,  0.0768, -0.9543],\n",
      "         [-0.6079, -0.3806,  0.1015, -0.9830]]], grad_fn=<ViewBackward0>) torch.Size([1, 4, 4])\n",
      "Q reshaped =  tensor([[[-0.5945, -0.7982]],\n",
      "\n",
      "        [[ 0.2541, -1.1856]],\n",
      "\n",
      "        [[-0.5907, -0.8145]],\n",
      "\n",
      "        [[ 0.2590, -1.1908]],\n",
      "\n",
      "        [[-0.5886, -0.3429]],\n",
      "\n",
      "        [[ 0.0768, -0.9543]],\n",
      "\n",
      "        [[-0.6079, -0.3806]],\n",
      "\n",
      "        [[ 0.1015, -0.9830]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "K reshaped =  tensor([[[-0.7237, -0.3619]],\n",
      "\n",
      "        [[ 1.0531, -0.5075]],\n",
      "\n",
      "        [[-0.6827, -0.4790]],\n",
      "\n",
      "        [[ 1.2487, -0.4996]],\n",
      "\n",
      "        [[-0.5655, -0.3325]],\n",
      "\n",
      "        [[ 0.8726, -0.2977]],\n",
      "\n",
      "        [[-0.0128, -0.9119]],\n",
      "\n",
      "        [[ 1.5721,  0.1687]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "V reshaped =  tensor([[[-0.9121,  1.2251]],\n",
      "\n",
      "        [[-0.1781,  1.1367]],\n",
      "\n",
      "        [[-1.1346,  1.1204]],\n",
      "\n",
      "        [[-0.0801,  1.2016]],\n",
      "\n",
      "        [[-0.6177,  1.2510]],\n",
      "\n",
      "        [[-0.4478,  0.9994]],\n",
      "\n",
      "        [[-1.2460,  0.6969]],\n",
      "\n",
      "        [[-0.4321,  1.0623]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "torch.Size([4, 2, 1, 2]) torch.Size([4, 2, 1, 2]) torch.Size([4, 2, 1, 2])\n",
      "\n",
      "scaled_dot_product_attention\n",
      "attn_output = \n",
      "tensor([[-0.9121,  1.2251, -0.1781,  1.1367],\n",
      "        [-1.1346,  1.1204, -0.0801,  1.2016],\n",
      "        [-0.6177,  1.2510, -0.4478,  0.9994],\n",
      "        [-1.2460,  0.6969, -0.4321,  1.0623]], grad_fn=<ViewBackward0>)\n",
      "\n",
      "Decoder norm2(x + mha(x, mem)) = \n",
      "tensor([[[ 0.9655, -1.6739,  0.2678,  0.4405],\n",
      "         [ 0.9760, -1.6225,  0.0026,  0.6439],\n",
      "         [ 1.0582, -1.5949, -0.0417,  0.5785],\n",
      "         [ 0.8551, -1.5567, -0.1965,  0.8982]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "\n",
      "Decoder norm3(x + ff(x)) = \n",
      "tensor([[[ 1.0891, -1.6247,  0.3917,  0.1440],\n",
      "         [ 1.1280, -1.6081,  0.1253,  0.3547],\n",
      "         [ 1.2079, -1.5667,  0.0743,  0.2844],\n",
      "         [ 1.0352, -1.5867, -0.0839,  0.6353]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "\n",
      "\n",
      "################### Transformer Decoder start #####################\n",
      "\n",
      "query =  tensor([[[ 1.0891, -1.6247,  0.3917,  0.1440],\n",
      "         [ 1.1280, -1.6081,  0.1253,  0.3547],\n",
      "         [ 1.2079, -1.5667,  0.0743,  0.2844],\n",
      "         [ 1.0352, -1.5867, -0.0839,  0.6353]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "query shape =  torch.Size([1, 4, 4])\n",
      "\n",
      " _in_projection_packed \n",
      "\n",
      "Q :-  tensor([[[ 0.6075, -0.0098,  1.2060,  0.9822],\n",
      "         [ 0.4839,  0.0549,  1.0884,  0.8157],\n",
      "         [ 0.4348,  0.1743,  1.0518,  0.8525],\n",
      "         [ 0.4025, -0.0228,  0.9782,  0.5796]]], grad_fn=<SelectBackward0>)\n",
      "\n",
      "K :-  tensor([[[-0.1669, -0.3601,  0.0291,  0.3061],\n",
      "         [-0.1030, -0.1233,  0.1006,  0.2773],\n",
      "         [-0.1334, -0.0964,  0.1874,  0.2714],\n",
      "         [-0.0004,  0.0849,  0.0663,  0.2464]]], grad_fn=<SelectBackward0>)\n",
      "\n",
      "V :-  tensor([[[ 0.7972, -0.7642, -0.5206, -1.0952],\n",
      "         [ 0.8066, -0.9265, -0.4583, -0.9729],\n",
      "         [ 0.8741, -0.9646, -0.4691, -0.9737],\n",
      "         [ 0.7097, -1.0123, -0.3685, -0.8177]]], grad_fn=<SelectBackward0>)\n",
      "\n",
      "tgt_len =  1\n",
      "bsz =  4\n",
      "num_heads =  2\n",
      "head_dim =  2\n",
      "\n",
      "tensor([[[ 0.6075, -0.0098,  1.2060,  0.9822],\n",
      "         [ 0.4839,  0.0549,  1.0884,  0.8157],\n",
      "         [ 0.4348,  0.1743,  1.0518,  0.8525],\n",
      "         [ 0.4025, -0.0228,  0.9782,  0.5796]]], grad_fn=<SelectBackward0>) torch.Size([1, 4, 4])\n",
      "Q reshaped =  tensor([[[ 0.6075, -0.0098]],\n",
      "\n",
      "        [[ 1.2060,  0.9822]],\n",
      "\n",
      "        [[ 0.4839,  0.0549]],\n",
      "\n",
      "        [[ 1.0884,  0.8157]],\n",
      "\n",
      "        [[ 0.4348,  0.1743]],\n",
      "\n",
      "        [[ 1.0518,  0.8525]],\n",
      "\n",
      "        [[ 0.4025, -0.0228]],\n",
      "\n",
      "        [[ 0.9782,  0.5796]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "K reshaped =  tensor([[[-0.1669, -0.3601]],\n",
      "\n",
      "        [[ 0.0291,  0.3061]],\n",
      "\n",
      "        [[-0.1030, -0.1233]],\n",
      "\n",
      "        [[ 0.1006,  0.2773]],\n",
      "\n",
      "        [[-0.1334, -0.0964]],\n",
      "\n",
      "        [[ 0.1874,  0.2714]],\n",
      "\n",
      "        [[-0.0004,  0.0849]],\n",
      "\n",
      "        [[ 0.0663,  0.2464]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "V reshaped =  tensor([[[ 0.7972, -0.7642]],\n",
      "\n",
      "        [[-0.5206, -1.0952]],\n",
      "\n",
      "        [[ 0.8066, -0.9265]],\n",
      "\n",
      "        [[-0.4583, -0.9729]],\n",
      "\n",
      "        [[ 0.8741, -0.9646]],\n",
      "\n",
      "        [[-0.4691, -0.9737]],\n",
      "\n",
      "        [[ 0.7097, -1.0123]],\n",
      "\n",
      "        [[-0.3685, -0.8177]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "torch.Size([4, 2, 1, 2]) torch.Size([4, 2, 1, 2]) torch.Size([4, 2, 1, 2])\n",
      "\n",
      "scaled_dot_product_attention\n",
      "attn_output = \n",
      "tensor([[ 0.7972, -0.7642, -0.5206, -1.0952],\n",
      "        [ 0.8066, -0.9265, -0.4583, -0.9729],\n",
      "        [ 0.8741, -0.9646, -0.4691, -0.9737],\n",
      "        [ 0.7097, -1.0123, -0.3685, -0.8177]], grad_fn=<ViewBackward0>)\n",
      "\n",
      "x =  tensor([[[ 1.0891, -1.6247,  0.3917,  0.1440],\n",
      "         [ 1.1280, -1.6081,  0.1253,  0.3547],\n",
      "         [ 1.2079, -1.5667,  0.0743,  0.2844],\n",
      "         [ 1.0352, -1.5867, -0.0839,  0.6353]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "\n",
      "sa_op =  tensor([[[ 2.3596e+00,  9.7316e-02,  9.3889e-01,  3.7141e-02],\n",
      "         [ 2.3653e+00,  1.3900e-02,  1.0530e+00,  9.5688e-02],\n",
      "         [ 2.4474e+00, -5.3176e-04,  1.1310e+00,  1.0151e-01],\n",
      "         [ 2.1957e+00, -5.2617e-02,  1.0397e+00,  1.4393e-01]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "\n",
      "temop =  tensor([[[ 3.4486, -1.5274,  1.3305,  0.1811],\n",
      "         [ 3.4933, -1.5942,  1.1783,  0.4504],\n",
      "         [ 3.6554, -1.5672,  1.2053,  0.3859],\n",
      "         [ 3.2309, -1.6393,  0.9558,  0.7793]]], grad_fn=<AddBackward0>)\n",
      "\n",
      "Decoder norm1(x + sa(x)) = \n",
      "tensor([[[ 1.4323, -1.3191,  0.2612, -0.3744],\n",
      "         [ 1.4362, -1.3618,  0.1630, -0.2373],\n",
      "         [ 1.4604, -1.3277,  0.1524, -0.2850],\n",
      "         [ 1.3922, -1.4338,  0.0720, -0.0304]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "\n",
      "Memory = \n",
      "tensor([[[-0.6045, -0.3659, -0.7456,  1.7160],\n",
      "         [-0.3525, -0.5226, -0.8313,  1.7063],\n",
      "         [-0.9077, -0.3700, -0.4154,  1.6931],\n",
      "         [-0.0998, -1.2328, -0.2232,  1.5558]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "\n",
      "query =  tensor([[[ 1.4323, -1.3191,  0.2612, -0.3744],\n",
      "         [ 1.4362, -1.3618,  0.1630, -0.2373],\n",
      "         [ 1.4604, -1.3277,  0.1524, -0.2850],\n",
      "         [ 1.3922, -1.4338,  0.0720, -0.0304]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "query shape =  torch.Size([1, 4, 4])\n",
      "\n",
      " _in_projection_packed \n",
      "\n",
      "Q :-  tensor([[[ 0.3737,  1.1011,  1.0474, -0.2670],\n",
      "         [ 0.4162,  1.0716,  1.0037, -0.2485],\n",
      "         [ 0.4251,  1.0817,  1.0072, -0.3037],\n",
      "         [ 0.4442,  1.0150,  0.9416, -0.1556]]], grad_fn=<ViewBackward0>)\n",
      "\n",
      "K :-  tensor([[[ 0.5865, -1.5370, -0.0593,  0.3249],\n",
      "         [ 0.6022, -1.4020, -0.0278,  0.1153],\n",
      "         [ 0.4098, -1.5959, -0.1676,  0.5825],\n",
      "         [ 0.1369, -0.9322, -0.2051, -0.0747]]], grad_fn=<SelectBackward0>)\n",
      "\n",
      "V :-  tensor([[[ 1.4120, -0.6568, -0.7945, -1.0526],\n",
      "         [ 1.3624, -0.6216, -0.8882, -1.2764],\n",
      "         [ 1.4343, -0.6807, -0.5334, -0.8036],\n",
      "         [ 1.1730, -0.5183, -0.5200, -1.5246]]], grad_fn=<SelectBackward0>)\n",
      "\n",
      "tgt_len =  1\n",
      "bsz =  4\n",
      "num_heads =  2\n",
      "head_dim =  2\n",
      "\n",
      "tensor([[[ 0.3737,  1.1011,  1.0474, -0.2670],\n",
      "         [ 0.4162,  1.0716,  1.0037, -0.2485],\n",
      "         [ 0.4251,  1.0817,  1.0072, -0.3037],\n",
      "         [ 0.4442,  1.0150,  0.9416, -0.1556]]], grad_fn=<ViewBackward0>) torch.Size([1, 4, 4])\n",
      "Q reshaped =  tensor([[[ 0.3737,  1.1011]],\n",
      "\n",
      "        [[ 1.0474, -0.2670]],\n",
      "\n",
      "        [[ 0.4162,  1.0716]],\n",
      "\n",
      "        [[ 1.0037, -0.2485]],\n",
      "\n",
      "        [[ 0.4251,  1.0817]],\n",
      "\n",
      "        [[ 1.0072, -0.3037]],\n",
      "\n",
      "        [[ 0.4442,  1.0150]],\n",
      "\n",
      "        [[ 0.9416, -0.1556]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "K reshaped =  tensor([[[ 0.5865, -1.5370]],\n",
      "\n",
      "        [[-0.0593,  0.3249]],\n",
      "\n",
      "        [[ 0.6022, -1.4020]],\n",
      "\n",
      "        [[-0.0278,  0.1153]],\n",
      "\n",
      "        [[ 0.4098, -1.5959]],\n",
      "\n",
      "        [[-0.1676,  0.5825]],\n",
      "\n",
      "        [[ 0.1369, -0.9322]],\n",
      "\n",
      "        [[-0.2051, -0.0747]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "V reshaped =  tensor([[[ 1.4120, -0.6568]],\n",
      "\n",
      "        [[-0.7945, -1.0526]],\n",
      "\n",
      "        [[ 1.3624, -0.6216]],\n",
      "\n",
      "        [[-0.8882, -1.2764]],\n",
      "\n",
      "        [[ 1.4343, -0.6807]],\n",
      "\n",
      "        [[-0.5334, -0.8036]],\n",
      "\n",
      "        [[ 1.1730, -0.5183]],\n",
      "\n",
      "        [[-0.5200, -1.5246]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "torch.Size([4, 2, 1, 2]) torch.Size([4, 2, 1, 2]) torch.Size([4, 2, 1, 2])\n",
      "\n",
      "scaled_dot_product_attention\n",
      "attn_output = \n",
      "tensor([[ 1.4120, -0.6568, -0.7945, -1.0526],\n",
      "        [ 1.3624, -0.6216, -0.8882, -1.2764],\n",
      "        [ 1.4343, -0.6807, -0.5334, -0.8036],\n",
      "        [ 1.1730, -0.5183, -0.5200, -1.5246]], grad_fn=<ViewBackward0>)\n",
      "\n",
      "Decoder norm2(x + mha(x, mem)) = \n",
      "tensor([[[ 1.6425, -0.1400, -0.4741, -1.0285],\n",
      "         [ 1.6183, -0.0168, -0.5787, -1.0228],\n",
      "         [ 1.7071, -0.3861, -0.4812, -0.8398],\n",
      "         [ 1.6379, -0.0619, -0.5991, -0.9769]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "\n",
      "Decoder norm3(x + ff(x)) = \n",
      "tensor([[[ 1.6675, -0.3092, -0.3619, -0.9964],\n",
      "         [ 1.6573, -0.1863, -0.4741, -0.9969],\n",
      "         [ 1.7130, -0.5573, -0.3694, -0.7864],\n",
      "         [ 1.6747, -0.2326, -0.4961, -0.9461]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "\n",
      "\n",
      "################### Transformer Decoder start #####################\n",
      "\n",
      "query =  tensor([[[ 1.6675, -0.3092, -0.3619, -0.9964],\n",
      "         [ 1.6573, -0.1863, -0.4741, -0.9969],\n",
      "         [ 1.7130, -0.5573, -0.3694, -0.7864],\n",
      "         [ 1.6747, -0.2326, -0.4961, -0.9461]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "query shape =  torch.Size([1, 4, 4])\n",
      "\n",
      " _in_projection_packed \n",
      "\n",
      "Q :-  tensor([[[-0.4564,  0.4040, -0.0769, -0.1547],\n",
      "         [-0.4492,  0.3951, -0.1052, -0.1132],\n",
      "         [-0.4883,  0.5261, -0.0320, -0.0809],\n",
      "         [-0.4584,  0.4274, -0.0984, -0.0877]]], grad_fn=<SelectBackward0>)\n",
      "\n",
      "K :-  tensor([[[-0.1585,  1.1721,  0.5348, -0.1444],\n",
      "         [-0.0538,  1.1324,  0.4825, -0.1531],\n",
      "         [-0.3281,  1.0430,  0.7170, -0.1352],\n",
      "         [-0.0818,  1.0969,  0.5240, -0.1527]]], grad_fn=<SelectBackward0>)\n",
      "\n",
      "V :-  tensor([[[-0.6207,  0.2833,  1.2686,  0.4266],\n",
      "         [-0.6454,  0.3579,  1.2686,  0.4896],\n",
      "         [-0.6633,  0.1246,  1.0597,  0.2788],\n",
      "         [-0.6633,  0.3297,  1.2191,  0.4629]]], grad_fn=<SelectBackward0>)\n",
      "\n",
      "tgt_len =  1\n",
      "bsz =  4\n",
      "num_heads =  2\n",
      "head_dim =  2\n",
      "\n",
      "tensor([[[-0.4564,  0.4040, -0.0769, -0.1547],\n",
      "         [-0.4492,  0.3951, -0.1052, -0.1132],\n",
      "         [-0.4883,  0.5261, -0.0320, -0.0809],\n",
      "         [-0.4584,  0.4274, -0.0984, -0.0877]]], grad_fn=<SelectBackward0>) torch.Size([1, 4, 4])\n",
      "Q reshaped =  tensor([[[-0.4564,  0.4040]],\n",
      "\n",
      "        [[-0.0769, -0.1547]],\n",
      "\n",
      "        [[-0.4492,  0.3951]],\n",
      "\n",
      "        [[-0.1052, -0.1132]],\n",
      "\n",
      "        [[-0.4883,  0.5261]],\n",
      "\n",
      "        [[-0.0320, -0.0809]],\n",
      "\n",
      "        [[-0.4584,  0.4274]],\n",
      "\n",
      "        [[-0.0984, -0.0877]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "K reshaped =  tensor([[[-0.1585,  1.1721]],\n",
      "\n",
      "        [[ 0.5348, -0.1444]],\n",
      "\n",
      "        [[-0.0538,  1.1324]],\n",
      "\n",
      "        [[ 0.4825, -0.1531]],\n",
      "\n",
      "        [[-0.3281,  1.0430]],\n",
      "\n",
      "        [[ 0.7170, -0.1352]],\n",
      "\n",
      "        [[-0.0818,  1.0969]],\n",
      "\n",
      "        [[ 0.5240, -0.1527]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "V reshaped =  tensor([[[-0.6207,  0.2833]],\n",
      "\n",
      "        [[ 1.2686,  0.4266]],\n",
      "\n",
      "        [[-0.6454,  0.3579]],\n",
      "\n",
      "        [[ 1.2686,  0.4896]],\n",
      "\n",
      "        [[-0.6633,  0.1246]],\n",
      "\n",
      "        [[ 1.0597,  0.2788]],\n",
      "\n",
      "        [[-0.6633,  0.3297]],\n",
      "\n",
      "        [[ 1.2191,  0.4629]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "torch.Size([4, 2, 1, 2]) torch.Size([4, 2, 1, 2]) torch.Size([4, 2, 1, 2])\n",
      "\n",
      "scaled_dot_product_attention\n",
      "attn_output = \n",
      "tensor([[-0.6207,  0.2833,  1.2686,  0.4266],\n",
      "        [-0.6454,  0.3579,  1.2686,  0.4896],\n",
      "        [-0.6633,  0.1246,  1.0597,  0.2788],\n",
      "        [-0.6633,  0.3297,  1.2191,  0.4629]], grad_fn=<ViewBackward0>)\n",
      "\n",
      "x =  tensor([[[ 1.6675, -0.3092, -0.3619, -0.9964],\n",
      "         [ 1.6573, -0.1863, -0.4741, -0.9969],\n",
      "         [ 1.7130, -0.5573, -0.3694, -0.7864],\n",
      "         [ 1.6747, -0.2326, -0.4961, -0.9461]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "\n",
      "sa_op =  tensor([[[-0.8042,  0.0264, -0.7331,  0.3777],\n",
      "         [-0.8095,  0.1128, -0.7847,  0.4764],\n",
      "         [-0.7536, -0.0026, -0.5104,  0.3038],\n",
      "         [-0.8005,  0.1196, -0.7378,  0.4749]]], grad_fn=<ViewBackward0>)\n",
      "\n",
      "temop =  tensor([[[ 0.8633, -0.2827, -1.0950, -0.6187],\n",
      "         [ 0.8478, -0.0734, -1.2588, -0.5205],\n",
      "         [ 0.9595, -0.5599, -0.8797, -0.4826],\n",
      "         [ 0.8743, -0.1130, -1.2339, -0.4711]]], grad_fn=<AddBackward0>)\n",
      "\n",
      "Decoder norm1(x + sa(x)) = \n",
      "tensor([[[ 1.5877e+00,  8.0138e-04, -1.1240e+00, -4.6449e-01],\n",
      "         [ 1.4409e+00,  2.3309e-01, -1.3209e+00, -3.5301e-01],\n",
      "         [ 1.6934e+00, -4.5037e-01, -9.0165e-01, -3.4137e-01],\n",
      "         [ 1.4644e+00,  1.6219e-01, -1.3164e+00, -3.1023e-01]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "\n",
      "Memory = \n",
      "tensor([[[-0.6045, -0.3659, -0.7456,  1.7160],\n",
      "         [-0.3525, -0.5226, -0.8313,  1.7063],\n",
      "         [-0.9077, -0.3700, -0.4154,  1.6931],\n",
      "         [-0.0998, -1.2328, -0.2232,  1.5558]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "\n",
      "query =  tensor([[[ 1.5877e+00,  8.0138e-04, -1.1240e+00, -4.6449e-01],\n",
      "         [ 1.4409e+00,  2.3309e-01, -1.3209e+00, -3.5301e-01],\n",
      "         [ 1.6934e+00, -4.5037e-01, -9.0165e-01, -3.4137e-01],\n",
      "         [ 1.4644e+00,  1.6219e-01, -1.3164e+00, -3.1023e-01]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "query shape =  torch.Size([1, 4, 4])\n",
      "\n",
      " _in_projection_packed \n",
      "\n",
      "Q :-  tensor([[[ 0.1865, -0.5781,  0.6103, -0.1993],\n",
      "         [ 0.1109, -0.3085,  0.5972, -0.3766],\n",
      "         [ 0.3719, -0.8744,  0.7502,  0.1472],\n",
      "         [ 0.1462, -0.3432,  0.6381, -0.3249]]], grad_fn=<ViewBackward0>)\n",
      "\n",
      "K :-  tensor([[[-0.9461, -0.4201,  0.6541, -0.5913],\n",
      "         [-0.9505, -0.4142,  0.5800, -0.6170],\n",
      "         [-0.7898, -0.4899,  0.6221, -0.3727],\n",
      "         [-0.4857, -0.5845,  0.1362, -0.0720]]], grad_fn=<SelectBackward0>)\n",
      "\n",
      "V :-  tensor([[[-0.1976,  0.1209, -0.5734, -1.4607],\n",
      "         [-0.1951,  0.1337, -0.7044, -1.4275],\n",
      "         [-0.2685,  0.0153, -0.3249, -1.3999],\n",
      "         [-0.3893, -0.1243, -0.5464, -1.0646]]], grad_fn=<SelectBackward0>)\n",
      "\n",
      "tgt_len =  1\n",
      "bsz =  4\n",
      "num_heads =  2\n",
      "head_dim =  2\n",
      "\n",
      "tensor([[[ 0.1865, -0.5781,  0.6103, -0.1993],\n",
      "         [ 0.1109, -0.3085,  0.5972, -0.3766],\n",
      "         [ 0.3719, -0.8744,  0.7502,  0.1472],\n",
      "         [ 0.1462, -0.3432,  0.6381, -0.3249]]], grad_fn=<ViewBackward0>) torch.Size([1, 4, 4])\n",
      "Q reshaped =  tensor([[[ 0.1865, -0.5781]],\n",
      "\n",
      "        [[ 0.6103, -0.1993]],\n",
      "\n",
      "        [[ 0.1109, -0.3085]],\n",
      "\n",
      "        [[ 0.5972, -0.3766]],\n",
      "\n",
      "        [[ 0.3719, -0.8744]],\n",
      "\n",
      "        [[ 0.7502,  0.1472]],\n",
      "\n",
      "        [[ 0.1462, -0.3432]],\n",
      "\n",
      "        [[ 0.6381, -0.3249]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "K reshaped =  tensor([[[-0.9461, -0.4201]],\n",
      "\n",
      "        [[ 0.6541, -0.5913]],\n",
      "\n",
      "        [[-0.9505, -0.4142]],\n",
      "\n",
      "        [[ 0.5800, -0.6170]],\n",
      "\n",
      "        [[-0.7898, -0.4899]],\n",
      "\n",
      "        [[ 0.6221, -0.3727]],\n",
      "\n",
      "        [[-0.4857, -0.5845]],\n",
      "\n",
      "        [[ 0.1362, -0.0720]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "V reshaped =  tensor([[[-0.1976,  0.1209]],\n",
      "\n",
      "        [[-0.5734, -1.4607]],\n",
      "\n",
      "        [[-0.1951,  0.1337]],\n",
      "\n",
      "        [[-0.7044, -1.4275]],\n",
      "\n",
      "        [[-0.2685,  0.0153]],\n",
      "\n",
      "        [[-0.3249, -1.3999]],\n",
      "\n",
      "        [[-0.3893, -0.1243]],\n",
      "\n",
      "        [[-0.5464, -1.0646]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "torch.Size([4, 2, 1, 2]) torch.Size([4, 2, 1, 2]) torch.Size([4, 2, 1, 2])\n",
      "\n",
      "scaled_dot_product_attention\n",
      "attn_output = \n",
      "tensor([[-0.1976,  0.1209, -0.5734, -1.4607],\n",
      "        [-0.1951,  0.1337, -0.7044, -1.4275],\n",
      "        [-0.2685,  0.0153, -0.3249, -1.3999],\n",
      "        [-0.3893, -0.1243, -0.5464, -1.0646]], grad_fn=<ViewBackward0>)\n",
      "\n",
      "Decoder norm2(x + mha(x, mem)) = \n",
      "tensor([[[ 1.5265, -0.1745, -1.2783, -0.0736],\n",
      "         [ 1.4271, -0.0056, -1.4010, -0.0205],\n",
      "         [ 1.6042, -0.4852, -1.0910, -0.0281],\n",
      "         [ 1.5221, -0.0578, -1.2834, -0.1809]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "\n",
      "Decoder norm3(x + ff(x)) = \n",
      "tensor([[[ 1.5900, -0.2046, -1.1776, -0.2079],\n",
      "         [ 1.4772,  0.0127, -1.3398, -0.1502],\n",
      "         [ 1.6628, -0.5940, -0.9286, -0.1402],\n",
      "         [ 1.5767, -0.0540, -1.1808, -0.3419]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "\n",
      "tensor([[[-0.1736, -0.6635, -0.7838, -0.4535, -1.0496, -0.8957,  0.4070,\n",
      "           0.4802,  0.3982, -0.3234],\n",
      "         [-0.1878, -0.6338, -0.7410, -0.4279, -1.0766, -0.9809,  0.3347,\n",
      "           0.5342,  0.4060, -0.4664],\n",
      "         [-0.1235, -0.6708, -0.8959, -0.4473, -0.9361, -0.7919,  0.5501,\n",
      "           0.3801,  0.3818, -0.0212],\n",
      "         [-0.1777, -0.6718, -0.6719, -0.4529, -1.0911, -0.8548,  0.3259,\n",
      "           0.5181,  0.4016, -0.4310]]], grad_fn=<ViewBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sreevaatsav/.pyenv/versions/project_env/lib/python3.10/site-packages/torch/nn/modules/transformer.py:282: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=512, dropout=0):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.encoding = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1).float()\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(torch.log(torch.tensor(10000.0)) / d_model))\n",
    "        self.encoding[:, 0::2] = torch.sin(position * div_term)\n",
    "        self.encoding[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.encoding = self.encoding.unsqueeze(0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.encoding[:, :x.size(1)].detach()\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, src_vocab_size, tgt_vocab_size, d_model=4, nhead=2, num_encoder_layers=1, num_decoder_layers=1):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.src_embedding = nn.Embedding(src_vocab_size, d_model)\n",
    "        self.tgt_embedding = nn.Embedding(tgt_vocab_size, d_model)\n",
    "        self.positional_encoding = PositionalEncoding(d_model, dropout=0)\n",
    "        self.transformer = nn.Transformer(\n",
    "            d_model=d_model,\n",
    "            nhead=nhead,\n",
    "            num_encoder_layers=num_encoder_layers,\n",
    "            num_decoder_layers=num_decoder_layers,\n",
    "            dropout=0\n",
    "        )\n",
    "        self.fc = nn.Linear(d_model, tgt_vocab_size)\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "\n",
    "        print(\"Source embeddings :- \\n\")\n",
    "        print(self.src_embedding(src))\n",
    "        print()\n",
    "\n",
    "        print(\"Target embeddings :- \\n\")\n",
    "        print(self.tgt_embedding(tgt))\n",
    "        print()\n",
    "\n",
    "\n",
    "        src = self.src_embedding(src) + self.positional_encoding(src)\n",
    "        tgt = self.tgt_embedding(tgt) + self.positional_encoding(tgt)\n",
    "\n",
    "        print(\"Positional encoded source embeddings:\")\n",
    "        print(src)\n",
    "        print()\n",
    "\n",
    "        print(\"Positional encoded target embeddings:\")\n",
    "        print(tgt)\n",
    "        print()\n",
    "\n",
    "        output = self.transformer(src, tgt)\n",
    "        output = self.fc(output)\n",
    "        return output\n",
    "\n",
    "    \n",
    "\n",
    "src_vocab_size = 10  # Source language vocabulary size\n",
    "tgt_vocab_size = 10  # Target language vocabulary size\n",
    "d_model = 4  # Dimension of the model\n",
    "num_heads = 2\n",
    "num_encoder_layers = 3\n",
    "num_decoder_layers = 3\n",
    "\n",
    "need_weights = False\n",
    "\n",
    "src_mask = None\n",
    "tgt_mask = None\n",
    "memory_mask = None\n",
    "\n",
    "\n",
    "model = TransformerModel(src_vocab_size, tgt_vocab_size, d_model=d_model, nhead=num_heads, num_encoder_layers = num_encoder_layers, num_decoder_layers=num_decoder_layers)\n",
    "# Source sentence in the source language\n",
    "src_sentence = torch.tensor([[0, 1, 2, 3]])  # Source sequence\n",
    "\n",
    "# Target sentence in the target language (translation of the source sentence)\n",
    "tgt_sentence = torch.tensor([[1, 0, 3, 3]])  # Target sequence\n",
    "\n",
    "# Forward pass\n",
    "output = model(src_sentence, tgt_sentence)\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_intermediate_outputs(src_sentence, tgt_sentence,d_model, model, num_encoder_layers , num_decoder_layers):\n",
    "\n",
    "    state_dict = model.state_dict()\n",
    "\n",
    "    pe_src_embeds, pe_tgt_embeds = get_embedding_outputs(src_sentence,  tgt_sentence, state_dict, d_model = d_model)\n",
    "\n",
    "    print(\"###\"*25)\n",
    "    print(\"### Encoder Start ###\")\n",
    "    print()\n",
    "\n",
    "    x_enc = pe_src_embeds\n",
    "\n",
    "    for lno in range(num_encoder_layers):\n",
    "        attn_enc_output, src_len, head_dim, attn_weights = encoder_block_attn_output(x_enc, state_dict, layer_num = lno, need_weights = False, embed_dim = d_model, num_heads= num_heads, src_mask=None)\n",
    "\n",
    "        if lno == 0:\n",
    "            tgt_len, bsz, embed_dim = x_enc.shape\n",
    "\n",
    "        output_enc_final = encoder_block_post_attn_output(x_enc, attn_enc_output, state_dict, layer_num = lno , bsz = bsz, tgt_len = tgt_len)\n",
    "\n",
    "        x_enc = output_enc_final\n",
    "\n",
    "    \n",
    "    print(\"### Encoder Done ###\")\n",
    "    print(\"\\n\\n\\n\")\n",
    "    print(\"### Decoder Start ###\")\n",
    "\n",
    "    x_dec = pe_tgt_embeds\n",
    "    memory = x_enc\n",
    "\n",
    "    for lno in range(num_decoder_layers):\n",
    "\n",
    "        self_attn_dec, dec_sa_wts = decoder_block_self_attn_output(x_dec, state_dict, layer_num = lno, need_weights = False, tgt_mask=None)\n",
    "        x_dec = dec_post_self_attn(self_attn_dec, x_dec, state_dict, layer_num = lno)\n",
    "\n",
    "        attn_dec_mha_output, attn_dec_mha_wts = decoder_block_cross_attn_output(x_dec, memory, state_dict, layer_num = lno, tgt_len = tgt_len, src_len = src_len, head_dim = head_dim, need_weights = False, memory_mask=None)\n",
    "        final_op = decoder_block_post_attn_output(x_dec, attn_dec_mha_output, state_dict, layer_num = lno)\n",
    "\n",
    "        print(pe_tgt_embeds.shape, final_op.shape)\n",
    "        x_dec = final_op\n",
    "\n",
    "\n",
    "    final_op = feef_fwd_transformer(final_op, state_dict)\n",
    "\n",
    "    print(\"### Decoder Done ###\")\n",
    "\n",
    "\n",
    "    return final_op\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source sentence embedding\n",
      "Word index: 0, Embedding: tensor([-0.2205,  1.7253, -0.2514,  0.9972])\n",
      "Word index: 1, Embedding: tensor([ 0.1335, -0.5650, -0.7537,  0.0105])\n",
      "Word index: 2, Embedding: tensor([-0.8226,  0.9250,  1.7916, -0.4149])\n",
      "Word index: 3, Embedding: tensor([ 0.2400, -2.0239, -0.6187, -0.7110])\n",
      "\n",
      "Target sentence embedding\n",
      "Word index: 1, Embedding: tensor([ 0.0958, -0.3552,  1.2360, -0.8826])\n",
      "Word index: 0, Embedding: tensor([ 0.0204,  1.1688, -0.0480, -1.3756])\n",
      "Word index: 3, Embedding: tensor([-0.1728,  0.3034, -1.1816, -1.1548])\n",
      "Word index: 3, Embedding: tensor([-0.1728,  0.3034, -1.1816, -1.1548])\n",
      "\n",
      "PE source embeddings : \n",
      "\n",
      "tensor([[[-0.2205,  3.7253,  1.7486,  4.9972],\n",
      "         [ 0.9750,  0.9753,  1.2563,  4.0105],\n",
      "         [ 0.0867,  1.5088,  3.8116,  3.5849],\n",
      "         [ 0.3811, -2.0139,  1.4113,  3.2885]]])\n",
      "\n",
      "PE target embeddings : \n",
      "\n",
      "tensor([[[ 1.0958,  0.6448,  4.2360,  3.1174],\n",
      "         [ 1.8619,  1.7091,  2.9620,  2.6244],\n",
      "         [ 1.7365, -0.1128,  1.8384,  2.8450],\n",
      "         [ 0.9683, -0.6866,  1.8484,  2.8448]]])\n",
      "\n",
      "###########################################################################\n",
      "### Encoder Start ###\n",
      "\n",
      "Q_enc_0 = \n",
      "tensor([[[ 0.6200, -0.9966]],\n",
      "\n",
      "        [[-0.1113,  1.0026]],\n",
      "\n",
      "        [[-0.1846, -0.6229]],\n",
      "\n",
      "        [[ 0.4113, -0.8594]],\n",
      "\n",
      "        [[-1.4594, -0.9644]],\n",
      "\n",
      "        [[ 0.6233, -0.4554]],\n",
      "\n",
      "        [[-1.3783, -0.2546]],\n",
      "\n",
      "        [[ 1.3028, -2.3128]]])\n",
      "\n",
      "K_enc_0 = \n",
      "tensor([[[ 1.9172, -1.6883]],\n",
      "\n",
      "        [[ 0.3543,  3.3778]],\n",
      "\n",
      "        [[ 1.4162, -1.2422]],\n",
      "\n",
      "        [[ 0.8147,  1.4920]],\n",
      "\n",
      "        [[ 0.0826, -1.0527]],\n",
      "\n",
      "        [[-0.6941,  2.5530]],\n",
      "\n",
      "        [[ 0.7099, -1.9596]],\n",
      "\n",
      "        [[ 0.4214,  0.1708]]])\n",
      "\n",
      "V_enc_0 = \n",
      "tensor([[[ 3.0345, -4.3447]],\n",
      "\n",
      "        [[ 4.4862, -0.7491]],\n",
      "\n",
      "        [[ 2.3662, -3.3168]],\n",
      "\n",
      "        [[ 2.7636, -0.3210]],\n",
      "\n",
      "        [[ 3.1573, -4.3181]],\n",
      "\n",
      "        [[ 2.9714,  0.0482]],\n",
      "\n",
      "        [[ 2.5920, -2.3741]],\n",
      "\n",
      "        [[ 0.7878,  0.8075]]])\n",
      "\n",
      "Encoder Attention output = \n",
      "tensor([[ 3.0345, -4.3447,  4.4862, -0.7491],\n",
      "        [ 2.3662, -3.3168,  2.7636, -0.3210],\n",
      "        [ 3.1573, -4.3181,  2.9714,  0.0482],\n",
      "        [ 2.5920, -2.3741,  0.7878,  0.8075]])\n",
      "\n",
      "Final Encoder 0 Output :\n",
      "norm2(norm1(x + self_atten(x)) + feed_fwd_op)\n",
      "\n",
      "tensor([[[-0.9361,  1.1250, -1.0537,  0.8648],\n",
      "         [-0.7420,  0.7963, -1.2133,  1.1590],\n",
      "         [-1.2530,  0.9626, -0.7095,  1.0000],\n",
      "         [-0.6221, -0.6651, -0.4386,  1.7258]]], grad_fn=<AddBackward0>)\n",
      "\n",
      "Q_enc_1 = \n",
      "tensor([[[ 0.3627,  0.2891]],\n",
      "\n",
      "        [[ 0.1520, -1.9940]],\n",
      "\n",
      "        [[ 0.3101, -0.0514]],\n",
      "\n",
      "        [[-0.1626, -1.9222]],\n",
      "\n",
      "        [[ 0.6701,  0.3549]],\n",
      "\n",
      "        [[ 0.3880, -1.8755]],\n",
      "\n",
      "        [[ 0.6906, -0.7477]],\n",
      "\n",
      "        [[-0.4020, -0.7598]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "K_enc_1 = \n",
      "tensor([[[ 0.4935, -0.7928]],\n",
      "\n",
      "        [[-0.7312,  1.2474]],\n",
      "\n",
      "        [[ 0.4188, -0.6940]],\n",
      "\n",
      "        [[-0.7778,  1.1314]],\n",
      "\n",
      "        [[ 0.5690, -1.0316]],\n",
      "\n",
      "        [[-0.6252,  1.4893]],\n",
      "\n",
      "        [[ 0.2258, -0.6703]],\n",
      "\n",
      "        [[-0.3773,  0.9025]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "V_enc_1 = \n",
      "tensor([[[ 0.4030, -0.8576]],\n",
      "\n",
      "        [[-1.5051, -0.5695]],\n",
      "\n",
      "        [[ 0.4024, -0.5434]],\n",
      "\n",
      "        [[-1.3514, -0.2802]],\n",
      "\n",
      "        [[ 0.4675, -0.8433]],\n",
      "\n",
      "        [[-1.5063, -0.5631]],\n",
      "\n",
      "        [[ 0.3788,  0.4803]],\n",
      "\n",
      "        [[-0.4493,  0.5641]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "Encoder Attention output = \n",
      "tensor([[ 0.4030, -0.8576, -1.5051, -0.5695],\n",
      "        [ 0.4024, -0.5434, -1.3514, -0.2802],\n",
      "        [ 0.4675, -0.8433, -1.5063, -0.5631],\n",
      "        [ 0.3788,  0.4803, -0.4493,  0.5641]], grad_fn=<ViewBackward0>)\n",
      "\n",
      "Final Encoder 1 Output :\n",
      "norm2(norm1(x + self_atten(x)) + feed_fwd_op)\n",
      "\n",
      "tensor([[[-0.5963, -0.4956, -0.6378,  1.7297],\n",
      "         [-0.3472, -0.6382, -0.7293,  1.7147],\n",
      "         [-0.7637, -0.6084, -0.3400,  1.7120],\n",
      "         [ 0.0533, -1.4090, -0.0613,  1.4170]]], grad_fn=<AddBackward0>)\n",
      "\n",
      "Q_enc_2 = \n",
      "tensor([[[ 0.0405,  0.5774]],\n",
      "\n",
      "        [[-0.2596,  0.5393]],\n",
      "\n",
      "        [[ 0.1239,  0.6862]],\n",
      "\n",
      "        [[-0.3060,  0.4950]],\n",
      "\n",
      "        [[ 0.1620,  0.5662]],\n",
      "\n",
      "        [[-0.1612,  0.4365]],\n",
      "\n",
      "        [[ 0.7716,  0.9559]],\n",
      "\n",
      "        [[-0.1349, -0.0280]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "K_enc_2 = \n",
      "tensor([[[ 0.0710,  0.5148]],\n",
      "\n",
      "        [[-1.2144,  0.3167]],\n",
      "\n",
      "        [[ 0.1238,  0.3417]],\n",
      "\n",
      "        [[-1.1581,  0.3155]],\n",
      "\n",
      "        [[ 0.0116,  0.5938]],\n",
      "\n",
      "        [[-1.1497,  0.1432]],\n",
      "\n",
      "        [[ 0.1141, -0.0613]],\n",
      "\n",
      "        [[-0.6354, -0.2518]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "V_enc_2 = \n",
      "tensor([[[0.8298, 0.5688]],\n",
      "\n",
      "        [[0.2635, 0.6075]],\n",
      "\n",
      "        [[0.8487, 0.6739]],\n",
      "\n",
      "        [[0.1504, 0.7465]],\n",
      "\n",
      "        [[0.9210, 0.4544]],\n",
      "\n",
      "        [[0.4513, 0.4206]],\n",
      "\n",
      "        [[1.0978, 0.6180]],\n",
      "\n",
      "        [[0.3093, 0.5803]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "Encoder Attention output = \n",
      "tensor([[0.8298, 0.5688, 0.2635, 0.6075],\n",
      "        [0.8487, 0.6739, 0.1504, 0.7465],\n",
      "        [0.9210, 0.4544, 0.4513, 0.4206],\n",
      "        [1.0978, 0.6180, 0.3093, 0.5803]], grad_fn=<ViewBackward0>)\n",
      "\n",
      "Final Encoder 2 Output :\n",
      "norm2(norm1(x + self_atten(x)) + feed_fwd_op)\n",
      "\n",
      "tensor([[[-0.6045, -0.3659, -0.7456,  1.7160],\n",
      "         [-0.3525, -0.5226, -0.8313,  1.7063],\n",
      "         [-0.9077, -0.3700, -0.4154,  1.6931],\n",
      "         [-0.0998, -1.2328, -0.2232,  1.5558]]], grad_fn=<AddBackward0>)\n",
      "\n",
      "### Encoder Done ###\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "### Decoder Start ###\n",
      "Q_dec_0 = \n",
      "tensor([[[-3.1998,  2.1538]],\n",
      "\n",
      "        [[-1.0128,  0.7383]],\n",
      "\n",
      "        [[-2.6388,  1.2949]],\n",
      "\n",
      "        [[-0.3135,  0.9396]],\n",
      "\n",
      "        [[-2.5799,  0.7808]],\n",
      "\n",
      "        [[-0.3913, -0.4072]],\n",
      "\n",
      "        [[-2.3275,  1.0272]],\n",
      "\n",
      "        [[-0.7647, -0.5439]]])\n",
      "\n",
      "K_dec_0 = \n",
      "tensor([[[ 1.0097,  1.4010]],\n",
      "\n",
      "        [[ 0.8216, -3.9416]],\n",
      "\n",
      "        [[ 1.1088,  2.1519]],\n",
      "\n",
      "        [[ 0.0169, -3.9404]],\n",
      "\n",
      "        [[ 0.6041,  1.1918]],\n",
      "\n",
      "        [[-0.2197, -2.3607]],\n",
      "\n",
      "        [[ 0.5968,  0.5446]],\n",
      "\n",
      "        [[ 0.1456, -1.7790]]])\n",
      "\n",
      "V_dec_0 = \n",
      "tensor([[[-2.4218,  1.2453]],\n",
      "\n",
      "        [[ 0.6700,  1.9331]],\n",
      "\n",
      "        [[-1.5839,  0.7873]],\n",
      "\n",
      "        [[ 0.8833,  1.9419]],\n",
      "\n",
      "        [[-2.1300,  0.6044]],\n",
      "\n",
      "        [[ 0.7146,  0.7506]],\n",
      "\n",
      "        [[-2.1505,  0.7811]],\n",
      "\n",
      "        [[ 0.6343,  0.5329]]])\n",
      "\n",
      "Decoder Self Attention = \n",
      "tensor([[-2.4218,  1.2453,  0.6700,  1.9331],\n",
      "        [-1.5839,  0.7873,  0.8833,  1.9419],\n",
      "        [-2.1300,  0.6044,  0.7146,  0.7506],\n",
      "        [-2.1505,  0.7811,  0.6343,  0.5329]])\n",
      "\n",
      "Decoder_0 norm1(x + sa(x))\n",
      "tensor([[[-1.1561, -0.8215,  1.1174,  0.8602],\n",
      "         [-1.1617, -0.8118,  1.1353,  0.8382],\n",
      "         [-0.7746, -1.1629,  0.6767,  1.2608],\n",
      "         [-0.8526, -1.0996,  0.6949,  1.2574]]], grad_fn=<AddBackward0>)\n",
      "\n",
      "Q_dec_0 = \n",
      "tensor([[[-0.5945, -0.7982]],\n",
      "\n",
      "        [[ 0.2541, -1.1856]],\n",
      "\n",
      "        [[-0.5907, -0.8145]],\n",
      "\n",
      "        [[ 0.2590, -1.1908]],\n",
      "\n",
      "        [[-0.5886, -0.3429]],\n",
      "\n",
      "        [[ 0.0768, -0.9543]],\n",
      "\n",
      "        [[-0.6079, -0.3806]],\n",
      "\n",
      "        [[ 0.1015, -0.9830]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "K_dec_0 = \n",
      "tensor([[[-0.7237, -0.3619]],\n",
      "\n",
      "        [[ 1.0531, -0.5075]],\n",
      "\n",
      "        [[-0.6827, -0.4790]],\n",
      "\n",
      "        [[ 1.2487, -0.4995]],\n",
      "\n",
      "        [[-0.5655, -0.3325]],\n",
      "\n",
      "        [[ 0.8726, -0.2977]],\n",
      "\n",
      "        [[-0.0128, -0.9119]],\n",
      "\n",
      "        [[ 1.5721,  0.1687]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "V_dec_0 = \n",
      "tensor([[[-0.9120,  1.2251]],\n",
      "\n",
      "        [[-0.1781,  1.1367]],\n",
      "\n",
      "        [[-1.1346,  1.1204]],\n",
      "\n",
      "        [[-0.0801,  1.2016]],\n",
      "\n",
      "        [[-0.6177,  1.2510]],\n",
      "\n",
      "        [[-0.4478,  0.9994]],\n",
      "\n",
      "        [[-1.2460,  0.6969]],\n",
      "\n",
      "        [[-0.4321,  1.0623]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "Cross attention in decoder_0\n",
      "tensor([[-0.9120,  1.2251, -0.1781,  1.1367],\n",
      "        [-1.1346,  1.1204, -0.0801,  1.2016],\n",
      "        [-0.6177,  1.2510, -0.4478,  0.9994],\n",
      "        [-1.2460,  0.6969, -0.4321,  1.0623]], grad_fn=<ViewBackward0>)\n",
      "\n",
      "norm2(x' + mha(x', mem)), \n",
      " where x' = Decoder_curr_layer norm1(x + sa(x))\n",
      "tensor([[[ 0.9655, -1.6739,  0.2678,  0.4405],\n",
      "         [ 0.9760, -1.6225,  0.0026,  0.6439],\n",
      "         [ 1.0582, -1.5949, -0.0417,  0.5785],\n",
      "         [ 0.8551, -1.5567, -0.1965,  0.8982]]], grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "\n",
      "norm3(x'' + ff(x'')) \n",
      " where, x'' = Decoder_curr_layer norm2(x' + mha(x'))\n",
      "tensor([[[ 1.0891, -1.6247,  0.3917,  0.1440],\n",
      "         [ 1.1280, -1.6081,  0.1253,  0.3547],\n",
      "         [ 1.2079, -1.5667,  0.0743,  0.2844],\n",
      "         [ 1.0352, -1.5867, -0.0839,  0.6353]]], grad_fn=<AddBackward0>)\n",
      "\n",
      "torch.Size([1, 4, 4]) torch.Size([1, 4, 4])\n",
      "Q_dec_1 = \n",
      "tensor([[[ 0.6075, -0.0099]],\n",
      "\n",
      "        [[ 1.2060,  0.9822]],\n",
      "\n",
      "        [[ 0.4839,  0.0549]],\n",
      "\n",
      "        [[ 1.0884,  0.8157]],\n",
      "\n",
      "        [[ 0.4348,  0.1743]],\n",
      "\n",
      "        [[ 1.0518,  0.8525]],\n",
      "\n",
      "        [[ 0.4025, -0.0228]],\n",
      "\n",
      "        [[ 0.9782,  0.5796]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "K_dec_1 = \n",
      "tensor([[[-0.1669, -0.3601]],\n",
      "\n",
      "        [[ 0.0291,  0.3061]],\n",
      "\n",
      "        [[-0.1030, -0.1233]],\n",
      "\n",
      "        [[ 0.1006,  0.2773]],\n",
      "\n",
      "        [[-0.1334, -0.0964]],\n",
      "\n",
      "        [[ 0.1874,  0.2714]],\n",
      "\n",
      "        [[-0.0004,  0.0849]],\n",
      "\n",
      "        [[ 0.0663,  0.2464]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "V_dec_1 = \n",
      "tensor([[[ 0.7971, -0.7642]],\n",
      "\n",
      "        [[-0.5205, -1.0952]],\n",
      "\n",
      "        [[ 0.8066, -0.9265]],\n",
      "\n",
      "        [[-0.4583, -0.9728]],\n",
      "\n",
      "        [[ 0.8741, -0.9646]],\n",
      "\n",
      "        [[-0.4691, -0.9737]],\n",
      "\n",
      "        [[ 0.7096, -1.0123]],\n",
      "\n",
      "        [[-0.3685, -0.8177]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "Decoder Self Attention = \n",
      "tensor([[ 0.7971, -0.7642, -0.5205, -1.0952],\n",
      "        [ 0.8066, -0.9265, -0.4583, -0.9728],\n",
      "        [ 0.8741, -0.9646, -0.4691, -0.9737],\n",
      "        [ 0.7096, -1.0123, -0.3685, -0.8177]], grad_fn=<ViewBackward0>)\n",
      "\n",
      "Decoder_1 norm1(x + sa(x))\n",
      "tensor([[[ 1.4323, -1.3191,  0.2612, -0.3744],\n",
      "         [ 1.4362, -1.3618,  0.1630, -0.2373],\n",
      "         [ 1.4604, -1.3277,  0.1524, -0.2850],\n",
      "         [ 1.3922, -1.4338,  0.0720, -0.0304]]], grad_fn=<AddBackward0>)\n",
      "\n",
      "Q_dec_1 = \n",
      "tensor([[[ 0.3737,  1.1011]],\n",
      "\n",
      "        [[ 1.0474, -0.2670]],\n",
      "\n",
      "        [[ 0.4162,  1.0716]],\n",
      "\n",
      "        [[ 1.0037, -0.2485]],\n",
      "\n",
      "        [[ 0.4251,  1.0817]],\n",
      "\n",
      "        [[ 1.0072, -0.3037]],\n",
      "\n",
      "        [[ 0.4442,  1.0150]],\n",
      "\n",
      "        [[ 0.9416, -0.1556]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "K_dec_1 = \n",
      "tensor([[[ 0.5865, -1.5370]],\n",
      "\n",
      "        [[-0.0593,  0.3249]],\n",
      "\n",
      "        [[ 0.6022, -1.4020]],\n",
      "\n",
      "        [[-0.0278,  0.1153]],\n",
      "\n",
      "        [[ 0.4098, -1.5959]],\n",
      "\n",
      "        [[-0.1676,  0.5825]],\n",
      "\n",
      "        [[ 0.1368, -0.9322]],\n",
      "\n",
      "        [[-0.2051, -0.0747]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "V_dec_1 = \n",
      "tensor([[[ 1.4120, -0.6568]],\n",
      "\n",
      "        [[-0.7945, -1.0526]],\n",
      "\n",
      "        [[ 1.3624, -0.6216]],\n",
      "\n",
      "        [[-0.8882, -1.2764]],\n",
      "\n",
      "        [[ 1.4343, -0.6807]],\n",
      "\n",
      "        [[-0.5334, -0.8036]],\n",
      "\n",
      "        [[ 1.1730, -0.5183]],\n",
      "\n",
      "        [[-0.5200, -1.5246]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "Cross attention in decoder_1\n",
      "tensor([[ 1.4120, -0.6568, -0.7945, -1.0526],\n",
      "        [ 1.3624, -0.6216, -0.8882, -1.2764],\n",
      "        [ 1.4343, -0.6807, -0.5334, -0.8036],\n",
      "        [ 1.1730, -0.5183, -0.5200, -1.5246]], grad_fn=<ViewBackward0>)\n",
      "\n",
      "norm2(x' + mha(x', mem)), \n",
      " where x' = Decoder_curr_layer norm1(x + sa(x))\n",
      "tensor([[[ 1.6425, -0.1400, -0.4741, -1.0285],\n",
      "         [ 1.6183, -0.0168, -0.5787, -1.0228],\n",
      "         [ 1.7071, -0.3861, -0.4812, -0.8398],\n",
      "         [ 1.6379, -0.0619, -0.5991, -0.9769]]], grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "\n",
      "norm3(x'' + ff(x'')) \n",
      " where, x'' = Decoder_curr_layer norm2(x' + mha(x'))\n",
      "tensor([[[ 1.6675, -0.3092, -0.3619, -0.9964],\n",
      "         [ 1.6573, -0.1863, -0.4741, -0.9969],\n",
      "         [ 1.7130, -0.5573, -0.3694, -0.7864],\n",
      "         [ 1.6747, -0.2326, -0.4961, -0.9460]]], grad_fn=<AddBackward0>)\n",
      "\n",
      "torch.Size([1, 4, 4]) torch.Size([1, 4, 4])\n",
      "Q_dec_2 = \n",
      "tensor([[[-0.4564,  0.4040]],\n",
      "\n",
      "        [[-0.0769, -0.1547]],\n",
      "\n",
      "        [[-0.4492,  0.3951]],\n",
      "\n",
      "        [[-0.1052, -0.1132]],\n",
      "\n",
      "        [[-0.4883,  0.5261]],\n",
      "\n",
      "        [[-0.0320, -0.0809]],\n",
      "\n",
      "        [[-0.4584,  0.4274]],\n",
      "\n",
      "        [[-0.0984, -0.0877]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "K_dec_2 = \n",
      "tensor([[[-0.1585,  1.1721]],\n",
      "\n",
      "        [[ 0.5348, -0.1444]],\n",
      "\n",
      "        [[-0.0538,  1.1324]],\n",
      "\n",
      "        [[ 0.4825, -0.1531]],\n",
      "\n",
      "        [[-0.3281,  1.0430]],\n",
      "\n",
      "        [[ 0.7169, -0.1352]],\n",
      "\n",
      "        [[-0.0818,  1.0969]],\n",
      "\n",
      "        [[ 0.5240, -0.1527]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "V_dec_2 = \n",
      "tensor([[[-0.6207,  0.2833]],\n",
      "\n",
      "        [[ 1.2686,  0.4266]],\n",
      "\n",
      "        [[-0.6454,  0.3579]],\n",
      "\n",
      "        [[ 1.2686,  0.4896]],\n",
      "\n",
      "        [[-0.6633,  0.1246]],\n",
      "\n",
      "        [[ 1.0597,  0.2788]],\n",
      "\n",
      "        [[-0.6633,  0.3297]],\n",
      "\n",
      "        [[ 1.2191,  0.4629]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "Decoder Self Attention = \n",
      "tensor([[-0.6207,  0.2833,  1.2686,  0.4266],\n",
      "        [-0.6454,  0.3579,  1.2686,  0.4896],\n",
      "        [-0.6633,  0.1246,  1.0597,  0.2788],\n",
      "        [-0.6633,  0.3297,  1.2191,  0.4629]], grad_fn=<ViewBackward0>)\n",
      "\n",
      "Decoder_2 norm1(x + sa(x))\n",
      "tensor([[[ 1.5877e+00,  7.9300e-04, -1.1240e+00, -4.6449e-01],\n",
      "         [ 1.4409e+00,  2.3308e-01, -1.3209e+00, -3.5300e-01],\n",
      "         [ 1.6934e+00, -4.5037e-01, -9.0165e-01, -3.4137e-01],\n",
      "         [ 1.4644e+00,  1.6218e-01, -1.3164e+00, -3.1023e-01]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "\n",
      "Q_dec_2 = \n",
      "tensor([[[ 0.1865, -0.5781]],\n",
      "\n",
      "        [[ 0.6103, -0.1993]],\n",
      "\n",
      "        [[ 0.1109, -0.3085]],\n",
      "\n",
      "        [[ 0.5972, -0.3766]],\n",
      "\n",
      "        [[ 0.3719, -0.8744]],\n",
      "\n",
      "        [[ 0.7502,  0.1472]],\n",
      "\n",
      "        [[ 0.1462, -0.3432]],\n",
      "\n",
      "        [[ 0.6381, -0.3249]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "K_dec_2 = \n",
      "tensor([[[-0.9461, -0.4201]],\n",
      "\n",
      "        [[ 0.6541, -0.5913]],\n",
      "\n",
      "        [[-0.9505, -0.4142]],\n",
      "\n",
      "        [[ 0.5800, -0.6170]],\n",
      "\n",
      "        [[-0.7898, -0.4899]],\n",
      "\n",
      "        [[ 0.6221, -0.3727]],\n",
      "\n",
      "        [[-0.4857, -0.5845]],\n",
      "\n",
      "        [[ 0.1362, -0.0720]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "V_dec_2 = \n",
      "tensor([[[-0.1976,  0.1209]],\n",
      "\n",
      "        [[-0.5734, -1.4606]],\n",
      "\n",
      "        [[-0.1951,  0.1337]],\n",
      "\n",
      "        [[-0.7044, -1.4274]],\n",
      "\n",
      "        [[-0.2685,  0.0153]],\n",
      "\n",
      "        [[-0.3249, -1.3999]],\n",
      "\n",
      "        [[-0.3893, -0.1243]],\n",
      "\n",
      "        [[-0.5464, -1.0646]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "Cross attention in decoder_2\n",
      "tensor([[-0.1976,  0.1209, -0.5734, -1.4606],\n",
      "        [-0.1951,  0.1337, -0.7044, -1.4274],\n",
      "        [-0.2685,  0.0153, -0.3249, -1.3999],\n",
      "        [-0.3893, -0.1243, -0.5464, -1.0646]], grad_fn=<ViewBackward0>)\n",
      "\n",
      "norm2(x' + mha(x', mem)), \n",
      " where x' = Decoder_curr_layer norm1(x + sa(x))\n",
      "tensor([[[ 1.5265, -0.1745, -1.2783, -0.0736],\n",
      "         [ 1.4271, -0.0056, -1.4010, -0.0205],\n",
      "         [ 1.6042, -0.4852, -1.0910, -0.0281],\n",
      "         [ 1.5221, -0.0578, -1.2834, -0.1809]]], grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "\n",
      "norm3(x'' + ff(x'')) \n",
      " where, x'' = Decoder_curr_layer norm2(x' + mha(x'))\n",
      "tensor([[[ 1.5900, -0.2046, -1.1776, -0.2079],\n",
      "         [ 1.4772,  0.0127, -1.3398, -0.1502],\n",
      "         [ 1.6628, -0.5940, -0.9286, -0.1402],\n",
      "         [ 1.5767, -0.0540, -1.1808, -0.3419]]], grad_fn=<AddBackward0>)\n",
      "\n",
      "torch.Size([1, 4, 4]) torch.Size([1, 4, 4])\n",
      "### Decoder Done ###\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.1736, -0.6635, -0.7838, -0.4535, -1.0496, -0.8957,  0.4070,\n",
       "           0.4802,  0.3982, -0.3234],\n",
       "         [-0.1878, -0.6338, -0.7410, -0.4279, -1.0766, -0.9809,  0.3347,\n",
       "           0.5342,  0.4060, -0.4664],\n",
       "         [-0.1235, -0.6708, -0.8959, -0.4473, -0.9361, -0.7919,  0.5501,\n",
       "           0.3801,  0.3818, -0.0212],\n",
       "         [-0.1777, -0.6718, -0.6719, -0.4529, -1.0911, -0.8548,  0.3259,\n",
       "           0.5181,  0.4016, -0.4310]]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_model = 4\n",
    "num_encoder_layers = 3\n",
    "num_decoder_layers = 3\n",
    "\n",
    "get_all_intermediate_outputs(src_sentence, tgt_sentence, model = model, num_encoder_layers = num_encoder_layers , num_decoder_layers = num_encoder_layers, d_model=d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Attention Weights:\n",
      "tensor([[[0.2494, 0.1807, 0.5699],\n",
      "         [0.5087, 0.4590, 0.0323],\n",
      "         [0.3180, 0.3567, 0.3253]]])\n",
      "\n",
      "Sum of Attention Weights along the last dimension:\n",
      "tensor([[1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "batch_size = 1\n",
    "sequence_length = 3\n",
    "embedding_size = 2\n",
    "\n",
    "\n",
    "# Assuming Q_enc1 and K_enc1 are your input tensors\n",
    "Q_enc1 = torch.randn((batch_size, sequence_length, embedding_size))\n",
    "K_enc1 = torch.randn((batch_size, sequence_length, embedding_size))\n",
    "\n",
    "\n",
    "# Calculate attention weights\n",
    "attn_weight = Q_enc1 @ K_enc1.transpose(-2, -1) \n",
    "attn_weight = torch.softmax(attn_weight, dim=-1)\n",
    "\n",
    "# Display the result\n",
    "print(\"Original Attention Weights:\")\n",
    "print(attn_weight)\n",
    "\n",
    "print(\"\\nSum of Attention Weights along the last dimension:\")\n",
    "print(torch.sum(attn_weight, dim=-1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3., 1., 1.],\n",
      "        [4., 1., 3.]])\n",
      "tensor([[0.2689, 0.5000, 0.1192],\n",
      "        [0.7311, 0.5000, 0.8808]])\n"
     ]
    }
   ],
   "source": [
    "m = nn.Softmax(dim=0)\n",
    "input = torch.randint(low= 1, high=5, size= (2, 3), dtype = torch.float32)\n",
    "\n",
    "print(input)\n",
    "\n",
    "output = m(input)\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.7870, 0.1065, 0.1065],\n",
      "        [0.7054, 0.0351, 0.2595]])\n"
     ]
    }
   ],
   "source": [
    "m = nn.Softmax(dim=1)\n",
    "output = m(input)\n",
    "\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.7870, 0.1065, 0.1065],\n",
      "        [0.7054, 0.0351, 0.2595]])\n"
     ]
    }
   ],
   "source": [
    "m = nn.Softmax(dim=-1)\n",
    "output = m(input)\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "my_project_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
