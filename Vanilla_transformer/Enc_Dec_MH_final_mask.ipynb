{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-head attention transformer\n",
    "### Encoder and Decoder\n",
    "### (With masking)\n",
    "\n",
    "Pytorch's implementation (in built)\n",
    "\n",
    "NOTE :- A new exmple must be used for testing the masked attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source embeddings :- \n",
      "\n",
      "tensor([[[-0.1965,  0.4001, -1.4294,  1.2002],\n",
      "         [ 0.2407,  1.6451,  0.5767,  0.7134],\n",
      "         [ 0.3766, -0.3972, -1.4901,  0.3670],\n",
      "         [-0.1228,  0.0718, -0.7737, -0.0868]]], grad_fn=<EmbeddingBackward0>)\n",
      "\n",
      "Target embeddings :- \n",
      "\n",
      "tensor([[[-0.2492, -1.9798, -0.3900, -0.5383],\n",
      "         [ 1.2019,  2.0036, -0.5215,  1.9132],\n",
      "         [-0.7919, -1.0487,  0.2215, -0.2312],\n",
      "         [-0.7919, -1.0487,  0.2215, -0.2312]]], grad_fn=<EmbeddingBackward0>)\n",
      "\n",
      "Positional encoded source embeddings:\n",
      "tensor([[[-0.1965,  2.4001,  0.5706,  5.2002],\n",
      "         [ 1.0822,  3.1854,  2.5867,  4.7134],\n",
      "         [ 1.2859,  0.1867,  0.5299,  4.3668],\n",
      "         [ 0.0183,  0.0818,  1.2563,  3.9128]]], grad_fn=<AddBackward0>)\n",
      "\n",
      "Positional encoded target embeddings:\n",
      "tensor([[[ 0.7508, -0.9798,  2.6100,  3.4617],\n",
      "         [ 3.0434,  2.5439,  2.4885,  5.9131],\n",
      "         [ 1.1174, -1.4648,  3.2415,  3.7686],\n",
      "         [ 0.3492, -2.0387,  3.2515,  3.7683]]], grad_fn=<AddBackward0>)\n",
      "\n",
      "query =  tensor([[[-0.1965,  2.4001,  0.5706,  5.2002],\n",
      "         [ 1.0822,  3.1854,  2.5867,  4.7134],\n",
      "         [ 1.2859,  0.1867,  0.5299,  4.3668],\n",
      "         [ 0.0183,  0.0818,  1.2563,  3.9128]]], grad_fn=<AddBackward0>)\n",
      "query shape =  torch.Size([1, 4, 4])\n",
      "\n",
      " _in_projection_packed \n",
      "\n",
      "Q :-  tensor([[[-1.8032, -0.8990,  2.2382,  4.2951],\n",
      "         [-0.3593, -0.7384,  2.4799,  4.1834],\n",
      "         [-1.3313, -2.1788,  1.5391,  2.7540],\n",
      "         [-1.4253, -1.8787,  1.9033,  2.2597]]], grad_fn=<SelectBackward0>)\n",
      "\n",
      "K :-  tensor([[[ 0.5545, -2.5529,  1.4227, -1.0407],\n",
      "         [ 1.7060, -3.1221,  1.1894, -1.5360],\n",
      "         [-0.3429, -1.8059,  0.9272, -1.7877],\n",
      "         [-0.2644, -2.0345,  0.4344, -1.2102]]], grad_fn=<SelectBackward0>)\n",
      "\n",
      "V :-  tensor([[[-1.7160, -1.7735,  2.4448, -2.5212],\n",
      "         [ 0.2962, -1.9649,  2.1180, -2.5557],\n",
      "         [-1.1142, -0.9874,  2.7670, -0.9002],\n",
      "         [-1.3984, -0.4769,  2.3564, -1.3284]]], grad_fn=<SelectBackward0>)\n",
      "\n",
      "tgt_len =  1\n",
      "bsz =  4\n",
      "num_heads =  2\n",
      "head_dim =  2\n",
      "\n",
      "tensor([[[-1.8032, -0.8990,  2.2382,  4.2951],\n",
      "         [-0.3593, -0.7384,  2.4799,  4.1834],\n",
      "         [-1.3313, -2.1788,  1.5391,  2.7540],\n",
      "         [-1.4253, -1.8787,  1.9033,  2.2597]]], grad_fn=<SelectBackward0>) torch.Size([1, 4, 4])\n",
      "Q reshaped =  tensor([[[-1.8032, -0.8990]],\n",
      "\n",
      "        [[ 2.2382,  4.2951]],\n",
      "\n",
      "        [[-0.3593, -0.7384]],\n",
      "\n",
      "        [[ 2.4799,  4.1834]],\n",
      "\n",
      "        [[-1.3313, -2.1788]],\n",
      "\n",
      "        [[ 1.5391,  2.7540]],\n",
      "\n",
      "        [[-1.4253, -1.8787]],\n",
      "\n",
      "        [[ 1.9033,  2.2597]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "K reshaped =  tensor([[[ 0.5545, -2.5529]],\n",
      "\n",
      "        [[ 1.4227, -1.0407]],\n",
      "\n",
      "        [[ 1.7060, -3.1221]],\n",
      "\n",
      "        [[ 1.1894, -1.5360]],\n",
      "\n",
      "        [[-0.3429, -1.8059]],\n",
      "\n",
      "        [[ 0.9272, -1.7877]],\n",
      "\n",
      "        [[-0.2644, -2.0345]],\n",
      "\n",
      "        [[ 0.4344, -1.2102]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "V reshaped =  tensor([[[-1.7160, -1.7735]],\n",
      "\n",
      "        [[ 2.4448, -2.5212]],\n",
      "\n",
      "        [[ 0.2962, -1.9649]],\n",
      "\n",
      "        [[ 2.1180, -2.5557]],\n",
      "\n",
      "        [[-1.1142, -0.9874]],\n",
      "\n",
      "        [[ 2.7670, -0.9002]],\n",
      "\n",
      "        [[-1.3984, -0.4769]],\n",
      "\n",
      "        [[ 2.3564, -1.3284]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "torch.Size([4, 2, 1, 2]) torch.Size([4, 2, 1, 2]) torch.Size([4, 2, 1, 2])\n",
      "\n",
      "scaled_dot_product_attention\n",
      "attn_output = \n",
      "tensor([[-1.7160, -1.7735,  2.4448, -2.5212],\n",
      "        [ 0.2962, -1.9649,  2.1180, -2.5557],\n",
      "        [-1.1142, -0.9874,  2.7670, -0.9002],\n",
      "        [-1.3984, -0.4769,  2.3564, -1.3284]], grad_fn=<ViewBackward0>)\n",
      "\n",
      "x + self_atten(x) = \n",
      "tensor([[[-2.6744, -0.4120, -1.3760,  3.0762],\n",
      "         [-0.4678,  0.1436,  0.8659,  3.5650],\n",
      "         [-1.1954, -1.9379, -1.3329,  2.5755],\n",
      "         [-2.2205, -1.9074, -0.1597,  2.2600]]], grad_fn=<AddBackward0>)\n",
      "\n",
      "norm1(x + self_atten(x)) = \n",
      "tensor([[[-1.0914, -0.0307, -0.4826,  1.6047],\n",
      "         [-0.9706, -0.5736, -0.1044,  1.6486],\n",
      "         [-0.4056, -0.8223, -0.4828,  1.7106],\n",
      "         [-0.9627, -0.7868,  0.1950,  1.5544]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "\n",
      "feed_fwd_op =  tensor([[[-0.1871, -0.1101, -0.1648, -0.2785],\n",
      "         [-0.1748, -0.1302, -0.1951, -0.2789],\n",
      "         [-0.1622, -0.1335, -0.2141, -0.2479],\n",
      "         [-0.1692, -0.1353, -0.2021, -0.2888]]], grad_fn=<ViewBackward0>)\n",
      "\n",
      "norm2(norm1(x + self_atten(x)) + feed_fwd_op) = \n",
      "tensor([[[-1.1376,  0.0461, -0.4810,  1.5724],\n",
      "         [-0.9991, -0.5350, -0.1102,  1.6442],\n",
      "         [-0.3924, -0.7949, -0.5264,  1.7138],\n",
      "         [-0.9869, -0.7650,  0.2028,  1.5491]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "\n",
      "##################################\n",
      "\n",
      "################### Transformer Decoder start #####################\n",
      "\n",
      "query =  tensor([[[ 0.7508, -0.9798,  2.6100,  3.4617],\n",
      "         [ 3.0434,  2.5439,  2.4885,  5.9131],\n",
      "         [ 1.1174, -1.4648,  3.2415,  3.7686],\n",
      "         [ 0.3492, -2.0387,  3.2515,  3.7683]]], grad_fn=<AddBackward0>)\n",
      "query shape =  torch.Size([1, 4, 4])\n",
      "\n",
      " _in_projection_packed \n",
      "\n",
      "Q :-  tensor([[[-0.5715, -0.9803, -0.2668, -2.1467],\n",
      "         [ 0.6712,  1.4975, -1.1324, -1.3539],\n",
      "         [-1.1546, -1.6399, -0.3026, -2.6020],\n",
      "         [-0.9442, -1.7529, -0.1179, -2.8550]]], grad_fn=<SelectBackward0>)\n",
      "\n",
      "K :-  tensor([[[ 0.5695, -0.4099,  1.4165,  1.8115],\n",
      "         [-0.6667,  2.4925,  2.2106,  0.3611],\n",
      "         [ 0.8026, -0.7249,  1.6418,  2.0580],\n",
      "         [ 0.9950, -1.1761,  1.5863,  2.6246]]], grad_fn=<SelectBackward0>)\n",
      "\n",
      "V :-  tensor([[[-0.6472,  1.7969, -1.4070, -0.1967],\n",
      "         [ 0.9499, -1.1671, -4.2108, -1.5491],\n",
      "         [-1.0828,  2.1850, -1.5985, -0.3161],\n",
      "         [-1.1503,  2.9487, -1.0779,  0.0906]]], grad_fn=<SelectBackward0>)\n",
      "\n",
      "tgt_len =  1\n",
      "bsz =  4\n",
      "num_heads =  2\n",
      "head_dim =  2\n",
      "\n",
      "tensor([[[-0.5715, -0.9803, -0.2668, -2.1467],\n",
      "         [ 0.6712,  1.4975, -1.1324, -1.3539],\n",
      "         [-1.1546, -1.6399, -0.3026, -2.6020],\n",
      "         [-0.9442, -1.7529, -0.1179, -2.8550]]], grad_fn=<SelectBackward0>) torch.Size([1, 4, 4])\n",
      "Q reshaped =  tensor([[[-0.5715, -0.9803]],\n",
      "\n",
      "        [[-0.2668, -2.1467]],\n",
      "\n",
      "        [[ 0.6712,  1.4975]],\n",
      "\n",
      "        [[-1.1324, -1.3539]],\n",
      "\n",
      "        [[-1.1546, -1.6399]],\n",
      "\n",
      "        [[-0.3026, -2.6020]],\n",
      "\n",
      "        [[-0.9442, -1.7529]],\n",
      "\n",
      "        [[-0.1179, -2.8550]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "K reshaped =  tensor([[[ 0.5695, -0.4099]],\n",
      "\n",
      "        [[ 1.4165,  1.8115]],\n",
      "\n",
      "        [[-0.6667,  2.4925]],\n",
      "\n",
      "        [[ 2.2106,  0.3611]],\n",
      "\n",
      "        [[ 0.8026, -0.7249]],\n",
      "\n",
      "        [[ 1.6418,  2.0580]],\n",
      "\n",
      "        [[ 0.9950, -1.1761]],\n",
      "\n",
      "        [[ 1.5863,  2.6246]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "V reshaped =  tensor([[[-0.6472,  1.7969]],\n",
      "\n",
      "        [[-1.4070, -0.1967]],\n",
      "\n",
      "        [[ 0.9499, -1.1671]],\n",
      "\n",
      "        [[-4.2108, -1.5491]],\n",
      "\n",
      "        [[-1.0828,  2.1850]],\n",
      "\n",
      "        [[-1.5985, -0.3161]],\n",
      "\n",
      "        [[-1.1503,  2.9487]],\n",
      "\n",
      "        [[-1.0779,  0.0906]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "torch.Size([4, 2, 1, 2]) torch.Size([4, 2, 1, 2]) torch.Size([4, 2, 1, 2])\n",
      "\n",
      "scaled_dot_product_attention\n",
      "attn_output = \n",
      "tensor([[-0.6472,  1.7969, -1.4070, -0.1967],\n",
      "        [ 0.9499, -1.1671, -4.2108, -1.5491],\n",
      "        [-1.0828,  2.1850, -1.5985, -0.3161],\n",
      "        [-1.1503,  2.9487, -1.0779,  0.0906]], grad_fn=<ViewBackward0>)\n",
      "\n",
      "x =  tensor([[[ 0.7508, -0.9798,  2.6100,  3.4617],\n",
      "         [ 3.0434,  2.5439,  2.4885,  5.9131],\n",
      "         [ 1.1174, -1.4648,  3.2415,  3.7686],\n",
      "         [ 0.3492, -2.0387,  3.2515,  3.7683]]], grad_fn=<AddBackward0>)\n",
      "\n",
      "sa_op =  tensor([[[-0.7972, -0.8369, -2.9348,  1.6432],\n",
      "         [ 0.5649,  3.8928, -2.7056, -2.2094],\n",
      "         [-0.8494, -1.3390, -3.7335,  2.1888],\n",
      "         [-1.3322, -2.1464, -3.6756,  2.9461]]], grad_fn=<ViewBackward0>)\n",
      "\n",
      "temop =  tensor([[[-0.0464, -1.8167, -0.3248,  5.1049],\n",
      "         [ 3.6082,  6.4368, -0.2171,  3.7038],\n",
      "         [ 0.2680, -2.8039, -0.4920,  5.9574],\n",
      "         [-0.9830, -4.1850, -0.4241,  6.7144]]], grad_fn=<AddBackward0>)\n",
      "\n",
      "Decoder norm1(x + sa(x)) = \n",
      "tensor([[[-0.2967, -0.9738, -0.4032,  1.6737],\n",
      "         [ 0.0951,  1.2893, -1.5199,  0.1355],\n",
      "         [-0.1441, -1.0976, -0.3800,  1.6218],\n",
      "         [-0.3173, -1.1214, -0.1770,  1.6157]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "\n",
      "Memory = \n",
      "tensor([[[-1.1376,  0.0461, -0.4810,  1.5724],\n",
      "         [-0.9991, -0.5350, -0.1102,  1.6442],\n",
      "         [-0.3924, -0.7949, -0.5264,  1.7138],\n",
      "         [-0.9869, -0.7650,  0.2028,  1.5491]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "\n",
      "query =  tensor([[[-0.2967, -0.9738, -0.4032,  1.6737],\n",
      "         [ 0.0951,  1.2893, -1.5199,  0.1355],\n",
      "         [-0.1441, -1.0976, -0.3800,  1.6218],\n",
      "         [-0.3173, -1.1214, -0.1770,  1.6157]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "query shape =  torch.Size([1, 4, 4])\n",
      "\n",
      " _in_projection_packed \n",
      "\n",
      "Q :-  tensor([[[ 1.1551, -0.5112,  0.2386, -0.9912],\n",
      "         [ 0.2198, -0.2264, -0.3958,  0.5742],\n",
      "         [ 1.1903, -0.4910,  0.3076, -1.0038],\n",
      "         [ 1.0901, -0.4674,  0.2824, -1.0465]]], grad_fn=<ViewBackward0>)\n",
      "\n",
      "K :-  tensor([[[-0.0704,  0.2770,  1.2327, -0.4538],\n",
      "         [-0.1660,  0.3082,  0.8527, -0.2363],\n",
      "         [-0.4686, -0.0289,  0.3370, -0.2865],\n",
      "         [-0.1387,  0.3806,  0.6938, -0.0846]]], grad_fn=<SelectBackward0>)\n",
      "\n",
      "V :-  tensor([[[ 0.1339,  0.0898,  0.1141,  0.5882],\n",
      "         [ 0.0868,  0.4066, -0.0548,  0.3838],\n",
      "         [ 0.3645,  0.3931, -0.1983, -0.0611],\n",
      "         [-0.0098,  0.5592, -0.1135,  0.3421]]], grad_fn=<SelectBackward0>)\n",
      "\n",
      "tgt_len =  1\n",
      "bsz =  4\n",
      "num_heads =  2\n",
      "head_dim =  2\n",
      "\n",
      "tensor([[[ 1.1551, -0.5112,  0.2386, -0.9912],\n",
      "         [ 0.2198, -0.2264, -0.3958,  0.5742],\n",
      "         [ 1.1903, -0.4910,  0.3076, -1.0038],\n",
      "         [ 1.0901, -0.4674,  0.2824, -1.0465]]], grad_fn=<ViewBackward0>) torch.Size([1, 4, 4])\n",
      "Q reshaped =  tensor([[[ 1.1551, -0.5112]],\n",
      "\n",
      "        [[ 0.2386, -0.9912]],\n",
      "\n",
      "        [[ 0.2198, -0.2264]],\n",
      "\n",
      "        [[-0.3958,  0.5742]],\n",
      "\n",
      "        [[ 1.1903, -0.4910]],\n",
      "\n",
      "        [[ 0.3076, -1.0038]],\n",
      "\n",
      "        [[ 1.0901, -0.4674]],\n",
      "\n",
      "        [[ 0.2824, -1.0465]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "K reshaped =  tensor([[[-0.0704,  0.2770]],\n",
      "\n",
      "        [[ 1.2327, -0.4538]],\n",
      "\n",
      "        [[-0.1660,  0.3082]],\n",
      "\n",
      "        [[ 0.8527, -0.2363]],\n",
      "\n",
      "        [[-0.4686, -0.0289]],\n",
      "\n",
      "        [[ 0.3370, -0.2865]],\n",
      "\n",
      "        [[-0.1387,  0.3806]],\n",
      "\n",
      "        [[ 0.6938, -0.0846]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "V reshaped =  tensor([[[ 0.1339,  0.0898]],\n",
      "\n",
      "        [[ 0.1141,  0.5882]],\n",
      "\n",
      "        [[ 0.0868,  0.4066]],\n",
      "\n",
      "        [[-0.0548,  0.3838]],\n",
      "\n",
      "        [[ 0.3645,  0.3931]],\n",
      "\n",
      "        [[-0.1983, -0.0611]],\n",
      "\n",
      "        [[-0.0098,  0.5592]],\n",
      "\n",
      "        [[-0.1135,  0.3421]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "torch.Size([4, 2, 1, 2]) torch.Size([4, 2, 1, 2]) torch.Size([4, 2, 1, 2])\n",
      "\n",
      "scaled_dot_product_attention\n",
      "attn_output = \n",
      "tensor([[ 0.1339,  0.0898,  0.1141,  0.5882],\n",
      "        [ 0.0868,  0.4066, -0.0548,  0.3838],\n",
      "        [ 0.3645,  0.3931, -0.1983, -0.0611],\n",
      "        [-0.0098,  0.5592, -0.1135,  0.3421]], grad_fn=<ViewBackward0>)\n",
      "\n",
      "Decoder norm2(x + mha(x, mem)) = \n",
      "tensor([[[-0.0528, -1.2301, -0.2699,  1.5528],\n",
      "         [ 0.2214,  1.0573, -1.6432,  0.3646],\n",
      "         [-0.2337, -0.7955, -0.6650,  1.6942],\n",
      "         [-0.1521, -1.0451, -0.4425,  1.6397]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "\n",
      "Decoder norm3(x + ff(x)) = \n",
      "tensor([[[ 0.0823, -1.1172, -0.5336,  1.5685],\n",
      "         [ 0.3409,  0.7710, -1.7116,  0.5997],\n",
      "         [-0.0431, -0.7461, -0.8564,  1.6456],\n",
      "         [ 0.0110, -0.9569, -0.6753,  1.6212]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "\n",
      "tensor([[[-0.8690,  0.2952,  0.6433,  0.9624, -0.4184,  1.0310, -0.2743,\n",
      "           0.2156,  0.6588, -0.5275],\n",
      "         [-0.1098,  0.0060, -0.0221,  0.2819, -0.9641,  0.5592, -0.5691,\n",
      "          -0.1397,  0.5262,  0.3902],\n",
      "         [-0.7065,  0.2620,  0.5209,  0.8902, -0.5664,  1.0184, -0.2785,\n",
      "           0.0787,  0.5858, -0.3458],\n",
      "         [-0.7971,  0.2843,  0.5912,  0.9347, -0.4833,  1.0307, -0.2683,\n",
      "           0.1500,  0.6215, -0.4467]]], grad_fn=<ViewBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sreevaatsav/.pyenv/versions/project_env/lib/python3.10/site-packages/torch/nn/modules/transformer.py:282: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=512, dropout=0):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.encoding = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1).float()\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(torch.log(torch.tensor(10000.0)) / d_model))\n",
    "        self.encoding[:, 0::2] = torch.sin(position * div_term)\n",
    "        self.encoding[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.encoding = self.encoding.unsqueeze(0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.encoding[:, :x.size(1)].detach()\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, src_vocab_size, tgt_vocab_size, d_model=4, nhead=2, num_encoder_layers=1, num_decoder_layers=1):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.src_embedding = nn.Embedding(src_vocab_size, d_model)\n",
    "        self.tgt_embedding = nn.Embedding(tgt_vocab_size, d_model)\n",
    "        self.positional_encoding = PositionalEncoding(d_model, dropout=0)\n",
    "        self.transformer = nn.Transformer(\n",
    "            d_model=d_model,\n",
    "            nhead=nhead,\n",
    "            num_encoder_layers=num_encoder_layers,\n",
    "            num_decoder_layers=num_decoder_layers,\n",
    "            dropout=0\n",
    "        )\n",
    "        self.fc = nn.Linear(d_model, tgt_vocab_size)\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "\n",
    "        print(\"Source embeddings :- \\n\")\n",
    "        print(self.src_embedding(src))\n",
    "        print()\n",
    "\n",
    "        print(\"Target embeddings :- \\n\")\n",
    "        print(self.tgt_embedding(tgt))\n",
    "        print()\n",
    "\n",
    "\n",
    "        src = self.src_embedding(src) + self.positional_encoding(src)\n",
    "        tgt = self.tgt_embedding(tgt) + self.positional_encoding(tgt)\n",
    "\n",
    "        print(\"Positional encoded source embeddings:\")\n",
    "        print(src)\n",
    "        print()\n",
    "\n",
    "        print(\"Positional encoded target embeddings:\")\n",
    "        print(tgt)\n",
    "        print()\n",
    "\n",
    "        output = self.transformer(src, tgt)\n",
    "        output = self.fc(output)\n",
    "        return output\n",
    "\n",
    "    \n",
    "\n",
    "src_vocab_size = 10  # Source language vocabulary size\n",
    "tgt_vocab_size = 10  # Target language vocabulary size\n",
    "d_model = 4  # Dimension of the model\n",
    "num_heads = 2\n",
    "model = TransformerModel(src_vocab_size, tgt_vocab_size, d_model=d_model, nhead=num_heads)\n",
    "# Source sentence in the source language\n",
    "src_sentence = torch.tensor([[0, 1, 2, 3]])  # Source sequence\n",
    "\n",
    "# Target sentence in the target language (translation of the source sentence)\n",
    "tgt_sentence = torch.tensor([[1, 0, 3, 3]])  # Target sequence\n",
    "\n",
    "# Forward pass\n",
    "output = model(src_sentence, tgt_sentence)\n",
    "print(output)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions to get the manual calulations for each component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import functional as F\n",
    "\n",
    "# Using the state dictionary to get the intermediate outputs\n",
    "state_dict = model.state_dict()\n",
    "\n",
    "need_weights = False\n",
    "\n",
    "src_mask = None\n",
    "\n",
    "tgt_mask = None\n",
    "\n",
    "memory_mask = None\n",
    "\n",
    "embed_dim = 4\n",
    "\n",
    "num_heads = 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embeddings and Positional encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding_outputs(src_sentence,  tgt_sentence, state_dict, d_model):\n",
    "\n",
    "    src_vocab_embeds = state_dict[\"src_embedding.weight\"]\n",
    "\n",
    "    src_embedding = torch.zeros(src_sentence.size(0), src_sentence.size(1), d_model)\n",
    "\n",
    "    print(\"Source sentence embedding\")\n",
    "    for i in range(src_sentence.size(0)):\n",
    "        for j in range(src_sentence.size(1)):\n",
    "            # Get the index for the current word in the sequence\n",
    "            word_index = src_sentence[i, j].item()\n",
    "\n",
    "            # Check if the index is within valid range\n",
    "            if word_index < 0 or word_index >= src_vocab_embeds.size(0):\n",
    "                raise ValueError(f\"Invalid word index: {word_index}\")\n",
    "\n",
    "            # Lookup the corresponding embedding vector for the word\n",
    "            src_embedding[i, j, :] = src_vocab_embeds[word_index, :]\n",
    "\n",
    "            # Print intermediate results for debugging\n",
    "            \n",
    "            print(f\"Word index: {word_index}, Embedding: {src_vocab_embeds[word_index, :]}\")\n",
    "    print()\n",
    "\n",
    "\n",
    "    tgt_vocab_embeds = state_dict[\"tgt_embedding.weight\"]\n",
    "\n",
    "    tgt_embedding = torch.zeros(tgt_sentence.size(0), tgt_sentence.size(1), d_model)\n",
    "\n",
    "\n",
    "    print(\"Target sentence embedding\")\n",
    "    for i in range(tgt_sentence.size(0)):\n",
    "        for j in range(tgt_sentence.size(1)):\n",
    "            # Get the index for the current word in the sequence\n",
    "            word_index = tgt_sentence[i, j].item()\n",
    "\n",
    "            # Check if the index is within valid range\n",
    "            if word_index < 0 or word_index >= tgt_vocab_embeds.size(0):\n",
    "                raise ValueError(f\"Invalid word index: {word_index}\")\n",
    "\n",
    "            # Lookup the corresponding embedding vector for the word\n",
    "            tgt_embedding[i, j, :] = tgt_vocab_embeds[word_index, :]\n",
    "\n",
    "            # Print intermediate results for debugging\n",
    "            print(f\"Word index: {word_index}, Embedding: {tgt_vocab_embeds[word_index, :]}\")\n",
    "    print()\n",
    "\n",
    "\n",
    "    pe = PositionalEncoding(d_model = d_model, dropout=0)\n",
    "\n",
    "    pe_src_embeds = src_embedding + pe(src_sentence)\n",
    "\n",
    "    pe_tgt_embeds = tgt_embedding + pe(tgt_sentence)\n",
    "\n",
    "    print(\"PE source embeddings : \\n\")\n",
    "    print(pe_src_embeds)\n",
    "    print()\n",
    "\n",
    "    print(\"PE target embeddings : \\n\")\n",
    "    print(pe_tgt_embeds)\n",
    "    print()\n",
    "\n",
    "    return pe_src_embeds, pe_tgt_embeds\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source sentence embedding\n",
      "Word index: 0, Embedding: tensor([-0.1965,  0.4001, -1.4294,  1.2002])\n",
      "Word index: 1, Embedding: tensor([0.2407, 1.6451, 0.5767, 0.7134])\n",
      "Word index: 2, Embedding: tensor([ 0.3766, -0.3972, -1.4901,  0.3670])\n",
      "Word index: 3, Embedding: tensor([-0.1228,  0.0718, -0.7737, -0.0868])\n",
      "\n",
      "Target sentence embedding\n",
      "Word index: 1, Embedding: tensor([-0.2492, -1.9798, -0.3900, -0.5383])\n",
      "Word index: 0, Embedding: tensor([ 1.2019,  2.0036, -0.5215,  1.9132])\n",
      "Word index: 3, Embedding: tensor([-0.7919, -1.0487,  0.2215, -0.2312])\n",
      "Word index: 3, Embedding: tensor([-0.7919, -1.0487,  0.2215, -0.2312])\n",
      "\n",
      "PE source embeddings : \n",
      "\n",
      "tensor([[[-0.1965,  2.4001,  0.5706,  5.2002],\n",
      "         [ 1.0822,  3.1854,  2.5867,  4.7134],\n",
      "         [ 1.2859,  0.1867,  0.5299,  4.3668],\n",
      "         [ 0.0183,  0.0818,  1.2563,  3.9128]]])\n",
      "\n",
      "PE target embeddings : \n",
      "\n",
      "tensor([[[ 0.7508, -0.9798,  2.6100,  3.4617],\n",
      "         [ 3.0434,  2.5439,  2.4885,  5.9131],\n",
      "         [ 1.1174, -1.4648,  3.2415,  3.7686],\n",
      "         [ 0.3492, -2.0387,  3.2515,  3.7683]]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pe_src_embeds, pe_tgt_embeds = get_embedding_outputs(src_sentence,  tgt_sentence, state_dict, d_model = d_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder function to display the intermediate outputs and get the final outputs from the encoder\n",
    "\n",
    "### Self attention "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder_block_attn_output(x, state_dict, layer_num, embed_dim, num_heads, need_weights = False, src_mask = None):\n",
    "\n",
    "    \n",
    "    query_enc = key_enc = value_enc = x\n",
    "\n",
    "    tgt_len, bsz, embed_dim = x.shape\n",
    "\n",
    "\n",
    "    W_enc = state_dict[\"transformer.encoder.layers.{}.self_attn.in_proj_weight\".format(layer_num)]\n",
    "    b_enc = state_dict[\"transformer.encoder.layers.{}.self_attn.in_proj_bias\".format(layer_num)]\n",
    "\n",
    "\n",
    "    head_dim = embed_dim//num_heads\n",
    "\n",
    "    tempop1 = query_enc[0]@W_enc.T\n",
    "    tempop1 = tempop1.T\n",
    "\n",
    "    Q_enc,K_enc,V_enc = tempop1.T.chunk(3, dim= -1)\n",
    "\n",
    "\n",
    "    Q_enc = Q_enc.unsqueeze(0)\n",
    "    K_enc = K_enc.unsqueeze(0)\n",
    "    V_enc = V_enc.unsqueeze(0)\n",
    "\n",
    "\n",
    "    Q_enc = Q_enc.reshape(tgt_len, bsz * num_heads, head_dim).transpose(0, 1)\n",
    "    K_enc = K_enc.reshape(tgt_len, bsz * num_heads, head_dim).transpose(0, 1)\n",
    "    V_enc = V_enc.reshape(tgt_len, bsz * num_heads, head_dim).transpose(0, 1)\n",
    "\n",
    "    print(\"Q_enc_{} = \".format(layer_num))\n",
    "    print(Q_enc)\n",
    "    print()\n",
    "\n",
    "    print(\"K_enc_{} = \".format(layer_num))\n",
    "    print(K_enc)\n",
    "    print()\n",
    "\n",
    "    print(\"V_enc_{} = \".format(layer_num))\n",
    "    print(V_enc)\n",
    "    print()\n",
    "\n",
    "    src_len = K_enc.size(1)\n",
    "\n",
    "\n",
    "    attn_mask = src_mask\n",
    "    if attn_mask is not None:\n",
    "\n",
    "        # Ensuring attn_mask's dim is 3\n",
    "        if attn_mask.dim() == 2:\n",
    "            correct_2d_size = (tgt_len, src_len)\n",
    "            if attn_mask.shape != correct_2d_size:\n",
    "                raise RuntimeError(f\"The shape of the 2D attn_mask is {attn_mask.shape}, but should be {correct_2d_size}.\")\n",
    "            attn_mask = attn_mask.unsqueeze(0)\n",
    "        elif attn_mask.dim() == 3:\n",
    "            correct_3d_size = (bsz * num_heads, tgt_len, src_len)\n",
    "            if attn_mask.shape != correct_3d_size:\n",
    "                raise RuntimeError(f\"The shape of the 3D attn_mask is {attn_mask.shape}, but should be {correct_3d_size}.\")\n",
    "        else:\n",
    "            raise RuntimeError(f\"attn_mask's dimension {attn_mask.dim()} is not supported\")\n",
    "\n",
    "    # attn_mask can be either (L,S) or (N*num_heads, L, S)\n",
    "    # if attn_mask's shape is (1, L, S) we need to unsqueeze to (1, 1, L, S)\n",
    "    # in order to match the input for SDPA of (N, num_heads, L, S)\n",
    "    if attn_mask is not None:\n",
    "        if attn_mask.size(0) == 1 and attn_mask.dim() == 3:\n",
    "            attn_mask = attn_mask.unsqueeze(0)\n",
    "        else:\n",
    "            attn_mask = attn_mask.view(bsz, num_heads, -1, src_len)\n",
    "\n",
    "\n",
    "    if need_weights is False:\n",
    "        Q_enc1 = Q_enc.view(bsz, num_heads, tgt_len, head_dim)\n",
    "        K_enc1 = K_enc.view(bsz, num_heads, src_len, head_dim)\n",
    "        V_enc1 = V_enc.view(bsz, num_heads, src_len, head_dim)\n",
    "\n",
    "\n",
    "        L, S = Q_enc1.size(-2), K_enc1.size(-2)\n",
    "\n",
    "        scale_factor = 1 / math.sqrt(Q_enc1.size(-1)) \n",
    "        # scale_factor = 1\n",
    "        attn_bias = torch.zeros(L, S, dtype=Q_enc1.dtype)\n",
    "\n",
    "        if attn_mask is not None:\n",
    "            if attn_mask.dtype == torch.bool:\n",
    "                attn_mask.masked_fill_(attn_mask.logical_not(), float(\"-inf\"))\n",
    "            else:\n",
    "                attn_bias += attn_mask\n",
    "\n",
    "        attn_weight = Q_enc1 @ K_enc1.transpose(-2, -1) * scale_factor\n",
    "        attn_weight += attn_bias\n",
    "        attn_weight = torch.softmax(attn_weight, dim=-1)\n",
    "        \n",
    "        attn_output = attn_weight @ V_enc1\n",
    "\n",
    "        attn_output = attn_output.permute(2, 0, 1, 3).contiguous().view(bsz * tgt_len, embed_dim)\n",
    "\n",
    "        print(\"Encoder Attention output = \")\n",
    "        print(attn_output)\n",
    "        print()\n",
    "\n",
    "        # print(src_len, pe_src_embeds.shape)\n",
    "        return attn_output, src_len, head_dim, None\n",
    "    \n",
    "    else:\n",
    "\n",
    "        B, Nt, E = Q_enc.shape\n",
    "\n",
    "        Q_enc_scaled = Q_enc / math.sqrt(E)\n",
    "\n",
    "        Q_enc_scaled\n",
    "\n",
    "        if attn_mask is not None:\n",
    "            temp_pdt_matrix_enc = torch.baddbmm(attn_mask, Q_enc_scaled, K_enc.transpose(-2, -1))\n",
    "        else:\n",
    "            temp_pdt_matrix_enc = torch.bmm(Q_enc_scaled, K_enc.transpose(-2, -1))\n",
    "\n",
    "\n",
    "        # Obtaining the diagonal entries of the un-normalised product (K.Q_scaled)\n",
    "        pdt_matrix_enc = torch.diagonal(temp_pdt_matrix_enc, dim1=-2, dim2=-1).unsqueeze(-1)\n",
    "\n",
    "        attn_wt_matrix_enc = torch.nn.functional.softmax(pdt_matrix_enc, dim=-1)\n",
    "\n",
    "        attn_enc_output = torch.bmm(attn_wt_matrix_enc, V_enc)\n",
    "\n",
    "        attn_enc_output = attn_enc_output.transpose(0, 1).contiguous().view(tgt_len * bsz, embed_dim)\n",
    "\n",
    "        print(\"Encoder Attention output = \")\n",
    "        print(attn_enc_output)\n",
    "        print()\n",
    "\n",
    "        return attn_enc_output, src_len, head_dim, attn_wt_matrix_enc\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q_enc_0 = \n",
      "tensor([[[-1.8032, -0.8990]],\n",
      "\n",
      "        [[ 2.2382,  4.2951]],\n",
      "\n",
      "        [[-0.3593, -0.7384]],\n",
      "\n",
      "        [[ 2.4799,  4.1834]],\n",
      "\n",
      "        [[-1.3313, -2.1788]],\n",
      "\n",
      "        [[ 1.5391,  2.7540]],\n",
      "\n",
      "        [[-1.4253, -1.8787]],\n",
      "\n",
      "        [[ 1.9033,  2.2597]]])\n",
      "\n",
      "K_enc_0 = \n",
      "tensor([[[ 0.5545, -2.5529]],\n",
      "\n",
      "        [[ 1.4227, -1.0407]],\n",
      "\n",
      "        [[ 1.7060, -3.1221]],\n",
      "\n",
      "        [[ 1.1894, -1.5360]],\n",
      "\n",
      "        [[-0.3429, -1.8059]],\n",
      "\n",
      "        [[ 0.9272, -1.7877]],\n",
      "\n",
      "        [[-0.2644, -2.0345]],\n",
      "\n",
      "        [[ 0.4344, -1.2102]]])\n",
      "\n",
      "V_enc_0 = \n",
      "tensor([[[-1.7160, -1.7735]],\n",
      "\n",
      "        [[ 2.4448, -2.5212]],\n",
      "\n",
      "        [[ 0.2962, -1.9649]],\n",
      "\n",
      "        [[ 2.1180, -2.5557]],\n",
      "\n",
      "        [[-1.1142, -0.9874]],\n",
      "\n",
      "        [[ 2.7670, -0.9002]],\n",
      "\n",
      "        [[-1.3984, -0.4769]],\n",
      "\n",
      "        [[ 2.3564, -1.3284]]])\n",
      "\n",
      "Encoder Attention output = \n",
      "tensor([[-1.7160, -1.7735,  2.4448, -2.5212],\n",
      "        [ 0.2962, -1.9649,  2.1180, -2.5557],\n",
      "        [-1.1142, -0.9874,  2.7670, -0.9002],\n",
      "        [-1.3984, -0.4769,  2.3564, -1.3284]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# need_weights = True\n",
    "\n",
    "attn_enc_output, src_len, head_dim, attn_weights = encoder_block_attn_output(pe_src_embeds, state_dict, layer_num = 0, need_weights = need_weights, embed_dim=embed_dim, num_heads=num_heads)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Post self attention in encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder_block_post_attn_output(x, attn_enc_output, state_dict, layer_num, bsz, tgt_len):\n",
    "\n",
    "    op_enc_1 = torch.matmul(attn_enc_output, state_dict[\"transformer.encoder.layers.{}.self_attn.out_proj.weight\".format(layer_num)].t()) + state_dict[\"transformer.encoder.layers.{}.self_attn.out_proj.bias\".format(layer_num)]\n",
    "\n",
    "    attn_enc_output = op_enc_1.view(tgt_len, bsz, attn_enc_output.size(1))\n",
    "\n",
    "\n",
    "    # Here src is the original passed inputs to the 1st transformer encoder layer which \n",
    "    # are pe_src_embeds \n",
    "    output_enc_1 = attn_enc_output + x\n",
    "    output_enc_1\n",
    "\n",
    "    linear_result_enc_1 = output_enc_1*state_dict[\"transformer.encoder.layers.{}.norm1.weight\".format(layer_num)] + state_dict[\"transformer.encoder.layers.{}.norm1.bias\".format(layer_num)]\n",
    "\n",
    "    # Layer normalization from Torch's implementation\n",
    "    layernorm_enc_1 = torch.nn.LayerNorm(normalized_shape=linear_result_enc_1.shape[2:])\n",
    "    linear_op_enc_1 = layernorm_enc_1(linear_result_enc_1)\n",
    "\n",
    "\n",
    "    # Manual Layer Normalization\n",
    "    x = linear_result_enc_1\n",
    "\n",
    "    # Obtained layer norm weights and biases (learnable)\n",
    "    w = layernorm_enc_1.weight\n",
    "    b = layernorm_enc_1.bias\n",
    "\n",
    "    linear_result_enc_1f = w*x + b\n",
    "\n",
    "    epsilon = 1e-5  \n",
    "    mean = linear_result_enc_1f.mean(dim=-1, keepdim=True)\n",
    "    std = linear_result_enc_1f.std(dim=-1, unbiased=False, keepdim=True)\n",
    "    normalized_result_enc_1 = (linear_result_enc_1f - mean) / (std + epsilon) * w + b\n",
    "\n",
    "\n",
    "    op_enc_1 = torch.matmul(normalized_result_enc_1, state_dict[\"transformer.encoder.layers.{}.linear1.weight\".format(layer_num)].t()) + state_dict[\"transformer.encoder.layers.{}.linear1.bias\".format(layer_num)]\n",
    "    op_enc_1_relu = torch.nn.functional.relu(op_enc_1)\n",
    "    op_enc_2 = torch.matmul(op_enc_1_relu, state_dict[\"transformer.encoder.layers.{}.linear2.weight\".format(layer_num)].t()) + state_dict[\"transformer.encoder.layers.{}.linear2.bias\".format(layer_num)]\n",
    "\n",
    "\n",
    "    output_enc_2 = op_enc_2 + linear_op_enc_1\n",
    "    output_enc_2_norm = output_enc_2*state_dict[\"transformer.encoder.layers.{}.norm2.weight\".format(layer_num)] + state_dict[\"transformer.encoder.layers.{}.norm2.bias\".format(layer_num)]\n",
    "\n",
    "    # Layer normalization from Torch's implementation\n",
    "    layernorm_enc_final = torch.nn.LayerNorm(normalized_shape=output_enc_2_norm.shape[2:])\n",
    "    output_enc_final = layernorm_enc_final(output_enc_2_norm)\n",
    "\n",
    "\n",
    "    # Manual Layer Normalization \n",
    "    x = output_enc_2_norm\n",
    "\n",
    "    # Obtained layer norm weights and biases (learnable)\n",
    "    w = layernorm_enc_final.weight\n",
    "    b = layernorm_enc_final.bias\n",
    "\n",
    "    linear_result_enc_2 = w*x + b\n",
    "\n",
    "    epsilon = 1e-5  \n",
    "    mean = linear_result_enc_2.mean(dim=-1, keepdim=True)\n",
    "    std = linear_result_enc_2.std(dim=-1, unbiased=False, keepdim=True)\n",
    "    output_enc_final = (linear_result_enc_2 - mean) / (std + epsilon) * w + b\n",
    "\n",
    "    print(\"Final Encoder {} Output :\".format(layer_num))\n",
    "    print(\"norm2(norm1(x + self_atten(x)) + feed_fwd_op)\\n\")\n",
    "    print(output_enc_final)\n",
    "    print()\n",
    "\n",
    "    return output_enc_final\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Encoder 0 Output :\n",
      "norm2(norm1(x + self_atten(x)) + feed_fwd_op)\n",
      "\n",
      "tensor([[[-1.1375,  0.0461, -0.4810,  1.5724],\n",
      "         [-0.9991, -0.5350, -0.1102,  1.6442],\n",
      "         [-0.3924, -0.7949, -0.5264,  1.7137],\n",
      "         [-0.9869, -0.7650,  0.2028,  1.5491]]], grad_fn=<AddBackward0>)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tgt_len, bsz, embed_dim = pe_src_embeds.shape\n",
    "output_enc_final = encoder_block_post_attn_output(pe_src_embeds, attn_enc_output, state_dict, layer_num = 0 , bsz = bsz, tgt_len = tgt_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder functions to display the intermediate outputs and get the final outputs from the decoder\n",
    "\n",
    "\n",
    "### Self attention outputs from a decoder block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoder_block_self_attn_output(x, state_dict, layer_num, tgt_mask = None,need_weights = False):\n",
    "\n",
    "    query_dec = key_dec = value_dec = x\n",
    "\n",
    "    tgt_len, bsz, embed_dim = x.shape\n",
    "\n",
    "    W_dec = state_dict[\"transformer.decoder.layers.{}.self_attn.in_proj_weight\".format(layer_num)]\n",
    "    b_dec = state_dict[\"transformer.decoder.layers.{}.self_attn.in_proj_bias\".format(layer_num)]\n",
    "\n",
    "\n",
    "    head_dim = embed_dim//num_heads\n",
    "\n",
    "    tempop1 = query_dec[0]@W_dec.T\n",
    "    tempop1 = tempop1.T\n",
    "\n",
    "    Q_dec,K_dec,V_dec = tempop1.T.chunk(3, dim= -1)\n",
    "\n",
    "\n",
    "    Q_dec = Q_dec.unsqueeze(0)\n",
    "    K_dec = K_dec.unsqueeze(0)\n",
    "    V_dec = V_dec.unsqueeze(0)\n",
    "\n",
    "\n",
    "    Q_dec = Q_dec.reshape(tgt_len, bsz * num_heads, head_dim).transpose(0, 1)\n",
    "    K_dec = K_dec.reshape(tgt_len, bsz * num_heads, head_dim).transpose(0, 1)\n",
    "    V_dec = V_dec.reshape(tgt_len, bsz * num_heads, head_dim).transpose(0, 1)\n",
    "\n",
    "    print(\"Q_dec_{} = \".format(layer_num))\n",
    "    print(Q_dec)\n",
    "    print()\n",
    "\n",
    "    print(\"K_dec_{} = \".format(layer_num))\n",
    "    print(K_dec)\n",
    "    print()\n",
    "\n",
    "    print(\"V_dec_{} = \".format(layer_num))\n",
    "    print(V_dec)\n",
    "    print()\n",
    "\n",
    "    src_len = K_dec.size(1)\n",
    "\n",
    "\n",
    "    attn_mask = tgt_mask\n",
    "    if attn_mask is not None:\n",
    "\n",
    "        # Ensuring attn_mask's dim is 3\n",
    "        if attn_mask.dim() == 2:\n",
    "            correct_2d_size = (tgt_len, src_len)\n",
    "            if attn_mask.shape != correct_2d_size:\n",
    "                raise RuntimeError(f\"The shape of the 2D attn_mask is {attn_mask.shape}, but should be {correct_2d_size}.\")\n",
    "            attn_mask = attn_mask.unsqueeze(0)\n",
    "        elif attn_mask.dim() == 3:\n",
    "            correct_3d_size = (bsz * num_heads, tgt_len, src_len)\n",
    "            if attn_mask.shape != correct_3d_size:\n",
    "                raise RuntimeError(f\"The shape of the 3D attn_mask is {attn_mask.shape}, but should be {correct_3d_size}.\")\n",
    "        else:\n",
    "            raise RuntimeError(f\"attn_mask's dimension {attn_mask.dim()} is not supported\")\n",
    "\n",
    "    # attn_mask can be either (L,S) or (N*num_heads, L, S)\n",
    "    # if attn_mask's shape is (1, L, S) we need to unsqueeze to (1, 1, L, S)\n",
    "    # in order to match the input for SDPA of (N, num_heads, L, S)\n",
    "    if attn_mask is not None:\n",
    "        if attn_mask.size(0) == 1 and attn_mask.dim() == 3:\n",
    "            attn_mask = attn_mask.unsqueeze(0)\n",
    "        else:\n",
    "            attn_mask = attn_mask.view(bsz, num_heads, -1, src_len)\n",
    "\n",
    "\n",
    "\n",
    "    if need_weights is False:\n",
    "        Q_dec1 = Q_dec.view(bsz, num_heads, tgt_len, head_dim)\n",
    "        K_dec1 = K_dec.view(bsz, num_heads, src_len, head_dim)\n",
    "        V_dec1 = V_dec.view(bsz, num_heads, src_len, head_dim)\n",
    "\n",
    "\n",
    "        L, S = Q_dec1.size(-2), K_dec1.size(-2)\n",
    "\n",
    "        scale_factor = 1 / math.sqrt(Q_dec1.size(-1)) \n",
    "        # scale_factor = 1\n",
    "        attn_bias = torch.zeros(L, S, dtype=Q_dec1.dtype)\n",
    "\n",
    "        if attn_mask is not None:\n",
    "            if attn_mask.dtype == torch.bool:\n",
    "                attn_mask.masked_fill_(attn_mask.logical_not(), float(\"-inf\"))\n",
    "            else:\n",
    "                attn_bias += attn_mask\n",
    "\n",
    "\n",
    "        attn_weight = Q_dec1 @ K_dec1.transpose(-2, -1) * scale_factor\n",
    "        attn_weight += attn_bias\n",
    "        attn_weight = torch.softmax(attn_weight, dim=-1)\n",
    "        \n",
    "        attn_output = attn_weight @ V_dec1\n",
    "\n",
    "        attn_output = attn_output.permute(2, 0, 1, 3).contiguous().view(bsz * tgt_len, embed_dim)\n",
    "\n",
    "        print(\"Decoder Self Attention = \")\n",
    "        print(attn_output)\n",
    "        print()\n",
    "\n",
    "        op_dec_1 = torch.matmul(attn_output, state_dict[\"transformer.decoder.layers.{}.self_attn.out_proj.weight\".format(layer_num)].t()) + state_dict[\"transformer.decoder.layers.{}.self_attn.out_proj.bias\".format(layer_num)]\n",
    "        attn_dec_output = op_dec_1.view(tgt_len, bsz, attn_output.size(1))\n",
    "\n",
    "        return attn_dec_output, None\n",
    "    \n",
    "    else:\n",
    "\n",
    "\n",
    "        B, Nt, E = Q_dec.shape\n",
    "\n",
    "        Q_dec_scaled = Q_dec / math.sqrt(E)\n",
    "\n",
    "\n",
    "        if attn_mask is not None:\n",
    "            temp_pdt_matrix_dec = torch.baddbmm(attn_mask, Q_dec_scaled, K_dec.transpose(-2, -1))\n",
    "        else:\n",
    "            temp_pdt_matrix_dec = torch.bmm(Q_dec_scaled, K_dec.transpose(-2, -1))\n",
    "            \n",
    "\n",
    "        # Obtaining the diagonal entries of the un-normalised product (K.Q_scaled)\n",
    "        pdt_matrix_dec = torch.diagonal(temp_pdt_matrix_dec, dim1=-2, dim2=-1).unsqueeze(-1)\n",
    "\n",
    "        print(\"Q_scaled @ Kt = \", pdt_matrix_dec)\n",
    "\n",
    "        attn_wt_matrix_dec = torch.nn.functional.softmax(pdt_matrix_dec, dim=-1)\n",
    "\n",
    "        attn_dec_output = torch.bmm(attn_wt_matrix_dec, V_dec)\n",
    "\n",
    "        attn_dec_output = attn_dec_output.transpose(0, 1).contiguous().view(tgt_len * bsz, embed_dim)\n",
    "\n",
    "        print(\"Decoder Attention output = \")\n",
    "        print(attn_wt_matrix_dec)\n",
    "        print()\n",
    "\n",
    "\n",
    "        op_dec_1 = torch.matmul(attn_dec_output, state_dict[\"transformer.decoder.layers.{}.self_attn.out_proj.weight\".format(layer_num)].t()) + state_dict[\"transformer.decoder.layers.{}.self_attn.out_proj.bias\".format(layer_num)]\n",
    "        attn_dec_output = op_dec_1.view(tgt_len, bsz, attn_dec_output.size(1))\n",
    "    \n",
    "\n",
    "        return attn_dec_output, attn_wt_matrix_dec\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# query_dec = key_dec = value_dec = pe_tgt_embeds \n",
    "\n",
    "# tgt_len, bsz, embed_dim = pe_src_embeds.shape\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "need_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q_dec_0 = \n",
      "tensor([[[-0.5715, -0.9803]],\n",
      "\n",
      "        [[-0.2668, -2.1467]],\n",
      "\n",
      "        [[ 0.6712,  1.4975]],\n",
      "\n",
      "        [[-1.1324, -1.3539]],\n",
      "\n",
      "        [[-1.1546, -1.6399]],\n",
      "\n",
      "        [[-0.3026, -2.6020]],\n",
      "\n",
      "        [[-0.9442, -1.7529]],\n",
      "\n",
      "        [[-0.1179, -2.8550]]])\n",
      "\n",
      "K_dec_0 = \n",
      "tensor([[[ 0.5695, -0.4099]],\n",
      "\n",
      "        [[ 1.4165,  1.8115]],\n",
      "\n",
      "        [[-0.6667,  2.4925]],\n",
      "\n",
      "        [[ 2.2106,  0.3611]],\n",
      "\n",
      "        [[ 0.8026, -0.7249]],\n",
      "\n",
      "        [[ 1.6418,  2.0580]],\n",
      "\n",
      "        [[ 0.9950, -1.1761]],\n",
      "\n",
      "        [[ 1.5863,  2.6246]]])\n",
      "\n",
      "V_dec_0 = \n",
      "tensor([[[-0.6472,  1.7969]],\n",
      "\n",
      "        [[-1.4070, -0.1967]],\n",
      "\n",
      "        [[ 0.9499, -1.1671]],\n",
      "\n",
      "        [[-4.2108, -1.5491]],\n",
      "\n",
      "        [[-1.0828,  2.1850]],\n",
      "\n",
      "        [[-1.5985, -0.3161]],\n",
      "\n",
      "        [[-1.1503,  2.9487]],\n",
      "\n",
      "        [[-1.0779,  0.0906]]])\n",
      "\n",
      "Decoder Self Attention = \n",
      "tensor([[-0.6472,  1.7969, -1.4070, -0.1967],\n",
      "        [ 0.9499, -1.1671, -4.2108, -1.5491],\n",
      "        [-1.0828,  2.1850, -1.5985, -0.3161],\n",
      "        [-1.1503,  2.9487, -1.0779,  0.0906]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# need_weights = True\n",
    "\n",
    "self_attn_dec, dec_sa_wts = decoder_block_self_attn_output(pe_tgt_embeds, state_dict, layer_num = 0, need_weights = need_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.7972, -0.8369, -2.9348,  1.6432],\n",
       "         [ 0.5649,  3.8928, -2.7056, -2.2094],\n",
       "         [-0.8494, -1.3390, -3.7335,  2.1888],\n",
       "         [-1.3322, -2.1464, -3.6756,  2.9461]]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "self_attn_dec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dec_post_self_attn(self_attn_dec, x, state_dict, layer_num):\n",
    "    output_dec_1 = self_attn_dec + x\n",
    "\n",
    "    linear_result_dec_1 = output_dec_1*state_dict[\"transformer.decoder.layers.{}.norm1.weight\".format(layer_num)] + state_dict[\"transformer.decoder.layers.{}.norm1.bias\".format(layer_num)]\n",
    "\n",
    "    layernorm_dec_1 = torch.nn.LayerNorm(normalized_shape=linear_result_dec_1.shape[2:])\n",
    "    linear_op_dec_1 = layernorm_dec_1(linear_result_dec_1)\n",
    "\n",
    "    # From torch's implementation\n",
    "    # print(linear_op_dec_1)\n",
    "\n",
    "    x = linear_result_dec_1\n",
    "\n",
    "    # Obtained layer norm weights and biases (learnable)\n",
    "    w = layernorm_dec_1.weight\n",
    "    b = layernorm_dec_1.bias\n",
    "\n",
    "    linear_result_dec_1f = w*x + b\n",
    "\n",
    "    epsilon = 1e-5  \n",
    "    mean = linear_result_dec_1f.mean(dim=-1, keepdim=True)\n",
    "    std = linear_result_dec_1f.std(dim=-1, unbiased=False, keepdim=True)\n",
    "    normalized_result_dec_1 = (linear_result_dec_1f - mean) / (std + epsilon) * w + b\n",
    "\n",
    "\n",
    "    print(\"Decoder_{} norm1(x + sa(x))\".format(layer_num))\n",
    "    print(normalized_result_dec_1)\n",
    "    print()\n",
    "\n",
    "    return normalized_result_dec_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoder_0 norm1(x + sa(x))\n",
      "tensor([[[-0.2967, -0.9738, -0.4032,  1.6737],\n",
      "         [ 0.0951,  1.2893, -1.5199,  0.1355],\n",
      "         [-0.1441, -1.0976, -0.3800,  1.6218],\n",
      "         [-0.3173, -1.1214, -0.1770,  1.6157]]], grad_fn=<AddBackward0>)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x_dec = dec_post_self_attn(self_attn_dec, pe_tgt_embeds, state_dict, layer_num = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross attention in decoder \n",
    "### query, key, value =  x, mem, mem \n",
    "\n",
    "### Cross attention between the encoder's final output and the decoder layer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.2967, -0.9738, -0.4032,  1.6737],\n",
       "         [ 0.0951,  1.2893, -1.5199,  0.1355],\n",
       "         [-0.1441, -1.0976, -0.3800,  1.6218],\n",
       "         [-0.3173, -1.1214, -0.1770,  1.6157]]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "memory = output_enc_final\n",
    "x_dec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoder_block_cross_attn_output(x_dec, memory, state_dict, layer_num, tgt_len, src_len, head_dim, memory_mask = None,need_weights = False):\n",
    "\n",
    "    query_dec_mha = x_dec\n",
    "    key_dec_mha, value_dec_mha = memory, memory\n",
    "\n",
    "\n",
    "    W_dec_mha = state_dict[\"transformer.decoder.layers.{}.multihead_attn.in_proj_weight\".format(layer_num)]\n",
    "    b_dec_mha = state_dict[\"transformer.decoder.layers.{}.multihead_attn.in_proj_bias\".format(layer_num)]\n",
    "\n",
    "\n",
    "    ########## CHANGE FROM SELF ATTENTION ############\n",
    "\n",
    "    W_q, W_k, W_v = W_dec_mha.chunk(3)\n",
    "\n",
    "    Q_dec_mha = query_dec_mha[0]@W_q.T\n",
    "\n",
    "    K_dec_mha = key_dec_mha[0]@W_k.T\n",
    "\n",
    "    V_dec_mha = value_dec_mha[0]@W_v.T\n",
    "\n",
    "    #################################################\n",
    "\n",
    "    K_dec_mha = K_dec_mha.unsqueeze(0)\n",
    "    V_dec_mha = V_dec_mha.unsqueeze(0)\n",
    "    Q_dec_mha = Q_dec_mha.unsqueeze(0)\n",
    "\n",
    "    Q_dec_mha = Q_dec_mha.reshape(tgt_len, bsz * num_heads, head_dim).transpose(0, 1)\n",
    "    K_dec_mha = K_dec_mha.reshape(tgt_len, bsz * num_heads, head_dim).transpose(0, 1)\n",
    "    V_dec_mha = V_dec_mha.reshape(tgt_len, bsz * num_heads, head_dim).transpose(0, 1)\n",
    "\n",
    "    print(\"Q_dec_{} = \".format(layer_num))\n",
    "    print(Q_dec_mha)\n",
    "    print()\n",
    "\n",
    "    print(\"K_dec_{} = \".format(layer_num))\n",
    "    print(K_dec_mha)\n",
    "    print()\n",
    "\n",
    "    print(\"V_dec_{} = \".format(layer_num))\n",
    "    print(V_dec_mha)\n",
    "    print()\n",
    "\n",
    "\n",
    "    attn_mask = memory_mask\n",
    "    if attn_mask is not None:\n",
    "\n",
    "        # Ensuring attn_mask's dim is 3\n",
    "        if attn_mask.dim() == 2:\n",
    "            correct_2d_size = (tgt_len, src_len)\n",
    "            if attn_mask.shape != correct_2d_size:\n",
    "                raise RuntimeError(f\"The shape of the 2D attn_mask is {attn_mask.shape}, but should be {correct_2d_size}.\")\n",
    "            attn_mask = attn_mask.unsqueeze(0)\n",
    "        elif attn_mask.dim() == 3:\n",
    "            correct_3d_size = (bsz * num_heads, tgt_len, src_len)\n",
    "            if attn_mask.shape != correct_3d_size:\n",
    "                raise RuntimeError(f\"The shape of the 3D attn_mask is {attn_mask.shape}, but should be {correct_3d_size}.\")\n",
    "        else:\n",
    "            raise RuntimeError(f\"attn_mask's dimension {attn_mask.dim()} is not supported\")\n",
    "\n",
    "    # attn_mask can be either (L,S) or (N*num_heads, L, S)\n",
    "    # if attn_mask's shape is (1, L, S) we need to unsqueeze to (1, 1, L, S)\n",
    "    # in order to match the input for SDPA of (N, num_heads, L, S)\n",
    "    if attn_mask is not None:\n",
    "        if attn_mask.size(0) == 1 and attn_mask.dim() == 3:\n",
    "            attn_mask = attn_mask.unsqueeze(0)\n",
    "        else:\n",
    "            attn_mask = attn_mask.view(bsz, num_heads, -1, src_len)\n",
    "\n",
    "\n",
    "    if need_weights is False:\n",
    "        Q_dec_mha1 = Q_dec_mha.view(bsz, num_heads, tgt_len, head_dim)\n",
    "        K_dec_mha1 = K_dec_mha.view(bsz, num_heads, src_len, head_dim)\n",
    "        V_dec_mha1 = V_dec_mha.view(bsz, num_heads, src_len, head_dim)\n",
    "\n",
    "        L, S = Q_dec_mha1.size(-2), K_dec_mha1.size(-2)\n",
    "\n",
    "        scale_factor = 1 / math.sqrt(Q_dec_mha1.size(-1)) \n",
    "        # scale_factor = 1\n",
    "\n",
    "        attn_bias = torch.zeros(L, S, dtype=Q_dec_mha1.dtype)\n",
    "        if attn_mask is not None:\n",
    "            if attn_mask.dtype == torch.bool:\n",
    "                attn_mask.masked_fill_(attn_mask.logical_not(), float(\"-inf\"))\n",
    "            else:\n",
    "                attn_bias += attn_mask\n",
    "\n",
    "\n",
    "        attn_weight = Q_dec_mha1 @ K_dec_mha1.transpose(-2, -1) * scale_factor\n",
    "        attn_weight += attn_bias\n",
    "        attn_weight = torch.softmax(attn_weight, dim=-1)\n",
    "        attn_output_dec_mha = attn_weight @ V_dec_mha1\n",
    "\n",
    "        attn_output_dec_mha = attn_output_dec_mha.permute(2, 0, 1, 3).view(bsz * tgt_len, embed_dim)\n",
    "        \n",
    "        print(\"Cross attention in decoder_{}\".format(layer_num))\n",
    "        print(attn_output_dec_mha)\n",
    "        print()\n",
    "\n",
    "        op_dec_mha_1 = torch.matmul(attn_output_dec_mha, state_dict[\"transformer.decoder.layers.{}.multihead_attn.out_proj.weight\".format(layer_num)].t()) + state_dict[\"transformer.decoder.layers.{}.multihead_attn.out_proj.bias\".format(layer_num)]\n",
    "\n",
    "        attn_dec_mha_output = op_dec_mha_1.view(tgt_len, bsz, attn_output_dec_mha.size(1))\n",
    "\n",
    "        return attn_dec_mha_output, None\n",
    "    \n",
    "    else: \n",
    "\n",
    "        B, Nt, E = Q_dec_mha.shape\n",
    "\n",
    "        Q_dec_mha_scaled = Q_dec_mha / math.sqrt(E)\n",
    "\n",
    "\n",
    "        if attn_mask is not None:\n",
    "            temp_pdt_matrix_dec_mha = torch.baddbmm(attn_mask, Q_dec_mha_scaled, K_dec_mha.transpose(-2, -1))\n",
    "        else:\n",
    "            temp_pdt_matrix_dec_mha = torch.bmm(Q_dec_mha_scaled, K_dec_mha.transpose(-2, -1))\n",
    "            \n",
    "\n",
    "        # Obtaining the diagonal entries of the un-normalised product (K.Q_scaled)\n",
    "        pdt_matrix_dec_mha = torch.diagonal(temp_pdt_matrix_dec_mha, dim1=-2, dim2=-1).unsqueeze(-1)\n",
    "\n",
    "        print(\"Q_scaled @ Kt = \", pdt_matrix_dec_mha)\n",
    "\n",
    "        attn_wt_matrix_dec_mha = torch.nn.functional.softmax(pdt_matrix_dec_mha, dim=-1)\n",
    "\n",
    "        attn_dec_mha_output = torch.bmm(attn_wt_matrix_dec_mha, V_dec_mha)\n",
    "\n",
    "        attn_dec_mha_output = attn_dec_mha_output.transpose(0, 1).contiguous().view(tgt_len * bsz, embed_dim)\n",
    "\n",
    "        print(\"Decoder mha Attention output = \")\n",
    "        print(attn_dec_mha_output)\n",
    "        print()\n",
    "\n",
    "\n",
    "        op_dec_mha = torch.matmul(attn_dec_mha_output, state_dict[\"transformer.decoder.layers.{}.multihead_attn.out_proj.weight\".format(layer_num)].t()) + state_dict[\"transformer.decoder.layers.{}.multihead_attn.out_proj.bias\".format(layer_num)]\n",
    "        attn_dec_output = op_dec_mha.view(tgt_len, bsz, attn_dec_mha_output.size(1))\n",
    "\n",
    "\n",
    "        return attn_dec_output , attn_wt_matrix_dec_mha\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q_dec_0 = \n",
      "tensor([[[ 1.1551, -0.5112]],\n",
      "\n",
      "        [[ 0.2386, -0.9911]],\n",
      "\n",
      "        [[ 0.2198, -0.2264]],\n",
      "\n",
      "        [[-0.3958,  0.5742]],\n",
      "\n",
      "        [[ 1.1903, -0.4910]],\n",
      "\n",
      "        [[ 0.3076, -1.0038]],\n",
      "\n",
      "        [[ 1.0901, -0.4674]],\n",
      "\n",
      "        [[ 0.2824, -1.0465]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "K_dec_0 = \n",
      "tensor([[[-0.0704,  0.2770]],\n",
      "\n",
      "        [[ 1.2327, -0.4538]],\n",
      "\n",
      "        [[-0.1660,  0.3082]],\n",
      "\n",
      "        [[ 0.8527, -0.2363]],\n",
      "\n",
      "        [[-0.4686, -0.0289]],\n",
      "\n",
      "        [[ 0.3370, -0.2864]],\n",
      "\n",
      "        [[-0.1386,  0.3806]],\n",
      "\n",
      "        [[ 0.6938, -0.0846]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "V_dec_0 = \n",
      "tensor([[[ 0.1339,  0.0898]],\n",
      "\n",
      "        [[ 0.1141,  0.5882]],\n",
      "\n",
      "        [[ 0.0868,  0.4066]],\n",
      "\n",
      "        [[-0.0548,  0.3838]],\n",
      "\n",
      "        [[ 0.3645,  0.3931]],\n",
      "\n",
      "        [[-0.1983, -0.0611]],\n",
      "\n",
      "        [[-0.0098,  0.5592]],\n",
      "\n",
      "        [[-0.1135,  0.3421]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "Cross attention in decoder_0\n",
      "tensor([[ 0.1339,  0.0898,  0.1141,  0.5882],\n",
      "        [ 0.0868,  0.4066, -0.0548,  0.3838],\n",
      "        [ 0.3645,  0.3931, -0.1983, -0.0611],\n",
      "        [-0.0098,  0.5592, -0.1135,  0.3421]], grad_fn=<ViewBackward0>)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# need_weights = True\n",
    "\n",
    "attn_dec_mha_output, attn_dec_mha_wts = decoder_block_cross_attn_output(x_dec, memory, state_dict, layer_num = 0, tgt_len = tgt_len, src_len = src_len, head_dim = head_dim, need_weights = need_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoder_block_post_attn_output(x_dec, attn_dec_mha_output, state_dict, layer_num):\n",
    "\n",
    "    output_dec_2 = attn_dec_mha_output + x_dec\n",
    "\n",
    "    linear_result_dec_2 = output_dec_2*state_dict[\"transformer.decoder.layers.{}.norm2.weight\".format(layer_num)] + state_dict[\"transformer.decoder.layers.{}.norm2.bias\".format(layer_num)]\n",
    "\n",
    "    # Layer normalization from Torch's implementation \n",
    "    layernorm_dec_2 = torch.nn.LayerNorm(normalized_shape=linear_result_dec_2.shape[2:])\n",
    "    linear_op_dec_2 = layernorm_dec_2(linear_result_dec_2)\n",
    "\n",
    "    x = linear_result_dec_2\n",
    "    w = layernorm_dec_2.weight\n",
    "    b = layernorm_dec_2.bias\n",
    "\n",
    "    linear_result_dec_2f = w*x + b\n",
    "\n",
    "    epsilon = 1e-5  \n",
    "    mean = linear_result_dec_2f.mean(dim=-1, keepdim=True)\n",
    "    std = linear_result_dec_2f.std(dim=-1, unbiased=False, keepdim=True)\n",
    "    normalized_result_dec_2 = (linear_result_dec_2f - mean) / (std + epsilon) * w + b\n",
    "\n",
    "    print(\"norm2(x' + mha(x', mem)), \\n where x' = Decoder_curr_layer norm1(x + sa(x))\")\n",
    "    print(normalized_result_dec_2)\n",
    "    print(\"\\n\\n\")\n",
    "\n",
    "    x_dec2_norm = normalized_result_dec_2\n",
    "\n",
    "    op_dec_1 = torch.matmul(x_dec2_norm, state_dict[\"transformer.decoder.layers.{}.linear1.weight\".format(layer_num)].t()) + state_dict[\"transformer.decoder.layers.{}.linear1.bias\".format(layer_num)]\n",
    "    op_dec_1_relu = torch.nn.functional.relu(op_dec_1)\n",
    "    op_dec_2 = torch.matmul(op_dec_1_relu, state_dict[\"transformer.decoder.layers.{}.linear2.weight\".format(layer_num)].t()) + state_dict[\"transformer.decoder.layers.{}.linear2.bias\".format(layer_num)]\n",
    "\n",
    "    ff_dec = op_dec_2\n",
    "\n",
    "    x_dec3_unorm = x_dec2_norm + ff_dec\n",
    "\n",
    "    linear_result_dec_3 = x_dec3_unorm*state_dict[\"transformer.decoder.layers.0.norm3.weight\"] + state_dict[\"transformer.decoder.layers.0.norm3.bias\"]\n",
    "\n",
    "    # Layer normalization from Torch's implementation \n",
    "    layernorm_dec_3 = torch.nn.LayerNorm(normalized_shape=linear_result_dec_3.shape[2:])\n",
    "    linear_op_dec_3 = layernorm_dec_3(linear_result_dec_3)\n",
    "\n",
    "    # From torch's implementation\n",
    "    x = linear_result_dec_3\n",
    "\n",
    "    # Obtained layer norm weights and biases (learnable)\n",
    "    w = layernorm_dec_3.weight\n",
    "    b = layernorm_dec_3.bias\n",
    "\n",
    "    linear_result_dec_3f = w*x + b\n",
    "\n",
    "    epsilon = 1e-5  \n",
    "    mean = linear_result_dec_3f.mean(dim=-1, keepdim=True)\n",
    "    std = linear_result_dec_3f.std(dim=-1, unbiased=False, keepdim=True)\n",
    "    normalized_result_dec_3 = (linear_result_dec_3f - mean) / (std + epsilon) * w + b\n",
    "\n",
    "    print(\"norm3(x'' + ff(x'')) \\n where, x'' = Decoder_curr_layer norm2(x' + mha(x'))\")\n",
    "    print(normalized_result_dec_3)\n",
    "    print()\n",
    "\n",
    "    return normalized_result_dec_3\n",
    "\n",
    "def feef_fwd_transformer(dec_output_final, state_dict):\n",
    "\n",
    "    final_op = dec_output_final@state_dict[\"fc.weight\"].T + state_dict[\"fc.bias\"]\n",
    "\n",
    "    return final_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "norm2(x' + mha(x', mem)), \n",
      " where x' = Decoder_curr_layer norm1(x + sa(x))\n",
      "tensor([[[-0.0528, -1.2301, -0.2699,  1.5528],\n",
      "         [ 0.2214,  1.0573, -1.6432,  0.3646],\n",
      "         [-0.2337, -0.7955, -0.6650,  1.6942],\n",
      "         [-0.1521, -1.0451, -0.4425,  1.6397]]], grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "\n",
      "norm3(x'' + ff(x'')) \n",
      " where, x'' = Decoder_curr_layer norm2(x' + mha(x'))\n",
      "tensor([[[ 0.0823, -1.1172, -0.5336,  1.5685],\n",
      "         [ 0.3409,  0.7710, -1.7116,  0.5997],\n",
      "         [-0.0431, -0.7461, -0.8564,  1.6456],\n",
      "         [ 0.0110, -0.9569, -0.6753,  1.6212]]], grad_fn=<AddBackward0>)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final_attn_op_decoder = decoder_block_post_attn_output(x_dec, attn_dec_mha_output, state_dict, layer_num = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_op = feef_fwd_transformer(final_attn_op_decoder, state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.8690,  0.2952,  0.6433,  0.9624, -0.4184,  1.0310, -0.2743,\n",
       "           0.2156,  0.6588, -0.5275],\n",
       "         [-0.1098,  0.0060, -0.0221,  0.2819, -0.9641,  0.5592, -0.5691,\n",
       "          -0.1397,  0.5262,  0.3902],\n",
       "         [-0.7065,  0.2620,  0.5209,  0.8902, -0.5664,  1.0184, -0.2785,\n",
       "           0.0787,  0.5858, -0.3458],\n",
       "         [-0.7971,  0.2843,  0.5912,  0.9347, -0.4833,  1.0307, -0.2683,\n",
       "           0.1500,  0.6215, -0.4467]]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_op"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "########################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examaple :- \n",
    "\n",
    "Transformer with 3 encoders and 3 decoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source embeddings :- \n",
      "\n",
      "tensor([[[-0.2205,  1.7253, -0.2514,  0.9972],\n",
      "         [ 0.1335, -0.5650, -0.7537,  0.0105],\n",
      "         [-0.8226,  0.9250,  1.7916, -0.4149],\n",
      "         [ 0.2400, -2.0239, -0.6187, -0.7110]]], grad_fn=<EmbeddingBackward0>)\n",
      "\n",
      "Target embeddings :- \n",
      "\n",
      "tensor([[[ 0.0958, -0.3552,  1.2360, -0.8826],\n",
      "         [ 0.0204,  1.1688, -0.0480, -1.3756],\n",
      "         [-0.1728,  0.3034, -1.1816, -1.1548],\n",
      "         [-0.1728,  0.3034, -1.1816, -1.1548]]], grad_fn=<EmbeddingBackward0>)\n",
      "\n",
      "Positional encoded source embeddings:\n",
      "tensor([[[-0.2205,  3.7253,  1.7486,  4.9972],\n",
      "         [ 0.9750,  0.9753,  1.2563,  4.0105],\n",
      "         [ 0.0867,  1.5088,  3.8116,  3.5849],\n",
      "         [ 0.3811, -2.0139,  1.4113,  3.2885]]], grad_fn=<AddBackward0>)\n",
      "\n",
      "Positional encoded target embeddings:\n",
      "tensor([[[ 1.0958,  0.6448,  4.2360,  3.1174],\n",
      "         [ 1.8619,  1.7091,  2.9620,  2.6244],\n",
      "         [ 1.7365, -0.1128,  1.8384,  2.8450],\n",
      "         [ 0.9683, -0.6866,  1.8484,  2.8448]]], grad_fn=<AddBackward0>)\n",
      "\n",
      "query =  tensor([[[-0.2205,  3.7253,  1.7486,  4.9972],\n",
      "         [ 0.9750,  0.9753,  1.2563,  4.0105],\n",
      "         [ 0.0867,  1.5088,  3.8116,  3.5849],\n",
      "         [ 0.3811, -2.0139,  1.4113,  3.2885]]], grad_fn=<AddBackward0>)\n",
      "query shape =  torch.Size([1, 4, 4])\n",
      "\n",
      " _in_projection_packed \n",
      "\n",
      "Q :-  tensor([[[ 0.6200, -0.9966, -0.1113,  1.0026],\n",
      "         [-0.1846, -0.6229,  0.4113, -0.8594],\n",
      "         [-1.4594, -0.9644,  0.6233, -0.4554],\n",
      "         [-1.3783, -0.2546,  1.3028, -2.3128]]], grad_fn=<SelectBackward0>)\n",
      "\n",
      "K :-  tensor([[[ 1.9172, -1.6883,  0.3543,  3.3778],\n",
      "         [ 1.4162, -1.2422,  0.8147,  1.4920],\n",
      "         [ 0.0826, -1.0527, -0.6941,  2.5530],\n",
      "         [ 0.7099, -1.9596,  0.4214,  0.1708]]], grad_fn=<SelectBackward0>)\n",
      "\n",
      "V :-  tensor([[[ 3.0345, -4.3447,  4.4862, -0.7491],\n",
      "         [ 2.3662, -3.3168,  2.7636, -0.3210],\n",
      "         [ 3.1573, -4.3181,  2.9714,  0.0482],\n",
      "         [ 2.5920, -2.3741,  0.7878,  0.8075]]], grad_fn=<SelectBackward0>)\n",
      "\n",
      "tgt_len =  1\n",
      "bsz =  4\n",
      "num_heads =  2\n",
      "head_dim =  2\n",
      "\n",
      "tensor([[[ 0.6200, -0.9966, -0.1113,  1.0026],\n",
      "         [-0.1846, -0.6229,  0.4113, -0.8594],\n",
      "         [-1.4594, -0.9644,  0.6233, -0.4554],\n",
      "         [-1.3783, -0.2546,  1.3028, -2.3128]]], grad_fn=<SelectBackward0>) torch.Size([1, 4, 4])\n",
      "Q reshaped =  tensor([[[ 0.6200, -0.9966]],\n",
      "\n",
      "        [[-0.1113,  1.0026]],\n",
      "\n",
      "        [[-0.1846, -0.6229]],\n",
      "\n",
      "        [[ 0.4113, -0.8594]],\n",
      "\n",
      "        [[-1.4594, -0.9644]],\n",
      "\n",
      "        [[ 0.6233, -0.4554]],\n",
      "\n",
      "        [[-1.3783, -0.2546]],\n",
      "\n",
      "        [[ 1.3028, -2.3128]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "K reshaped =  tensor([[[ 1.9172, -1.6883]],\n",
      "\n",
      "        [[ 0.3543,  3.3778]],\n",
      "\n",
      "        [[ 1.4162, -1.2422]],\n",
      "\n",
      "        [[ 0.8147,  1.4920]],\n",
      "\n",
      "        [[ 0.0826, -1.0527]],\n",
      "\n",
      "        [[-0.6941,  2.5530]],\n",
      "\n",
      "        [[ 0.7099, -1.9596]],\n",
      "\n",
      "        [[ 0.4214,  0.1708]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "V reshaped =  tensor([[[ 3.0345, -4.3447]],\n",
      "\n",
      "        [[ 4.4862, -0.7491]],\n",
      "\n",
      "        [[ 2.3662, -3.3168]],\n",
      "\n",
      "        [[ 2.7636, -0.3210]],\n",
      "\n",
      "        [[ 3.1573, -4.3181]],\n",
      "\n",
      "        [[ 2.9714,  0.0482]],\n",
      "\n",
      "        [[ 2.5920, -2.3741]],\n",
      "\n",
      "        [[ 0.7878,  0.8075]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "torch.Size([4, 2, 1, 2]) torch.Size([4, 2, 1, 2]) torch.Size([4, 2, 1, 2])\n",
      "\n",
      "scaled_dot_product_attention\n",
      "attn_output = \n",
      "tensor([[ 3.0345, -4.3447,  4.4862, -0.7491],\n",
      "        [ 2.3662, -3.3168,  2.7636, -0.3210],\n",
      "        [ 3.1573, -4.3181,  2.9714,  0.0482],\n",
      "        [ 2.5920, -2.3741,  0.7878,  0.8075]], grad_fn=<ViewBackward0>)\n",
      "\n",
      "x + self_atten(x) = \n",
      "tensor([[[ 0.3759, 10.1658, -0.2625,  9.7144],\n",
      "         [ 1.3622,  5.3116, -0.1058,  7.1487],\n",
      "         [ 0.5270,  6.4245,  2.3854,  7.1627],\n",
      "         [ 0.7453, -0.1984,  1.3652,  5.1093]]], grad_fn=<AddBackward0>)\n",
      "\n",
      "norm1(x + self_atten(x)) = \n",
      "tensor([[[-0.9339,  1.0440, -1.0629,  0.9528],\n",
      "         [-0.7074,  0.6443, -1.2099,  1.2730],\n",
      "         [-1.3032,  0.8330, -0.6301,  1.1004],\n",
      "         [-0.5013, -0.9697, -0.1936,  1.6646]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "\n",
      "feed_fwd_op =  tensor([[[-0.1663,  0.2646, -0.1747,  0.0518],\n",
      "         [-0.1308,  0.2821, -0.1690,  0.0694],\n",
      "         [-0.1481,  0.2724, -0.1941,  0.0482],\n",
      "         [-0.0589,  0.3677, -0.1883,  0.0552]]], grad_fn=<ViewBackward0>)\n",
      "\n",
      "norm2(norm1(x + self_atten(x)) + feed_fwd_op) = \n",
      "tensor([[[-0.9361,  1.1250, -1.0537,  0.8648],\n",
      "         [-0.7420,  0.7963, -1.2133,  1.1590],\n",
      "         [-1.2530,  0.9626, -0.7096,  1.0000],\n",
      "         [-0.6221, -0.6651, -0.4386,  1.7258]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "\n",
      "##################################\n",
      "query =  tensor([[[-0.9361,  1.1250, -1.0537,  0.8648],\n",
      "         [-0.7420,  0.7963, -1.2133,  1.1590],\n",
      "         [-1.2530,  0.9626, -0.7096,  1.0000],\n",
      "         [-0.6221, -0.6651, -0.4386,  1.7258]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "query shape =  torch.Size([1, 4, 4])\n",
      "\n",
      " _in_projection_packed \n",
      "\n",
      "Q :-  tensor([[[ 0.3627,  0.2891,  0.1520, -1.9940],\n",
      "         [ 0.3101, -0.0514, -0.1626, -1.9222],\n",
      "         [ 0.6701,  0.3549,  0.3880, -1.8755],\n",
      "         [ 0.6906, -0.7477, -0.4020, -0.7598]]], grad_fn=<SelectBackward0>)\n",
      "\n",
      "K :-  tensor([[[ 0.4935, -0.7928, -0.7312,  1.2475],\n",
      "         [ 0.4188, -0.6940, -0.7778,  1.1314],\n",
      "         [ 0.5690, -1.0316, -0.6252,  1.4893],\n",
      "         [ 0.2258, -0.6703, -0.3773,  0.9025]]], grad_fn=<SelectBackward0>)\n",
      "\n",
      "V :-  tensor([[[ 0.4030, -0.8576, -1.5052, -0.5695],\n",
      "         [ 0.4024, -0.5434, -1.3514, -0.2802],\n",
      "         [ 0.4675, -0.8433, -1.5064, -0.5631],\n",
      "         [ 0.3788,  0.4803, -0.4493,  0.5641]]], grad_fn=<SelectBackward0>)\n",
      "\n",
      "tgt_len =  1\n",
      "bsz =  4\n",
      "num_heads =  2\n",
      "head_dim =  2\n",
      "\n",
      "tensor([[[ 0.3627,  0.2891,  0.1520, -1.9940],\n",
      "         [ 0.3101, -0.0514, -0.1626, -1.9222],\n",
      "         [ 0.6701,  0.3549,  0.3880, -1.8755],\n",
      "         [ 0.6906, -0.7477, -0.4020, -0.7598]]], grad_fn=<SelectBackward0>) torch.Size([1, 4, 4])\n",
      "Q reshaped =  tensor([[[ 0.3627,  0.2891]],\n",
      "\n",
      "        [[ 0.1520, -1.9940]],\n",
      "\n",
      "        [[ 0.3101, -0.0514]],\n",
      "\n",
      "        [[-0.1626, -1.9222]],\n",
      "\n",
      "        [[ 0.6701,  0.3549]],\n",
      "\n",
      "        [[ 0.3880, -1.8755]],\n",
      "\n",
      "        [[ 0.6906, -0.7477]],\n",
      "\n",
      "        [[-0.4020, -0.7598]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "K reshaped =  tensor([[[ 0.4935, -0.7928]],\n",
      "\n",
      "        [[-0.7312,  1.2475]],\n",
      "\n",
      "        [[ 0.4188, -0.6940]],\n",
      "\n",
      "        [[-0.7778,  1.1314]],\n",
      "\n",
      "        [[ 0.5690, -1.0316]],\n",
      "\n",
      "        [[-0.6252,  1.4893]],\n",
      "\n",
      "        [[ 0.2258, -0.6703]],\n",
      "\n",
      "        [[-0.3773,  0.9025]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "V reshaped =  tensor([[[ 0.4030, -0.8576]],\n",
      "\n",
      "        [[-1.5052, -0.5695]],\n",
      "\n",
      "        [[ 0.4024, -0.5434]],\n",
      "\n",
      "        [[-1.3514, -0.2802]],\n",
      "\n",
      "        [[ 0.4675, -0.8433]],\n",
      "\n",
      "        [[-1.5064, -0.5631]],\n",
      "\n",
      "        [[ 0.3788,  0.4803]],\n",
      "\n",
      "        [[-0.4493,  0.5641]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "torch.Size([4, 2, 1, 2]) torch.Size([4, 2, 1, 2]) torch.Size([4, 2, 1, 2])\n",
      "\n",
      "scaled_dot_product_attention\n",
      "attn_output = \n",
      "tensor([[ 0.4030, -0.8576, -1.5052, -0.5695],\n",
      "        [ 0.4024, -0.5434, -1.3514, -0.2802],\n",
      "        [ 0.4675, -0.8433, -1.5064, -0.5631],\n",
      "        [ 0.3788,  0.4803, -0.4493,  0.5641]], grad_fn=<ViewBackward0>)\n",
      "\n",
      "x + self_atten(x) = \n",
      "tensor([[[-1.6024, -0.0855, -0.8318,  2.0456],\n",
      "         [-1.1145, -0.1943, -0.8227,  2.1095],\n",
      "         [-1.8601, -0.1886, -0.4596,  2.1959],\n",
      "         [ 0.0045, -0.6032,  0.2809,  1.7661]]], grad_fn=<AddBackward0>)\n",
      "\n",
      "norm1(x + self_atten(x)) = \n",
      "tensor([[[-1.0913,  0.0243, -0.5246,  1.5916],\n",
      "         [-0.8763, -0.1492, -0.6457,  1.6712],\n",
      "         [-1.2221, -0.0757, -0.2616,  1.5595],\n",
      "         [-0.4103, -1.1077, -0.0931,  1.6112]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "\n",
      "feed_fwd_op =  tensor([[[ 0.4470, -0.5749, -0.1584, -0.0712],\n",
      "         [ 0.4435, -0.5669, -0.1590, -0.0966],\n",
      "         [ 0.4216, -0.5813, -0.1474, -0.0723],\n",
      "         [ 0.3456, -0.5575, -0.0970, -0.1832]]], grad_fn=<ViewBackward0>)\n",
      "\n",
      "norm2(norm1(x + self_atten(x)) + feed_fwd_op) = \n",
      "tensor([[[-0.5963, -0.4956, -0.6379,  1.7297],\n",
      "         [-0.3472, -0.6382, -0.7293,  1.7147],\n",
      "         [-0.7637, -0.6084, -0.3400,  1.7120],\n",
      "         [ 0.0533, -1.4090, -0.0613,  1.4170]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "\n",
      "##################################\n",
      "query =  tensor([[[-0.5963, -0.4956, -0.6379,  1.7297],\n",
      "         [-0.3472, -0.6382, -0.7293,  1.7147],\n",
      "         [-0.7637, -0.6084, -0.3400,  1.7120],\n",
      "         [ 0.0533, -1.4090, -0.0613,  1.4170]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "query shape =  torch.Size([1, 4, 4])\n",
      "\n",
      " _in_projection_packed \n",
      "\n",
      "Q :-  tensor([[[ 0.0405,  0.5774, -0.2596,  0.5393],\n",
      "         [ 0.1239,  0.6862, -0.3060,  0.4950],\n",
      "         [ 0.1620,  0.5662, -0.1612,  0.4365],\n",
      "         [ 0.7716,  0.9559, -0.1349, -0.0280]]], grad_fn=<SelectBackward0>)\n",
      "\n",
      "K :-  tensor([[[ 0.0710,  0.5148, -1.2144,  0.3167],\n",
      "         [ 0.1238,  0.3417, -1.1581,  0.3155],\n",
      "         [ 0.0116,  0.5938, -1.1497,  0.1432],\n",
      "         [ 0.1141, -0.0613, -0.6354, -0.2518]]], grad_fn=<SelectBackward0>)\n",
      "\n",
      "V :-  tensor([[[0.8298, 0.5688, 0.2635, 0.6075],\n",
      "         [0.8487, 0.6739, 0.1504, 0.7465],\n",
      "         [0.9210, 0.4544, 0.4513, 0.4206],\n",
      "         [1.0978, 0.6180, 0.3093, 0.5803]]], grad_fn=<SelectBackward0>)\n",
      "\n",
      "tgt_len =  1\n",
      "bsz =  4\n",
      "num_heads =  2\n",
      "head_dim =  2\n",
      "\n",
      "tensor([[[ 0.0405,  0.5774, -0.2596,  0.5393],\n",
      "         [ 0.1239,  0.6862, -0.3060,  0.4950],\n",
      "         [ 0.1620,  0.5662, -0.1612,  0.4365],\n",
      "         [ 0.7716,  0.9559, -0.1349, -0.0280]]], grad_fn=<SelectBackward0>) torch.Size([1, 4, 4])\n",
      "Q reshaped =  tensor([[[ 0.0405,  0.5774]],\n",
      "\n",
      "        [[-0.2596,  0.5393]],\n",
      "\n",
      "        [[ 0.1239,  0.6862]],\n",
      "\n",
      "        [[-0.3060,  0.4950]],\n",
      "\n",
      "        [[ 0.1620,  0.5662]],\n",
      "\n",
      "        [[-0.1612,  0.4365]],\n",
      "\n",
      "        [[ 0.7716,  0.9559]],\n",
      "\n",
      "        [[-0.1349, -0.0280]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "K reshaped =  tensor([[[ 0.0710,  0.5148]],\n",
      "\n",
      "        [[-1.2144,  0.3167]],\n",
      "\n",
      "        [[ 0.1238,  0.3417]],\n",
      "\n",
      "        [[-1.1581,  0.3155]],\n",
      "\n",
      "        [[ 0.0116,  0.5938]],\n",
      "\n",
      "        [[-1.1497,  0.1432]],\n",
      "\n",
      "        [[ 0.1141, -0.0613]],\n",
      "\n",
      "        [[-0.6354, -0.2518]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "V reshaped =  tensor([[[0.8298, 0.5688]],\n",
      "\n",
      "        [[0.2635, 0.6075]],\n",
      "\n",
      "        [[0.8487, 0.6739]],\n",
      "\n",
      "        [[0.1504, 0.7465]],\n",
      "\n",
      "        [[0.9210, 0.4544]],\n",
      "\n",
      "        [[0.4513, 0.4206]],\n",
      "\n",
      "        [[1.0978, 0.6180]],\n",
      "\n",
      "        [[0.3093, 0.5803]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "torch.Size([4, 2, 1, 2]) torch.Size([4, 2, 1, 2]) torch.Size([4, 2, 1, 2])\n",
      "\n",
      "scaled_dot_product_attention\n",
      "attn_output = \n",
      "tensor([[0.8298, 0.5688, 0.2635, 0.6075],\n",
      "        [0.8487, 0.6739, 0.1504, 0.7465],\n",
      "        [0.9210, 0.4544, 0.4513, 0.4206],\n",
      "        [1.0978, 0.6180, 0.3093, 0.5803]], grad_fn=<ViewBackward0>)\n",
      "\n",
      "x + self_atten(x) = \n",
      "tensor([[[-0.6962, -0.3005, -0.8981,  1.6969],\n",
      "         [-0.5437, -0.5735, -1.1419,  1.7736],\n",
      "         [-0.7530, -0.1603, -0.3758,  1.5299],\n",
      "         [-0.1229, -1.0374, -0.2774,  1.3355]]], grad_fn=<AddBackward0>)\n",
      "\n",
      "norm1(x + self_atten(x)) = \n",
      "tensor([[[-0.6273, -0.2435, -0.8232,  1.6940],\n",
      "         [-0.3771, -0.4038, -0.9114,  1.6923],\n",
      "         [-0.9298, -0.2521, -0.4984,  1.6803],\n",
      "         [-0.1133, -1.1784, -0.2933,  1.5851]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "\n",
      "feed_fwd_op =  tensor([[[-0.0765, -0.2176, -0.0241, -0.0379],\n",
      "         [-0.0726, -0.2200, -0.0284, -0.0344],\n",
      "         [-0.0856, -0.2137, -0.0139, -0.0377],\n",
      "         [-0.0755, -0.2121, -0.0265, -0.0180]]], grad_fn=<ViewBackward0>)\n",
      "\n",
      "norm2(norm1(x + self_atten(x)) + feed_fwd_op) = \n",
      "tensor([[[-0.6045, -0.3659, -0.7456,  1.7160],\n",
      "         [-0.3525, -0.5226, -0.8313,  1.7063],\n",
      "         [-0.9077, -0.3700, -0.4154,  1.6931],\n",
      "         [-0.0998, -1.2328, -0.2232,  1.5558]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "\n",
      "##################################\n",
      "\n",
      "################### Transformer Decoder start #####################\n",
      "\n",
      "query =  tensor([[[ 1.0958,  0.6448,  4.2360,  3.1174],\n",
      "         [ 1.8619,  1.7091,  2.9620,  2.6244],\n",
      "         [ 1.7365, -0.1128,  1.8384,  2.8450],\n",
      "         [ 0.9683, -0.6866,  1.8484,  2.8448]]], grad_fn=<AddBackward0>)\n",
      "query shape =  torch.Size([1, 4, 4])\n",
      "\n",
      " _in_projection_packed \n",
      "\n",
      "Q :-  tensor([[[-3.1998,  2.1538, -1.0128,  0.7383],\n",
      "         [-2.6388,  1.2949, -0.3135,  0.9396],\n",
      "         [-2.5799,  0.7808, -0.3913, -0.4072],\n",
      "         [-2.3275,  1.0272, -0.7647, -0.5439]]], grad_fn=<SelectBackward0>)\n",
      "\n",
      "K :-  tensor([[[ 1.0097,  1.4010,  0.8216, -3.9416],\n",
      "         [ 1.1088,  2.1519,  0.0169, -3.9404],\n",
      "         [ 0.6041,  1.1918, -0.2197, -2.3607],\n",
      "         [ 0.5968,  0.5446,  0.1456, -1.7790]]], grad_fn=<SelectBackward0>)\n",
      "\n",
      "V :-  tensor([[[-2.4218,  1.2453,  0.6700,  1.9331],\n",
      "         [-1.5839,  0.7873,  0.8833,  1.9419],\n",
      "         [-2.1300,  0.6044,  0.7146,  0.7506],\n",
      "         [-2.1505,  0.7811,  0.6343,  0.5329]]], grad_fn=<SelectBackward0>)\n",
      "\n",
      "tgt_len =  1\n",
      "bsz =  4\n",
      "num_heads =  2\n",
      "head_dim =  2\n",
      "\n",
      "tensor([[[-3.1998,  2.1538, -1.0128,  0.7383],\n",
      "         [-2.6388,  1.2949, -0.3135,  0.9396],\n",
      "         [-2.5799,  0.7808, -0.3913, -0.4072],\n",
      "         [-2.3275,  1.0272, -0.7647, -0.5439]]], grad_fn=<SelectBackward0>) torch.Size([1, 4, 4])\n",
      "Q reshaped =  tensor([[[-3.1998,  2.1538]],\n",
      "\n",
      "        [[-1.0128,  0.7383]],\n",
      "\n",
      "        [[-2.6388,  1.2949]],\n",
      "\n",
      "        [[-0.3135,  0.9396]],\n",
      "\n",
      "        [[-2.5799,  0.7808]],\n",
      "\n",
      "        [[-0.3913, -0.4072]],\n",
      "\n",
      "        [[-2.3275,  1.0272]],\n",
      "\n",
      "        [[-0.7647, -0.5439]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "K reshaped =  tensor([[[ 1.0097,  1.4010]],\n",
      "\n",
      "        [[ 0.8216, -3.9416]],\n",
      "\n",
      "        [[ 1.1088,  2.1519]],\n",
      "\n",
      "        [[ 0.0169, -3.9404]],\n",
      "\n",
      "        [[ 0.6041,  1.1918]],\n",
      "\n",
      "        [[-0.2197, -2.3607]],\n",
      "\n",
      "        [[ 0.5968,  0.5446]],\n",
      "\n",
      "        [[ 0.1456, -1.7790]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "V reshaped =  tensor([[[-2.4218,  1.2453]],\n",
      "\n",
      "        [[ 0.6700,  1.9331]],\n",
      "\n",
      "        [[-1.5839,  0.7873]],\n",
      "\n",
      "        [[ 0.8833,  1.9419]],\n",
      "\n",
      "        [[-2.1300,  0.6044]],\n",
      "\n",
      "        [[ 0.7146,  0.7506]],\n",
      "\n",
      "        [[-2.1505,  0.7811]],\n",
      "\n",
      "        [[ 0.6343,  0.5329]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "torch.Size([4, 2, 1, 2]) torch.Size([4, 2, 1, 2]) torch.Size([4, 2, 1, 2])\n",
      "\n",
      "scaled_dot_product_attention\n",
      "attn_output = \n",
      "tensor([[-2.4218,  1.2453,  0.6700,  1.9331],\n",
      "        [-1.5839,  0.7873,  0.8833,  1.9419],\n",
      "        [-2.1300,  0.6044,  0.7146,  0.7506],\n",
      "        [-2.1505,  0.7811,  0.6343,  0.5329]], grad_fn=<ViewBackward0>)\n",
      "\n",
      "x =  tensor([[[ 1.0958,  0.6448,  4.2360,  3.1174],\n",
      "         [ 1.8619,  1.7091,  2.9620,  2.6244],\n",
      "         [ 1.7365, -0.1128,  1.8384,  2.8450],\n",
      "         [ 0.9683, -0.6866,  1.8484,  2.8448]]], grad_fn=<AddBackward0>)\n",
      "\n",
      "sa_op =  tensor([[[-2.5407, -0.7950,  3.1156,  3.2390],\n",
      "         [-1.9186, -0.9082,  2.6121,  2.2215],\n",
      "         [-1.5350, -0.6587,  1.9988,  2.4555],\n",
      "         [-1.5773, -0.6308,  1.9817,  2.5988]]], grad_fn=<ViewBackward0>)\n",
      "\n",
      "temop =  tensor([[[-1.4449, -0.1502,  7.3516,  6.3564],\n",
      "         [-0.0567,  0.8009,  5.5741,  4.8459],\n",
      "         [ 0.2016, -0.7714,  3.8372,  5.3005],\n",
      "         [-0.6090, -1.3174,  3.8301,  5.4436]]], grad_fn=<AddBackward0>)\n",
      "\n",
      "Decoder norm1(x + sa(x)) = \n",
      "tensor([[[-1.1561, -0.8215,  1.1174,  0.8602],\n",
      "         [-1.1617, -0.8118,  1.1353,  0.8382],\n",
      "         [-0.7746, -1.1630,  0.6767,  1.2608],\n",
      "         [-0.8526, -1.0996,  0.6949,  1.2574]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "\n",
      "Memory = \n",
      "tensor([[[-0.6045, -0.3659, -0.7456,  1.7160],\n",
      "         [-0.3525, -0.5226, -0.8313,  1.7063],\n",
      "         [-0.9077, -0.3700, -0.4154,  1.6931],\n",
      "         [-0.0998, -1.2328, -0.2232,  1.5558]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "\n",
      "query =  tensor([[[-1.1561, -0.8215,  1.1174,  0.8602],\n",
      "         [-1.1617, -0.8118,  1.1353,  0.8382],\n",
      "         [-0.7746, -1.1630,  0.6767,  1.2608],\n",
      "         [-0.8526, -1.0996,  0.6949,  1.2574]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "query shape =  torch.Size([1, 4, 4])\n",
      "\n",
      " _in_projection_packed \n",
      "\n",
      "Q :-  tensor([[[-0.5945, -0.7982,  0.2541, -1.1856],\n",
      "         [-0.5907, -0.8145,  0.2590, -1.1908],\n",
      "         [-0.5886, -0.3429,  0.0768, -0.9543],\n",
      "         [-0.6079, -0.3806,  0.1015, -0.9830]]], grad_fn=<ViewBackward0>)\n",
      "\n",
      "K :-  tensor([[[-0.7237, -0.3619,  1.0531, -0.5075],\n",
      "         [-0.6827, -0.4790,  1.2487, -0.4996],\n",
      "         [-0.5655, -0.3325,  0.8726, -0.2977],\n",
      "         [-0.0128, -0.9119,  1.5721,  0.1687]]], grad_fn=<SelectBackward0>)\n",
      "\n",
      "V :-  tensor([[[-0.9121,  1.2251, -0.1781,  1.1367],\n",
      "         [-1.1346,  1.1204, -0.0801,  1.2016],\n",
      "         [-0.6177,  1.2510, -0.4478,  0.9994],\n",
      "         [-1.2460,  0.6969, -0.4321,  1.0623]]], grad_fn=<SelectBackward0>)\n",
      "\n",
      "tgt_len =  1\n",
      "bsz =  4\n",
      "num_heads =  2\n",
      "head_dim =  2\n",
      "\n",
      "tensor([[[-0.5945, -0.7982,  0.2541, -1.1856],\n",
      "         [-0.5907, -0.8145,  0.2590, -1.1908],\n",
      "         [-0.5886, -0.3429,  0.0768, -0.9543],\n",
      "         [-0.6079, -0.3806,  0.1015, -0.9830]]], grad_fn=<ViewBackward0>) torch.Size([1, 4, 4])\n",
      "Q reshaped =  tensor([[[-0.5945, -0.7982]],\n",
      "\n",
      "        [[ 0.2541, -1.1856]],\n",
      "\n",
      "        [[-0.5907, -0.8145]],\n",
      "\n",
      "        [[ 0.2590, -1.1908]],\n",
      "\n",
      "        [[-0.5886, -0.3429]],\n",
      "\n",
      "        [[ 0.0768, -0.9543]],\n",
      "\n",
      "        [[-0.6079, -0.3806]],\n",
      "\n",
      "        [[ 0.1015, -0.9830]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "K reshaped =  tensor([[[-0.7237, -0.3619]],\n",
      "\n",
      "        [[ 1.0531, -0.5075]],\n",
      "\n",
      "        [[-0.6827, -0.4790]],\n",
      "\n",
      "        [[ 1.2487, -0.4996]],\n",
      "\n",
      "        [[-0.5655, -0.3325]],\n",
      "\n",
      "        [[ 0.8726, -0.2977]],\n",
      "\n",
      "        [[-0.0128, -0.9119]],\n",
      "\n",
      "        [[ 1.5721,  0.1687]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "V reshaped =  tensor([[[-0.9121,  1.2251]],\n",
      "\n",
      "        [[-0.1781,  1.1367]],\n",
      "\n",
      "        [[-1.1346,  1.1204]],\n",
      "\n",
      "        [[-0.0801,  1.2016]],\n",
      "\n",
      "        [[-0.6177,  1.2510]],\n",
      "\n",
      "        [[-0.4478,  0.9994]],\n",
      "\n",
      "        [[-1.2460,  0.6969]],\n",
      "\n",
      "        [[-0.4321,  1.0623]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "torch.Size([4, 2, 1, 2]) torch.Size([4, 2, 1, 2]) torch.Size([4, 2, 1, 2])\n",
      "\n",
      "scaled_dot_product_attention\n",
      "attn_output = \n",
      "tensor([[-0.9121,  1.2251, -0.1781,  1.1367],\n",
      "        [-1.1346,  1.1204, -0.0801,  1.2016],\n",
      "        [-0.6177,  1.2510, -0.4478,  0.9994],\n",
      "        [-1.2460,  0.6969, -0.4321,  1.0623]], grad_fn=<ViewBackward0>)\n",
      "\n",
      "Decoder norm2(x + mha(x, mem)) = \n",
      "tensor([[[ 0.9655, -1.6739,  0.2678,  0.4405],\n",
      "         [ 0.9760, -1.6225,  0.0026,  0.6439],\n",
      "         [ 1.0582, -1.5949, -0.0417,  0.5785],\n",
      "         [ 0.8551, -1.5567, -0.1965,  0.8982]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "\n",
      "Decoder norm3(x + ff(x)) = \n",
      "tensor([[[ 1.0891, -1.6247,  0.3917,  0.1440],\n",
      "         [ 1.1280, -1.6081,  0.1253,  0.3547],\n",
      "         [ 1.2079, -1.5667,  0.0743,  0.2844],\n",
      "         [ 1.0352, -1.5867, -0.0839,  0.6353]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "\n",
      "\n",
      "################### Transformer Decoder start #####################\n",
      "\n",
      "query =  tensor([[[ 1.0891, -1.6247,  0.3917,  0.1440],\n",
      "         [ 1.1280, -1.6081,  0.1253,  0.3547],\n",
      "         [ 1.2079, -1.5667,  0.0743,  0.2844],\n",
      "         [ 1.0352, -1.5867, -0.0839,  0.6353]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "query shape =  torch.Size([1, 4, 4])\n",
      "\n",
      " _in_projection_packed \n",
      "\n",
      "Q :-  tensor([[[ 0.6075, -0.0098,  1.2060,  0.9822],\n",
      "         [ 0.4839,  0.0549,  1.0884,  0.8157],\n",
      "         [ 0.4348,  0.1743,  1.0518,  0.8525],\n",
      "         [ 0.4025, -0.0228,  0.9782,  0.5796]]], grad_fn=<SelectBackward0>)\n",
      "\n",
      "K :-  tensor([[[-0.1669, -0.3601,  0.0291,  0.3061],\n",
      "         [-0.1030, -0.1233,  0.1006,  0.2773],\n",
      "         [-0.1334, -0.0964,  0.1874,  0.2714],\n",
      "         [-0.0004,  0.0849,  0.0663,  0.2464]]], grad_fn=<SelectBackward0>)\n",
      "\n",
      "V :-  tensor([[[ 0.7972, -0.7642, -0.5206, -1.0952],\n",
      "         [ 0.8066, -0.9265, -0.4583, -0.9729],\n",
      "         [ 0.8741, -0.9646, -0.4691, -0.9737],\n",
      "         [ 0.7097, -1.0123, -0.3685, -0.8177]]], grad_fn=<SelectBackward0>)\n",
      "\n",
      "tgt_len =  1\n",
      "bsz =  4\n",
      "num_heads =  2\n",
      "head_dim =  2\n",
      "\n",
      "tensor([[[ 0.6075, -0.0098,  1.2060,  0.9822],\n",
      "         [ 0.4839,  0.0549,  1.0884,  0.8157],\n",
      "         [ 0.4348,  0.1743,  1.0518,  0.8525],\n",
      "         [ 0.4025, -0.0228,  0.9782,  0.5796]]], grad_fn=<SelectBackward0>) torch.Size([1, 4, 4])\n",
      "Q reshaped =  tensor([[[ 0.6075, -0.0098]],\n",
      "\n",
      "        [[ 1.2060,  0.9822]],\n",
      "\n",
      "        [[ 0.4839,  0.0549]],\n",
      "\n",
      "        [[ 1.0884,  0.8157]],\n",
      "\n",
      "        [[ 0.4348,  0.1743]],\n",
      "\n",
      "        [[ 1.0518,  0.8525]],\n",
      "\n",
      "        [[ 0.4025, -0.0228]],\n",
      "\n",
      "        [[ 0.9782,  0.5796]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "K reshaped =  tensor([[[-0.1669, -0.3601]],\n",
      "\n",
      "        [[ 0.0291,  0.3061]],\n",
      "\n",
      "        [[-0.1030, -0.1233]],\n",
      "\n",
      "        [[ 0.1006,  0.2773]],\n",
      "\n",
      "        [[-0.1334, -0.0964]],\n",
      "\n",
      "        [[ 0.1874,  0.2714]],\n",
      "\n",
      "        [[-0.0004,  0.0849]],\n",
      "\n",
      "        [[ 0.0663,  0.2464]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "V reshaped =  tensor([[[ 0.7972, -0.7642]],\n",
      "\n",
      "        [[-0.5206, -1.0952]],\n",
      "\n",
      "        [[ 0.8066, -0.9265]],\n",
      "\n",
      "        [[-0.4583, -0.9729]],\n",
      "\n",
      "        [[ 0.8741, -0.9646]],\n",
      "\n",
      "        [[-0.4691, -0.9737]],\n",
      "\n",
      "        [[ 0.7097, -1.0123]],\n",
      "\n",
      "        [[-0.3685, -0.8177]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "torch.Size([4, 2, 1, 2]) torch.Size([4, 2, 1, 2]) torch.Size([4, 2, 1, 2])\n",
      "\n",
      "scaled_dot_product_attention\n",
      "attn_output = \n",
      "tensor([[ 0.7972, -0.7642, -0.5206, -1.0952],\n",
      "        [ 0.8066, -0.9265, -0.4583, -0.9729],\n",
      "        [ 0.8741, -0.9646, -0.4691, -0.9737],\n",
      "        [ 0.7097, -1.0123, -0.3685, -0.8177]], grad_fn=<ViewBackward0>)\n",
      "\n",
      "x =  tensor([[[ 1.0891, -1.6247,  0.3917,  0.1440],\n",
      "         [ 1.1280, -1.6081,  0.1253,  0.3547],\n",
      "         [ 1.2079, -1.5667,  0.0743,  0.2844],\n",
      "         [ 1.0352, -1.5867, -0.0839,  0.6353]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "\n",
      "sa_op =  tensor([[[ 2.3596e+00,  9.7316e-02,  9.3889e-01,  3.7141e-02],\n",
      "         [ 2.3653e+00,  1.3900e-02,  1.0530e+00,  9.5688e-02],\n",
      "         [ 2.4474e+00, -5.3176e-04,  1.1310e+00,  1.0151e-01],\n",
      "         [ 2.1957e+00, -5.2617e-02,  1.0397e+00,  1.4393e-01]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "\n",
      "temop =  tensor([[[ 3.4486, -1.5274,  1.3305,  0.1811],\n",
      "         [ 3.4933, -1.5942,  1.1783,  0.4504],\n",
      "         [ 3.6554, -1.5672,  1.2053,  0.3859],\n",
      "         [ 3.2309, -1.6393,  0.9558,  0.7793]]], grad_fn=<AddBackward0>)\n",
      "\n",
      "Decoder norm1(x + sa(x)) = \n",
      "tensor([[[ 1.4323, -1.3191,  0.2612, -0.3744],\n",
      "         [ 1.4362, -1.3618,  0.1630, -0.2373],\n",
      "         [ 1.4604, -1.3277,  0.1524, -0.2850],\n",
      "         [ 1.3922, -1.4338,  0.0720, -0.0304]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "\n",
      "Memory = \n",
      "tensor([[[-0.6045, -0.3659, -0.7456,  1.7160],\n",
      "         [-0.3525, -0.5226, -0.8313,  1.7063],\n",
      "         [-0.9077, -0.3700, -0.4154,  1.6931],\n",
      "         [-0.0998, -1.2328, -0.2232,  1.5558]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "\n",
      "query =  tensor([[[ 1.4323, -1.3191,  0.2612, -0.3744],\n",
      "         [ 1.4362, -1.3618,  0.1630, -0.2373],\n",
      "         [ 1.4604, -1.3277,  0.1524, -0.2850],\n",
      "         [ 1.3922, -1.4338,  0.0720, -0.0304]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "query shape =  torch.Size([1, 4, 4])\n",
      "\n",
      " _in_projection_packed \n",
      "\n",
      "Q :-  tensor([[[ 0.3737,  1.1011,  1.0474, -0.2670],\n",
      "         [ 0.4162,  1.0716,  1.0037, -0.2485],\n",
      "         [ 0.4251,  1.0817,  1.0072, -0.3037],\n",
      "         [ 0.4442,  1.0150,  0.9416, -0.1556]]], grad_fn=<ViewBackward0>)\n",
      "\n",
      "K :-  tensor([[[ 0.5865, -1.5370, -0.0593,  0.3249],\n",
      "         [ 0.6022, -1.4020, -0.0278,  0.1153],\n",
      "         [ 0.4098, -1.5959, -0.1676,  0.5825],\n",
      "         [ 0.1369, -0.9322, -0.2051, -0.0747]]], grad_fn=<SelectBackward0>)\n",
      "\n",
      "V :-  tensor([[[ 1.4120, -0.6568, -0.7945, -1.0526],\n",
      "         [ 1.3624, -0.6216, -0.8882, -1.2764],\n",
      "         [ 1.4343, -0.6807, -0.5334, -0.8036],\n",
      "         [ 1.1730, -0.5183, -0.5200, -1.5246]]], grad_fn=<SelectBackward0>)\n",
      "\n",
      "tgt_len =  1\n",
      "bsz =  4\n",
      "num_heads =  2\n",
      "head_dim =  2\n",
      "\n",
      "tensor([[[ 0.3737,  1.1011,  1.0474, -0.2670],\n",
      "         [ 0.4162,  1.0716,  1.0037, -0.2485],\n",
      "         [ 0.4251,  1.0817,  1.0072, -0.3037],\n",
      "         [ 0.4442,  1.0150,  0.9416, -0.1556]]], grad_fn=<ViewBackward0>) torch.Size([1, 4, 4])\n",
      "Q reshaped =  tensor([[[ 0.3737,  1.1011]],\n",
      "\n",
      "        [[ 1.0474, -0.2670]],\n",
      "\n",
      "        [[ 0.4162,  1.0716]],\n",
      "\n",
      "        [[ 1.0037, -0.2485]],\n",
      "\n",
      "        [[ 0.4251,  1.0817]],\n",
      "\n",
      "        [[ 1.0072, -0.3037]],\n",
      "\n",
      "        [[ 0.4442,  1.0150]],\n",
      "\n",
      "        [[ 0.9416, -0.1556]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "K reshaped =  tensor([[[ 0.5865, -1.5370]],\n",
      "\n",
      "        [[-0.0593,  0.3249]],\n",
      "\n",
      "        [[ 0.6022, -1.4020]],\n",
      "\n",
      "        [[-0.0278,  0.1153]],\n",
      "\n",
      "        [[ 0.4098, -1.5959]],\n",
      "\n",
      "        [[-0.1676,  0.5825]],\n",
      "\n",
      "        [[ 0.1369, -0.9322]],\n",
      "\n",
      "        [[-0.2051, -0.0747]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "V reshaped =  tensor([[[ 1.4120, -0.6568]],\n",
      "\n",
      "        [[-0.7945, -1.0526]],\n",
      "\n",
      "        [[ 1.3624, -0.6216]],\n",
      "\n",
      "        [[-0.8882, -1.2764]],\n",
      "\n",
      "        [[ 1.4343, -0.6807]],\n",
      "\n",
      "        [[-0.5334, -0.8036]],\n",
      "\n",
      "        [[ 1.1730, -0.5183]],\n",
      "\n",
      "        [[-0.5200, -1.5246]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "torch.Size([4, 2, 1, 2]) torch.Size([4, 2, 1, 2]) torch.Size([4, 2, 1, 2])\n",
      "\n",
      "scaled_dot_product_attention\n",
      "attn_output = \n",
      "tensor([[ 1.4120, -0.6568, -0.7945, -1.0526],\n",
      "        [ 1.3624, -0.6216, -0.8882, -1.2764],\n",
      "        [ 1.4343, -0.6807, -0.5334, -0.8036],\n",
      "        [ 1.1730, -0.5183, -0.5200, -1.5246]], grad_fn=<ViewBackward0>)\n",
      "\n",
      "Decoder norm2(x + mha(x, mem)) = \n",
      "tensor([[[ 1.6425, -0.1400, -0.4741, -1.0285],\n",
      "         [ 1.6183, -0.0168, -0.5787, -1.0228],\n",
      "         [ 1.7071, -0.3861, -0.4812, -0.8398],\n",
      "         [ 1.6379, -0.0619, -0.5991, -0.9769]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "\n",
      "Decoder norm3(x + ff(x)) = \n",
      "tensor([[[ 1.6675, -0.3092, -0.3619, -0.9964],\n",
      "         [ 1.6573, -0.1863, -0.4741, -0.9969],\n",
      "         [ 1.7130, -0.5573, -0.3694, -0.7864],\n",
      "         [ 1.6747, -0.2326, -0.4961, -0.9461]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "\n",
      "\n",
      "################### Transformer Decoder start #####################\n",
      "\n",
      "query =  tensor([[[ 1.6675, -0.3092, -0.3619, -0.9964],\n",
      "         [ 1.6573, -0.1863, -0.4741, -0.9969],\n",
      "         [ 1.7130, -0.5573, -0.3694, -0.7864],\n",
      "         [ 1.6747, -0.2326, -0.4961, -0.9461]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "query shape =  torch.Size([1, 4, 4])\n",
      "\n",
      " _in_projection_packed \n",
      "\n",
      "Q :-  tensor([[[-0.4564,  0.4040, -0.0769, -0.1547],\n",
      "         [-0.4492,  0.3951, -0.1052, -0.1132],\n",
      "         [-0.4883,  0.5261, -0.0320, -0.0809],\n",
      "         [-0.4584,  0.4274, -0.0984, -0.0877]]], grad_fn=<SelectBackward0>)\n",
      "\n",
      "K :-  tensor([[[-0.1585,  1.1721,  0.5348, -0.1444],\n",
      "         [-0.0538,  1.1324,  0.4825, -0.1531],\n",
      "         [-0.3281,  1.0430,  0.7170, -0.1352],\n",
      "         [-0.0818,  1.0969,  0.5240, -0.1527]]], grad_fn=<SelectBackward0>)\n",
      "\n",
      "V :-  tensor([[[-0.6207,  0.2833,  1.2686,  0.4266],\n",
      "         [-0.6454,  0.3579,  1.2686,  0.4896],\n",
      "         [-0.6633,  0.1246,  1.0597,  0.2788],\n",
      "         [-0.6633,  0.3297,  1.2191,  0.4629]]], grad_fn=<SelectBackward0>)\n",
      "\n",
      "tgt_len =  1\n",
      "bsz =  4\n",
      "num_heads =  2\n",
      "head_dim =  2\n",
      "\n",
      "tensor([[[-0.4564,  0.4040, -0.0769, -0.1547],\n",
      "         [-0.4492,  0.3951, -0.1052, -0.1132],\n",
      "         [-0.4883,  0.5261, -0.0320, -0.0809],\n",
      "         [-0.4584,  0.4274, -0.0984, -0.0877]]], grad_fn=<SelectBackward0>) torch.Size([1, 4, 4])\n",
      "Q reshaped =  tensor([[[-0.4564,  0.4040]],\n",
      "\n",
      "        [[-0.0769, -0.1547]],\n",
      "\n",
      "        [[-0.4492,  0.3951]],\n",
      "\n",
      "        [[-0.1052, -0.1132]],\n",
      "\n",
      "        [[-0.4883,  0.5261]],\n",
      "\n",
      "        [[-0.0320, -0.0809]],\n",
      "\n",
      "        [[-0.4584,  0.4274]],\n",
      "\n",
      "        [[-0.0984, -0.0877]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "K reshaped =  tensor([[[-0.1585,  1.1721]],\n",
      "\n",
      "        [[ 0.5348, -0.1444]],\n",
      "\n",
      "        [[-0.0538,  1.1324]],\n",
      "\n",
      "        [[ 0.4825, -0.1531]],\n",
      "\n",
      "        [[-0.3281,  1.0430]],\n",
      "\n",
      "        [[ 0.7170, -0.1352]],\n",
      "\n",
      "        [[-0.0818,  1.0969]],\n",
      "\n",
      "        [[ 0.5240, -0.1527]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "V reshaped =  tensor([[[-0.6207,  0.2833]],\n",
      "\n",
      "        [[ 1.2686,  0.4266]],\n",
      "\n",
      "        [[-0.6454,  0.3579]],\n",
      "\n",
      "        [[ 1.2686,  0.4896]],\n",
      "\n",
      "        [[-0.6633,  0.1246]],\n",
      "\n",
      "        [[ 1.0597,  0.2788]],\n",
      "\n",
      "        [[-0.6633,  0.3297]],\n",
      "\n",
      "        [[ 1.2191,  0.4629]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "torch.Size([4, 2, 1, 2]) torch.Size([4, 2, 1, 2]) torch.Size([4, 2, 1, 2])\n",
      "\n",
      "scaled_dot_product_attention\n",
      "attn_output = \n",
      "tensor([[-0.6207,  0.2833,  1.2686,  0.4266],\n",
      "        [-0.6454,  0.3579,  1.2686,  0.4896],\n",
      "        [-0.6633,  0.1246,  1.0597,  0.2788],\n",
      "        [-0.6633,  0.3297,  1.2191,  0.4629]], grad_fn=<ViewBackward0>)\n",
      "\n",
      "x =  tensor([[[ 1.6675, -0.3092, -0.3619, -0.9964],\n",
      "         [ 1.6573, -0.1863, -0.4741, -0.9969],\n",
      "         [ 1.7130, -0.5573, -0.3694, -0.7864],\n",
      "         [ 1.6747, -0.2326, -0.4961, -0.9461]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "\n",
      "sa_op =  tensor([[[-0.8042,  0.0264, -0.7331,  0.3777],\n",
      "         [-0.8095,  0.1128, -0.7847,  0.4764],\n",
      "         [-0.7536, -0.0026, -0.5104,  0.3038],\n",
      "         [-0.8005,  0.1196, -0.7378,  0.4749]]], grad_fn=<ViewBackward0>)\n",
      "\n",
      "temop =  tensor([[[ 0.8633, -0.2827, -1.0950, -0.6187],\n",
      "         [ 0.8478, -0.0734, -1.2588, -0.5205],\n",
      "         [ 0.9595, -0.5599, -0.8797, -0.4826],\n",
      "         [ 0.8743, -0.1130, -1.2339, -0.4711]]], grad_fn=<AddBackward0>)\n",
      "\n",
      "Decoder norm1(x + sa(x)) = \n",
      "tensor([[[ 1.5877e+00,  8.0138e-04, -1.1240e+00, -4.6449e-01],\n",
      "         [ 1.4409e+00,  2.3309e-01, -1.3209e+00, -3.5301e-01],\n",
      "         [ 1.6934e+00, -4.5037e-01, -9.0165e-01, -3.4137e-01],\n",
      "         [ 1.4644e+00,  1.6219e-01, -1.3164e+00, -3.1023e-01]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "\n",
      "Memory = \n",
      "tensor([[[-0.6045, -0.3659, -0.7456,  1.7160],\n",
      "         [-0.3525, -0.5226, -0.8313,  1.7063],\n",
      "         [-0.9077, -0.3700, -0.4154,  1.6931],\n",
      "         [-0.0998, -1.2328, -0.2232,  1.5558]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "\n",
      "query =  tensor([[[ 1.5877e+00,  8.0138e-04, -1.1240e+00, -4.6449e-01],\n",
      "         [ 1.4409e+00,  2.3309e-01, -1.3209e+00, -3.5301e-01],\n",
      "         [ 1.6934e+00, -4.5037e-01, -9.0165e-01, -3.4137e-01],\n",
      "         [ 1.4644e+00,  1.6219e-01, -1.3164e+00, -3.1023e-01]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "query shape =  torch.Size([1, 4, 4])\n",
      "\n",
      " _in_projection_packed \n",
      "\n",
      "Q :-  tensor([[[ 0.1865, -0.5781,  0.6103, -0.1993],\n",
      "         [ 0.1109, -0.3085,  0.5972, -0.3766],\n",
      "         [ 0.3719, -0.8744,  0.7502,  0.1472],\n",
      "         [ 0.1462, -0.3432,  0.6381, -0.3249]]], grad_fn=<ViewBackward0>)\n",
      "\n",
      "K :-  tensor([[[-0.9461, -0.4201,  0.6541, -0.5913],\n",
      "         [-0.9505, -0.4142,  0.5800, -0.6170],\n",
      "         [-0.7898, -0.4899,  0.6221, -0.3727],\n",
      "         [-0.4857, -0.5845,  0.1362, -0.0720]]], grad_fn=<SelectBackward0>)\n",
      "\n",
      "V :-  tensor([[[-0.1976,  0.1209, -0.5734, -1.4607],\n",
      "         [-0.1951,  0.1337, -0.7044, -1.4275],\n",
      "         [-0.2685,  0.0153, -0.3249, -1.3999],\n",
      "         [-0.3893, -0.1243, -0.5464, -1.0646]]], grad_fn=<SelectBackward0>)\n",
      "\n",
      "tgt_len =  1\n",
      "bsz =  4\n",
      "num_heads =  2\n",
      "head_dim =  2\n",
      "\n",
      "tensor([[[ 0.1865, -0.5781,  0.6103, -0.1993],\n",
      "         [ 0.1109, -0.3085,  0.5972, -0.3766],\n",
      "         [ 0.3719, -0.8744,  0.7502,  0.1472],\n",
      "         [ 0.1462, -0.3432,  0.6381, -0.3249]]], grad_fn=<ViewBackward0>) torch.Size([1, 4, 4])\n",
      "Q reshaped =  tensor([[[ 0.1865, -0.5781]],\n",
      "\n",
      "        [[ 0.6103, -0.1993]],\n",
      "\n",
      "        [[ 0.1109, -0.3085]],\n",
      "\n",
      "        [[ 0.5972, -0.3766]],\n",
      "\n",
      "        [[ 0.3719, -0.8744]],\n",
      "\n",
      "        [[ 0.7502,  0.1472]],\n",
      "\n",
      "        [[ 0.1462, -0.3432]],\n",
      "\n",
      "        [[ 0.6381, -0.3249]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "K reshaped =  tensor([[[-0.9461, -0.4201]],\n",
      "\n",
      "        [[ 0.6541, -0.5913]],\n",
      "\n",
      "        [[-0.9505, -0.4142]],\n",
      "\n",
      "        [[ 0.5800, -0.6170]],\n",
      "\n",
      "        [[-0.7898, -0.4899]],\n",
      "\n",
      "        [[ 0.6221, -0.3727]],\n",
      "\n",
      "        [[-0.4857, -0.5845]],\n",
      "\n",
      "        [[ 0.1362, -0.0720]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "V reshaped =  tensor([[[-0.1976,  0.1209]],\n",
      "\n",
      "        [[-0.5734, -1.4607]],\n",
      "\n",
      "        [[-0.1951,  0.1337]],\n",
      "\n",
      "        [[-0.7044, -1.4275]],\n",
      "\n",
      "        [[-0.2685,  0.0153]],\n",
      "\n",
      "        [[-0.3249, -1.3999]],\n",
      "\n",
      "        [[-0.3893, -0.1243]],\n",
      "\n",
      "        [[-0.5464, -1.0646]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "torch.Size([4, 2, 1, 2]) torch.Size([4, 2, 1, 2]) torch.Size([4, 2, 1, 2])\n",
      "\n",
      "scaled_dot_product_attention\n",
      "attn_output = \n",
      "tensor([[-0.1976,  0.1209, -0.5734, -1.4607],\n",
      "        [-0.1951,  0.1337, -0.7044, -1.4275],\n",
      "        [-0.2685,  0.0153, -0.3249, -1.3999],\n",
      "        [-0.3893, -0.1243, -0.5464, -1.0646]], grad_fn=<ViewBackward0>)\n",
      "\n",
      "Decoder norm2(x + mha(x, mem)) = \n",
      "tensor([[[ 1.5265, -0.1745, -1.2783, -0.0736],\n",
      "         [ 1.4271, -0.0056, -1.4010, -0.0205],\n",
      "         [ 1.6042, -0.4852, -1.0910, -0.0281],\n",
      "         [ 1.5221, -0.0578, -1.2834, -0.1809]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "\n",
      "Decoder norm3(x + ff(x)) = \n",
      "tensor([[[ 1.5900, -0.2046, -1.1776, -0.2079],\n",
      "         [ 1.4772,  0.0127, -1.3398, -0.1502],\n",
      "         [ 1.6628, -0.5940, -0.9286, -0.1402],\n",
      "         [ 1.5767, -0.0540, -1.1808, -0.3419]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "\n",
      "tensor([[[-0.1736, -0.6635, -0.7838, -0.4535, -1.0496, -0.8957,  0.4070,\n",
      "           0.4802,  0.3982, -0.3234],\n",
      "         [-0.1878, -0.6338, -0.7410, -0.4279, -1.0766, -0.9809,  0.3347,\n",
      "           0.5342,  0.4060, -0.4664],\n",
      "         [-0.1235, -0.6708, -0.8959, -0.4473, -0.9361, -0.7919,  0.5501,\n",
      "           0.3801,  0.3818, -0.0212],\n",
      "         [-0.1777, -0.6718, -0.6719, -0.4529, -1.0911, -0.8548,  0.3259,\n",
      "           0.5181,  0.4016, -0.4310]]], grad_fn=<ViewBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sreevaatsav/.pyenv/versions/project_env/lib/python3.10/site-packages/torch/nn/modules/transformer.py:282: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=512, dropout=0):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.encoding = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1).float()\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(torch.log(torch.tensor(10000.0)) / d_model))\n",
    "        self.encoding[:, 0::2] = torch.sin(position * div_term)\n",
    "        self.encoding[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.encoding = self.encoding.unsqueeze(0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.encoding[:, :x.size(1)].detach()\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, src_vocab_size, tgt_vocab_size, d_model=4, nhead=2, num_encoder_layers=1, num_decoder_layers=1):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.src_embedding = nn.Embedding(src_vocab_size, d_model)\n",
    "        self.tgt_embedding = nn.Embedding(tgt_vocab_size, d_model)\n",
    "        self.positional_encoding = PositionalEncoding(d_model, dropout=0)\n",
    "        self.transformer = nn.Transformer(\n",
    "            d_model=d_model,\n",
    "            nhead=nhead,\n",
    "            num_encoder_layers=num_encoder_layers,\n",
    "            num_decoder_layers=num_decoder_layers,\n",
    "            dropout=0\n",
    "        )\n",
    "        self.fc = nn.Linear(d_model, tgt_vocab_size)\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "\n",
    "        print(\"Source embeddings :- \\n\")\n",
    "        print(self.src_embedding(src))\n",
    "        print()\n",
    "\n",
    "        print(\"Target embeddings :- \\n\")\n",
    "        print(self.tgt_embedding(tgt))\n",
    "        print()\n",
    "\n",
    "\n",
    "        src = self.src_embedding(src) + self.positional_encoding(src)\n",
    "        tgt = self.tgt_embedding(tgt) + self.positional_encoding(tgt)\n",
    "\n",
    "        print(\"Positional encoded source embeddings:\")\n",
    "        print(src)\n",
    "        print()\n",
    "\n",
    "        print(\"Positional encoded target embeddings:\")\n",
    "        print(tgt)\n",
    "        print()\n",
    "\n",
    "        output = self.transformer(src, tgt)\n",
    "        output = self.fc(output)\n",
    "        return output\n",
    "\n",
    "    \n",
    "\n",
    "src_vocab_size = 10  # Source language vocabulary size\n",
    "tgt_vocab_size = 10  # Target language vocabulary size\n",
    "d_model = 4  # Dimension of the model\n",
    "num_heads = 2\n",
    "num_encoder_layers = 3\n",
    "num_decoder_layers = 3\n",
    "\n",
    "need_weights = False\n",
    "\n",
    "src_mask = None\n",
    "tgt_mask = None\n",
    "memory_mask = None\n",
    "\n",
    "\n",
    "model = TransformerModel(src_vocab_size, tgt_vocab_size, d_model=d_model, nhead=num_heads, num_encoder_layers = num_encoder_layers, num_decoder_layers=num_decoder_layers)\n",
    "# Source sentence in the source language\n",
    "src_sentence = torch.tensor([[0, 1, 2, 3]])  # Source sequence\n",
    "\n",
    "# Target sentence in the target language (translation of the source sentence)\n",
    "tgt_sentence = torch.tensor([[1, 0, 3, 3]])  # Target sequence\n",
    "\n",
    "# Forward pass\n",
    "output = model(src_sentence, tgt_sentence)\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_intermediate_outputs(src_sentence, tgt_sentence,d_model, model, num_encoder_layers , num_decoder_layers):\n",
    "\n",
    "    state_dict = model.state_dict()\n",
    "\n",
    "    pe_src_embeds, pe_tgt_embeds = get_embedding_outputs(src_sentence,  tgt_sentence, state_dict, d_model = d_model)\n",
    "\n",
    "    print(\"###\"*25)\n",
    "    print(\"### Encoder Start ###\")\n",
    "    print()\n",
    "\n",
    "    x_enc = pe_src_embeds\n",
    "\n",
    "    for lno in range(num_encoder_layers):\n",
    "        attn_enc_output, src_len, head_dim, attn_weights = encoder_block_attn_output(x_enc, state_dict, layer_num = lno, need_weights = False, embed_dim = d_model, num_heads= num_heads, src_mask=None)\n",
    "\n",
    "        if lno == 0:\n",
    "            tgt_len, bsz, embed_dim = x_enc.shape\n",
    "\n",
    "        output_enc_final = encoder_block_post_attn_output(x_enc, attn_enc_output, state_dict, layer_num = lno , bsz = bsz, tgt_len = tgt_len)\n",
    "\n",
    "        x_enc = output_enc_final\n",
    "\n",
    "    \n",
    "    print(\"### Encoder Done ###\")\n",
    "    print(\"\\n\\n\\n\")\n",
    "    print(\"### Decoder Start ###\")\n",
    "\n",
    "    x_dec = pe_tgt_embeds\n",
    "    memory = x_enc\n",
    "\n",
    "    for lno in range(num_decoder_layers):\n",
    "\n",
    "        self_attn_dec, dec_sa_wts = decoder_block_self_attn_output(x_dec, state_dict, layer_num = lno, need_weights = False, tgt_mask=None)\n",
    "        x_dec = dec_post_self_attn(self_attn_dec, x_dec, state_dict, layer_num = lno)\n",
    "\n",
    "        attn_dec_mha_output, attn_dec_mha_wts = decoder_block_cross_attn_output(x_dec, memory, state_dict, layer_num = lno, tgt_len = tgt_len, src_len = src_len, head_dim = head_dim, need_weights = False, memory_mask=None)\n",
    "        final_op = decoder_block_post_attn_output(x_dec, attn_dec_mha_output, state_dict, layer_num = lno)\n",
    "\n",
    "        print(pe_tgt_embeds.shape, final_op.shape)\n",
    "        x_dec = final_op\n",
    "\n",
    "\n",
    "    final_op = feef_fwd_transformer(final_op, state_dict)\n",
    "\n",
    "    print(\"### Decoder Done ###\")\n",
    "\n",
    "\n",
    "    return final_op\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source sentence embedding\n",
      "Word index: 0, Embedding: tensor([-0.2205,  1.7253, -0.2514,  0.9972])\n",
      "Word index: 1, Embedding: tensor([ 0.1335, -0.5650, -0.7537,  0.0105])\n",
      "Word index: 2, Embedding: tensor([-0.8226,  0.9250,  1.7916, -0.4149])\n",
      "Word index: 3, Embedding: tensor([ 0.2400, -2.0239, -0.6187, -0.7110])\n",
      "\n",
      "Target sentence embedding\n",
      "Word index: 1, Embedding: tensor([ 0.0958, -0.3552,  1.2360, -0.8826])\n",
      "Word index: 0, Embedding: tensor([ 0.0204,  1.1688, -0.0480, -1.3756])\n",
      "Word index: 3, Embedding: tensor([-0.1728,  0.3034, -1.1816, -1.1548])\n",
      "Word index: 3, Embedding: tensor([-0.1728,  0.3034, -1.1816, -1.1548])\n",
      "\n",
      "PE source embeddings : \n",
      "\n",
      "tensor([[[-0.2205,  3.7253,  1.7486,  4.9972],\n",
      "         [ 0.9750,  0.9753,  1.2563,  4.0105],\n",
      "         [ 0.0867,  1.5088,  3.8116,  3.5849],\n",
      "         [ 0.3811, -2.0139,  1.4113,  3.2885]]])\n",
      "\n",
      "PE target embeddings : \n",
      "\n",
      "tensor([[[ 1.0958,  0.6448,  4.2360,  3.1174],\n",
      "         [ 1.8619,  1.7091,  2.9620,  2.6244],\n",
      "         [ 1.7365, -0.1128,  1.8384,  2.8450],\n",
      "         [ 0.9683, -0.6866,  1.8484,  2.8448]]])\n",
      "\n",
      "###########################################################################\n",
      "### Encoder Start ###\n",
      "\n",
      "Q_enc_0 = \n",
      "tensor([[[ 0.6200, -0.9966]],\n",
      "\n",
      "        [[-0.1113,  1.0026]],\n",
      "\n",
      "        [[-0.1846, -0.6229]],\n",
      "\n",
      "        [[ 0.4113, -0.8594]],\n",
      "\n",
      "        [[-1.4594, -0.9644]],\n",
      "\n",
      "        [[ 0.6233, -0.4554]],\n",
      "\n",
      "        [[-1.3783, -0.2546]],\n",
      "\n",
      "        [[ 1.3028, -2.3128]]])\n",
      "\n",
      "K_enc_0 = \n",
      "tensor([[[ 1.9172, -1.6883]],\n",
      "\n",
      "        [[ 0.3543,  3.3778]],\n",
      "\n",
      "        [[ 1.4162, -1.2422]],\n",
      "\n",
      "        [[ 0.8147,  1.4920]],\n",
      "\n",
      "        [[ 0.0826, -1.0527]],\n",
      "\n",
      "        [[-0.6941,  2.5530]],\n",
      "\n",
      "        [[ 0.7099, -1.9596]],\n",
      "\n",
      "        [[ 0.4214,  0.1708]]])\n",
      "\n",
      "V_enc_0 = \n",
      "tensor([[[ 3.0345, -4.3447]],\n",
      "\n",
      "        [[ 4.4862, -0.7491]],\n",
      "\n",
      "        [[ 2.3662, -3.3168]],\n",
      "\n",
      "        [[ 2.7636, -0.3210]],\n",
      "\n",
      "        [[ 3.1573, -4.3181]],\n",
      "\n",
      "        [[ 2.9714,  0.0482]],\n",
      "\n",
      "        [[ 2.5920, -2.3741]],\n",
      "\n",
      "        [[ 0.7878,  0.8075]]])\n",
      "\n",
      "Encoder Attention output = \n",
      "tensor([[ 3.0345, -4.3447,  4.4862, -0.7491],\n",
      "        [ 2.3662, -3.3168,  2.7636, -0.3210],\n",
      "        [ 3.1573, -4.3181,  2.9714,  0.0482],\n",
      "        [ 2.5920, -2.3741,  0.7878,  0.8075]])\n",
      "\n",
      "Final Encoder 0 Output :\n",
      "norm2(norm1(x + self_atten(x)) + feed_fwd_op)\n",
      "\n",
      "tensor([[[-0.9361,  1.1250, -1.0537,  0.8648],\n",
      "         [-0.7420,  0.7963, -1.2133,  1.1590],\n",
      "         [-1.2530,  0.9626, -0.7095,  1.0000],\n",
      "         [-0.6221, -0.6651, -0.4386,  1.7258]]], grad_fn=<AddBackward0>)\n",
      "\n",
      "Q_enc_1 = \n",
      "tensor([[[ 0.3627,  0.2891]],\n",
      "\n",
      "        [[ 0.1520, -1.9940]],\n",
      "\n",
      "        [[ 0.3101, -0.0514]],\n",
      "\n",
      "        [[-0.1626, -1.9222]],\n",
      "\n",
      "        [[ 0.6701,  0.3549]],\n",
      "\n",
      "        [[ 0.3880, -1.8755]],\n",
      "\n",
      "        [[ 0.6906, -0.7477]],\n",
      "\n",
      "        [[-0.4020, -0.7598]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "K_enc_1 = \n",
      "tensor([[[ 0.4935, -0.7928]],\n",
      "\n",
      "        [[-0.7312,  1.2474]],\n",
      "\n",
      "        [[ 0.4188, -0.6940]],\n",
      "\n",
      "        [[-0.7778,  1.1314]],\n",
      "\n",
      "        [[ 0.5690, -1.0316]],\n",
      "\n",
      "        [[-0.6252,  1.4893]],\n",
      "\n",
      "        [[ 0.2258, -0.6703]],\n",
      "\n",
      "        [[-0.3773,  0.9025]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "V_enc_1 = \n",
      "tensor([[[ 0.4030, -0.8576]],\n",
      "\n",
      "        [[-1.5051, -0.5695]],\n",
      "\n",
      "        [[ 0.4024, -0.5434]],\n",
      "\n",
      "        [[-1.3514, -0.2802]],\n",
      "\n",
      "        [[ 0.4675, -0.8433]],\n",
      "\n",
      "        [[-1.5063, -0.5631]],\n",
      "\n",
      "        [[ 0.3788,  0.4803]],\n",
      "\n",
      "        [[-0.4493,  0.5641]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "Encoder Attention output = \n",
      "tensor([[ 0.4030, -0.8576, -1.5051, -0.5695],\n",
      "        [ 0.4024, -0.5434, -1.3514, -0.2802],\n",
      "        [ 0.4675, -0.8433, -1.5063, -0.5631],\n",
      "        [ 0.3788,  0.4803, -0.4493,  0.5641]], grad_fn=<ViewBackward0>)\n",
      "\n",
      "Final Encoder 1 Output :\n",
      "norm2(norm1(x + self_atten(x)) + feed_fwd_op)\n",
      "\n",
      "tensor([[[-0.5963, -0.4956, -0.6378,  1.7297],\n",
      "         [-0.3472, -0.6382, -0.7293,  1.7147],\n",
      "         [-0.7637, -0.6084, -0.3400,  1.7120],\n",
      "         [ 0.0533, -1.4090, -0.0613,  1.4170]]], grad_fn=<AddBackward0>)\n",
      "\n",
      "Q_enc_2 = \n",
      "tensor([[[ 0.0405,  0.5774]],\n",
      "\n",
      "        [[-0.2596,  0.5393]],\n",
      "\n",
      "        [[ 0.1239,  0.6862]],\n",
      "\n",
      "        [[-0.3060,  0.4950]],\n",
      "\n",
      "        [[ 0.1620,  0.5662]],\n",
      "\n",
      "        [[-0.1612,  0.4365]],\n",
      "\n",
      "        [[ 0.7716,  0.9559]],\n",
      "\n",
      "        [[-0.1349, -0.0280]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "K_enc_2 = \n",
      "tensor([[[ 0.0710,  0.5148]],\n",
      "\n",
      "        [[-1.2144,  0.3167]],\n",
      "\n",
      "        [[ 0.1238,  0.3417]],\n",
      "\n",
      "        [[-1.1581,  0.3155]],\n",
      "\n",
      "        [[ 0.0116,  0.5938]],\n",
      "\n",
      "        [[-1.1497,  0.1432]],\n",
      "\n",
      "        [[ 0.1141, -0.0613]],\n",
      "\n",
      "        [[-0.6354, -0.2518]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "V_enc_2 = \n",
      "tensor([[[0.8298, 0.5688]],\n",
      "\n",
      "        [[0.2635, 0.6075]],\n",
      "\n",
      "        [[0.8487, 0.6739]],\n",
      "\n",
      "        [[0.1504, 0.7465]],\n",
      "\n",
      "        [[0.9210, 0.4544]],\n",
      "\n",
      "        [[0.4513, 0.4206]],\n",
      "\n",
      "        [[1.0978, 0.6180]],\n",
      "\n",
      "        [[0.3093, 0.5803]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "Encoder Attention output = \n",
      "tensor([[0.8298, 0.5688, 0.2635, 0.6075],\n",
      "        [0.8487, 0.6739, 0.1504, 0.7465],\n",
      "        [0.9210, 0.4544, 0.4513, 0.4206],\n",
      "        [1.0978, 0.6180, 0.3093, 0.5803]], grad_fn=<ViewBackward0>)\n",
      "\n",
      "Final Encoder 2 Output :\n",
      "norm2(norm1(x + self_atten(x)) + feed_fwd_op)\n",
      "\n",
      "tensor([[[-0.6045, -0.3659, -0.7456,  1.7160],\n",
      "         [-0.3525, -0.5226, -0.8313,  1.7063],\n",
      "         [-0.9077, -0.3700, -0.4154,  1.6931],\n",
      "         [-0.0998, -1.2328, -0.2232,  1.5558]]], grad_fn=<AddBackward0>)\n",
      "\n",
      "### Encoder Done ###\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "### Decoder Start ###\n",
      "Q_dec_0 = \n",
      "tensor([[[-3.1998,  2.1538]],\n",
      "\n",
      "        [[-1.0128,  0.7383]],\n",
      "\n",
      "        [[-2.6388,  1.2949]],\n",
      "\n",
      "        [[-0.3135,  0.9396]],\n",
      "\n",
      "        [[-2.5799,  0.7808]],\n",
      "\n",
      "        [[-0.3913, -0.4072]],\n",
      "\n",
      "        [[-2.3275,  1.0272]],\n",
      "\n",
      "        [[-0.7647, -0.5439]]])\n",
      "\n",
      "K_dec_0 = \n",
      "tensor([[[ 1.0097,  1.4010]],\n",
      "\n",
      "        [[ 0.8216, -3.9416]],\n",
      "\n",
      "        [[ 1.1088,  2.1519]],\n",
      "\n",
      "        [[ 0.0169, -3.9404]],\n",
      "\n",
      "        [[ 0.6041,  1.1918]],\n",
      "\n",
      "        [[-0.2197, -2.3607]],\n",
      "\n",
      "        [[ 0.5968,  0.5446]],\n",
      "\n",
      "        [[ 0.1456, -1.7790]]])\n",
      "\n",
      "V_dec_0 = \n",
      "tensor([[[-2.4218,  1.2453]],\n",
      "\n",
      "        [[ 0.6700,  1.9331]],\n",
      "\n",
      "        [[-1.5839,  0.7873]],\n",
      "\n",
      "        [[ 0.8833,  1.9419]],\n",
      "\n",
      "        [[-2.1300,  0.6044]],\n",
      "\n",
      "        [[ 0.7146,  0.7506]],\n",
      "\n",
      "        [[-2.1505,  0.7811]],\n",
      "\n",
      "        [[ 0.6343,  0.5329]]])\n",
      "\n",
      "Decoder Self Attention = \n",
      "tensor([[-2.4218,  1.2453,  0.6700,  1.9331],\n",
      "        [-1.5839,  0.7873,  0.8833,  1.9419],\n",
      "        [-2.1300,  0.6044,  0.7146,  0.7506],\n",
      "        [-2.1505,  0.7811,  0.6343,  0.5329]])\n",
      "\n",
      "Decoder_0 norm1(x + sa(x))\n",
      "tensor([[[-1.1561, -0.8215,  1.1174,  0.8602],\n",
      "         [-1.1617, -0.8118,  1.1353,  0.8382],\n",
      "         [-0.7746, -1.1629,  0.6767,  1.2608],\n",
      "         [-0.8526, -1.0996,  0.6949,  1.2574]]], grad_fn=<AddBackward0>)\n",
      "\n",
      "Q_dec_0 = \n",
      "tensor([[[-0.5945, -0.7982]],\n",
      "\n",
      "        [[ 0.2541, -1.1856]],\n",
      "\n",
      "        [[-0.5907, -0.8145]],\n",
      "\n",
      "        [[ 0.2590, -1.1908]],\n",
      "\n",
      "        [[-0.5886, -0.3429]],\n",
      "\n",
      "        [[ 0.0768, -0.9543]],\n",
      "\n",
      "        [[-0.6079, -0.3806]],\n",
      "\n",
      "        [[ 0.1015, -0.9830]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "K_dec_0 = \n",
      "tensor([[[-0.7237, -0.3619]],\n",
      "\n",
      "        [[ 1.0531, -0.5075]],\n",
      "\n",
      "        [[-0.6827, -0.4790]],\n",
      "\n",
      "        [[ 1.2487, -0.4995]],\n",
      "\n",
      "        [[-0.5655, -0.3325]],\n",
      "\n",
      "        [[ 0.8726, -0.2977]],\n",
      "\n",
      "        [[-0.0128, -0.9119]],\n",
      "\n",
      "        [[ 1.5721,  0.1687]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "V_dec_0 = \n",
      "tensor([[[-0.9120,  1.2251]],\n",
      "\n",
      "        [[-0.1781,  1.1367]],\n",
      "\n",
      "        [[-1.1346,  1.1204]],\n",
      "\n",
      "        [[-0.0801,  1.2016]],\n",
      "\n",
      "        [[-0.6177,  1.2510]],\n",
      "\n",
      "        [[-0.4478,  0.9994]],\n",
      "\n",
      "        [[-1.2460,  0.6969]],\n",
      "\n",
      "        [[-0.4321,  1.0623]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "Cross attention in decoder_0\n",
      "tensor([[-0.9120,  1.2251, -0.1781,  1.1367],\n",
      "        [-1.1346,  1.1204, -0.0801,  1.2016],\n",
      "        [-0.6177,  1.2510, -0.4478,  0.9994],\n",
      "        [-1.2460,  0.6969, -0.4321,  1.0623]], grad_fn=<ViewBackward0>)\n",
      "\n",
      "norm2(x' + mha(x', mem)), \n",
      " where x' = Decoder_curr_layer norm1(x + sa(x))\n",
      "tensor([[[ 0.9655, -1.6739,  0.2678,  0.4405],\n",
      "         [ 0.9760, -1.6225,  0.0026,  0.6439],\n",
      "         [ 1.0582, -1.5949, -0.0417,  0.5785],\n",
      "         [ 0.8551, -1.5567, -0.1965,  0.8982]]], grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "\n",
      "norm3(x'' + ff(x'')) \n",
      " where, x'' = Decoder_curr_layer norm2(x' + mha(x'))\n",
      "tensor([[[ 1.0891, -1.6247,  0.3917,  0.1440],\n",
      "         [ 1.1280, -1.6081,  0.1253,  0.3547],\n",
      "         [ 1.2079, -1.5667,  0.0743,  0.2844],\n",
      "         [ 1.0352, -1.5867, -0.0839,  0.6353]]], grad_fn=<AddBackward0>)\n",
      "\n",
      "torch.Size([1, 4, 4]) torch.Size([1, 4, 4])\n",
      "Q_dec_1 = \n",
      "tensor([[[ 0.6075, -0.0099]],\n",
      "\n",
      "        [[ 1.2060,  0.9822]],\n",
      "\n",
      "        [[ 0.4839,  0.0549]],\n",
      "\n",
      "        [[ 1.0884,  0.8157]],\n",
      "\n",
      "        [[ 0.4348,  0.1743]],\n",
      "\n",
      "        [[ 1.0518,  0.8525]],\n",
      "\n",
      "        [[ 0.4025, -0.0228]],\n",
      "\n",
      "        [[ 0.9782,  0.5796]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "K_dec_1 = \n",
      "tensor([[[-0.1669, -0.3601]],\n",
      "\n",
      "        [[ 0.0291,  0.3061]],\n",
      "\n",
      "        [[-0.1030, -0.1233]],\n",
      "\n",
      "        [[ 0.1006,  0.2773]],\n",
      "\n",
      "        [[-0.1334, -0.0964]],\n",
      "\n",
      "        [[ 0.1874,  0.2714]],\n",
      "\n",
      "        [[-0.0004,  0.0849]],\n",
      "\n",
      "        [[ 0.0663,  0.2464]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "V_dec_1 = \n",
      "tensor([[[ 0.7971, -0.7642]],\n",
      "\n",
      "        [[-0.5205, -1.0952]],\n",
      "\n",
      "        [[ 0.8066, -0.9265]],\n",
      "\n",
      "        [[-0.4583, -0.9728]],\n",
      "\n",
      "        [[ 0.8741, -0.9646]],\n",
      "\n",
      "        [[-0.4691, -0.9737]],\n",
      "\n",
      "        [[ 0.7096, -1.0123]],\n",
      "\n",
      "        [[-0.3685, -0.8177]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "Decoder Self Attention = \n",
      "tensor([[ 0.7971, -0.7642, -0.5205, -1.0952],\n",
      "        [ 0.8066, -0.9265, -0.4583, -0.9728],\n",
      "        [ 0.8741, -0.9646, -0.4691, -0.9737],\n",
      "        [ 0.7096, -1.0123, -0.3685, -0.8177]], grad_fn=<ViewBackward0>)\n",
      "\n",
      "Decoder_1 norm1(x + sa(x))\n",
      "tensor([[[ 1.4323, -1.3191,  0.2612, -0.3744],\n",
      "         [ 1.4362, -1.3618,  0.1630, -0.2373],\n",
      "         [ 1.4604, -1.3277,  0.1524, -0.2850],\n",
      "         [ 1.3922, -1.4338,  0.0720, -0.0304]]], grad_fn=<AddBackward0>)\n",
      "\n",
      "Q_dec_1 = \n",
      "tensor([[[ 0.3737,  1.1011]],\n",
      "\n",
      "        [[ 1.0474, -0.2670]],\n",
      "\n",
      "        [[ 0.4162,  1.0716]],\n",
      "\n",
      "        [[ 1.0037, -0.2485]],\n",
      "\n",
      "        [[ 0.4251,  1.0817]],\n",
      "\n",
      "        [[ 1.0072, -0.3037]],\n",
      "\n",
      "        [[ 0.4442,  1.0150]],\n",
      "\n",
      "        [[ 0.9416, -0.1556]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "K_dec_1 = \n",
      "tensor([[[ 0.5865, -1.5370]],\n",
      "\n",
      "        [[-0.0593,  0.3249]],\n",
      "\n",
      "        [[ 0.6022, -1.4020]],\n",
      "\n",
      "        [[-0.0278,  0.1153]],\n",
      "\n",
      "        [[ 0.4098, -1.5959]],\n",
      "\n",
      "        [[-0.1676,  0.5825]],\n",
      "\n",
      "        [[ 0.1368, -0.9322]],\n",
      "\n",
      "        [[-0.2051, -0.0747]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "V_dec_1 = \n",
      "tensor([[[ 1.4120, -0.6568]],\n",
      "\n",
      "        [[-0.7945, -1.0526]],\n",
      "\n",
      "        [[ 1.3624, -0.6216]],\n",
      "\n",
      "        [[-0.8882, -1.2764]],\n",
      "\n",
      "        [[ 1.4343, -0.6807]],\n",
      "\n",
      "        [[-0.5334, -0.8036]],\n",
      "\n",
      "        [[ 1.1730, -0.5183]],\n",
      "\n",
      "        [[-0.5200, -1.5246]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "Cross attention in decoder_1\n",
      "tensor([[ 1.4120, -0.6568, -0.7945, -1.0526],\n",
      "        [ 1.3624, -0.6216, -0.8882, -1.2764],\n",
      "        [ 1.4343, -0.6807, -0.5334, -0.8036],\n",
      "        [ 1.1730, -0.5183, -0.5200, -1.5246]], grad_fn=<ViewBackward0>)\n",
      "\n",
      "norm2(x' + mha(x', mem)), \n",
      " where x' = Decoder_curr_layer norm1(x + sa(x))\n",
      "tensor([[[ 1.6425, -0.1400, -0.4741, -1.0285],\n",
      "         [ 1.6183, -0.0168, -0.5787, -1.0228],\n",
      "         [ 1.7071, -0.3861, -0.4812, -0.8398],\n",
      "         [ 1.6379, -0.0619, -0.5991, -0.9769]]], grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "\n",
      "norm3(x'' + ff(x'')) \n",
      " where, x'' = Decoder_curr_layer norm2(x' + mha(x'))\n",
      "tensor([[[ 1.6675, -0.3092, -0.3619, -0.9964],\n",
      "         [ 1.6573, -0.1863, -0.4741, -0.9969],\n",
      "         [ 1.7130, -0.5573, -0.3694, -0.7864],\n",
      "         [ 1.6747, -0.2326, -0.4961, -0.9460]]], grad_fn=<AddBackward0>)\n",
      "\n",
      "torch.Size([1, 4, 4]) torch.Size([1, 4, 4])\n",
      "Q_dec_2 = \n",
      "tensor([[[-0.4564,  0.4040]],\n",
      "\n",
      "        [[-0.0769, -0.1547]],\n",
      "\n",
      "        [[-0.4492,  0.3951]],\n",
      "\n",
      "        [[-0.1052, -0.1132]],\n",
      "\n",
      "        [[-0.4883,  0.5261]],\n",
      "\n",
      "        [[-0.0320, -0.0809]],\n",
      "\n",
      "        [[-0.4584,  0.4274]],\n",
      "\n",
      "        [[-0.0984, -0.0877]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "K_dec_2 = \n",
      "tensor([[[-0.1585,  1.1721]],\n",
      "\n",
      "        [[ 0.5348, -0.1444]],\n",
      "\n",
      "        [[-0.0538,  1.1324]],\n",
      "\n",
      "        [[ 0.4825, -0.1531]],\n",
      "\n",
      "        [[-0.3281,  1.0430]],\n",
      "\n",
      "        [[ 0.7169, -0.1352]],\n",
      "\n",
      "        [[-0.0818,  1.0969]],\n",
      "\n",
      "        [[ 0.5240, -0.1527]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "V_dec_2 = \n",
      "tensor([[[-0.6207,  0.2833]],\n",
      "\n",
      "        [[ 1.2686,  0.4266]],\n",
      "\n",
      "        [[-0.6454,  0.3579]],\n",
      "\n",
      "        [[ 1.2686,  0.4896]],\n",
      "\n",
      "        [[-0.6633,  0.1246]],\n",
      "\n",
      "        [[ 1.0597,  0.2788]],\n",
      "\n",
      "        [[-0.6633,  0.3297]],\n",
      "\n",
      "        [[ 1.2191,  0.4629]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "Decoder Self Attention = \n",
      "tensor([[-0.6207,  0.2833,  1.2686,  0.4266],\n",
      "        [-0.6454,  0.3579,  1.2686,  0.4896],\n",
      "        [-0.6633,  0.1246,  1.0597,  0.2788],\n",
      "        [-0.6633,  0.3297,  1.2191,  0.4629]], grad_fn=<ViewBackward0>)\n",
      "\n",
      "Decoder_2 norm1(x + sa(x))\n",
      "tensor([[[ 1.5877e+00,  7.9300e-04, -1.1240e+00, -4.6449e-01],\n",
      "         [ 1.4409e+00,  2.3308e-01, -1.3209e+00, -3.5300e-01],\n",
      "         [ 1.6934e+00, -4.5037e-01, -9.0165e-01, -3.4137e-01],\n",
      "         [ 1.4644e+00,  1.6218e-01, -1.3164e+00, -3.1023e-01]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "\n",
      "Q_dec_2 = \n",
      "tensor([[[ 0.1865, -0.5781]],\n",
      "\n",
      "        [[ 0.6103, -0.1993]],\n",
      "\n",
      "        [[ 0.1109, -0.3085]],\n",
      "\n",
      "        [[ 0.5972, -0.3766]],\n",
      "\n",
      "        [[ 0.3719, -0.8744]],\n",
      "\n",
      "        [[ 0.7502,  0.1472]],\n",
      "\n",
      "        [[ 0.1462, -0.3432]],\n",
      "\n",
      "        [[ 0.6381, -0.3249]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "K_dec_2 = \n",
      "tensor([[[-0.9461, -0.4201]],\n",
      "\n",
      "        [[ 0.6541, -0.5913]],\n",
      "\n",
      "        [[-0.9505, -0.4142]],\n",
      "\n",
      "        [[ 0.5800, -0.6170]],\n",
      "\n",
      "        [[-0.7898, -0.4899]],\n",
      "\n",
      "        [[ 0.6221, -0.3727]],\n",
      "\n",
      "        [[-0.4857, -0.5845]],\n",
      "\n",
      "        [[ 0.1362, -0.0720]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "V_dec_2 = \n",
      "tensor([[[-0.1976,  0.1209]],\n",
      "\n",
      "        [[-0.5734, -1.4606]],\n",
      "\n",
      "        [[-0.1951,  0.1337]],\n",
      "\n",
      "        [[-0.7044, -1.4274]],\n",
      "\n",
      "        [[-0.2685,  0.0153]],\n",
      "\n",
      "        [[-0.3249, -1.3999]],\n",
      "\n",
      "        [[-0.3893, -0.1243]],\n",
      "\n",
      "        [[-0.5464, -1.0646]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "Cross attention in decoder_2\n",
      "tensor([[-0.1976,  0.1209, -0.5734, -1.4606],\n",
      "        [-0.1951,  0.1337, -0.7044, -1.4274],\n",
      "        [-0.2685,  0.0153, -0.3249, -1.3999],\n",
      "        [-0.3893, -0.1243, -0.5464, -1.0646]], grad_fn=<ViewBackward0>)\n",
      "\n",
      "norm2(x' + mha(x', mem)), \n",
      " where x' = Decoder_curr_layer norm1(x + sa(x))\n",
      "tensor([[[ 1.5265, -0.1745, -1.2783, -0.0736],\n",
      "         [ 1.4271, -0.0056, -1.4010, -0.0205],\n",
      "         [ 1.6042, -0.4852, -1.0910, -0.0281],\n",
      "         [ 1.5221, -0.0578, -1.2834, -0.1809]]], grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "\n",
      "norm3(x'' + ff(x'')) \n",
      " where, x'' = Decoder_curr_layer norm2(x' + mha(x'))\n",
      "tensor([[[ 1.5900, -0.2046, -1.1776, -0.2079],\n",
      "         [ 1.4772,  0.0127, -1.3398, -0.1502],\n",
      "         [ 1.6628, -0.5940, -0.9286, -0.1402],\n",
      "         [ 1.5767, -0.0540, -1.1808, -0.3419]]], grad_fn=<AddBackward0>)\n",
      "\n",
      "torch.Size([1, 4, 4]) torch.Size([1, 4, 4])\n",
      "### Decoder Done ###\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.1736, -0.6635, -0.7838, -0.4535, -1.0496, -0.8957,  0.4070,\n",
       "           0.4802,  0.3982, -0.3234],\n",
       "         [-0.1878, -0.6338, -0.7410, -0.4279, -1.0766, -0.9809,  0.3347,\n",
       "           0.5342,  0.4060, -0.4664],\n",
       "         [-0.1235, -0.6708, -0.8959, -0.4473, -0.9361, -0.7919,  0.5501,\n",
       "           0.3801,  0.3818, -0.0212],\n",
       "         [-0.1777, -0.6718, -0.6719, -0.4529, -1.0911, -0.8548,  0.3259,\n",
       "           0.5181,  0.4016, -0.4310]]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_model = 4\n",
    "num_encoder_layers = 3\n",
    "num_decoder_layers = 3\n",
    "\n",
    "get_all_intermediate_outputs(src_sentence, tgt_sentence, model = model, num_encoder_layers = num_encoder_layers , num_decoder_layers = num_encoder_layers, d_model=d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "my_project_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
