{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-head attention \n",
    "### Encoder and Decoder\n",
    "### (Without masking)\n",
    "\n",
    "Pytorch's implementation (in built)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source embeddings :- \n",
      "\n",
      "tensor([[[-0.7902,  0.2501,  1.5278, -0.5044],\n",
      "         [-1.6202, -0.6824,  0.4107,  0.1833],\n",
      "         [ 0.4261, -1.3366, -1.2978, -0.8236],\n",
      "         [-0.4685,  1.9740,  0.3854, -0.9693]]], grad_fn=<EmbeddingBackward0>)\n",
      "\n",
      "Target embeddings :- \n",
      "\n",
      "tensor([[[-0.7213,  0.7379,  1.2300,  1.0213],\n",
      "         [ 1.3452,  0.7265,  1.7796, -1.9332],\n",
      "         [ 2.4084,  1.7481, -1.9929,  0.3660],\n",
      "         [ 2.4084,  1.7481, -1.9929,  0.3660]]], grad_fn=<EmbeddingBackward0>)\n",
      "\n",
      "Positional encoded source embeddings:\n",
      "tensor([[[-0.7902,  2.2501,  3.5278,  3.4956],\n",
      "         [-0.7787,  0.8579,  2.4207,  4.1832],\n",
      "         [ 1.3354, -0.7528,  0.7222,  3.1762],\n",
      "         [-0.3274,  1.9840,  2.4154,  3.0303]]], grad_fn=<AddBackward0>)\n",
      "\n",
      "Positional encoded target embeddings:\n",
      "tensor([[[0.2787, 1.7379, 4.2300, 5.0213],\n",
      "         [3.1867, 1.2668, 4.7896, 2.0668],\n",
      "         [4.3177, 1.3320, 1.0271, 4.3658],\n",
      "         [3.5496, 0.7582, 1.0371, 4.3655]]], grad_fn=<AddBackward0>)\n",
      "\n",
      "query =  tensor([[[-0.7902,  2.2501,  3.5278,  3.4956],\n",
      "         [-0.7787,  0.8579,  2.4207,  4.1832],\n",
      "         [ 1.3354, -0.7528,  0.7222,  3.1762],\n",
      "         [-0.3274,  1.9840,  2.4154,  3.0303]]], grad_fn=<AddBackward0>)\n",
      "query shape =  torch.Size([1, 4, 4])\n",
      "\n",
      " _in_projection_packed \n",
      "\n",
      "Q :-  tensor([[[ 0.5279, -2.2983, -1.0550, -1.1707],\n",
      "         [ 0.6662, -1.4912, -1.2663, -1.0478],\n",
      "         [ 1.1158,  0.0281, -0.6286, -1.5645],\n",
      "         [ 0.7990, -1.7561, -0.9101, -1.1756]]], grad_fn=<SelectBackward0>)\n",
      "\n",
      "K :-  tensor([[[-0.6694,  0.4451, -0.7058,  4.3720],\n",
      "         [-0.7618,  0.1292, -1.4209,  3.7035],\n",
      "         [-0.5826, -0.2429, -1.7014,  1.3370],\n",
      "         [-0.9576,  0.1478, -0.8108,  3.5906]]], grad_fn=<SelectBackward0>)\n",
      "\n",
      "V :-  tensor([[[ 4.4475,  0.1953,  1.3194,  2.5191],\n",
      "         [ 4.0303,  0.9956, -0.0675,  1.8946],\n",
      "         [ 1.9893,  1.7287, -0.5388, -0.3410],\n",
      "         [ 3.4502,  0.4872,  0.9050,  1.7593]]], grad_fn=<SelectBackward0>)\n",
      "\n",
      "tgt_len =  1\n",
      "bsz =  4\n",
      "num_heads =  2\n",
      "head_dim =  2\n",
      "\n",
      "tensor([[[ 0.5279, -2.2983, -1.0550, -1.1707],\n",
      "         [ 0.6662, -1.4912, -1.2663, -1.0478],\n",
      "         [ 1.1158,  0.0281, -0.6286, -1.5645],\n",
      "         [ 0.7990, -1.7561, -0.9101, -1.1756]]], grad_fn=<SelectBackward0>) torch.Size([1, 4, 4])\n",
      "Q reshaped =  tensor([[[ 0.5279, -2.2983]],\n",
      "\n",
      "        [[-1.0550, -1.1707]],\n",
      "\n",
      "        [[ 0.6662, -1.4912]],\n",
      "\n",
      "        [[-1.2663, -1.0478]],\n",
      "\n",
      "        [[ 1.1158,  0.0281]],\n",
      "\n",
      "        [[-0.6286, -1.5645]],\n",
      "\n",
      "        [[ 0.7990, -1.7561]],\n",
      "\n",
      "        [[-0.9101, -1.1756]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "K reshaped =  tensor([[[-0.6694,  0.4451]],\n",
      "\n",
      "        [[-0.7058,  4.3720]],\n",
      "\n",
      "        [[-0.7618,  0.1292]],\n",
      "\n",
      "        [[-1.4209,  3.7035]],\n",
      "\n",
      "        [[-0.5826, -0.2429]],\n",
      "\n",
      "        [[-1.7014,  1.3370]],\n",
      "\n",
      "        [[-0.9576,  0.1478]],\n",
      "\n",
      "        [[-0.8108,  3.5906]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "V reshaped =  tensor([[[ 4.4475,  0.1953]],\n",
      "\n",
      "        [[ 1.3194,  2.5191]],\n",
      "\n",
      "        [[ 4.0303,  0.9956]],\n",
      "\n",
      "        [[-0.0675,  1.8946]],\n",
      "\n",
      "        [[ 1.9893,  1.7287]],\n",
      "\n",
      "        [[-0.5388, -0.3410]],\n",
      "\n",
      "        [[ 3.4502,  0.4872]],\n",
      "\n",
      "        [[ 0.9050,  1.7593]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "torch.Size([4, 2, 1, 2]) torch.Size([4, 2, 1, 2]) torch.Size([4, 2, 1, 2])\n",
      "\n",
      "scaled_dot_product_attention\n",
      "attn_output = \n",
      "tensor([[ 4.4475,  0.1953,  1.3194,  2.5191],\n",
      "        [ 4.0303,  0.9956, -0.0675,  1.8946],\n",
      "        [ 1.9893,  1.7287, -0.5388, -0.3410],\n",
      "        [ 3.4502,  0.4872,  0.9050,  1.7593]], grad_fn=<ViewBackward0>)\n",
      "\n",
      "x + self_atten(x) = \n",
      "tensor([[[-1.4862,  1.8661,  4.0148,  2.5770],\n",
      "         [-0.8138,  2.0059,  2.1528,  1.9926],\n",
      "         [ 1.1531,  1.4032, -0.4664,  0.8864],\n",
      "         [-0.7956,  2.0351,  2.5134,  1.9907]]], grad_fn=<AddBackward0>)\n",
      "\n",
      "norm1(x + self_atten(x)) = \n",
      "tensor([[[-1.5997,  0.0610,  1.1254,  0.4132],\n",
      "         [-1.7298,  0.5408,  0.6590,  0.5300],\n",
      "         [ 0.5662,  0.9124, -1.6757,  0.1971],\n",
      "         [-1.7105,  0.4593,  0.8259,  0.4253]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "\n",
      "feed_fwd_op =  tensor([[[-0.3322,  0.1667,  0.5269, -0.7270],\n",
      "         [-0.3123,  0.1495,  0.5369, -0.7439],\n",
      "         [-0.1717,  0.0792,  0.6190, -0.5591],\n",
      "         [-0.3209,  0.1534,  0.5329, -0.7450]]], grad_fn=<ViewBackward0>)\n",
      "\n",
      "norm2(norm1(x + self_atten(x)) + feed_fwd_op) = \n",
      "tensor([[[-1.4351,  0.2488,  1.3596, -0.1734],\n",
      "         [-1.5803,  0.6344,  1.0443, -0.0984],\n",
      "         [ 0.5213,  1.2944, -1.3575, -0.4581],\n",
      "         [-1.5292,  0.5588,  1.1479, -0.1776]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "\n",
      "##################################\n",
      "\n",
      "################### Transformer Decoder start #####################\n",
      "\n",
      "query =  tensor([[[0.2787, 1.7379, 4.2300, 5.0213],\n",
      "         [3.1867, 1.2668, 4.7896, 2.0668],\n",
      "         [4.3177, 1.3320, 1.0271, 4.3658],\n",
      "         [3.5496, 0.7582, 1.0371, 4.3655]]], grad_fn=<AddBackward0>)\n",
      "query shape =  torch.Size([1, 4, 4])\n",
      "\n",
      " _in_projection_packed \n",
      "\n",
      "Q :-  tensor([[[ 0.5871, -3.8831,  4.2142, -2.5878],\n",
      "         [ 2.9569, -1.3021,  4.5938,  0.1096],\n",
      "         [ 0.0119, -0.3000,  4.3522, -0.2060],\n",
      "         [-0.4440, -0.9314,  3.7144, -0.5702]]], grad_fn=<SelectBackward0>)\n",
      "\n",
      "K :-  tensor([[[0.9020, 3.8072, 2.5476, 2.3639],\n",
      "         [3.1392, 2.3370, 3.9712, 2.5925],\n",
      "         [1.8144, 2.4839, 2.6009, 2.5871],\n",
      "         [1.1623, 2.4910, 2.2101, 2.6568]]], grad_fn=<SelectBackward0>)\n",
      "\n",
      "V :-  tensor([[[ 1.6530, -3.7049, -0.6983, -0.4213],\n",
      "         [ 0.4364, -3.0483,  0.2365, -1.1687],\n",
      "         [-1.8952, -1.8379, -3.1507, -2.0810],\n",
      "         [-1.3617, -1.7368, -2.8310, -1.5591]]], grad_fn=<SelectBackward0>)\n",
      "\n",
      "tgt_len =  1\n",
      "bsz =  4\n",
      "num_heads =  2\n",
      "head_dim =  2\n",
      "\n",
      "tensor([[[ 0.5871, -3.8831,  4.2142, -2.5878],\n",
      "         [ 2.9569, -1.3021,  4.5938,  0.1096],\n",
      "         [ 0.0119, -0.3000,  4.3522, -0.2060],\n",
      "         [-0.4440, -0.9314,  3.7144, -0.5702]]], grad_fn=<SelectBackward0>) torch.Size([1, 4, 4])\n",
      "Q reshaped =  tensor([[[ 0.5871, -3.8831]],\n",
      "\n",
      "        [[ 4.2142, -2.5878]],\n",
      "\n",
      "        [[ 2.9569, -1.3021]],\n",
      "\n",
      "        [[ 4.5938,  0.1096]],\n",
      "\n",
      "        [[ 0.0119, -0.3000]],\n",
      "\n",
      "        [[ 4.3522, -0.2060]],\n",
      "\n",
      "        [[-0.4440, -0.9314]],\n",
      "\n",
      "        [[ 3.7144, -0.5702]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "K reshaped =  tensor([[[0.9020, 3.8072]],\n",
      "\n",
      "        [[2.5476, 2.3639]],\n",
      "\n",
      "        [[3.1392, 2.3370]],\n",
      "\n",
      "        [[3.9712, 2.5925]],\n",
      "\n",
      "        [[1.8144, 2.4839]],\n",
      "\n",
      "        [[2.6009, 2.5871]],\n",
      "\n",
      "        [[1.1623, 2.4910]],\n",
      "\n",
      "        [[2.2101, 2.6568]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "V reshaped =  tensor([[[ 1.6530, -3.7049]],\n",
      "\n",
      "        [[-0.6983, -0.4213]],\n",
      "\n",
      "        [[ 0.4364, -3.0483]],\n",
      "\n",
      "        [[ 0.2365, -1.1687]],\n",
      "\n",
      "        [[-1.8952, -1.8379]],\n",
      "\n",
      "        [[-3.1507, -2.0810]],\n",
      "\n",
      "        [[-1.3617, -1.7368]],\n",
      "\n",
      "        [[-2.8310, -1.5591]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "torch.Size([4, 2, 1, 2]) torch.Size([4, 2, 1, 2]) torch.Size([4, 2, 1, 2])\n",
      "\n",
      "scaled_dot_product_attention\n",
      "attn_output = \n",
      "tensor([[ 1.6530, -3.7049, -0.6983, -0.4213],\n",
      "        [ 0.4364, -3.0483,  0.2365, -1.1687],\n",
      "        [-1.8952, -1.8379, -3.1507, -2.0810],\n",
      "        [-1.3617, -1.7368, -2.8310, -1.5591]], grad_fn=<ViewBackward0>)\n",
      "\n",
      "x =  tensor([[[0.2787, 1.7379, 4.2300, 5.0213],\n",
      "         [3.1867, 1.2668, 4.7896, 2.0668],\n",
      "         [4.3177, 1.3320, 1.0271, 4.3658],\n",
      "         [3.5496, 0.7582, 1.0371, 4.3655]]], grad_fn=<AddBackward0>)\n",
      "\n",
      "sa_op =  tensor([[[ 0.8414, -2.6533,  1.9088,  1.6049],\n",
      "         [ 0.7398, -2.4941,  2.2667,  0.5465],\n",
      "         [-0.7669, -2.4307, -0.1783, -1.5623],\n",
      "         [-0.6269, -2.1283, -0.1805, -1.0618]]], grad_fn=<ViewBackward0>)\n",
      "\n",
      "temop =  tensor([[[ 1.1201, -0.9154,  6.1388,  6.6262],\n",
      "         [ 3.9264, -1.2273,  7.0563,  2.6133],\n",
      "         [ 3.5508, -1.0987,  0.8488,  2.8034],\n",
      "         [ 2.9227, -1.3702,  0.8567,  3.3037]]], grad_fn=<AddBackward0>)\n",
      "\n",
      "Decoder norm1(x + sa(x)) = \n",
      "tensor([[[-0.6579, -1.2888,  0.8978,  1.0489],\n",
      "         [ 0.2808, -1.4541,  1.3345, -0.1612],\n",
      "         [ 1.1197, -1.4515, -0.3745,  0.7064],\n",
      "         [ 0.8014, -1.5007, -0.3065,  1.0058]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "\n",
      "Memory = \n",
      "tensor([[[-1.4351,  0.2488,  1.3596, -0.1734],\n",
      "         [-1.5803,  0.6344,  1.0443, -0.0984],\n",
      "         [ 0.5213,  1.2944, -1.3575, -0.4582],\n",
      "         [-1.5292,  0.5588,  1.1479, -0.1776]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "\n",
      "query =  tensor([[[-0.6579, -1.2888,  0.8978,  1.0489],\n",
      "         [ 0.2808, -1.4541,  1.3345, -0.1612],\n",
      "         [ 1.1197, -1.4515, -0.3745,  0.7064],\n",
      "         [ 0.8014, -1.5007, -0.3065,  1.0058]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "query shape =  torch.Size([1, 4, 4])\n",
      "\n",
      " _in_projection_packed \n",
      "\n",
      "Q :-  tensor([[[ 0.9136, -0.6283, -0.8274, -0.4683],\n",
      "         [ 1.0450, -0.3151, -0.8564, -0.7002],\n",
      "         [ 0.8594,  0.4279, -0.9335, -0.7778],\n",
      "         [ 0.9027,  0.2643, -0.9781, -0.7504]]], grad_fn=<ViewBackward0>)\n",
      "\n",
      "K :-  tensor([[[-0.0191, -0.7460, -1.2817,  1.1603],\n",
      "         [-0.1216, -0.6533, -1.0173,  1.2253],\n",
      "         [-0.2807,  0.9705,  1.7984, -0.6181],\n",
      "         [-0.0946, -0.6538, -1.0587,  1.1941]]], grad_fn=<SelectBackward0>)\n",
      "\n",
      "V :-  tensor([[[ 0.6794,  0.3024,  0.1560, -1.2189],\n",
      "         [ 0.8529, -0.0325,  0.1457, -1.3982],\n",
      "         [-0.2030, -1.4923, -0.1055, -0.4424],\n",
      "         [ 0.7814,  0.0261,  0.1489, -1.3966]]], grad_fn=<SelectBackward0>)\n",
      "\n",
      "tgt_len =  1\n",
      "bsz =  4\n",
      "num_heads =  2\n",
      "head_dim =  2\n",
      "\n",
      "tensor([[[ 0.9136, -0.6283, -0.8274, -0.4683],\n",
      "         [ 1.0450, -0.3151, -0.8564, -0.7002],\n",
      "         [ 0.8594,  0.4279, -0.9335, -0.7778],\n",
      "         [ 0.9027,  0.2643, -0.9781, -0.7504]]], grad_fn=<ViewBackward0>) torch.Size([1, 4, 4])\n",
      "Q reshaped =  tensor([[[ 0.9136, -0.6283]],\n",
      "\n",
      "        [[-0.8274, -0.4683]],\n",
      "\n",
      "        [[ 1.0450, -0.3151]],\n",
      "\n",
      "        [[-0.8564, -0.7002]],\n",
      "\n",
      "        [[ 0.8594,  0.4279]],\n",
      "\n",
      "        [[-0.9335, -0.7778]],\n",
      "\n",
      "        [[ 0.9027,  0.2643]],\n",
      "\n",
      "        [[-0.9781, -0.7504]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "K reshaped =  tensor([[[-0.0191, -0.7460]],\n",
      "\n",
      "        [[-1.2817,  1.1603]],\n",
      "\n",
      "        [[-0.1216, -0.6533]],\n",
      "\n",
      "        [[-1.0173,  1.2253]],\n",
      "\n",
      "        [[-0.2807,  0.9705]],\n",
      "\n",
      "        [[ 1.7984, -0.6181]],\n",
      "\n",
      "        [[-0.0946, -0.6538]],\n",
      "\n",
      "        [[-1.0587,  1.1941]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "V reshaped =  tensor([[[ 0.6794,  0.3024]],\n",
      "\n",
      "        [[ 0.1560, -1.2189]],\n",
      "\n",
      "        [[ 0.8529, -0.0325]],\n",
      "\n",
      "        [[ 0.1457, -1.3982]],\n",
      "\n",
      "        [[-0.2030, -1.4923]],\n",
      "\n",
      "        [[-0.1055, -0.4424]],\n",
      "\n",
      "        [[ 0.7814,  0.0261]],\n",
      "\n",
      "        [[ 0.1489, -1.3966]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "torch.Size([4, 2, 1, 2]) torch.Size([4, 2, 1, 2]) torch.Size([4, 2, 1, 2])\n",
      "\n",
      "scaled_dot_product_attention\n",
      "attn_output = \n",
      "tensor([[ 0.6794,  0.3024,  0.1560, -1.2189],\n",
      "        [ 0.8529, -0.0325,  0.1457, -1.3982],\n",
      "        [-0.2030, -1.4923, -0.1055, -0.4424],\n",
      "        [ 0.7814,  0.0261,  0.1489, -1.3966]], grad_fn=<ViewBackward0>)\n",
      "\n",
      "Decoder norm2(x + mha(x, mem)) = \n",
      "tensor([[[-0.9941, -0.6906,  0.0954,  1.5892],\n",
      "         [ 0.9460, -1.5990,  0.7356, -0.0825],\n",
      "         [ 0.9940, -1.4994, -0.3113,  0.8166],\n",
      "         [ 0.8530, -0.6956, -1.2569,  1.0995]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "\n",
      "Decoder norm3(x + ff(x)) = \n",
      "tensor([[[-0.4839, -1.0977, -0.0186,  1.6002],\n",
      "         [ 1.0259, -1.6139,  0.5856,  0.0024],\n",
      "         [ 1.1085, -1.4980, -0.2804,  0.6698],\n",
      "         [ 1.0902, -0.9253, -1.0678,  0.9029]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "\n",
      "tensor([[[-0.4911,  0.1506,  0.7609,  1.1714,  0.0706,  0.0303,  0.0905,\n",
      "           0.0423, -0.8750, -0.0365],\n",
      "         [-0.6612, -0.7883,  0.2774,  0.2475, -0.7074,  0.0212, -1.3313,\n",
      "          -0.3360, -0.1797,  0.7370],\n",
      "         [-1.0149, -0.8154,  0.5574,  0.2583,  0.0395,  0.0466, -0.6874,\n",
      "          -0.8217, -0.5338,  0.7056],\n",
      "         [-1.2632, -0.8038,  0.6587,  0.1799,  0.6946, -0.0544, -0.0378,\n",
      "          -1.0047, -0.7610,  0.4892]]], grad_fn=<ViewBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sreevaatsav/.pyenv/versions/project_env/lib/python3.10/site-packages/torch/nn/modules/transformer.py:282: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=512, dropout=0):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.encoding = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1).float()\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(torch.log(torch.tensor(10000.0)) / d_model))\n",
    "        self.encoding[:, 0::2] = torch.sin(position * div_term)\n",
    "        self.encoding[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.encoding = self.encoding.unsqueeze(0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.encoding[:, :x.size(1)].detach()\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, src_vocab_size, tgt_vocab_size, d_model=4, nhead=2, num_encoder_layers=1, num_decoder_layers=1):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.src_embedding = nn.Embedding(src_vocab_size, d_model)\n",
    "        self.tgt_embedding = nn.Embedding(tgt_vocab_size, d_model)\n",
    "        self.positional_encoding = PositionalEncoding(d_model, dropout=0)\n",
    "        self.transformer = nn.Transformer(\n",
    "            d_model=d_model,\n",
    "            nhead=nhead,\n",
    "            num_encoder_layers=num_encoder_layers,\n",
    "            num_decoder_layers=num_decoder_layers,\n",
    "            dropout=0\n",
    "        )\n",
    "        self.fc = nn.Linear(d_model, tgt_vocab_size)\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "\n",
    "        print(\"Source embeddings :- \\n\")\n",
    "        print(self.src_embedding(src))\n",
    "        print()\n",
    "\n",
    "        print(\"Target embeddings :- \\n\")\n",
    "        print(self.tgt_embedding(tgt))\n",
    "        print()\n",
    "\n",
    "\n",
    "        src = self.src_embedding(src) + self.positional_encoding(src)\n",
    "        tgt = self.tgt_embedding(tgt) + self.positional_encoding(tgt)\n",
    "\n",
    "        print(\"Positional encoded source embeddings:\")\n",
    "        print(src)\n",
    "        print()\n",
    "\n",
    "        print(\"Positional encoded target embeddings:\")\n",
    "        print(tgt)\n",
    "        print()\n",
    "\n",
    "        output = self.transformer(src, tgt)\n",
    "        output = self.fc(output)\n",
    "        return output\n",
    "\n",
    "    \n",
    "\n",
    "src_vocab_size = 10  # Source language vocabulary size\n",
    "tgt_vocab_size = 10  # Target language vocabulary size\n",
    "d_model = 4  # Dimension of the model\n",
    "num_heads = 2\n",
    "model = TransformerModel(src_vocab_size, tgt_vocab_size, d_model=d_model, nhead=num_heads)\n",
    "# Source sentence in the source language\n",
    "src_sentence = torch.tensor([[0, 1, 2, 3]])  # Source sequence\n",
    "\n",
    "# Target sentence in the target language (translation of the source sentence)\n",
    "tgt_sentence = torch.tensor([[1, 0, 3, 3]])  # Target sequence\n",
    "\n",
    "# Forward pass\n",
    "output = model(src_sentence, tgt_sentence)\n",
    "print(output)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions to get the manual calulations for each component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import functional as F\n",
    "\n",
    "# Using the state dictionary to get the intermediate outputs\n",
    "state_dict = model.state_dict()\n",
    "\n",
    "need_weights = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embeddings and Positional encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding_outputs(src_sentence,  tgt_sentence, state_dict, d_model):\n",
    "\n",
    "    src_vocab_embeds = state_dict[\"src_embedding.weight\"]\n",
    "\n",
    "    src_embedding = torch.zeros(src_sentence.size(0), src_sentence.size(1), d_model)\n",
    "\n",
    "    print(\"Source sentence embedding\")\n",
    "    for i in range(src_sentence.size(0)):\n",
    "        for j in range(src_sentence.size(1)):\n",
    "            # Get the index for the current word in the sequence\n",
    "            word_index = src_sentence[i, j].item()\n",
    "\n",
    "            # Check if the index is within valid range\n",
    "            if word_index < 0 or word_index >= src_vocab_embeds.size(0):\n",
    "                raise ValueError(f\"Invalid word index: {word_index}\")\n",
    "\n",
    "            # Lookup the corresponding embedding vector for the word\n",
    "            src_embedding[i, j, :] = src_vocab_embeds[word_index, :]\n",
    "\n",
    "            # Print intermediate results for debugging\n",
    "            \n",
    "            print(f\"Word index: {word_index}, Embedding: {src_vocab_embeds[word_index, :]}\")\n",
    "    print()\n",
    "\n",
    "\n",
    "    tgt_vocab_embeds = state_dict[\"tgt_embedding.weight\"]\n",
    "\n",
    "    tgt_embedding = torch.zeros(tgt_sentence.size(0), tgt_sentence.size(1), d_model)\n",
    "\n",
    "\n",
    "    print(\"Target sentence embedding\")\n",
    "    for i in range(tgt_sentence.size(0)):\n",
    "        for j in range(tgt_sentence.size(1)):\n",
    "            # Get the index for the current word in the sequence\n",
    "            word_index = tgt_sentence[i, j].item()\n",
    "\n",
    "            # Check if the index is within valid range\n",
    "            if word_index < 0 or word_index >= tgt_vocab_embeds.size(0):\n",
    "                raise ValueError(f\"Invalid word index: {word_index}\")\n",
    "\n",
    "            # Lookup the corresponding embedding vector for the word\n",
    "            tgt_embedding[i, j, :] = tgt_vocab_embeds[word_index, :]\n",
    "\n",
    "            # Print intermediate results for debugging\n",
    "            print(f\"Word index: {word_index}, Embedding: {tgt_vocab_embeds[word_index, :]}\")\n",
    "    print()\n",
    "\n",
    "\n",
    "    pe = PositionalEncoding(d_model = d_model, dropout=0)\n",
    "\n",
    "    pe_src_embeds = src_embedding + pe(src_sentence)\n",
    "\n",
    "    pe_tgt_embeds = tgt_embedding + pe(tgt_sentence)\n",
    "\n",
    "    print(\"PE source embeddings : \\n\")\n",
    "    print(pe_src_embeds)\n",
    "    print()\n",
    "\n",
    "    print(\"PE target embeddings : \\n\")\n",
    "    print(pe_tgt_embeds)\n",
    "    print()\n",
    "\n",
    "    return pe_src_embeds, pe_tgt_embeds\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source sentence embedding\n",
      "Word index: 0, Embedding: tensor([-0.7902,  0.2501,  1.5278, -0.5044])\n",
      "Word index: 1, Embedding: tensor([-1.6202, -0.6824,  0.4107,  0.1833])\n",
      "Word index: 2, Embedding: tensor([ 0.4261, -1.3366, -1.2978, -0.8236])\n",
      "Word index: 3, Embedding: tensor([-0.4685,  1.9740,  0.3854, -0.9693])\n",
      "\n",
      "Target sentence embedding\n",
      "Word index: 1, Embedding: tensor([-0.7213,  0.7379,  1.2300,  1.0213])\n",
      "Word index: 0, Embedding: tensor([ 1.3452,  0.7265,  1.7796, -1.9332])\n",
      "Word index: 3, Embedding: tensor([ 2.4084,  1.7481, -1.9929,  0.3660])\n",
      "Word index: 3, Embedding: tensor([ 2.4084,  1.7481, -1.9929,  0.3660])\n",
      "\n",
      "PE source embeddings : \n",
      "\n",
      "tensor([[[-0.7902,  2.2501,  3.5278,  3.4956],\n",
      "         [-0.7787,  0.8579,  2.4207,  4.1832],\n",
      "         [ 1.3354, -0.7528,  0.7222,  3.1762],\n",
      "         [-0.3274,  1.9840,  2.4154,  3.0303]]])\n",
      "\n",
      "PE target embeddings : \n",
      "\n",
      "tensor([[[0.2787, 1.7379, 4.2300, 5.0213],\n",
      "         [3.1867, 1.2668, 4.7896, 2.0668],\n",
      "         [4.3177, 1.3320, 1.0271, 4.3658],\n",
      "         [3.5496, 0.7582, 1.0371, 4.3655]]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pe_src_embeds, pe_tgt_embeds = get_embedding_outputs(src_sentence,  tgt_sentence, state_dict, d_model = d_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder function to display the intermediate outputs and get the final outputs from the encoder\n",
    "\n",
    "### Self attention "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder_block_attn_output(x, state_dict, layer_num, need_weights = False):\n",
    "\n",
    "    query_enc = key_enc = value_enc = x\n",
    "\n",
    "    tgt_len, bsz, embed_dim = x.shape\n",
    "\n",
    "    W_enc = state_dict[\"transformer.encoder.layers.{}.self_attn.in_proj_weight\".format(layer_num)]\n",
    "    b_enc = state_dict[\"transformer.encoder.layers.{}.self_attn.in_proj_bias\".format(layer_num)]\n",
    "\n",
    "\n",
    "    head_dim = embed_dim//num_heads\n",
    "\n",
    "    tempop1 = query_enc[0]@W_enc.T\n",
    "    tempop1 = tempop1.T\n",
    "\n",
    "    Q_enc,K_enc,V_enc = tempop1.T.chunk(3, dim= -1)\n",
    "\n",
    "\n",
    "    Q_enc = Q_enc.unsqueeze(0)\n",
    "    K_enc = K_enc.unsqueeze(0)\n",
    "    V_enc = V_enc.unsqueeze(0)\n",
    "\n",
    "\n",
    "    Q_enc = Q_enc.reshape(tgt_len, bsz * num_heads, head_dim).transpose(0, 1)\n",
    "    K_enc = K_enc.reshape(tgt_len, bsz * num_heads, head_dim).transpose(0, 1)\n",
    "    V_enc = V_enc.reshape(tgt_len, bsz * num_heads, head_dim).transpose(0, 1)\n",
    "\n",
    "    print(\"Q_enc_{} = \".format(layer_num))\n",
    "    print(Q_enc)\n",
    "    print()\n",
    "\n",
    "    print(\"K_enc_{} = \".format(layer_num))\n",
    "    print(K_enc)\n",
    "    print()\n",
    "\n",
    "    print(\"V_enc_{} = \".format(layer_num))\n",
    "    print(V_enc)\n",
    "    print()\n",
    "\n",
    "    src_len = K_enc.size(1)\n",
    "\n",
    "\n",
    "    if need_weights is False:\n",
    "        Q_enc1 = Q_enc.view(bsz, num_heads, tgt_len, head_dim)\n",
    "        K_enc1 = K_enc.view(bsz, num_heads, src_len, head_dim)\n",
    "        V_enc1 = V_enc.view(bsz, num_heads, src_len, head_dim)\n",
    "\n",
    "\n",
    "        L, S = Q_enc1.size(-2), K_enc1.size(-2)\n",
    "\n",
    "        scale_factor = 1 / math.sqrt(Q_enc1.size(-1)) \n",
    "        # scale_factor = 1\n",
    "        attn_bias = torch.zeros(L, S, dtype=Q_enc1.dtype)\n",
    "\n",
    "\n",
    "        attn_weight = Q_enc1 @ K_enc1.transpose(-2, -1) * scale_factor\n",
    "        attn_weight += attn_bias\n",
    "        attn_weight = torch.softmax(attn_weight, dim=-1)\n",
    "        \n",
    "        attn_output = attn_weight @ V_enc1\n",
    "\n",
    "        attn_output = attn_output.permute(2, 0, 1, 3).contiguous().view(bsz * tgt_len, embed_dim)\n",
    "\n",
    "        print(\"Encoder Attention output = \")\n",
    "        print(attn_output)\n",
    "        print()\n",
    "\n",
    "        # print(src_len, pe_src_embeds.shape)\n",
    "        return attn_output, src_len, head_dim, None\n",
    "    \n",
    "    else:\n",
    "\n",
    "        B, Nt, E = Q_enc.shape\n",
    "\n",
    "        Q_enc_scaled = Q_enc / math.sqrt(E)\n",
    "\n",
    "        Q_enc_scaled\n",
    "\n",
    "        temp_pdt_matrix_enc = torch.bmm(Q_enc_scaled, K_enc.transpose(-2, -1))\n",
    "\n",
    "        # Obtaining the diagonal entries of the un-normalised product (K.Q_scaled)\n",
    "        pdt_matrix_enc = torch.diagonal(temp_pdt_matrix_enc, dim1=-2, dim2=-1).unsqueeze(-1)\n",
    "\n",
    "        attn_wt_matrix_enc = torch.nn.functional.softmax(pdt_matrix_enc, dim=-1)\n",
    "\n",
    "        attn_enc_output = torch.bmm(attn_wt_matrix_enc, V_enc)\n",
    "\n",
    "        attn_enc_output = attn_enc_output.transpose(0, 1).contiguous().view(tgt_len * bsz, embed_dim)\n",
    "\n",
    "        print(\"Encoder Attention output = \")\n",
    "        print(attn_enc_output)\n",
    "        print()\n",
    "\n",
    "        return attn_enc_output, src_len, head_dim, attn_wt_matrix_enc\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q_enc_0 = \n",
      "tensor([[[ 0.5279, -2.2983]],\n",
      "\n",
      "        [[-1.0550, -1.1707]],\n",
      "\n",
      "        [[ 0.6662, -1.4912]],\n",
      "\n",
      "        [[-1.2663, -1.0478]],\n",
      "\n",
      "        [[ 1.1158,  0.0281]],\n",
      "\n",
      "        [[-0.6286, -1.5645]],\n",
      "\n",
      "        [[ 0.7990, -1.7561]],\n",
      "\n",
      "        [[-0.9101, -1.1756]]])\n",
      "\n",
      "K_enc_0 = \n",
      "tensor([[[-0.6694,  0.4451]],\n",
      "\n",
      "        [[-0.7058,  4.3720]],\n",
      "\n",
      "        [[-0.7618,  0.1292]],\n",
      "\n",
      "        [[-1.4209,  3.7035]],\n",
      "\n",
      "        [[-0.5826, -0.2429]],\n",
      "\n",
      "        [[-1.7014,  1.3370]],\n",
      "\n",
      "        [[-0.9576,  0.1478]],\n",
      "\n",
      "        [[-0.8108,  3.5906]]])\n",
      "\n",
      "V_enc_0 = \n",
      "tensor([[[ 4.4475,  0.1953]],\n",
      "\n",
      "        [[ 1.3194,  2.5191]],\n",
      "\n",
      "        [[ 4.0303,  0.9956]],\n",
      "\n",
      "        [[-0.0675,  1.8946]],\n",
      "\n",
      "        [[ 1.9893,  1.7287]],\n",
      "\n",
      "        [[-0.5388, -0.3410]],\n",
      "\n",
      "        [[ 3.4502,  0.4872]],\n",
      "\n",
      "        [[ 0.9050,  1.7593]]])\n",
      "\n",
      "Encoder Attention output = \n",
      "tensor([[ 4.4475,  0.1953,  1.3194,  2.5191],\n",
      "        [ 4.0303,  0.9956, -0.0675,  1.8946],\n",
      "        [ 1.9893,  1.7287, -0.5388, -0.3410],\n",
      "        [ 3.4502,  0.4872,  0.9050,  1.7593]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# need_weights = True\n",
    "\n",
    "attn_enc_output, src_len, head_dim, attn_weights = encoder_block_attn_output(pe_src_embeds, state_dict, layer_num = 0, need_weights = need_weights)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Post self attention in encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder_block_post_attn_output(x, attn_enc_output, state_dict, layer_num, bsz, tgt_len):\n",
    "\n",
    "    op_enc_1 = torch.matmul(attn_enc_output, state_dict[\"transformer.encoder.layers.{}.self_attn.out_proj.weight\".format(layer_num)].t()) + state_dict[\"transformer.encoder.layers.{}.self_attn.out_proj.bias\".format(layer_num)]\n",
    "\n",
    "    attn_enc_output = op_enc_1.view(tgt_len, bsz, attn_enc_output.size(1))\n",
    "\n",
    "\n",
    "    # Here src is the original passed inputs to the 1st transformer encoder layer which \n",
    "    # are pe_src_embeds \n",
    "    output_enc_1 = attn_enc_output + x\n",
    "    output_enc_1\n",
    "\n",
    "    linear_result_enc_1 = output_enc_1*state_dict[\"transformer.encoder.layers.{}.norm1.weight\".format(layer_num)] + state_dict[\"transformer.encoder.layers.{}.norm1.bias\".format(layer_num)]\n",
    "\n",
    "    # Layer normalization from Torch's implementation\n",
    "    layernorm_enc_1 = torch.nn.LayerNorm(normalized_shape=linear_result_enc_1.shape[2:])\n",
    "    linear_op_enc_1 = layernorm_enc_1(linear_result_enc_1)\n",
    "\n",
    "\n",
    "    # Manual Layer Normalization\n",
    "    x = linear_result_enc_1\n",
    "\n",
    "    # Obtained layer norm weights and biases (learnable)\n",
    "    w = layernorm_enc_1.weight\n",
    "    b = layernorm_enc_1.bias\n",
    "\n",
    "    linear_result_enc_1f = w*x + b\n",
    "\n",
    "    epsilon = 1e-5  \n",
    "    mean = linear_result_enc_1f.mean(dim=-1, keepdim=True)\n",
    "    std = linear_result_enc_1f.std(dim=-1, unbiased=False, keepdim=True)\n",
    "    normalized_result_enc_1 = (linear_result_enc_1f - mean) / (std + epsilon) * w + b\n",
    "\n",
    "\n",
    "    op_enc_1 = torch.matmul(normalized_result_enc_1, state_dict[\"transformer.encoder.layers.{}.linear1.weight\".format(layer_num)].t()) + state_dict[\"transformer.encoder.layers.{}.linear1.bias\".format(layer_num)]\n",
    "    op_enc_1_relu = torch.nn.functional.relu(op_enc_1)\n",
    "    op_enc_2 = torch.matmul(op_enc_1_relu, state_dict[\"transformer.encoder.layers.{}.linear2.weight\".format(layer_num)].t()) + state_dict[\"transformer.encoder.layers.{}.linear2.bias\".format(layer_num)]\n",
    "\n",
    "\n",
    "    output_enc_2 = op_enc_2 + linear_op_enc_1\n",
    "    output_enc_2_norm = output_enc_2*state_dict[\"transformer.encoder.layers.{}.norm2.weight\".format(layer_num)] + state_dict[\"transformer.encoder.layers.{}.norm2.bias\".format(layer_num)]\n",
    "\n",
    "    # Layer normalization from Torch's implementation\n",
    "    layernorm_enc_final = torch.nn.LayerNorm(normalized_shape=output_enc_2_norm.shape[2:])\n",
    "    output_enc_final = layernorm_enc_final(output_enc_2_norm)\n",
    "\n",
    "\n",
    "    # Manual Layer Normalization \n",
    "    x = output_enc_2_norm\n",
    "\n",
    "    # Obtained layer norm weights and biases (learnable)\n",
    "    w = layernorm_enc_final.weight\n",
    "    b = layernorm_enc_final.bias\n",
    "\n",
    "    linear_result_enc_2 = w*x + b\n",
    "\n",
    "    epsilon = 1e-5  \n",
    "    mean = linear_result_enc_2.mean(dim=-1, keepdim=True)\n",
    "    std = linear_result_enc_2.std(dim=-1, unbiased=False, keepdim=True)\n",
    "    output_enc_final = (linear_result_enc_2 - mean) / (std + epsilon) * w + b\n",
    "\n",
    "    print(\"Final Encoder {} Output :\".format(layer_num))\n",
    "    print(\"norm2(norm1(x + self_atten(x)) + feed_fwd_op)\\n\")\n",
    "    print(output_enc_final)\n",
    "    print()\n",
    "\n",
    "    return output_enc_final\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Encoder 0 Output :\n",
      "norm2(norm1(x + self_atten(x)) + feed_fwd_op)\n",
      "\n",
      "tensor([[[-1.4351,  0.2488,  1.3596, -0.1734],\n",
      "         [-1.5803,  0.6344,  1.0443, -0.0984],\n",
      "         [ 0.5213,  1.2944, -1.3575, -0.4581],\n",
      "         [-1.5292,  0.5588,  1.1479, -0.1776]]], grad_fn=<AddBackward0>)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tgt_len, bsz, embed_dim = pe_src_embeds.shape\n",
    "output_enc_final = encoder_block_post_attn_output(pe_src_embeds, attn_enc_output, state_dict, layer_num = 0 , bsz = bsz, tgt_len = tgt_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder functions to display the intermediate outputs and get the final outputs from the decoder\n",
    "\n",
    "\n",
    "### Self attention outputs from a decoder block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoder_block_self_attn_output(x, state_dict, layer_num, need_weights = False):\n",
    "\n",
    "    query_dec = key_dec = value_dec = x\n",
    "\n",
    "    tgt_len, bsz, embed_dim = x.shape\n",
    "\n",
    "    W_dec = state_dict[\"transformer.decoder.layers.{}.self_attn.in_proj_weight\".format(layer_num)]\n",
    "    b_dec = state_dict[\"transformer.decoder.layers.{}.self_attn.in_proj_bias\".format(layer_num)]\n",
    "\n",
    "\n",
    "    head_dim = embed_dim//num_heads\n",
    "\n",
    "    tempop1 = query_dec[0]@W_dec.T\n",
    "    tempop1 = tempop1.T\n",
    "\n",
    "    Q_dec,K_dec,V_dec = tempop1.T.chunk(3, dim= -1)\n",
    "\n",
    "\n",
    "    Q_dec = Q_dec.unsqueeze(0)\n",
    "    K_dec = K_dec.unsqueeze(0)\n",
    "    V_dec = V_dec.unsqueeze(0)\n",
    "\n",
    "\n",
    "    Q_dec = Q_dec.reshape(tgt_len, bsz * num_heads, head_dim).transpose(0, 1)\n",
    "    K_dec = K_dec.reshape(tgt_len, bsz * num_heads, head_dim).transpose(0, 1)\n",
    "    V_dec = V_dec.reshape(tgt_len, bsz * num_heads, head_dim).transpose(0, 1)\n",
    "\n",
    "    print(\"Q_dec_{} = \".format(layer_num))\n",
    "    print(Q_dec)\n",
    "    print()\n",
    "\n",
    "    print(\"K_dec_{} = \".format(layer_num))\n",
    "    print(K_dec)\n",
    "    print()\n",
    "\n",
    "    print(\"V_dec_{} = \".format(layer_num))\n",
    "    print(V_dec)\n",
    "    print()\n",
    "\n",
    "    src_len = K_dec.size(1)\n",
    "\n",
    "\n",
    "    if need_weights is False:\n",
    "        Q_dec1 = Q_dec.view(bsz, num_heads, tgt_len, head_dim)\n",
    "        K_dec1 = K_dec.view(bsz, num_heads, src_len, head_dim)\n",
    "        V_dec1 = V_dec.view(bsz, num_heads, src_len, head_dim)\n",
    "\n",
    "\n",
    "        L, S = Q_dec1.size(-2), K_dec1.size(-2)\n",
    "\n",
    "        scale_factor = 1 / math.sqrt(Q_dec1.size(-1)) \n",
    "        # scale_factor = 1\n",
    "        attn_bias = torch.zeros(L, S, dtype=Q_dec1.dtype)\n",
    "\n",
    "\n",
    "        attn_weight = Q_dec1 @ K_dec1.transpose(-2, -1) * scale_factor\n",
    "        attn_weight += attn_bias\n",
    "        attn_weight = torch.softmax(attn_weight, dim=-1)\n",
    "        \n",
    "        attn_output = attn_weight @ V_dec1\n",
    "\n",
    "        attn_output = attn_output.permute(2, 0, 1, 3).contiguous().view(bsz * tgt_len, embed_dim)\n",
    "\n",
    "        print(\"Decoder Self Attention = \")\n",
    "        print(attn_output)\n",
    "        print()\n",
    "\n",
    "        op_dec_1 = torch.matmul(attn_output, state_dict[\"transformer.decoder.layers.{}.self_attn.out_proj.weight\".format(layer_num)].t()) + state_dict[\"transformer.decoder.layers.{}.self_attn.out_proj.bias\".format(layer_num)]\n",
    "        attn_dec_output = op_dec_1.view(tgt_len, bsz, attn_output.size(1))\n",
    "\n",
    "        return attn_dec_output, None\n",
    "    \n",
    "    else:\n",
    "\n",
    "\n",
    "        B, Nt, E = Q_dec.shape\n",
    "\n",
    "        Q_dec_scaled = Q_dec / math.sqrt(E)\n",
    "\n",
    "        Q_dec_scaled\n",
    "\n",
    "        temp_pdt_matrix_dec = torch.bmm(Q_dec_scaled, K_dec.transpose(-2, -1))\n",
    "\n",
    "        # Obtaining the diagonal entries of the un-normalised product (K.Q_scaled)\n",
    "        pdt_matrix_dec = torch.diagonal(temp_pdt_matrix_dec, dim1=-2, dim2=-1).unsqueeze(-1)\n",
    "\n",
    "        print(\"Q_scaled @ Kt = \", pdt_matrix_dec)\n",
    "\n",
    "        attn_wt_matrix_dec = torch.nn.functional.softmax(pdt_matrix_dec, dim=-1)\n",
    "\n",
    "        attn_dec_output = torch.bmm(attn_wt_matrix_dec, V_dec)\n",
    "\n",
    "        attn_dec_output = attn_dec_output.transpose(0, 1).contiguous().view(tgt_len * bsz, embed_dim)\n",
    "\n",
    "        print(\"Decoder Attention output = \")\n",
    "        print(attn_wt_matrix_dec)\n",
    "        print()\n",
    "\n",
    "\n",
    "        op_dec_1 = torch.matmul(attn_dec_output, state_dict[\"transformer.decoder.layers.{}.self_attn.out_proj.weight\".format(layer_num)].t()) + state_dict[\"transformer.decoder.layers.{}.self_attn.out_proj.bias\".format(layer_num)]\n",
    "        attn_dec_output = op_dec_1.view(tgt_len, bsz, attn_dec_output.size(1))\n",
    "    \n",
    "\n",
    "        return attn_dec_output, attn_wt_matrix_dec\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# query_dec = key_dec = value_dec = pe_tgt_embeds \n",
    "\n",
    "# tgt_len, bsz, embed_dim = pe_src_embeds.shape\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "need_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q_dec_0 = \n",
      "tensor([[[ 0.5871, -3.8831]],\n",
      "\n",
      "        [[ 4.2142, -2.5878]],\n",
      "\n",
      "        [[ 2.9569, -1.3021]],\n",
      "\n",
      "        [[ 4.5938,  0.1096]],\n",
      "\n",
      "        [[ 0.0119, -0.3000]],\n",
      "\n",
      "        [[ 4.3522, -0.2060]],\n",
      "\n",
      "        [[-0.4440, -0.9314]],\n",
      "\n",
      "        [[ 3.7144, -0.5702]]])\n",
      "\n",
      "K_dec_0 = \n",
      "tensor([[[0.9020, 3.8072]],\n",
      "\n",
      "        [[2.5476, 2.3639]],\n",
      "\n",
      "        [[3.1392, 2.3370]],\n",
      "\n",
      "        [[3.9712, 2.5925]],\n",
      "\n",
      "        [[1.8144, 2.4839]],\n",
      "\n",
      "        [[2.6009, 2.5871]],\n",
      "\n",
      "        [[1.1623, 2.4910]],\n",
      "\n",
      "        [[2.2101, 2.6568]]])\n",
      "\n",
      "V_dec_0 = \n",
      "tensor([[[ 1.6530, -3.7049]],\n",
      "\n",
      "        [[-0.6983, -0.4213]],\n",
      "\n",
      "        [[ 0.4364, -3.0483]],\n",
      "\n",
      "        [[ 0.2365, -1.1687]],\n",
      "\n",
      "        [[-1.8952, -1.8379]],\n",
      "\n",
      "        [[-3.1507, -2.0810]],\n",
      "\n",
      "        [[-1.3617, -1.7368]],\n",
      "\n",
      "        [[-2.8310, -1.5591]]])\n",
      "\n",
      "Decoder Self Attention = \n",
      "tensor([[ 1.6530, -3.7049, -0.6983, -0.4213],\n",
      "        [ 0.4364, -3.0483,  0.2365, -1.1687],\n",
      "        [-1.8952, -1.8379, -3.1507, -2.0810],\n",
      "        [-1.3617, -1.7368, -2.8310, -1.5591]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# need_weights = True\n",
    "\n",
    "self_attn_dec, dec_sa_wts = decoder_block_self_attn_output(pe_tgt_embeds, state_dict, layer_num = 0, need_weights = need_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.8414, -2.6533,  1.9088,  1.6049],\n",
       "         [ 0.7398, -2.4941,  2.2667,  0.5465],\n",
       "         [-0.7669, -2.4307, -0.1783, -1.5623],\n",
       "         [-0.6269, -2.1283, -0.1805, -1.0618]]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "self_attn_dec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dec_post_self_attn(self_attn_dec, x, state_dict, layer_num):\n",
    "    output_dec_1 = self_attn_dec + x\n",
    "\n",
    "    linear_result_dec_1 = output_dec_1*state_dict[\"transformer.decoder.layers.{}.norm1.weight\".format(layer_num)] + state_dict[\"transformer.decoder.layers.{}.norm1.bias\".format(layer_num)]\n",
    "\n",
    "    layernorm_dec_1 = torch.nn.LayerNorm(normalized_shape=linear_result_dec_1.shape[2:])\n",
    "    linear_op_dec_1 = layernorm_dec_1(linear_result_dec_1)\n",
    "\n",
    "    # From torch's implementation\n",
    "    # print(linear_op_dec_1)\n",
    "\n",
    "    x = linear_result_dec_1\n",
    "\n",
    "    # Obtained layer norm weights and biases (learnable)\n",
    "    w = layernorm_dec_1.weight\n",
    "    b = layernorm_dec_1.bias\n",
    "\n",
    "    linear_result_dec_1f = w*x + b\n",
    "\n",
    "    epsilon = 1e-5  \n",
    "    mean = linear_result_dec_1f.mean(dim=-1, keepdim=True)\n",
    "    std = linear_result_dec_1f.std(dim=-1, unbiased=False, keepdim=True)\n",
    "    normalized_result_dec_1 = (linear_result_dec_1f - mean) / (std + epsilon) * w + b\n",
    "\n",
    "\n",
    "    print(\"Decoder_{} norm1(x + sa(x))\".format(layer_num))\n",
    "    print(normalized_result_dec_1)\n",
    "    print()\n",
    "\n",
    "    return normalized_result_dec_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoder_0 norm1(x + sa(x))\n",
      "tensor([[[-0.6579, -1.2888,  0.8978,  1.0489],\n",
      "         [ 0.2808, -1.4541,  1.3345, -0.1612],\n",
      "         [ 1.1197, -1.4515, -0.3745,  0.7064],\n",
      "         [ 0.8014, -1.5007, -0.3065,  1.0058]]], grad_fn=<AddBackward0>)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x_dec = dec_post_self_attn(self_attn_dec, pe_tgt_embeds, state_dict, layer_num = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross attention in decoder \n",
    "### query, key, value =  x, mem, mem \n",
    "\n",
    "### Cross attention between the encoder's final output and the decoder layer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.6579, -1.2888,  0.8978,  1.0489],\n",
       "         [ 0.2808, -1.4541,  1.3345, -0.1612],\n",
       "         [ 1.1197, -1.4515, -0.3745,  0.7064],\n",
       "         [ 0.8014, -1.5007, -0.3065,  1.0058]]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "memory = output_enc_final\n",
    "x_dec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoder_block_cross_attn_output(x_dec, memory, state_dict, layer_num, tgt_len, src_len, head_dim, need_weights = False):\n",
    "\n",
    "    query_dec_mha = x_dec\n",
    "    key_dec_mha, value_dec_mha = memory, memory\n",
    "\n",
    "\n",
    "    W_dec_mha = state_dict[\"transformer.decoder.layers.{}.multihead_attn.in_proj_weight\".format(layer_num)]\n",
    "    b_dec_mha = state_dict[\"transformer.decoder.layers.{}.multihead_attn.in_proj_bias\".format(layer_num)]\n",
    "\n",
    "\n",
    "    ########## CHANGE FROM SELF ATTENTION ############\n",
    "\n",
    "    W_q, W_k, W_v = W_dec_mha.chunk(3)\n",
    "\n",
    "    Q_dec_mha = query_dec_mha[0]@W_q.T\n",
    "\n",
    "    K_dec_mha = key_dec_mha[0]@W_k.T\n",
    "\n",
    "    V_dec_mha = value_dec_mha[0]@W_v.T\n",
    "\n",
    "    #################################################\n",
    "\n",
    "    K_dec_mha = K_dec_mha.unsqueeze(0)\n",
    "    V_dec_mha = V_dec_mha.unsqueeze(0)\n",
    "    Q_dec_mha = Q_dec_mha.unsqueeze(0)\n",
    "\n",
    "    Q_dec_mha = Q_dec_mha.reshape(tgt_len, bsz * num_heads, head_dim).transpose(0, 1)\n",
    "    K_dec_mha = K_dec_mha.reshape(tgt_len, bsz * num_heads, head_dim).transpose(0, 1)\n",
    "    V_dec_mha = V_dec_mha.reshape(tgt_len, bsz * num_heads, head_dim).transpose(0, 1)\n",
    "\n",
    "    print(\"Q_dec_{} = \".format(layer_num))\n",
    "    print(Q_dec_mha)\n",
    "    print()\n",
    "\n",
    "    print(\"K_dec_{} = \".format(layer_num))\n",
    "    print(K_dec_mha)\n",
    "    print()\n",
    "\n",
    "    print(\"V_dec_{} = \".format(layer_num))\n",
    "    print(V_dec_mha)\n",
    "    print()\n",
    "\n",
    "    ### With 'need_weights' = False\n",
    "\n",
    "    if need_weights is False:\n",
    "        Q_dec_mha1 = Q_dec_mha.view(bsz, num_heads, tgt_len, head_dim)\n",
    "        K_dec_mha1 = K_dec_mha.view(bsz, num_heads, src_len, head_dim)\n",
    "        V_dec_mha1 = V_dec_mha.view(bsz, num_heads, src_len, head_dim)\n",
    "\n",
    "\n",
    "        L, S = Q_dec_mha1.size(-2), K_dec_mha1.size(-2)\n",
    "\n",
    "\n",
    "        scale_factor = 1 / math.sqrt(Q_dec_mha1.size(-1)) \n",
    "        # scale_factor = 1\n",
    "\n",
    "        attn_bias = torch.zeros(L, S, dtype=Q_dec_mha1.dtype)\n",
    "\n",
    "\n",
    "        attn_weight = Q_dec_mha1 @ K_dec_mha1.transpose(-2, -1) * scale_factor\n",
    "        attn_weight += attn_bias\n",
    "        attn_weight = torch.softmax(attn_weight, dim=-1)\n",
    "        attn_output_dec_mha = attn_weight @ V_dec_mha1\n",
    "\n",
    "        attn_output_dec_mha = attn_output_dec_mha.permute(2, 0, 1, 3).view(bsz * tgt_len, embed_dim)\n",
    "        \n",
    "        print(\"Cross attention in decoder_{}\".format(layer_num))\n",
    "        print(attn_output_dec_mha)\n",
    "        print()\n",
    "\n",
    "        op_dec_mha_1 = torch.matmul(attn_output_dec_mha, state_dict[\"transformer.decoder.layers.{}.multihead_attn.out_proj.weight\".format(layer_num)].t()) + state_dict[\"transformer.decoder.layers.{}.multihead_attn.out_proj.bias\".format(layer_num)]\n",
    "\n",
    "        attn_dec_mha_output = op_dec_mha_1.view(tgt_len, bsz, attn_output_dec_mha.size(1))\n",
    "\n",
    "        return attn_dec_mha_output, None\n",
    "    \n",
    "    else: \n",
    "\n",
    "        B, Nt, E = Q_dec_mha.shape\n",
    "\n",
    "        Q_dec_mha_scaled = Q_dec_mha / math.sqrt(E)\n",
    "\n",
    "        Q_dec_mha_scaled\n",
    "\n",
    "        temp_pdt_matrix_dec_mha = torch.bmm(Q_dec_mha_scaled, K_dec_mha.transpose(-2, -1))\n",
    "\n",
    "        # Obtaining the diagonal entries of the un-normalised product (K.Q_scaled)\n",
    "        pdt_matrix_dec_mha = torch.diagonal(temp_pdt_matrix_dec_mha, dim1=-2, dim2=-1).unsqueeze(-1)\n",
    "\n",
    "        print(\"Q_scaled @ Kt = \", pdt_matrix_dec_mha)\n",
    "\n",
    "        attn_wt_matrix_dec_mha = torch.nn.functional.softmax(pdt_matrix_dec_mha, dim=-1)\n",
    "\n",
    "        attn_dec_mha_output = torch.bmm(attn_wt_matrix_dec_mha, V_dec_mha)\n",
    "\n",
    "        attn_dec_mha_output = attn_dec_mha_output.transpose(0, 1).contiguous().view(tgt_len * bsz, embed_dim)\n",
    "\n",
    "        print(\"Decoder mha Attention output = \")\n",
    "        print(attn_dec_mha_output)\n",
    "        print()\n",
    "\n",
    "\n",
    "        op_dec_mha = torch.matmul(attn_dec_mha_output, state_dict[\"transformer.decoder.layers.{}.multihead_attn.out_proj.weight\".format(layer_num)].t()) + state_dict[\"transformer.decoder.layers.{}.multihead_attn.out_proj.bias\".format(layer_num)]\n",
    "        attn_dec_output = op_dec_mha.view(tgt_len, bsz, attn_dec_mha_output.size(1))\n",
    "    \n",
    "\n",
    "\n",
    "        # B, Nt, E = Q_dec_mha.shape\n",
    "\n",
    "        # Q_dec_scaled = Q_dec_mha / math.sqrt(E)\n",
    "\n",
    "        # Q_dec_scaled\n",
    "\n",
    "        # temp_pdt_matrix_dec = torch.bmm(Q_dec_scaled, K_dec_mha.transpose(-2, -1))\n",
    "\n",
    "        # # Obtaining the diagonal entries of the un-normalised product (K.Q_scaled)\n",
    "        # pdt_matrix_dec = torch.diagonal(temp_pdt_matrix_dec, dim1=-2, dim2=-1).unsqueeze(-1)\n",
    "\n",
    "        # attn_wt_matrix_dec = F.softmax(pdt_matrix_dec, dim=-1)\n",
    "        # attn_wt_matrix_dec = attn_wt_matrix_dec.view(-1, 1, 1)\n",
    "\n",
    "        # attn_dec_output = torch.bmm(attn_wt_matrix_dec,V_dec_mha.permute(1, 0, 2))\n",
    "\n",
    "        # attn_dec_output = attn_dec_output.transpose(0, 1).view(tgt_len * bsz, embed_dim)\n",
    "\n",
    "        return attn_dec_output , attn_wt_matrix_dec_mha\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q_dec_0 = \n",
      "tensor([[[ 0.9136, -0.6283]],\n",
      "\n",
      "        [[-0.8274, -0.4683]],\n",
      "\n",
      "        [[ 1.0450, -0.3151]],\n",
      "\n",
      "        [[-0.8564, -0.7002]],\n",
      "\n",
      "        [[ 0.8594,  0.4279]],\n",
      "\n",
      "        [[-0.9335, -0.7778]],\n",
      "\n",
      "        [[ 0.9027,  0.2643]],\n",
      "\n",
      "        [[-0.9781, -0.7504]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "K_dec_0 = \n",
      "tensor([[[-0.0191, -0.7460]],\n",
      "\n",
      "        [[-1.2817,  1.1603]],\n",
      "\n",
      "        [[-0.1216, -0.6533]],\n",
      "\n",
      "        [[-1.0173,  1.2253]],\n",
      "\n",
      "        [[-0.2807,  0.9704]],\n",
      "\n",
      "        [[ 1.7984, -0.6181]],\n",
      "\n",
      "        [[-0.0946, -0.6538]],\n",
      "\n",
      "        [[-1.0587,  1.1941]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "V_dec_0 = \n",
      "tensor([[[ 0.6794,  0.3024]],\n",
      "\n",
      "        [[ 0.1560, -1.2189]],\n",
      "\n",
      "        [[ 0.8529, -0.0325]],\n",
      "\n",
      "        [[ 0.1457, -1.3982]],\n",
      "\n",
      "        [[-0.2030, -1.4923]],\n",
      "\n",
      "        [[-0.1055, -0.4424]],\n",
      "\n",
      "        [[ 0.7814,  0.0261]],\n",
      "\n",
      "        [[ 0.1489, -1.3966]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "Cross attention in decoder_0\n",
      "tensor([[ 0.6794,  0.3024,  0.1560, -1.2189],\n",
      "        [ 0.8529, -0.0325,  0.1457, -1.3982],\n",
      "        [-0.2030, -1.4923, -0.1055, -0.4424],\n",
      "        [ 0.7814,  0.0261,  0.1489, -1.3966]], grad_fn=<ViewBackward0>)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# need_weights = True\n",
    "\n",
    "attn_dec_mha_output, attn_dec_mha_wts = decoder_block_cross_attn_output(x_dec, memory, state_dict, layer_num = 0, tgt_len = tgt_len, src_len = src_len, head_dim = head_dim, need_weights = need_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoder_block_post_attn_output(x_dec, attn_dec_mha_output, state_dict, layer_num):\n",
    "\n",
    "    output_dec_2 = attn_dec_mha_output + x_dec\n",
    "\n",
    "    linear_result_dec_2 = output_dec_2*state_dict[\"transformer.decoder.layers.{}.norm2.weight\".format(layer_num)] + state_dict[\"transformer.decoder.layers.{}.norm2.bias\".format(layer_num)]\n",
    "\n",
    "    # Layer normalization from Torch's implementation \n",
    "    layernorm_dec_2 = torch.nn.LayerNorm(normalized_shape=linear_result_dec_2.shape[2:])\n",
    "    linear_op_dec_2 = layernorm_dec_2(linear_result_dec_2)\n",
    "\n",
    "    x = linear_result_dec_2\n",
    "    w = layernorm_dec_2.weight\n",
    "    b = layernorm_dec_2.bias\n",
    "\n",
    "    linear_result_dec_2f = w*x + b\n",
    "\n",
    "    epsilon = 1e-5  \n",
    "    mean = linear_result_dec_2f.mean(dim=-1, keepdim=True)\n",
    "    std = linear_result_dec_2f.std(dim=-1, unbiased=False, keepdim=True)\n",
    "    normalized_result_dec_2 = (linear_result_dec_2f - mean) / (std + epsilon) * w + b\n",
    "\n",
    "    print(\"norm2(x' + mha(x', mem)), \\n where x' = Decoder_curr_layer norm1(x + sa(x))\")\n",
    "    print(normalized_result_dec_2)\n",
    "    print(\"\\n\\n\")\n",
    "\n",
    "    x_dec2_norm = normalized_result_dec_2\n",
    "\n",
    "    op_dec_1 = torch.matmul(x_dec2_norm, state_dict[\"transformer.decoder.layers.{}.linear1.weight\".format(layer_num)].t()) + state_dict[\"transformer.decoder.layers.{}.linear1.bias\".format(layer_num)]\n",
    "    op_dec_1_relu = torch.nn.functional.relu(op_dec_1)\n",
    "    op_dec_2 = torch.matmul(op_dec_1_relu, state_dict[\"transformer.decoder.layers.{}.linear2.weight\".format(layer_num)].t()) + state_dict[\"transformer.decoder.layers.{}.linear2.bias\".format(layer_num)]\n",
    "\n",
    "    ff_dec = op_dec_2\n",
    "\n",
    "    x_dec3_unorm = x_dec2_norm + ff_dec\n",
    "\n",
    "    linear_result_dec_3 = x_dec3_unorm*state_dict[\"transformer.decoder.layers.0.norm3.weight\"] + state_dict[\"transformer.decoder.layers.0.norm3.bias\"]\n",
    "\n",
    "    # Layer normalization from Torch's implementation \n",
    "    layernorm_dec_3 = torch.nn.LayerNorm(normalized_shape=linear_result_dec_3.shape[2:])\n",
    "    linear_op_dec_3 = layernorm_dec_3(linear_result_dec_3)\n",
    "\n",
    "    # From torch's implementation\n",
    "    x = linear_result_dec_3\n",
    "\n",
    "    # Obtained layer norm weights and biases (learnable)\n",
    "    w = layernorm_dec_3.weight\n",
    "    b = layernorm_dec_3.bias\n",
    "\n",
    "    linear_result_dec_3f = w*x + b\n",
    "\n",
    "    epsilon = 1e-5  \n",
    "    mean = linear_result_dec_3f.mean(dim=-1, keepdim=True)\n",
    "    std = linear_result_dec_3f.std(dim=-1, unbiased=False, keepdim=True)\n",
    "    normalized_result_dec_3 = (linear_result_dec_3f - mean) / (std + epsilon) * w + b\n",
    "\n",
    "    print(\"norm3(x'' + ff(x'')) \\n where, x'' = Decoder_curr_layer norm2(x' + mha(x'))\")\n",
    "    print(normalized_result_dec_3)\n",
    "    print()\n",
    "\n",
    "    return normalized_result_dec_3\n",
    "\n",
    "def feef_fwd_transformer(dec_output_final, state_dict):\n",
    "\n",
    "    final_op = dec_output_final@state_dict[\"fc.weight\"].T + state_dict[\"fc.bias\"]\n",
    "\n",
    "    return final_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "norm2(x' + mha(x', mem)), \n",
      " where x' = Decoder_curr_layer norm1(x + sa(x))\n",
      "tensor([[[-0.9941, -0.6906,  0.0954,  1.5892],\n",
      "         [ 0.9460, -1.5991,  0.7356, -0.0825],\n",
      "         [ 0.9940, -1.4993, -0.3113,  0.8166],\n",
      "         [ 0.8530, -0.6956, -1.2569,  1.0995]]], grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "\n",
      "norm3(x'' + ff(x'')) \n",
      " where, x'' = Decoder_curr_layer norm2(x' + mha(x'))\n",
      "tensor([[[-0.4839, -1.0977, -0.0186,  1.6002],\n",
      "         [ 1.0258, -1.6139,  0.5856,  0.0024],\n",
      "         [ 1.1085, -1.4979, -0.2804,  0.6698],\n",
      "         [ 1.0902, -0.9253, -1.0677,  0.9029]]], grad_fn=<AddBackward0>)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final_attn_op_decoder = decoder_block_post_attn_output(x_dec, attn_dec_mha_output, state_dict, layer_num = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_op = feef_fwd_transformer(final_attn_op_decoder, state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.4911,  0.1506,  0.7609,  1.1714,  0.0706,  0.0303,  0.0905,\n",
       "           0.0423, -0.8750, -0.0365],\n",
       "         [-0.6612, -0.7883,  0.2774,  0.2475, -0.7074,  0.0212, -1.3313,\n",
       "          -0.3360, -0.1797,  0.7370],\n",
       "         [-1.0149, -0.8154,  0.5574,  0.2583,  0.0395,  0.0466, -0.6874,\n",
       "          -0.8217, -0.5338,  0.7056],\n",
       "         [-1.2632, -0.8038,  0.6587,  0.1799,  0.6946, -0.0544, -0.0378,\n",
       "          -1.0047, -0.7610,  0.4892]]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_op"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "########################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examaple :- \n",
    "\n",
    "Transformer with 3 encoders and 3 decoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source embeddings :- \n",
      "\n",
      "tensor([[[-0.4019,  1.1379, -0.5372,  1.8170],\n",
      "         [-0.9956,  0.5849, -0.1061,  0.5031],\n",
      "         [ 1.0417,  0.3042,  1.3250,  1.6455],\n",
      "         [ 0.5991, -1.5866,  0.1186,  0.1184]]], grad_fn=<EmbeddingBackward0>)\n",
      "\n",
      "Target embeddings :- \n",
      "\n",
      "tensor([[[ 0.6701, -0.7239,  0.8397,  0.7060],\n",
      "         [ 0.8538, -0.2595, -0.4081, -1.8921],\n",
      "         [-1.2405,  0.6807,  0.4640, -0.9147],\n",
      "         [-1.2405,  0.6807,  0.4640, -0.9147]]], grad_fn=<EmbeddingBackward0>)\n",
      "\n",
      "Positional encoded source embeddings:\n",
      "tensor([[[-0.4019,  3.1379,  1.4628,  5.8170],\n",
      "         [-0.1541,  2.1252,  1.9039,  4.5030],\n",
      "         [ 1.9510,  0.8881,  3.3450,  5.6453],\n",
      "         [ 0.7402, -1.5766,  2.1486,  4.1180]]], grad_fn=<AddBackward0>)\n",
      "\n",
      "Positional encoded target embeddings:\n",
      "tensor([[[ 1.6701,  0.2761,  3.8397,  4.7060],\n",
      "         [ 2.6952,  0.2808,  2.6019,  2.1078],\n",
      "         [ 0.6688,  0.2646,  3.4840,  3.0851],\n",
      "         [-0.0993, -0.3093,  3.4940,  3.0849]]], grad_fn=<AddBackward0>)\n",
      "\n",
      "query =  tensor([[[-0.4019,  3.1379,  1.4628,  5.8170],\n",
      "         [-0.1541,  2.1252,  1.9039,  4.5030],\n",
      "         [ 1.9510,  0.8881,  3.3450,  5.6453],\n",
      "         [ 0.7402, -1.5766,  2.1486,  4.1180]]], grad_fn=<AddBackward0>)\n",
      "query shape =  torch.Size([1, 4, 4])\n",
      "\n",
      " _in_projection_packed \n",
      "\n",
      "Q :-  tensor([[[-4.1784e+00, -4.0106e+00,  4.1455e+00,  5.2374e-02],\n",
      "         [-2.7286e+00, -3.1981e+00,  3.3991e+00,  3.0230e-03],\n",
      "         [-2.3743e+00, -4.9482e+00,  3.9994e+00,  3.7645e-01],\n",
      "         [-4.3266e-01, -2.7821e+00,  1.3830e+00, -2.1311e-02]]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "\n",
      "K :-  tensor([[[ 0.1276,  0.7274,  1.5029, -0.2525],\n",
      "         [ 0.1799,  0.1953,  1.1665, -0.4289],\n",
      "         [-0.2008, -0.5300,  2.1722,  0.3007],\n",
      "         [-1.0771, -0.0375,  0.8167,  0.6506]]], grad_fn=<SelectBackward0>)\n",
      "\n",
      "V :-  tensor([[[-3.9342,  3.3096,  1.6318, -1.3380],\n",
      "         [-3.0291,  2.4046,  0.9440, -1.6920],\n",
      "         [-3.4646,  3.5623,  0.8624, -4.8413],\n",
      "         [-1.4426,  1.8447,  1.5679, -4.1069]]], grad_fn=<SelectBackward0>)\n",
      "\n",
      "tgt_len =  1\n",
      "bsz =  4\n",
      "num_heads =  2\n",
      "head_dim =  2\n",
      "\n",
      "tensor([[[-4.1784e+00, -4.0106e+00,  4.1455e+00,  5.2374e-02],\n",
      "         [-2.7286e+00, -3.1981e+00,  3.3991e+00,  3.0230e-03],\n",
      "         [-2.3743e+00, -4.9482e+00,  3.9994e+00,  3.7645e-01],\n",
      "         [-4.3266e-01, -2.7821e+00,  1.3830e+00, -2.1311e-02]]],\n",
      "       grad_fn=<SelectBackward0>) torch.Size([1, 4, 4])\n",
      "Q reshaped =  tensor([[[-4.1784e+00, -4.0106e+00]],\n",
      "\n",
      "        [[ 4.1455e+00,  5.2374e-02]],\n",
      "\n",
      "        [[-2.7286e+00, -3.1981e+00]],\n",
      "\n",
      "        [[ 3.3991e+00,  3.0230e-03]],\n",
      "\n",
      "        [[-2.3743e+00, -4.9482e+00]],\n",
      "\n",
      "        [[ 3.9994e+00,  3.7645e-01]],\n",
      "\n",
      "        [[-4.3266e-01, -2.7821e+00]],\n",
      "\n",
      "        [[ 1.3830e+00, -2.1311e-02]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "K reshaped =  tensor([[[ 0.1276,  0.7274]],\n",
      "\n",
      "        [[ 1.5029, -0.2525]],\n",
      "\n",
      "        [[ 0.1799,  0.1953]],\n",
      "\n",
      "        [[ 1.1665, -0.4289]],\n",
      "\n",
      "        [[-0.2008, -0.5300]],\n",
      "\n",
      "        [[ 2.1722,  0.3007]],\n",
      "\n",
      "        [[-1.0771, -0.0375]],\n",
      "\n",
      "        [[ 0.8167,  0.6506]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "V reshaped =  tensor([[[-3.9342,  3.3096]],\n",
      "\n",
      "        [[ 1.6318, -1.3380]],\n",
      "\n",
      "        [[-3.0291,  2.4046]],\n",
      "\n",
      "        [[ 0.9440, -1.6920]],\n",
      "\n",
      "        [[-3.4646,  3.5623]],\n",
      "\n",
      "        [[ 0.8624, -4.8413]],\n",
      "\n",
      "        [[-1.4426,  1.8447]],\n",
      "\n",
      "        [[ 1.5679, -4.1069]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "torch.Size([4, 2, 1, 2]) torch.Size([4, 2, 1, 2]) torch.Size([4, 2, 1, 2])\n",
      "\n",
      "scaled_dot_product_attention\n",
      "attn_output = \n",
      "tensor([[-3.9342,  3.3096,  1.6318, -1.3380],\n",
      "        [-3.0291,  2.4046,  0.9440, -1.6920],\n",
      "        [-3.4646,  3.5623,  0.8624, -4.8413],\n",
      "        [-1.4426,  1.8447,  1.5679, -4.1069]], grad_fn=<ViewBackward0>)\n",
      "\n",
      "x + self_atten(x) = \n",
      "tensor([[[-0.8558,  2.7045,  5.2462,  7.4718],\n",
      "         [-0.3151,  2.2842,  5.0327,  6.1554],\n",
      "         [ 2.2185,  3.8096,  9.1808,  8.7983],\n",
      "         [ 1.4062,  1.0164,  6.1867,  5.3536]]], grad_fn=<AddBackward0>)\n",
      "\n",
      "norm1(x + self_atten(x)) = \n",
      "tensor([[[-1.4525, -0.3027,  0.5182,  1.2370],\n",
      "         [-1.4344, -0.4000,  0.6938,  1.1406],\n",
      "         [-1.2432, -0.7203,  1.0446,  0.9189],\n",
      "         [-0.9053, -1.0746,  1.1709,  0.8091]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "\n",
      "feed_fwd_op =  tensor([[[ 0.6593, -0.1599, -0.2553, -0.4965],\n",
      "         [ 0.6559, -0.1567, -0.2564, -0.4904],\n",
      "         [ 0.6460, -0.1504, -0.2512, -0.4776],\n",
      "         [ 0.6418, -0.1477, -0.2348, -0.4736]]], grad_fn=<ViewBackward0>)\n",
      "\n",
      "norm2(norm1(x + self_atten(x)) + feed_fwd_op) = \n",
      "tensor([[[-1.2149, -0.6647,  0.5425,  1.3371],\n",
      "         [-1.1645, -0.8040,  0.8113,  1.1572],\n",
      "         [-0.7768, -1.1709,  1.2276,  0.7201],\n",
      "         [-0.2635, -1.4664,  1.2417,  0.4882]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "\n",
      "##################################\n",
      "query =  tensor([[[-1.2149, -0.6647,  0.5425,  1.3371],\n",
      "         [-1.1645, -0.8040,  0.8113,  1.1572],\n",
      "         [-0.7768, -1.1709,  1.2276,  0.7201],\n",
      "         [-0.2635, -1.4664,  1.2417,  0.4882]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "query shape =  torch.Size([1, 4, 4])\n",
      "\n",
      " _in_projection_packed \n",
      "\n",
      "Q :-  tensor([[[-0.3071, -0.9261, -0.4274, -1.6287],\n",
      "         [-0.3712, -0.8321, -0.5106, -1.6461],\n",
      "         [-0.5273, -0.5971, -0.5360, -1.4053],\n",
      "         [-0.6403, -0.4727, -0.3634, -0.9819]]], grad_fn=<SelectBackward0>)\n",
      "\n",
      "K :-  tensor([[[-0.6060,  0.7348,  0.2426, -0.0995],\n",
      "         [-0.6149,  0.7130,  0.1095, -0.1689],\n",
      "         [-0.6346,  0.5822, -0.0855, -0.1482],\n",
      "         [-0.6683,  0.4347, -0.0533,  0.0817]]], grad_fn=<SelectBackward0>)\n",
      "\n",
      "V :-  tensor([[[0.0228, 0.8665, 0.8199, 0.5550],\n",
      "         [0.0629, 0.7502, 0.7990, 0.6160],\n",
      "         [0.2519, 0.4040, 0.4996, 0.6860],\n",
      "         [0.4813, 0.1391, 0.0481, 0.6612]]], grad_fn=<SelectBackward0>)\n",
      "\n",
      "tgt_len =  1\n",
      "bsz =  4\n",
      "num_heads =  2\n",
      "head_dim =  2\n",
      "\n",
      "tensor([[[-0.3071, -0.9261, -0.4274, -1.6287],\n",
      "         [-0.3712, -0.8321, -0.5106, -1.6461],\n",
      "         [-0.5273, -0.5971, -0.5360, -1.4053],\n",
      "         [-0.6403, -0.4727, -0.3634, -0.9819]]], grad_fn=<SelectBackward0>) torch.Size([1, 4, 4])\n",
      "Q reshaped =  tensor([[[-0.3071, -0.9261]],\n",
      "\n",
      "        [[-0.4274, -1.6287]],\n",
      "\n",
      "        [[-0.3712, -0.8321]],\n",
      "\n",
      "        [[-0.5106, -1.6461]],\n",
      "\n",
      "        [[-0.5273, -0.5971]],\n",
      "\n",
      "        [[-0.5360, -1.4053]],\n",
      "\n",
      "        [[-0.6403, -0.4727]],\n",
      "\n",
      "        [[-0.3634, -0.9819]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "K reshaped =  tensor([[[-0.6060,  0.7348]],\n",
      "\n",
      "        [[ 0.2426, -0.0995]],\n",
      "\n",
      "        [[-0.6149,  0.7130]],\n",
      "\n",
      "        [[ 0.1095, -0.1689]],\n",
      "\n",
      "        [[-0.6346,  0.5822]],\n",
      "\n",
      "        [[-0.0855, -0.1482]],\n",
      "\n",
      "        [[-0.6683,  0.4347]],\n",
      "\n",
      "        [[-0.0533,  0.0817]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "V reshaped =  tensor([[[0.0228, 0.8665]],\n",
      "\n",
      "        [[0.8199, 0.5550]],\n",
      "\n",
      "        [[0.0629, 0.7502]],\n",
      "\n",
      "        [[0.7990, 0.6160]],\n",
      "\n",
      "        [[0.2519, 0.4040]],\n",
      "\n",
      "        [[0.4996, 0.6860]],\n",
      "\n",
      "        [[0.4813, 0.1391]],\n",
      "\n",
      "        [[0.0481, 0.6612]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "torch.Size([4, 2, 1, 2]) torch.Size([4, 2, 1, 2]) torch.Size([4, 2, 1, 2])\n",
      "\n",
      "scaled_dot_product_attention\n",
      "attn_output = \n",
      "tensor([[0.0228, 0.8665, 0.8199, 0.5550],\n",
      "        [0.0629, 0.7502, 0.7990, 0.6160],\n",
      "        [0.2519, 0.4040, 0.4996, 0.6860],\n",
      "        [0.4813, 0.1391, 0.0481, 0.6612]], grad_fn=<ViewBackward0>)\n",
      "\n",
      "x + self_atten(x) = \n",
      "tensor([[[-0.8430, -0.7016,  0.6939,  0.6850],\n",
      "         [-0.9321, -0.7960,  1.0525,  0.6173],\n",
      "         [-1.0221, -1.0624,  1.6719,  0.5487],\n",
      "         [-0.9650, -1.3045,  1.8056,  0.6417]]], grad_fn=<AddBackward0>)\n",
      "\n",
      "norm1(x + self_atten(x)) = \n",
      "tensor([[[-1.0942, -0.9011,  1.0037,  0.9916],\n",
      "         [-1.0612, -0.9037,  1.2341,  0.7308],\n",
      "         [-0.9205, -0.9557,  1.4276,  0.4486],\n",
      "         [-0.8045, -1.0751,  1.4037,  0.4760]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "\n",
      "feed_fwd_op =  tensor([[[ 0.3246, -0.0696,  0.0884,  0.0391],\n",
      "         [ 0.3253, -0.0795,  0.0840,  0.0468],\n",
      "         [ 0.3254, -0.0892,  0.0829,  0.0611],\n",
      "         [ 0.3281, -0.0864,  0.0877,  0.0612]]], grad_fn=<ViewBackward0>)\n",
      "\n",
      "norm2(norm1(x + self_atten(x)) + feed_fwd_op) = \n",
      "tensor([[[-0.8932, -1.1009,  1.0288,  0.9653],\n",
      "         [-0.8499, -1.1032,  1.2533,  0.6998],\n",
      "         [-0.6944, -1.1470,  1.4241,  0.4172],\n",
      "         [-0.5705, -1.2513,  1.3849,  0.4368]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "\n",
      "##################################\n",
      "query =  tensor([[[-0.8932, -1.1009,  1.0288,  0.9653],\n",
      "         [-0.8499, -1.1032,  1.2533,  0.6998],\n",
      "         [-0.6944, -1.1470,  1.4241,  0.4172],\n",
      "         [-0.5705, -1.2513,  1.3849,  0.4368]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "query shape =  torch.Size([1, 4, 4])\n",
      "\n",
      " _in_projection_packed \n",
      "\n",
      "Q :-  tensor([[[ 1.0430,  1.0346,  0.1915,  0.7643],\n",
      "         [ 1.0802,  1.1768,  0.1347,  0.8348],\n",
      "         [ 1.1071,  1.2866,  0.0307,  0.8372],\n",
      "         [ 1.1341,  1.2796, -0.0283,  0.7693]]], grad_fn=<SelectBackward0>)\n",
      "\n",
      "K :-  tensor([[[ 0.2947, -0.6589, -0.3096, -0.3069],\n",
      "         [ 0.4526, -0.6829, -0.1179, -0.3080],\n",
      "         [ 0.6291, -0.6758,  0.1198, -0.3286],\n",
      "         [ 0.6650, -0.6575,  0.1700, -0.3691]]], grad_fn=<SelectBackward0>)\n",
      "\n",
      "V :-  tensor([[[ 0.4722, -0.2705,  0.1496, -0.1437],\n",
      "         [ 0.2148, -0.3004,  0.0540, -0.3001],\n",
      "         [-0.0661, -0.3265,  0.0216, -0.3693],\n",
      "         [-0.0760, -0.3316,  0.1204, -0.2472]]], grad_fn=<SelectBackward0>)\n",
      "\n",
      "tgt_len =  1\n",
      "bsz =  4\n",
      "num_heads =  2\n",
      "head_dim =  2\n",
      "\n",
      "tensor([[[ 1.0430,  1.0346,  0.1915,  0.7643],\n",
      "         [ 1.0802,  1.1768,  0.1347,  0.8348],\n",
      "         [ 1.1071,  1.2866,  0.0307,  0.8372],\n",
      "         [ 1.1341,  1.2796, -0.0283,  0.7693]]], grad_fn=<SelectBackward0>) torch.Size([1, 4, 4])\n",
      "Q reshaped =  tensor([[[ 1.0430,  1.0346]],\n",
      "\n",
      "        [[ 0.1915,  0.7643]],\n",
      "\n",
      "        [[ 1.0802,  1.1768]],\n",
      "\n",
      "        [[ 0.1347,  0.8348]],\n",
      "\n",
      "        [[ 1.1071,  1.2866]],\n",
      "\n",
      "        [[ 0.0307,  0.8372]],\n",
      "\n",
      "        [[ 1.1341,  1.2796]],\n",
      "\n",
      "        [[-0.0283,  0.7693]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "K reshaped =  tensor([[[ 0.2947, -0.6589]],\n",
      "\n",
      "        [[-0.3096, -0.3069]],\n",
      "\n",
      "        [[ 0.4526, -0.6829]],\n",
      "\n",
      "        [[-0.1179, -0.3080]],\n",
      "\n",
      "        [[ 0.6291, -0.6758]],\n",
      "\n",
      "        [[ 0.1198, -0.3286]],\n",
      "\n",
      "        [[ 0.6650, -0.6575]],\n",
      "\n",
      "        [[ 0.1700, -0.3691]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "V reshaped =  tensor([[[ 0.4722, -0.2705]],\n",
      "\n",
      "        [[ 0.1496, -0.1437]],\n",
      "\n",
      "        [[ 0.2148, -0.3004]],\n",
      "\n",
      "        [[ 0.0540, -0.3001]],\n",
      "\n",
      "        [[-0.0661, -0.3265]],\n",
      "\n",
      "        [[ 0.0216, -0.3693]],\n",
      "\n",
      "        [[-0.0760, -0.3316]],\n",
      "\n",
      "        [[ 0.1204, -0.2472]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "torch.Size([4, 2, 1, 2]) torch.Size([4, 2, 1, 2]) torch.Size([4, 2, 1, 2])\n",
      "\n",
      "scaled_dot_product_attention\n",
      "attn_output = \n",
      "tensor([[ 0.4722, -0.2705,  0.1496, -0.1437],\n",
      "        [ 0.2148, -0.3004,  0.0540, -0.3001],\n",
      "        [-0.0661, -0.3265,  0.0216, -0.3693],\n",
      "        [-0.0760, -0.3316,  0.1204, -0.2472]], grad_fn=<ViewBackward0>)\n",
      "\n",
      "x + self_atten(x) = \n",
      "tensor([[[-0.8504, -1.4650,  0.9579,  0.4356],\n",
      "         [-0.9411, -1.4618,  1.3932,  0.2156],\n",
      "         [-0.8325, -1.4240,  1.7809,  0.0221],\n",
      "         [-0.5794, -1.4343,  1.7439,  0.0820]]], grad_fn=<AddBackward0>)\n",
      "\n",
      "norm1(x + self_atten(x)) = \n",
      "tensor([[[-0.6391, -1.2726,  1.2250,  0.6867],\n",
      "         [-0.6742, -1.1469,  1.4452,  0.3760],\n",
      "         [-0.5951, -1.0846,  1.5675,  0.1121],\n",
      "         [-0.4569, -1.1905,  1.5368,  0.1106]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "\n",
      "feed_fwd_op =  tensor([[[-0.0512, -0.2150, -0.0642,  0.1403],\n",
      "         [-0.0405, -0.1937, -0.0798,  0.1402],\n",
      "         [-0.0352, -0.1823, -0.0888,  0.1502],\n",
      "         [-0.0423, -0.1874, -0.0877,  0.1605]]], grad_fn=<ViewBackward0>)\n",
      "\n",
      "norm2(norm1(x + self_atten(x)) + feed_fwd_op) = \n",
      "tensor([[[-0.5922, -1.3269,  1.1133,  0.8057],\n",
      "         [-0.6378, -1.2324,  1.3385,  0.5317],\n",
      "         [-0.5735, -1.1910,  1.4722,  0.2923],\n",
      "         [-0.4429, -1.2889,  1.4330,  0.2988]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "\n",
      "##################################\n",
      "\n",
      "################### Transformer Decoder start #####################\n",
      "\n",
      "query =  tensor([[[ 1.6701,  0.2761,  3.8397,  4.7060],\n",
      "         [ 2.6952,  0.2808,  2.6019,  2.1078],\n",
      "         [ 0.6688,  0.2646,  3.4840,  3.0851],\n",
      "         [-0.0993, -0.3093,  3.4940,  3.0849]]], grad_fn=<AddBackward0>)\n",
      "query shape =  torch.Size([1, 4, 4])\n",
      "\n",
      " _in_projection_packed \n",
      "\n",
      "Q :-  tensor([[[ 0.8298, -2.7723,  0.1053, -1.2076],\n",
      "         [-0.6300, -0.6314,  1.5452, -0.9409],\n",
      "         [ 0.9926, -2.3179,  0.2049, -1.0176],\n",
      "         [ 1.3943, -2.7760, -0.4507, -1.1836]]], grad_fn=<SelectBackward0>)\n",
      "\n",
      "K :-  tensor([[[-1.2786, -1.3001,  3.5688,  1.2169],\n",
      "         [ 0.2087, -1.3096,  2.0866,  0.6364],\n",
      "         [-0.9382, -1.3581,  2.7814,  0.4942],\n",
      "         [-1.1729, -1.2928,  2.9863,  0.6032]]], grad_fn=<SelectBackward0>)\n",
      "\n",
      "V :-  tensor([[[ 3.6230,  2.5086,  0.2786, -0.0958],\n",
      "         [ 3.1906,  1.8818, -0.5206, -0.1735],\n",
      "         [ 2.6893,  1.7036, -0.1949, -0.2417],\n",
      "         [ 2.3067,  1.7857,  0.1662, -0.5128]]], grad_fn=<SelectBackward0>)\n",
      "\n",
      "tgt_len =  1\n",
      "bsz =  4\n",
      "num_heads =  2\n",
      "head_dim =  2\n",
      "\n",
      "tensor([[[ 0.8298, -2.7723,  0.1053, -1.2076],\n",
      "         [-0.6300, -0.6314,  1.5452, -0.9409],\n",
      "         [ 0.9926, -2.3179,  0.2049, -1.0176],\n",
      "         [ 1.3943, -2.7760, -0.4507, -1.1836]]], grad_fn=<SelectBackward0>) torch.Size([1, 4, 4])\n",
      "Q reshaped =  tensor([[[ 0.8298, -2.7723]],\n",
      "\n",
      "        [[ 0.1053, -1.2076]],\n",
      "\n",
      "        [[-0.6300, -0.6314]],\n",
      "\n",
      "        [[ 1.5452, -0.9409]],\n",
      "\n",
      "        [[ 0.9926, -2.3179]],\n",
      "\n",
      "        [[ 0.2049, -1.0176]],\n",
      "\n",
      "        [[ 1.3943, -2.7760]],\n",
      "\n",
      "        [[-0.4507, -1.1836]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "K reshaped =  tensor([[[-1.2786, -1.3001]],\n",
      "\n",
      "        [[ 3.5688,  1.2169]],\n",
      "\n",
      "        [[ 0.2087, -1.3096]],\n",
      "\n",
      "        [[ 2.0866,  0.6364]],\n",
      "\n",
      "        [[-0.9382, -1.3581]],\n",
      "\n",
      "        [[ 2.7814,  0.4942]],\n",
      "\n",
      "        [[-1.1729, -1.2928]],\n",
      "\n",
      "        [[ 2.9863,  0.6032]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "V reshaped =  tensor([[[ 3.6230,  2.5086]],\n",
      "\n",
      "        [[ 0.2786, -0.0958]],\n",
      "\n",
      "        [[ 3.1906,  1.8818]],\n",
      "\n",
      "        [[-0.5206, -0.1735]],\n",
      "\n",
      "        [[ 2.6893,  1.7036]],\n",
      "\n",
      "        [[-0.1949, -0.2417]],\n",
      "\n",
      "        [[ 2.3067,  1.7857]],\n",
      "\n",
      "        [[ 0.1662, -0.5128]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "torch.Size([4, 2, 1, 2]) torch.Size([4, 2, 1, 2]) torch.Size([4, 2, 1, 2])\n",
      "\n",
      "scaled_dot_product_attention\n",
      "attn_output = \n",
      "tensor([[ 3.6230,  2.5086,  0.2786, -0.0958],\n",
      "        [ 3.1906,  1.8818, -0.5206, -0.1735],\n",
      "        [ 2.6893,  1.7036, -0.1949, -0.2417],\n",
      "        [ 2.3067,  1.7857,  0.1662, -0.5128]], grad_fn=<ViewBackward0>)\n",
      "\n",
      "x =  tensor([[[ 1.6701,  0.2761,  3.8397,  4.7060],\n",
      "         [ 2.6952,  0.2808,  2.6019,  2.1078],\n",
      "         [ 0.6688,  0.2646,  3.4840,  3.0851],\n",
      "         [-0.0993, -0.3093,  3.4940,  3.0849]]], grad_fn=<AddBackward0>)\n",
      "\n",
      "sa_op =  tensor([[[ 3.7430, -0.4384, -3.2883,  0.9628],\n",
      "         [ 3.6360, -0.4075, -2.1790,  1.5228],\n",
      "         [ 2.8786, -0.4051, -2.0207,  1.0439],\n",
      "         [ 2.1646, -0.4764, -1.9598,  0.4890]]], grad_fn=<ViewBackward0>)\n",
      "\n",
      "temop =  tensor([[[ 5.4130, -0.1623,  0.5514,  5.6688],\n",
      "         [ 6.3312, -0.1267,  0.4229,  3.6306],\n",
      "         [ 3.5475, -0.1405,  1.4633,  4.1290],\n",
      "         [ 2.0653, -0.7857,  1.5341,  3.5739]]], grad_fn=<AddBackward0>)\n",
      "\n",
      "Decoder norm1(x + sa(x)) = \n",
      "tensor([[[ 0.9474, -1.1278, -0.8622,  1.0426],\n",
      "         [ 1.4457, -1.0329, -0.8220,  0.4092],\n",
      "         [ 0.7637, -1.4068, -0.4629,  1.1060],\n",
      "         [ 0.2991, -1.5215, -0.0401,  1.2625]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "\n",
      "Memory = \n",
      "tensor([[[-0.5922, -1.3269,  1.1133,  0.8057],\n",
      "         [-0.6378, -1.2324,  1.3385,  0.5317],\n",
      "         [-0.5735, -1.1910,  1.4722,  0.2923],\n",
      "         [-0.4429, -1.2889,  1.4330,  0.2988]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "\n",
      "query =  tensor([[[ 0.9474, -1.1278, -0.8622,  1.0426],\n",
      "         [ 1.4457, -1.0329, -0.8220,  0.4092],\n",
      "         [ 0.7637, -1.4068, -0.4629,  1.1060],\n",
      "         [ 0.2991, -1.5215, -0.0401,  1.2625]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "query shape =  torch.Size([1, 4, 4])\n",
      "\n",
      " _in_projection_packed \n",
      "\n",
      "Q :-  tensor([[[0.9479, 0.5626, 1.2335, 0.6094],\n",
      "         [0.6321, 1.0295, 1.2887, 0.9829],\n",
      "         [0.8242, 0.5045, 1.0789, 0.4067],\n",
      "         [0.7195, 0.2046, 0.7696, 0.0188]]], grad_fn=<ViewBackward0>)\n",
      "\n",
      "K :-  tensor([[[-0.3500,  0.1593, -0.4738,  0.1080],\n",
      "         [-0.4297,  0.0768, -0.6427,  0.0108],\n",
      "         [-0.5195,  0.0223, -0.8347, -0.0350],\n",
      "         [-0.5670,  0.0474, -0.9287,  0.0190]]], grad_fn=<SelectBackward0>)\n",
      "\n",
      "V :-  tensor([[[ 0.0440, -0.3779, -0.0487,  1.3131],\n",
      "         [-0.0413, -0.1451,  0.1154,  1.4681],\n",
      "         [-0.0951,  0.0868,  0.2556,  1.5323],\n",
      "         [-0.0694,  0.1336,  0.2615,  1.4827]]], grad_fn=<SelectBackward0>)\n",
      "\n",
      "tgt_len =  1\n",
      "bsz =  4\n",
      "num_heads =  2\n",
      "head_dim =  2\n",
      "\n",
      "tensor([[[0.9479, 0.5626, 1.2335, 0.6094],\n",
      "         [0.6321, 1.0295, 1.2887, 0.9829],\n",
      "         [0.8242, 0.5045, 1.0789, 0.4067],\n",
      "         [0.7195, 0.2046, 0.7696, 0.0188]]], grad_fn=<ViewBackward0>) torch.Size([1, 4, 4])\n",
      "Q reshaped =  tensor([[[0.9479, 0.5626]],\n",
      "\n",
      "        [[1.2335, 0.6094]],\n",
      "\n",
      "        [[0.6321, 1.0295]],\n",
      "\n",
      "        [[1.2887, 0.9829]],\n",
      "\n",
      "        [[0.8242, 0.5045]],\n",
      "\n",
      "        [[1.0789, 0.4067]],\n",
      "\n",
      "        [[0.7195, 0.2046]],\n",
      "\n",
      "        [[0.7696, 0.0188]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "K reshaped =  tensor([[[-0.3500,  0.1593]],\n",
      "\n",
      "        [[-0.4738,  0.1080]],\n",
      "\n",
      "        [[-0.4297,  0.0768]],\n",
      "\n",
      "        [[-0.6427,  0.0108]],\n",
      "\n",
      "        [[-0.5195,  0.0223]],\n",
      "\n",
      "        [[-0.8347, -0.0350]],\n",
      "\n",
      "        [[-0.5670,  0.0474]],\n",
      "\n",
      "        [[-0.9287,  0.0190]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "V reshaped =  tensor([[[ 0.0440, -0.3779]],\n",
      "\n",
      "        [[-0.0487,  1.3131]],\n",
      "\n",
      "        [[-0.0413, -0.1451]],\n",
      "\n",
      "        [[ 0.1154,  1.4681]],\n",
      "\n",
      "        [[-0.0951,  0.0868]],\n",
      "\n",
      "        [[ 0.2556,  1.5323]],\n",
      "\n",
      "        [[-0.0694,  0.1336]],\n",
      "\n",
      "        [[ 0.2615,  1.4827]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "torch.Size([4, 2, 1, 2]) torch.Size([4, 2, 1, 2]) torch.Size([4, 2, 1, 2])\n",
      "\n",
      "scaled_dot_product_attention\n",
      "attn_output = \n",
      "tensor([[ 0.0440, -0.3779, -0.0487,  1.3131],\n",
      "        [-0.0413, -0.1451,  0.1154,  1.4681],\n",
      "        [-0.0951,  0.0868,  0.2556,  1.5323],\n",
      "        [-0.0694,  0.1336,  0.2615,  1.4827]], grad_fn=<ViewBackward0>)\n",
      "\n",
      "Decoder norm2(x + mha(x, mem)) = \n",
      "tensor([[[ 1.2606, -0.5207, -1.3359,  0.5960],\n",
      "         [ 1.4943, -0.3998, -1.2572,  0.1627],\n",
      "         [ 0.9907, -0.8290, -1.1574,  0.9958],\n",
      "         [ 0.6228, -1.0692, -0.8652,  1.3116]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "\n",
      "Decoder norm3(x + ff(x)) = \n",
      "tensor([[[ 1.3813, -0.3479, -1.3644,  0.3310],\n",
      "         [ 1.5767, -0.2089, -1.2010, -0.1669],\n",
      "         [ 1.1134, -0.7185, -1.2381,  0.8432],\n",
      "         [ 0.6920, -1.0099, -0.9481,  1.2659]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "\n",
      "\n",
      "################### Transformer Decoder start #####################\n",
      "\n",
      "query =  tensor([[[ 1.3813, -0.3479, -1.3644,  0.3310],\n",
      "         [ 1.5767, -0.2089, -1.2010, -0.1669],\n",
      "         [ 1.1134, -0.7185, -1.2381,  0.8432],\n",
      "         [ 0.6920, -1.0099, -0.9481,  1.2659]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "query shape =  torch.Size([1, 4, 4])\n",
      "\n",
      " _in_projection_packed \n",
      "\n",
      "Q :-  tensor([[[-1.2965, -0.3490, -0.0857,  0.0193],\n",
      "         [-1.0470, -0.5929,  0.3245, -0.0959],\n",
      "         [-1.4643, -0.3067, -0.5399,  0.1115],\n",
      "         [-1.4496, -0.2229, -0.9415,  0.1928]]], grad_fn=<SelectBackward0>)\n",
      "\n",
      "K :-  tensor([[[-0.5187, -0.4265, -1.0570, -0.0622],\n",
      "         [-0.7099, -0.5175, -1.1703, -0.2904],\n",
      "         [-0.1186, -0.0719, -0.9885,  0.1322],\n",
      "         [ 0.3114,  0.3048, -0.7898,  0.3170]]], grad_fn=<SelectBackward0>)\n",
      "\n",
      "V :-  tensor([[[ 0.3763, -0.4561, -0.1431, -1.1326],\n",
      "         [ 0.1440, -0.5796,  0.0036, -1.2818],\n",
      "         [ 0.4909, -0.1850, -0.1889, -0.9615],\n",
      "         [ 0.5467,  0.1198, -0.2122, -0.6601]]], grad_fn=<SelectBackward0>)\n",
      "\n",
      "tgt_len =  1\n",
      "bsz =  4\n",
      "num_heads =  2\n",
      "head_dim =  2\n",
      "\n",
      "tensor([[[-1.2965, -0.3490, -0.0857,  0.0193],\n",
      "         [-1.0470, -0.5929,  0.3245, -0.0959],\n",
      "         [-1.4643, -0.3067, -0.5399,  0.1115],\n",
      "         [-1.4496, -0.2229, -0.9415,  0.1928]]], grad_fn=<SelectBackward0>) torch.Size([1, 4, 4])\n",
      "Q reshaped =  tensor([[[-1.2965, -0.3490]],\n",
      "\n",
      "        [[-0.0857,  0.0193]],\n",
      "\n",
      "        [[-1.0470, -0.5929]],\n",
      "\n",
      "        [[ 0.3245, -0.0959]],\n",
      "\n",
      "        [[-1.4643, -0.3067]],\n",
      "\n",
      "        [[-0.5399,  0.1115]],\n",
      "\n",
      "        [[-1.4496, -0.2229]],\n",
      "\n",
      "        [[-0.9415,  0.1928]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "K reshaped =  tensor([[[-0.5187, -0.4265]],\n",
      "\n",
      "        [[-1.0570, -0.0622]],\n",
      "\n",
      "        [[-0.7099, -0.5175]],\n",
      "\n",
      "        [[-1.1703, -0.2904]],\n",
      "\n",
      "        [[-0.1186, -0.0719]],\n",
      "\n",
      "        [[-0.9885,  0.1322]],\n",
      "\n",
      "        [[ 0.3114,  0.3048]],\n",
      "\n",
      "        [[-0.7898,  0.3170]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "V reshaped =  tensor([[[ 0.3763, -0.4561]],\n",
      "\n",
      "        [[-0.1431, -1.1326]],\n",
      "\n",
      "        [[ 0.1440, -0.5796]],\n",
      "\n",
      "        [[ 0.0036, -1.2818]],\n",
      "\n",
      "        [[ 0.4909, -0.1850]],\n",
      "\n",
      "        [[-0.1889, -0.9615]],\n",
      "\n",
      "        [[ 0.5467,  0.1198]],\n",
      "\n",
      "        [[-0.2122, -0.6601]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "torch.Size([4, 2, 1, 2]) torch.Size([4, 2, 1, 2]) torch.Size([4, 2, 1, 2])\n",
      "\n",
      "scaled_dot_product_attention\n",
      "attn_output = \n",
      "tensor([[ 0.3763, -0.4561, -0.1431, -1.1326],\n",
      "        [ 0.1440, -0.5796,  0.0036, -1.2818],\n",
      "        [ 0.4909, -0.1850, -0.1889, -0.9615],\n",
      "        [ 0.5467,  0.1198, -0.2122, -0.6601]], grad_fn=<ViewBackward0>)\n",
      "\n",
      "x =  tensor([[[ 1.3813, -0.3479, -1.3644,  0.3310],\n",
      "         [ 1.5767, -0.2089, -1.2010, -0.1669],\n",
      "         [ 1.1134, -0.7185, -1.2381,  0.8432],\n",
      "         [ 0.6920, -1.0099, -0.9481,  1.2659]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "\n",
      "sa_op =  tensor([[[-0.1777, -0.9673,  0.5500,  0.6954],\n",
      "         [-0.1762, -1.0215,  0.8236,  0.9471],\n",
      "         [-0.0216, -0.7625,  0.3748,  0.4990],\n",
      "         [ 0.1428, -0.4521,  0.1445,  0.2307]]], grad_fn=<ViewBackward0>)\n",
      "\n",
      "temop =  tensor([[[ 1.2036, -1.3151, -0.8143,  1.0263],\n",
      "         [ 1.4005, -1.2304, -0.3773,  0.7803],\n",
      "         [ 1.0918, -1.4810, -0.8633,  1.3422],\n",
      "         [ 0.8349, -1.4619, -0.8036,  1.4966]]], grad_fn=<AddBackward0>)\n",
      "\n",
      "Decoder norm1(x + sa(x)) = \n",
      "tensor([[[ 1.0656, -1.2119, -0.7590,  0.9053],\n",
      "         [ 1.2352, -1.3495, -0.5115,  0.6258],\n",
      "         [ 0.8783, -1.2347, -0.7275,  1.0839],\n",
      "         [ 0.6844, -1.2364, -0.6859,  1.2379]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "\n",
      "Memory = \n",
      "tensor([[[-0.5922, -1.3269,  1.1133,  0.8057],\n",
      "         [-0.6378, -1.2324,  1.3385,  0.5317],\n",
      "         [-0.5735, -1.1910,  1.4722,  0.2923],\n",
      "         [-0.4429, -1.2889,  1.4330,  0.2988]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "\n",
      "query =  tensor([[[ 1.0656, -1.2119, -0.7590,  0.9053],\n",
      "         [ 1.2352, -1.3495, -0.5115,  0.6258],\n",
      "         [ 0.8783, -1.2347, -0.7275,  1.0839],\n",
      "         [ 0.6844, -1.2364, -0.6859,  1.2379]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "query shape =  torch.Size([1, 4, 4])\n",
      "\n",
      " _in_projection_packed \n",
      "\n",
      "Q :-  tensor([[[-1.9050,  0.5733,  0.1261, -1.5097],\n",
      "         [-1.8502,  0.6210, -0.3027, -1.3703],\n",
      "         [-1.9108,  0.4930,  0.2565, -1.4770],\n",
      "         [-1.8856,  0.4082,  0.3779, -1.4221]]], grad_fn=<ViewBackward0>)\n",
      "\n",
      "K :-  tensor([[[ 0.0333, -0.4676, -0.5632, -0.3663],\n",
      "         [ 0.2681, -0.3386, -0.5514, -0.6400],\n",
      "         [ 0.3998, -0.2434, -0.4949, -0.8048],\n",
      "         [ 0.3100, -0.2751, -0.4447, -0.7277]]], grad_fn=<SelectBackward0>)\n",
      "\n",
      "V :-  tensor([[[ 1.2107, -0.5438,  0.7414,  0.6407],\n",
      "         [ 1.2866, -0.6758,  0.5375,  0.4867],\n",
      "         [ 1.2856, -0.7591,  0.3497,  0.3336],\n",
      "         [ 1.2321, -0.7398,  0.3449,  0.3160]]], grad_fn=<SelectBackward0>)\n",
      "\n",
      "tgt_len =  1\n",
      "bsz =  4\n",
      "num_heads =  2\n",
      "head_dim =  2\n",
      "\n",
      "tensor([[[-1.9050,  0.5733,  0.1261, -1.5097],\n",
      "         [-1.8502,  0.6210, -0.3027, -1.3703],\n",
      "         [-1.9108,  0.4930,  0.2565, -1.4770],\n",
      "         [-1.8856,  0.4082,  0.3779, -1.4221]]], grad_fn=<ViewBackward0>) torch.Size([1, 4, 4])\n",
      "Q reshaped =  tensor([[[-1.9050,  0.5733]],\n",
      "\n",
      "        [[ 0.1261, -1.5097]],\n",
      "\n",
      "        [[-1.8502,  0.6210]],\n",
      "\n",
      "        [[-0.3027, -1.3703]],\n",
      "\n",
      "        [[-1.9108,  0.4930]],\n",
      "\n",
      "        [[ 0.2565, -1.4770]],\n",
      "\n",
      "        [[-1.8856,  0.4082]],\n",
      "\n",
      "        [[ 0.3779, -1.4221]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "K reshaped =  tensor([[[ 0.0333, -0.4676]],\n",
      "\n",
      "        [[-0.5632, -0.3663]],\n",
      "\n",
      "        [[ 0.2681, -0.3386]],\n",
      "\n",
      "        [[-0.5514, -0.6400]],\n",
      "\n",
      "        [[ 0.3998, -0.2434]],\n",
      "\n",
      "        [[-0.4949, -0.8048]],\n",
      "\n",
      "        [[ 0.3100, -0.2751]],\n",
      "\n",
      "        [[-0.4447, -0.7277]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "V reshaped =  tensor([[[ 1.2107, -0.5438]],\n",
      "\n",
      "        [[ 0.7414,  0.6407]],\n",
      "\n",
      "        [[ 1.2866, -0.6758]],\n",
      "\n",
      "        [[ 0.5375,  0.4867]],\n",
      "\n",
      "        [[ 1.2856, -0.7591]],\n",
      "\n",
      "        [[ 0.3497,  0.3336]],\n",
      "\n",
      "        [[ 1.2321, -0.7398]],\n",
      "\n",
      "        [[ 0.3449,  0.3160]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "torch.Size([4, 2, 1, 2]) torch.Size([4, 2, 1, 2]) torch.Size([4, 2, 1, 2])\n",
      "\n",
      "scaled_dot_product_attention\n",
      "attn_output = \n",
      "tensor([[ 1.2107, -0.5438,  0.7414,  0.6407],\n",
      "        [ 1.2866, -0.6758,  0.5375,  0.4867],\n",
      "        [ 1.2856, -0.7591,  0.3497,  0.3336],\n",
      "        [ 1.2321, -0.7398,  0.3449,  0.3160]], grad_fn=<ViewBackward0>)\n",
      "\n",
      "Decoder norm2(x + mha(x, mem)) = \n",
      "tensor([[[ 0.7457, -1.5881, -0.1114,  0.9537],\n",
      "         [ 0.8056, -1.6688,  0.1206,  0.7426],\n",
      "         [ 0.4675, -1.5729, -0.0375,  1.1428],\n",
      "         [ 0.3051, -1.5305, -0.0251,  1.2506]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "\n",
      "Decoder norm3(x + ff(x)) = \n",
      "tensor([[[ 1.2432, -1.4435, -0.3187,  0.5189],\n",
      "         [ 1.2748, -1.5108, -0.0617,  0.2978],\n",
      "         [ 1.0110, -1.5164, -0.2721,  0.7775],\n",
      "         [ 0.8611, -1.5185, -0.2781,  0.9356]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "\n",
      "\n",
      "################### Transformer Decoder start #####################\n",
      "\n",
      "query =  tensor([[[ 1.2432, -1.4435, -0.3187,  0.5189],\n",
      "         [ 1.2748, -1.5108, -0.0617,  0.2978],\n",
      "         [ 1.0110, -1.5164, -0.2721,  0.7775],\n",
      "         [ 0.8611, -1.5185, -0.2781,  0.9356]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "query shape =  torch.Size([1, 4, 4])\n",
      "\n",
      " _in_projection_packed \n",
      "\n",
      "Q :-  tensor([[[ 0.1661, -0.2710, -1.6732,  1.3372],\n",
      "         [ 0.0757, -0.1392, -1.5539,  1.2860],\n",
      "         [-0.0637, -0.2209, -1.6248,  1.4451],\n",
      "         [-0.1751, -0.2083, -1.5853,  1.4845]]], grad_fn=<SelectBackward0>)\n",
      "\n",
      "K :-  tensor([[[-0.1551,  0.1137,  1.2844, -1.4701],\n",
      "         [-0.1799,  0.1267,  1.3576, -1.5729],\n",
      "         [-0.1646, -0.0850,  1.2122, -1.4463],\n",
      "         [-0.1632, -0.1936,  1.1399, -1.3920]]], grad_fn=<SelectBackward0>)\n",
      "\n",
      "V :-  tensor([[[-0.6564,  0.5187,  0.5258, -0.1919],\n",
      "         [-0.7346,  0.4115,  0.6410,  0.0575],\n",
      "         [-0.6852,  0.6195,  0.3440, -0.4469],\n",
      "         [-0.6771,  0.6782,  0.2220, -0.6131]]], grad_fn=<SelectBackward0>)\n",
      "\n",
      "tgt_len =  1\n",
      "bsz =  4\n",
      "num_heads =  2\n",
      "head_dim =  2\n",
      "\n",
      "tensor([[[ 0.1661, -0.2710, -1.6732,  1.3372],\n",
      "         [ 0.0757, -0.1392, -1.5539,  1.2860],\n",
      "         [-0.0637, -0.2209, -1.6248,  1.4451],\n",
      "         [-0.1751, -0.2083, -1.5853,  1.4845]]], grad_fn=<SelectBackward0>) torch.Size([1, 4, 4])\n",
      "Q reshaped =  tensor([[[ 0.1661, -0.2710]],\n",
      "\n",
      "        [[-1.6732,  1.3372]],\n",
      "\n",
      "        [[ 0.0757, -0.1392]],\n",
      "\n",
      "        [[-1.5539,  1.2860]],\n",
      "\n",
      "        [[-0.0637, -0.2209]],\n",
      "\n",
      "        [[-1.6248,  1.4451]],\n",
      "\n",
      "        [[-0.1751, -0.2083]],\n",
      "\n",
      "        [[-1.5853,  1.4845]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "K reshaped =  tensor([[[-0.1551,  0.1137]],\n",
      "\n",
      "        [[ 1.2844, -1.4701]],\n",
      "\n",
      "        [[-0.1799,  0.1267]],\n",
      "\n",
      "        [[ 1.3576, -1.5729]],\n",
      "\n",
      "        [[-0.1646, -0.0850]],\n",
      "\n",
      "        [[ 1.2122, -1.4463]],\n",
      "\n",
      "        [[-0.1632, -0.1936]],\n",
      "\n",
      "        [[ 1.1399, -1.3920]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "V reshaped =  tensor([[[-0.6564,  0.5187]],\n",
      "\n",
      "        [[ 0.5258, -0.1919]],\n",
      "\n",
      "        [[-0.7346,  0.4115]],\n",
      "\n",
      "        [[ 0.6410,  0.0575]],\n",
      "\n",
      "        [[-0.6852,  0.6195]],\n",
      "\n",
      "        [[ 0.3440, -0.4469]],\n",
      "\n",
      "        [[-0.6771,  0.6782]],\n",
      "\n",
      "        [[ 0.2220, -0.6131]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "torch.Size([4, 2, 1, 2]) torch.Size([4, 2, 1, 2]) torch.Size([4, 2, 1, 2])\n",
      "\n",
      "scaled_dot_product_attention\n",
      "attn_output = \n",
      "tensor([[-0.6564,  0.5187,  0.5258, -0.1919],\n",
      "        [-0.7346,  0.4115,  0.6410,  0.0575],\n",
      "        [-0.6852,  0.6195,  0.3440, -0.4469],\n",
      "        [-0.6771,  0.6782,  0.2220, -0.6131]], grad_fn=<ViewBackward0>)\n",
      "\n",
      "x =  tensor([[[ 1.2432, -1.4435, -0.3187,  0.5189],\n",
      "         [ 1.2748, -1.5108, -0.0617,  0.2978],\n",
      "         [ 1.0110, -1.5164, -0.2721,  0.7775],\n",
      "         [ 0.8611, -1.5185, -0.2781,  0.9356]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "\n",
      "sa_op =  tensor([[[ 0.4319,  0.1096, -0.6072, -0.0048],\n",
      "         [ 0.2849,  0.0838, -0.7558, -0.0279],\n",
      "         [ 0.5061,  0.1449, -0.4310,  0.0644],\n",
      "         [ 0.5580,  0.1651, -0.3058,  0.1042]]], grad_fn=<ViewBackward0>)\n",
      "\n",
      "temop =  tensor([[[ 1.6751, -1.3338, -0.9258,  0.5141],\n",
      "         [ 1.5597, -1.4270, -0.8176,  0.2698],\n",
      "         [ 1.5170, -1.3715, -0.7031,  0.8419],\n",
      "         [ 1.4191, -1.3534, -0.5839,  1.0398]]], grad_fn=<AddBackward0>)\n",
      "\n",
      "Decoder norm1(x + sa(x)) = \n",
      "tensor([[[ 1.4173, -1.1021, -0.7605,  0.4452],\n",
      "         [ 1.4636, -1.1642, -0.6280,  0.3287],\n",
      "         [ 1.2485, -1.2456, -0.6685,  0.6655],\n",
      "         [ 1.1303, -1.3014, -0.6265,  0.7976]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "\n",
      "Memory = \n",
      "tensor([[[-0.5922, -1.3269,  1.1133,  0.8057],\n",
      "         [-0.6378, -1.2324,  1.3385,  0.5317],\n",
      "         [-0.5735, -1.1910,  1.4722,  0.2923],\n",
      "         [-0.4429, -1.2889,  1.4330,  0.2988]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "\n",
      "query =  tensor([[[ 1.4173, -1.1021, -0.7605,  0.4452],\n",
      "         [ 1.4636, -1.1642, -0.6280,  0.3287],\n",
      "         [ 1.2485, -1.2456, -0.6685,  0.6655],\n",
      "         [ 1.1303, -1.3014, -0.6265,  0.7976]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "query shape =  torch.Size([1, 4, 4])\n",
      "\n",
      " _in_projection_packed \n",
      "\n",
      "Q :-  tensor([[[-0.2698,  0.1260,  0.2377,  0.7464],\n",
      "         [-0.3526,  0.1167,  0.3490,  0.8109],\n",
      "         [-0.0388,  0.0720,  0.0799,  0.8639],\n",
      "         [ 0.1038,  0.0419, -0.0260,  0.9097]]], grad_fn=<ViewBackward0>)\n",
      "\n",
      "K :-  tensor([[[ 0.6570, -0.7510,  1.2360, -0.1739],\n",
      "         [ 0.8703, -0.8419,  1.1563, -0.4592],\n",
      "         [ 1.0523, -0.8574,  1.0817, -0.6802],\n",
      "         [ 1.0838, -0.7947,  1.1098, -0.6654]]], grad_fn=<SelectBackward0>)\n",
      "\n",
      "V :-  tensor([[[-0.7708, -0.6468, -0.6293,  0.0018],\n",
      "         [-0.8111, -0.7012, -0.5620, -0.2070],\n",
      "         [-0.8138, -0.7227, -0.4755, -0.3110],\n",
      "         [-0.7947, -0.7098, -0.4504, -0.2107]]], grad_fn=<SelectBackward0>)\n",
      "\n",
      "tgt_len =  1\n",
      "bsz =  4\n",
      "num_heads =  2\n",
      "head_dim =  2\n",
      "\n",
      "tensor([[[-0.2698,  0.1260,  0.2377,  0.7464],\n",
      "         [-0.3526,  0.1167,  0.3490,  0.8109],\n",
      "         [-0.0388,  0.0720,  0.0799,  0.8639],\n",
      "         [ 0.1038,  0.0419, -0.0260,  0.9097]]], grad_fn=<ViewBackward0>) torch.Size([1, 4, 4])\n",
      "Q reshaped =  tensor([[[-0.2698,  0.1260]],\n",
      "\n",
      "        [[ 0.2377,  0.7464]],\n",
      "\n",
      "        [[-0.3526,  0.1167]],\n",
      "\n",
      "        [[ 0.3490,  0.8109]],\n",
      "\n",
      "        [[-0.0388,  0.0720]],\n",
      "\n",
      "        [[ 0.0799,  0.8639]],\n",
      "\n",
      "        [[ 0.1038,  0.0419]],\n",
      "\n",
      "        [[-0.0260,  0.9097]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "K reshaped =  tensor([[[ 0.6570, -0.7510]],\n",
      "\n",
      "        [[ 1.2360, -0.1739]],\n",
      "\n",
      "        [[ 0.8703, -0.8419]],\n",
      "\n",
      "        [[ 1.1563, -0.4592]],\n",
      "\n",
      "        [[ 1.0523, -0.8574]],\n",
      "\n",
      "        [[ 1.0817, -0.6802]],\n",
      "\n",
      "        [[ 1.0838, -0.7947]],\n",
      "\n",
      "        [[ 1.1098, -0.6654]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "V reshaped =  tensor([[[-0.7708, -0.6468]],\n",
      "\n",
      "        [[-0.6293,  0.0018]],\n",
      "\n",
      "        [[-0.8111, -0.7012]],\n",
      "\n",
      "        [[-0.5620, -0.2070]],\n",
      "\n",
      "        [[-0.8138, -0.7227]],\n",
      "\n",
      "        [[-0.4755, -0.3110]],\n",
      "\n",
      "        [[-0.7947, -0.7098]],\n",
      "\n",
      "        [[-0.4504, -0.2107]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "torch.Size([4, 2, 1, 2]) torch.Size([4, 2, 1, 2]) torch.Size([4, 2, 1, 2])\n",
      "\n",
      "scaled_dot_product_attention\n",
      "attn_output = \n",
      "tensor([[-0.7708, -0.6468, -0.6293,  0.0018],\n",
      "        [-0.8111, -0.7012, -0.5620, -0.2070],\n",
      "        [-0.8138, -0.7227, -0.4755, -0.3110],\n",
      "        [-0.7947, -0.7098, -0.4504, -0.2107]], grad_fn=<ViewBackward0>)\n",
      "\n",
      "Decoder norm2(x + mha(x, mem)) = \n",
      "tensor([[[ 1.0417,  0.0129, -1.6132,  0.5586],\n",
      "         [ 1.1124, -0.1153, -1.5597,  0.5626],\n",
      "         [ 0.9172, -0.2676, -1.5240,  0.8744],\n",
      "         [ 0.8745, -0.3234, -1.4959,  0.9449]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "\n",
      "Decoder norm3(x + ff(x)) = \n",
      "tensor([[[ 0.9955,  0.4470, -1.6616,  0.2191],\n",
      "         [ 1.0988,  0.3046, -1.6277,  0.2242],\n",
      "         [ 0.9490,  0.1364, -1.6589,  0.5735],\n",
      "         [ 0.9191,  0.0695, -1.6479,  0.6593]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "\n",
      "tensor([[[ 0.9643, -1.2317,  0.7367, -0.0064, -0.7884, -0.7311, -0.0141,\n",
      "           0.7371, -0.5670,  0.1799],\n",
      "         [ 0.9586, -1.1972,  0.8049, -0.0248, -0.7074, -0.7126,  0.0332,\n",
      "           0.7759, -0.5306,  0.1491],\n",
      "         [ 0.8940, -0.9926,  0.8377,  0.0088, -0.8084, -0.8657, -0.0520,\n",
      "           0.7437, -0.4131,  0.0413],\n",
      "         [ 0.8677, -0.9303,  0.8531,  0.0138, -0.8124, -0.8961, -0.0622,\n",
      "           0.7322, -0.3766,  0.0090]]], grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=512, dropout=0):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.encoding = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1).float()\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(torch.log(torch.tensor(10000.0)) / d_model))\n",
    "        self.encoding[:, 0::2] = torch.sin(position * div_term)\n",
    "        self.encoding[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.encoding = self.encoding.unsqueeze(0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.encoding[:, :x.size(1)].detach()\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, src_vocab_size, tgt_vocab_size, d_model=4, nhead=2, num_encoder_layers=1, num_decoder_layers=1):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.src_embedding = nn.Embedding(src_vocab_size, d_model)\n",
    "        self.tgt_embedding = nn.Embedding(tgt_vocab_size, d_model)\n",
    "        self.positional_encoding = PositionalEncoding(d_model, dropout=0)\n",
    "        self.transformer = nn.Transformer(\n",
    "            d_model=d_model,\n",
    "            nhead=nhead,\n",
    "            num_encoder_layers=num_encoder_layers,\n",
    "            num_decoder_layers=num_decoder_layers,\n",
    "            dropout=0\n",
    "        )\n",
    "        self.fc = nn.Linear(d_model, tgt_vocab_size)\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "\n",
    "        print(\"Source embeddings :- \\n\")\n",
    "        print(self.src_embedding(src))\n",
    "        print()\n",
    "\n",
    "        print(\"Target embeddings :- \\n\")\n",
    "        print(self.tgt_embedding(tgt))\n",
    "        print()\n",
    "\n",
    "\n",
    "        src = self.src_embedding(src) + self.positional_encoding(src)\n",
    "        tgt = self.tgt_embedding(tgt) + self.positional_encoding(tgt)\n",
    "\n",
    "        print(\"Positional encoded source embeddings:\")\n",
    "        print(src)\n",
    "        print()\n",
    "\n",
    "        print(\"Positional encoded target embeddings:\")\n",
    "        print(tgt)\n",
    "        print()\n",
    "\n",
    "        output = self.transformer(src, tgt)\n",
    "        output = self.fc(output)\n",
    "        return output\n",
    "\n",
    "    \n",
    "\n",
    "src_vocab_size = 10  # Source language vocabulary size\n",
    "tgt_vocab_size = 10  # Target language vocabulary size\n",
    "d_model = 4  # Dimension of the model\n",
    "num_heads = 2\n",
    "num_encoder_layers = 3\n",
    "num_decoder_layers = 3\n",
    "model = TransformerModel(src_vocab_size, tgt_vocab_size, d_model=d_model, nhead=num_heads, num_encoder_layers = num_encoder_layers, num_decoder_layers=num_decoder_layers)\n",
    "# Source sentence in the source language\n",
    "src_sentence = torch.tensor([[0, 1, 2, 3]])  # Source sequence\n",
    "\n",
    "# Target sentence in the target language (translation of the source sentence)\n",
    "tgt_sentence = torch.tensor([[1, 0, 3, 3]])  # Target sequence\n",
    "\n",
    "# Forward pass\n",
    "output = model(src_sentence, tgt_sentence)\n",
    "print(output)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_intermediate_outputs(src_sentence, tgt_sentence,d_model, model, num_encoder_layers , num_decoder_layers):\n",
    "\n",
    "    state_dict = model.state_dict()\n",
    "\n",
    "    pe_src_embeds, pe_tgt_embeds = get_embedding_outputs(src_sentence,  tgt_sentence, state_dict, d_model = d_model)\n",
    "\n",
    "    print(\"###\"*25)\n",
    "    print(\"### Encoder Start ###\")\n",
    "    print()\n",
    "\n",
    "    x_enc = pe_src_embeds\n",
    "\n",
    "    for lno in range(num_encoder_layers):\n",
    "        attn_enc_output, src_len, head_dim, attn_weights = encoder_block_attn_output(x_enc, state_dict, layer_num = lno, need_weights = False)\n",
    "\n",
    "        if lno == 0:\n",
    "            tgt_len, bsz, embed_dim = x_enc.shape\n",
    "\n",
    "        output_enc_final = encoder_block_post_attn_output(x_enc, attn_enc_output, state_dict, layer_num = lno , bsz = bsz, tgt_len = tgt_len)\n",
    "\n",
    "        x_enc = output_enc_final\n",
    "\n",
    "    \n",
    "    print(\"### Encoder Done ###\")\n",
    "    print(\"\\n\\n\\n\")\n",
    "    print(\"### Decoder Start ###\")\n",
    "\n",
    "    x_dec = pe_tgt_embeds\n",
    "    memory = x_enc\n",
    "\n",
    "    for lno in range(num_decoder_layers):\n",
    "\n",
    "        self_attn_dec, dec_sa_wts = decoder_block_self_attn_output(x_dec, state_dict, layer_num = lno, need_weights = False)\n",
    "        x_dec = dec_post_self_attn(self_attn_dec, x_dec, state_dict, layer_num = lno)\n",
    "\n",
    "        attn_dec_mha_output, attn_dec_mha_wts = decoder_block_cross_attn_output(x_dec, memory, state_dict, layer_num = lno, tgt_len = tgt_len, src_len = src_len, head_dim = head_dim, need_weights = False)\n",
    "        final_op = decoder_block_post_attn_output(x_dec, attn_dec_mha_output, state_dict, layer_num = lno)\n",
    "\n",
    "        print(pe_tgt_embeds.shape, final_op.shape)\n",
    "        x_dec = final_op\n",
    "\n",
    "\n",
    "    final_op = feef_fwd_transformer(final_op, state_dict)\n",
    "\n",
    "    print(\"### Decoder Done ###\")\n",
    "\n",
    "\n",
    "    return final_op\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source sentence embedding\n",
      "Word index: 0, Embedding: tensor([-0.4019,  1.1379, -0.5372,  1.8170])\n",
      "Word index: 1, Embedding: tensor([-0.9956,  0.5849, -0.1061,  0.5031])\n",
      "Word index: 2, Embedding: tensor([1.0417, 0.3042, 1.3250, 1.6455])\n",
      "Word index: 3, Embedding: tensor([ 0.5991, -1.5866,  0.1186,  0.1184])\n",
      "\n",
      "Target sentence embedding\n",
      "Word index: 1, Embedding: tensor([ 0.6701, -0.7239,  0.8397,  0.7060])\n",
      "Word index: 0, Embedding: tensor([ 0.8538, -0.2595, -0.4081, -1.8921])\n",
      "Word index: 3, Embedding: tensor([-1.2405,  0.6807,  0.4640, -0.9147])\n",
      "Word index: 3, Embedding: tensor([-1.2405,  0.6807,  0.4640, -0.9147])\n",
      "\n",
      "PE source embeddings : \n",
      "\n",
      "tensor([[[-0.4019,  3.1379,  1.4628,  5.8170],\n",
      "         [-0.1541,  2.1252,  1.9039,  4.5030],\n",
      "         [ 1.9510,  0.8881,  3.3450,  5.6453],\n",
      "         [ 0.7402, -1.5766,  2.1486,  4.1180]]])\n",
      "\n",
      "PE target embeddings : \n",
      "\n",
      "tensor([[[ 1.6701,  0.2761,  3.8397,  4.7060],\n",
      "         [ 2.6952,  0.2808,  2.6019,  2.1078],\n",
      "         [ 0.6688,  0.2646,  3.4840,  3.0851],\n",
      "         [-0.0993, -0.3093,  3.4940,  3.0849]]])\n",
      "\n",
      "###########################################################################\n",
      "### Encoder Start ###\n",
      "\n",
      "Q_enc_0 = \n",
      "tensor([[[-4.1784e+00, -4.0106e+00]],\n",
      "\n",
      "        [[ 4.1455e+00,  5.2374e-02]],\n",
      "\n",
      "        [[-2.7286e+00, -3.1981e+00]],\n",
      "\n",
      "        [[ 3.3991e+00,  3.0230e-03]],\n",
      "\n",
      "        [[-2.3743e+00, -4.9482e+00]],\n",
      "\n",
      "        [[ 3.9994e+00,  3.7645e-01]],\n",
      "\n",
      "        [[-4.3266e-01, -2.7821e+00]],\n",
      "\n",
      "        [[ 1.3830e+00, -2.1311e-02]]])\n",
      "\n",
      "K_enc_0 = \n",
      "tensor([[[ 0.1276,  0.7274]],\n",
      "\n",
      "        [[ 1.5029, -0.2525]],\n",
      "\n",
      "        [[ 0.1799,  0.1953]],\n",
      "\n",
      "        [[ 1.1665, -0.4289]],\n",
      "\n",
      "        [[-0.2008, -0.5300]],\n",
      "\n",
      "        [[ 2.1722,  0.3007]],\n",
      "\n",
      "        [[-1.0771, -0.0375]],\n",
      "\n",
      "        [[ 0.8167,  0.6506]]])\n",
      "\n",
      "V_enc_0 = \n",
      "tensor([[[-3.9342,  3.3096]],\n",
      "\n",
      "        [[ 1.6318, -1.3380]],\n",
      "\n",
      "        [[-3.0291,  2.4046]],\n",
      "\n",
      "        [[ 0.9440, -1.6920]],\n",
      "\n",
      "        [[-3.4646,  3.5623]],\n",
      "\n",
      "        [[ 0.8624, -4.8413]],\n",
      "\n",
      "        [[-1.4426,  1.8447]],\n",
      "\n",
      "        [[ 1.5679, -4.1069]]])\n",
      "\n",
      "Encoder Attention output = \n",
      "tensor([[-3.9342,  3.3096,  1.6318, -1.3380],\n",
      "        [-3.0291,  2.4046,  0.9440, -1.6920],\n",
      "        [-3.4646,  3.5623,  0.8624, -4.8413],\n",
      "        [-1.4426,  1.8447,  1.5679, -4.1069]])\n",
      "\n",
      "Final Encoder 0 Output :\n",
      "norm2(norm1(x + self_atten(x)) + feed_fwd_op)\n",
      "\n",
      "tensor([[[-1.2149, -0.6647,  0.5425,  1.3371],\n",
      "         [-1.1645, -0.8040,  0.8113,  1.1572],\n",
      "         [-0.7768, -1.1709,  1.2276,  0.7201],\n",
      "         [-0.2635, -1.4664,  1.2417,  0.4882]]], grad_fn=<AddBackward0>)\n",
      "\n",
      "Q_enc_1 = \n",
      "tensor([[[-0.3071, -0.9261]],\n",
      "\n",
      "        [[-0.4274, -1.6286]],\n",
      "\n",
      "        [[-0.3712, -0.8321]],\n",
      "\n",
      "        [[-0.5106, -1.6461]],\n",
      "\n",
      "        [[-0.5273, -0.5971]],\n",
      "\n",
      "        [[-0.5360, -1.4053]],\n",
      "\n",
      "        [[-0.6403, -0.4727]],\n",
      "\n",
      "        [[-0.3634, -0.9819]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "K_enc_1 = \n",
      "tensor([[[-0.6060,  0.7348]],\n",
      "\n",
      "        [[ 0.2426, -0.0995]],\n",
      "\n",
      "        [[-0.6149,  0.7130]],\n",
      "\n",
      "        [[ 0.1095, -0.1689]],\n",
      "\n",
      "        [[-0.6346,  0.5822]],\n",
      "\n",
      "        [[-0.0855, -0.1482]],\n",
      "\n",
      "        [[-0.6683,  0.4347]],\n",
      "\n",
      "        [[-0.0533,  0.0817]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "V_enc_1 = \n",
      "tensor([[[0.0228, 0.8665]],\n",
      "\n",
      "        [[0.8199, 0.5550]],\n",
      "\n",
      "        [[0.0629, 0.7502]],\n",
      "\n",
      "        [[0.7990, 0.6160]],\n",
      "\n",
      "        [[0.2519, 0.4040]],\n",
      "\n",
      "        [[0.4996, 0.6859]],\n",
      "\n",
      "        [[0.4813, 0.1391]],\n",
      "\n",
      "        [[0.0481, 0.6612]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "Encoder Attention output = \n",
      "tensor([[0.0228, 0.8665, 0.8199, 0.5550],\n",
      "        [0.0629, 0.7502, 0.7990, 0.6160],\n",
      "        [0.2519, 0.4040, 0.4996, 0.6859],\n",
      "        [0.4813, 0.1391, 0.0481, 0.6612]], grad_fn=<ViewBackward0>)\n",
      "\n",
      "Final Encoder 1 Output :\n",
      "norm2(norm1(x + self_atten(x)) + feed_fwd_op)\n",
      "\n",
      "tensor([[[-0.8932, -1.1009,  1.0288,  0.9653],\n",
      "         [-0.8499, -1.1032,  1.2533,  0.6998],\n",
      "         [-0.6944, -1.1470,  1.4241,  0.4172],\n",
      "         [-0.5705, -1.2512,  1.3849,  0.4368]]], grad_fn=<AddBackward0>)\n",
      "\n",
      "Q_enc_2 = \n",
      "tensor([[[ 1.0430,  1.0346]],\n",
      "\n",
      "        [[ 0.1915,  0.7643]],\n",
      "\n",
      "        [[ 1.0802,  1.1768]],\n",
      "\n",
      "        [[ 0.1347,  0.8348]],\n",
      "\n",
      "        [[ 1.1071,  1.2866]],\n",
      "\n",
      "        [[ 0.0307,  0.8372]],\n",
      "\n",
      "        [[ 1.1341,  1.2796]],\n",
      "\n",
      "        [[-0.0283,  0.7693]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "K_enc_2 = \n",
      "tensor([[[ 0.2947, -0.6589]],\n",
      "\n",
      "        [[-0.3096, -0.3069]],\n",
      "\n",
      "        [[ 0.4526, -0.6829]],\n",
      "\n",
      "        [[-0.1179, -0.3080]],\n",
      "\n",
      "        [[ 0.6291, -0.6758]],\n",
      "\n",
      "        [[ 0.1198, -0.3286]],\n",
      "\n",
      "        [[ 0.6650, -0.6575]],\n",
      "\n",
      "        [[ 0.1700, -0.3691]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "V_enc_2 = \n",
      "tensor([[[ 0.4722, -0.2705]],\n",
      "\n",
      "        [[ 0.1496, -0.1437]],\n",
      "\n",
      "        [[ 0.2148, -0.3004]],\n",
      "\n",
      "        [[ 0.0540, -0.3001]],\n",
      "\n",
      "        [[-0.0661, -0.3265]],\n",
      "\n",
      "        [[ 0.0216, -0.3693]],\n",
      "\n",
      "        [[-0.0760, -0.3316]],\n",
      "\n",
      "        [[ 0.1204, -0.2472]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "Encoder Attention output = \n",
      "tensor([[ 0.4722, -0.2705,  0.1496, -0.1437],\n",
      "        [ 0.2148, -0.3004,  0.0540, -0.3001],\n",
      "        [-0.0661, -0.3265,  0.0216, -0.3693],\n",
      "        [-0.0760, -0.3316,  0.1204, -0.2472]], grad_fn=<ViewBackward0>)\n",
      "\n",
      "Final Encoder 2 Output :\n",
      "norm2(norm1(x + self_atten(x)) + feed_fwd_op)\n",
      "\n",
      "tensor([[[-0.5922, -1.3269,  1.1133,  0.8057],\n",
      "         [-0.6378, -1.2324,  1.3385,  0.5317],\n",
      "         [-0.5735, -1.1909,  1.4722,  0.2923],\n",
      "         [-0.4429, -1.2888,  1.4329,  0.2988]]], grad_fn=<AddBackward0>)\n",
      "\n",
      "### Encoder Done ###\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "### Decoder Start ###\n",
      "Q_dec_0 = \n",
      "tensor([[[ 0.8298, -2.7723]],\n",
      "\n",
      "        [[ 0.1053, -1.2076]],\n",
      "\n",
      "        [[-0.6300, -0.6314]],\n",
      "\n",
      "        [[ 1.5452, -0.9409]],\n",
      "\n",
      "        [[ 0.9926, -2.3179]],\n",
      "\n",
      "        [[ 0.2049, -1.0176]],\n",
      "\n",
      "        [[ 1.3943, -2.7760]],\n",
      "\n",
      "        [[-0.4507, -1.1836]]])\n",
      "\n",
      "K_dec_0 = \n",
      "tensor([[[-1.2786, -1.3001]],\n",
      "\n",
      "        [[ 3.5688,  1.2169]],\n",
      "\n",
      "        [[ 0.2087, -1.3096]],\n",
      "\n",
      "        [[ 2.0866,  0.6364]],\n",
      "\n",
      "        [[-0.9382, -1.3581]],\n",
      "\n",
      "        [[ 2.7814,  0.4942]],\n",
      "\n",
      "        [[-1.1729, -1.2928]],\n",
      "\n",
      "        [[ 2.9863,  0.6032]]])\n",
      "\n",
      "V_dec_0 = \n",
      "tensor([[[ 3.6230,  2.5086]],\n",
      "\n",
      "        [[ 0.2786, -0.0958]],\n",
      "\n",
      "        [[ 3.1906,  1.8818]],\n",
      "\n",
      "        [[-0.5206, -0.1735]],\n",
      "\n",
      "        [[ 2.6893,  1.7036]],\n",
      "\n",
      "        [[-0.1949, -0.2417]],\n",
      "\n",
      "        [[ 2.3067,  1.7857]],\n",
      "\n",
      "        [[ 0.1662, -0.5128]]])\n",
      "\n",
      "Decoder Self Attention = \n",
      "tensor([[ 3.6230,  2.5086,  0.2786, -0.0958],\n",
      "        [ 3.1906,  1.8818, -0.5206, -0.1735],\n",
      "        [ 2.6893,  1.7036, -0.1949, -0.2417],\n",
      "        [ 2.3067,  1.7857,  0.1662, -0.5128]])\n",
      "\n",
      "Decoder_0 norm1(x + sa(x))\n",
      "tensor([[[ 0.9474, -1.1278, -0.8622,  1.0426],\n",
      "         [ 1.4457, -1.0329, -0.8220,  0.4092],\n",
      "         [ 0.7637, -1.4068, -0.4629,  1.1060],\n",
      "         [ 0.2991, -1.5215, -0.0401,  1.2625]]], grad_fn=<AddBackward0>)\n",
      "\n",
      "Q_dec_0 = \n",
      "tensor([[[0.9479, 0.5626]],\n",
      "\n",
      "        [[1.2335, 0.6094]],\n",
      "\n",
      "        [[0.6321, 1.0294]],\n",
      "\n",
      "        [[1.2887, 0.9829]],\n",
      "\n",
      "        [[0.8242, 0.5045]],\n",
      "\n",
      "        [[1.0789, 0.4067]],\n",
      "\n",
      "        [[0.7195, 0.2046]],\n",
      "\n",
      "        [[0.7696, 0.0188]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "K_dec_0 = \n",
      "tensor([[[-0.3500,  0.1593]],\n",
      "\n",
      "        [[-0.4738,  0.1080]],\n",
      "\n",
      "        [[-0.4297,  0.0768]],\n",
      "\n",
      "        [[-0.6427,  0.0108]],\n",
      "\n",
      "        [[-0.5195,  0.0223]],\n",
      "\n",
      "        [[-0.8347, -0.0350]],\n",
      "\n",
      "        [[-0.5670,  0.0474]],\n",
      "\n",
      "        [[-0.9287,  0.0190]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "V_dec_0 = \n",
      "tensor([[[ 0.0440, -0.3779]],\n",
      "\n",
      "        [[-0.0487,  1.3131]],\n",
      "\n",
      "        [[-0.0413, -0.1451]],\n",
      "\n",
      "        [[ 0.1154,  1.4680]],\n",
      "\n",
      "        [[-0.0951,  0.0868]],\n",
      "\n",
      "        [[ 0.2556,  1.5323]],\n",
      "\n",
      "        [[-0.0694,  0.1336]],\n",
      "\n",
      "        [[ 0.2615,  1.4827]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "Cross attention in decoder_0\n",
      "tensor([[ 0.0440, -0.3779, -0.0487,  1.3131],\n",
      "        [-0.0413, -0.1451,  0.1154,  1.4680],\n",
      "        [-0.0951,  0.0868,  0.2556,  1.5323],\n",
      "        [-0.0694,  0.1336,  0.2615,  1.4827]], grad_fn=<ViewBackward0>)\n",
      "\n",
      "norm2(x' + mha(x', mem)), \n",
      " where x' = Decoder_curr_layer norm1(x + sa(x))\n",
      "tensor([[[ 1.2606, -0.5207, -1.3358,  0.5960],\n",
      "         [ 1.4943, -0.3998, -1.2572,  0.1627],\n",
      "         [ 0.9907, -0.8290, -1.1574,  0.9958],\n",
      "         [ 0.6228, -1.0692, -0.8652,  1.3116]]], grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "\n",
      "norm3(x'' + ff(x'')) \n",
      " where, x'' = Decoder_curr_layer norm2(x' + mha(x'))\n",
      "tensor([[[ 1.3813, -0.3479, -1.3644,  0.3310],\n",
      "         [ 1.5767, -0.2089, -1.2010, -0.1669],\n",
      "         [ 1.1134, -0.7185, -1.2381,  0.8432],\n",
      "         [ 0.6920, -1.0098, -0.9480,  1.2659]]], grad_fn=<AddBackward0>)\n",
      "\n",
      "torch.Size([1, 4, 4]) torch.Size([1, 4, 4])\n",
      "Q_dec_1 = \n",
      "tensor([[[-1.2965, -0.3490]],\n",
      "\n",
      "        [[-0.0857,  0.0193]],\n",
      "\n",
      "        [[-1.0470, -0.5929]],\n",
      "\n",
      "        [[ 0.3245, -0.0959]],\n",
      "\n",
      "        [[-1.4643, -0.3067]],\n",
      "\n",
      "        [[-0.5399,  0.1115]],\n",
      "\n",
      "        [[-1.4496, -0.2229]],\n",
      "\n",
      "        [[-0.9414,  0.1928]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "K_dec_1 = \n",
      "tensor([[[-0.5187, -0.4265]],\n",
      "\n",
      "        [[-1.0570, -0.0622]],\n",
      "\n",
      "        [[-0.7099, -0.5175]],\n",
      "\n",
      "        [[-1.1703, -0.2904]],\n",
      "\n",
      "        [[-0.1186, -0.0719]],\n",
      "\n",
      "        [[-0.9885,  0.1322]],\n",
      "\n",
      "        [[ 0.3114,  0.3048]],\n",
      "\n",
      "        [[-0.7898,  0.3170]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "V_dec_1 = \n",
      "tensor([[[ 0.3763, -0.4560]],\n",
      "\n",
      "        [[-0.1431, -1.1326]],\n",
      "\n",
      "        [[ 0.1440, -0.5796]],\n",
      "\n",
      "        [[ 0.0036, -1.2818]],\n",
      "\n",
      "        [[ 0.4909, -0.1850]],\n",
      "\n",
      "        [[-0.1889, -0.9615]],\n",
      "\n",
      "        [[ 0.5467,  0.1198]],\n",
      "\n",
      "        [[-0.2122, -0.6601]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "Decoder Self Attention = \n",
      "tensor([[ 0.3763, -0.4560, -0.1431, -1.1326],\n",
      "        [ 0.1440, -0.5796,  0.0036, -1.2818],\n",
      "        [ 0.4909, -0.1850, -0.1889, -0.9615],\n",
      "        [ 0.5467,  0.1198, -0.2122, -0.6601]], grad_fn=<ViewBackward0>)\n",
      "\n",
      "Decoder_1 norm1(x + sa(x))\n",
      "tensor([[[ 1.0656, -1.2119, -0.7590,  0.9053],\n",
      "         [ 1.2351, -1.3495, -0.5115,  0.6258],\n",
      "         [ 0.8783, -1.2347, -0.7274,  1.0839],\n",
      "         [ 0.6844, -1.2364, -0.6859,  1.2379]]], grad_fn=<AddBackward0>)\n",
      "\n",
      "Q_dec_1 = \n",
      "tensor([[[-1.9050,  0.5733]],\n",
      "\n",
      "        [[ 0.1261, -1.5097]],\n",
      "\n",
      "        [[-1.8502,  0.6209]],\n",
      "\n",
      "        [[-0.3027, -1.3703]],\n",
      "\n",
      "        [[-1.9108,  0.4930]],\n",
      "\n",
      "        [[ 0.2565, -1.4770]],\n",
      "\n",
      "        [[-1.8856,  0.4082]],\n",
      "\n",
      "        [[ 0.3779, -1.4221]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "K_dec_1 = \n",
      "tensor([[[ 0.0333, -0.4676]],\n",
      "\n",
      "        [[-0.5632, -0.3663]],\n",
      "\n",
      "        [[ 0.2681, -0.3386]],\n",
      "\n",
      "        [[-0.5514, -0.6400]],\n",
      "\n",
      "        [[ 0.3998, -0.2434]],\n",
      "\n",
      "        [[-0.4949, -0.8048]],\n",
      "\n",
      "        [[ 0.3100, -0.2751]],\n",
      "\n",
      "        [[-0.4447, -0.7276]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "V_dec_1 = \n",
      "tensor([[[ 1.2107, -0.5438]],\n",
      "\n",
      "        [[ 0.7414,  0.6407]],\n",
      "\n",
      "        [[ 1.2866, -0.6758]],\n",
      "\n",
      "        [[ 0.5375,  0.4867]],\n",
      "\n",
      "        [[ 1.2856, -0.7591]],\n",
      "\n",
      "        [[ 0.3497,  0.3336]],\n",
      "\n",
      "        [[ 1.2321, -0.7398]],\n",
      "\n",
      "        [[ 0.3449,  0.3160]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "Cross attention in decoder_1\n",
      "tensor([[ 1.2107, -0.5438,  0.7414,  0.6407],\n",
      "        [ 1.2866, -0.6758,  0.5375,  0.4867],\n",
      "        [ 1.2856, -0.7591,  0.3497,  0.3336],\n",
      "        [ 1.2321, -0.7398,  0.3449,  0.3160]], grad_fn=<ViewBackward0>)\n",
      "\n",
      "norm2(x' + mha(x', mem)), \n",
      " where x' = Decoder_curr_layer norm1(x + sa(x))\n",
      "tensor([[[ 0.7457, -1.5880, -0.1114,  0.9537],\n",
      "         [ 0.8056, -1.6688,  0.1206,  0.7426],\n",
      "         [ 0.4675, -1.5729, -0.0375,  1.1428],\n",
      "         [ 0.3051, -1.5305, -0.0251,  1.2506]]], grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "\n",
      "norm3(x'' + ff(x'')) \n",
      " where, x'' = Decoder_curr_layer norm2(x' + mha(x'))\n",
      "tensor([[[ 1.2432, -1.4434, -0.3187,  0.5189],\n",
      "         [ 1.2748, -1.5108, -0.0617,  0.2978],\n",
      "         [ 1.0110, -1.5164, -0.2721,  0.7775],\n",
      "         [ 0.8611, -1.5185, -0.2781,  0.9356]]], grad_fn=<AddBackward0>)\n",
      "\n",
      "torch.Size([1, 4, 4]) torch.Size([1, 4, 4])\n",
      "Q_dec_2 = \n",
      "tensor([[[ 0.1661, -0.2710]],\n",
      "\n",
      "        [[-1.6732,  1.3371]],\n",
      "\n",
      "        [[ 0.0757, -0.1392]],\n",
      "\n",
      "        [[-1.5539,  1.2860]],\n",
      "\n",
      "        [[-0.0637, -0.2209]],\n",
      "\n",
      "        [[-1.6248,  1.4451]],\n",
      "\n",
      "        [[-0.1751, -0.2083]],\n",
      "\n",
      "        [[-1.5853,  1.4844]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "K_dec_2 = \n",
      "tensor([[[-0.1551,  0.1137]],\n",
      "\n",
      "        [[ 1.2844, -1.4701]],\n",
      "\n",
      "        [[-0.1799,  0.1267]],\n",
      "\n",
      "        [[ 1.3576, -1.5729]],\n",
      "\n",
      "        [[-0.1646, -0.0850]],\n",
      "\n",
      "        [[ 1.2122, -1.4463]],\n",
      "\n",
      "        [[-0.1632, -0.1936]],\n",
      "\n",
      "        [[ 1.1399, -1.3919]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "V_dec_2 = \n",
      "tensor([[[-0.6564,  0.5187]],\n",
      "\n",
      "        [[ 0.5258, -0.1919]],\n",
      "\n",
      "        [[-0.7346,  0.4115]],\n",
      "\n",
      "        [[ 0.6410,  0.0575]],\n",
      "\n",
      "        [[-0.6852,  0.6195]],\n",
      "\n",
      "        [[ 0.3440, -0.4469]],\n",
      "\n",
      "        [[-0.6771,  0.6782]],\n",
      "\n",
      "        [[ 0.2220, -0.6131]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "Decoder Self Attention = \n",
      "tensor([[-0.6564,  0.5187,  0.5258, -0.1919],\n",
      "        [-0.7346,  0.4115,  0.6410,  0.0575],\n",
      "        [-0.6852,  0.6195,  0.3440, -0.4469],\n",
      "        [-0.6771,  0.6782,  0.2220, -0.6131]], grad_fn=<ViewBackward0>)\n",
      "\n",
      "Decoder_2 norm1(x + sa(x))\n",
      "tensor([[[ 1.4173, -1.1021, -0.7605,  0.4452],\n",
      "         [ 1.4636, -1.1642, -0.6280,  0.3287],\n",
      "         [ 1.2485, -1.2456, -0.6685,  0.6655],\n",
      "         [ 1.1303, -1.3014, -0.6265,  0.7976]]], grad_fn=<AddBackward0>)\n",
      "\n",
      "Q_dec_2 = \n",
      "tensor([[[-0.2698,  0.1260]],\n",
      "\n",
      "        [[ 0.2377,  0.7464]],\n",
      "\n",
      "        [[-0.3526,  0.1167]],\n",
      "\n",
      "        [[ 0.3490,  0.8109]],\n",
      "\n",
      "        [[-0.0388,  0.0720]],\n",
      "\n",
      "        [[ 0.0799,  0.8639]],\n",
      "\n",
      "        [[ 0.1038,  0.0419]],\n",
      "\n",
      "        [[-0.0260,  0.9097]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "K_dec_2 = \n",
      "tensor([[[ 0.6570, -0.7510]],\n",
      "\n",
      "        [[ 1.2360, -0.1739]],\n",
      "\n",
      "        [[ 0.8703, -0.8419]],\n",
      "\n",
      "        [[ 1.1563, -0.4592]],\n",
      "\n",
      "        [[ 1.0523, -0.8574]],\n",
      "\n",
      "        [[ 1.0817, -0.6802]],\n",
      "\n",
      "        [[ 1.0838, -0.7947]],\n",
      "\n",
      "        [[ 1.1097, -0.6654]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "V_dec_2 = \n",
      "tensor([[[-0.7708, -0.6468]],\n",
      "\n",
      "        [[-0.6293,  0.0018]],\n",
      "\n",
      "        [[-0.8111, -0.7012]],\n",
      "\n",
      "        [[-0.5620, -0.2070]],\n",
      "\n",
      "        [[-0.8138, -0.7227]],\n",
      "\n",
      "        [[-0.4755, -0.3110]],\n",
      "\n",
      "        [[-0.7947, -0.7098]],\n",
      "\n",
      "        [[-0.4504, -0.2107]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "Cross attention in decoder_2\n",
      "tensor([[-0.7708, -0.6468, -0.6293,  0.0018],\n",
      "        [-0.8111, -0.7012, -0.5620, -0.2070],\n",
      "        [-0.8138, -0.7227, -0.4755, -0.3110],\n",
      "        [-0.7947, -0.7098, -0.4504, -0.2107]], grad_fn=<ViewBackward0>)\n",
      "\n",
      "norm2(x' + mha(x', mem)), \n",
      " where x' = Decoder_curr_layer norm1(x + sa(x))\n",
      "tensor([[[ 1.0417,  0.0129, -1.6132,  0.5586],\n",
      "         [ 1.1124, -0.1153, -1.5597,  0.5626],\n",
      "         [ 0.9172, -0.2676, -1.5240,  0.8744],\n",
      "         [ 0.8745, -0.3234, -1.4959,  0.9449]]], grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "\n",
      "norm3(x'' + ff(x'')) \n",
      " where, x'' = Decoder_curr_layer norm2(x' + mha(x'))\n",
      "tensor([[[ 0.9955,  0.4470, -1.6616,  0.2191],\n",
      "         [ 1.0988,  0.3046, -1.6277,  0.2242],\n",
      "         [ 0.9490,  0.1364, -1.6589,  0.5734],\n",
      "         [ 0.9191,  0.0695, -1.6479,  0.6593]]], grad_fn=<AddBackward0>)\n",
      "\n",
      "torch.Size([1, 4, 4]) torch.Size([1, 4, 4])\n",
      "### Decoder Done ###\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.9643, -1.2317,  0.7367, -0.0064, -0.7884, -0.7311, -0.0141,\n",
       "           0.7371, -0.5670,  0.1799],\n",
       "         [ 0.9586, -1.1972,  0.8049, -0.0248, -0.7074, -0.7126,  0.0332,\n",
       "           0.7759, -0.5306,  0.1491],\n",
       "         [ 0.8940, -0.9926,  0.8377,  0.0088, -0.8084, -0.8657, -0.0520,\n",
       "           0.7437, -0.4131,  0.0413],\n",
       "         [ 0.8677, -0.9303,  0.8531,  0.0138, -0.8124, -0.8961, -0.0622,\n",
       "           0.7322, -0.3766,  0.0090]]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_model = 4\n",
    "num_encoder_layers = 3\n",
    "num_decoder_layers = 3\n",
    "\n",
    "get_all_intermediate_outputs(src_sentence, tgt_sentence, model = model, num_encoder_layers = num_encoder_layers , num_decoder_layers = num_encoder_layers, d_model=d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "my_project_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
