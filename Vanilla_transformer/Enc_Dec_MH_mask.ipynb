{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-head attention transformer\n",
    "### Encoder and Decoder\n",
    "### (With masking)\n",
    "\n",
    "Pytorch's implementation (in built)\n",
    "\n",
    "NOTE :- A new exmple must be used for testing the masked attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source embeddings :- \n",
      "\n",
      "tensor([[[-1.1258, -1.1524, -0.2506, -0.4339,  0.8487,  0.6920],\n",
      "         [ 0.1198,  1.2377,  1.1168, -0.2473, -1.3527, -1.6959]],\n",
      "\n",
      "        [[-0.3160, -2.1152,  0.3223, -1.2633,  0.3500,  0.3081],\n",
      "         [-1.1258, -1.1524, -0.2506, -0.4339,  0.8487,  0.6920]],\n",
      "\n",
      "        [[ 0.1198,  1.2377,  1.1168, -0.2473, -1.3527, -1.6959],\n",
      "         [ 0.1198,  1.2377,  1.1168, -0.2473, -1.3527, -1.6959]],\n",
      "\n",
      "        [[ 0.5667,  0.7935,  0.5988, -1.5551, -0.3414,  1.8530],\n",
      "         [ 0.9463, -0.8437, -0.6136,  0.0316, -0.4927,  0.2484]]],\n",
      "       grad_fn=<EmbeddingBackward0>)\n",
      "\n",
      "Target embeddings :- \n",
      "\n",
      "tensor([[[-8.3832e-01,  8.9182e-04, -7.5043e-01,  1.8541e-01,  6.2114e-01,\n",
      "           6.3818e-01],\n",
      "         [-3.7015e-01, -1.2103e+00,  1.1404e+00, -8.9882e-02,  7.2980e-01,\n",
      "          -1.8453e+00]],\n",
      "\n",
      "        [[-2.0252e-02, -4.3717e-01,  1.6459e+00, -1.3602e+00,  3.4457e-01,\n",
      "           5.1987e-01],\n",
      "         [-3.6562e-01, -1.3024e+00,  9.9403e-02,  4.4182e-01,  2.4693e-01,\n",
      "           7.6887e-02]],\n",
      "\n",
      "        [[ 3.3801e-01,  4.5440e-01, -8.0249e-01, -1.2952e+00, -7.5018e-01,\n",
      "          -1.3120e+00],\n",
      "         [-2.4600e-01,  2.3025e+00, -1.8817e+00, -4.9727e-02,  1.9415e+00,\n",
      "           7.9150e-01]],\n",
      "\n",
      "        [[-2.5020e-02,  1.3694e+00,  2.6570e+00,  9.8512e-01,  3.7718e-01,\n",
      "           1.1012e+00],\n",
      "         [-1.1256e+00, -3.1700e-01, -1.0925e+00, -8.5194e-02, -9.3348e-02,\n",
      "           6.8705e-01]],\n",
      "\n",
      "        [[-2.1883e-01, -2.4351e+00, -7.2915e-02, -3.3987e-02,  7.9689e-01,\n",
      "          -1.8484e-01],\n",
      "         [-8.3832e-01,  8.9182e-04, -7.5043e-01,  1.8541e-01,  6.2114e-01,\n",
      "           6.3818e-01]]], grad_fn=<EmbeddingBackward0>)\n",
      "\n",
      "Positional encoded source embeddings:\n",
      "tensor([[[-1.1258, -0.1524, -0.2506,  0.5661,  0.8487,  1.6920],\n",
      "         [ 0.9613,  1.7780,  1.1632,  0.7516, -1.3505, -0.6959]],\n",
      "\n",
      "        [[-0.3160, -1.1152,  0.3223, -0.2633,  0.3500,  1.3081],\n",
      "         [-0.2844, -0.6121, -0.2042,  0.5650,  0.8509,  1.6920]],\n",
      "\n",
      "        [[ 0.1198,  2.2377,  1.1168,  0.7527, -1.3527, -0.6959],\n",
      "         [ 0.9613,  1.7780,  1.1632,  0.7516, -1.3505, -0.6959]],\n",
      "\n",
      "        [[ 0.5667,  1.7935,  0.5988, -0.5551, -0.3414,  2.8530],\n",
      "         [ 1.7878, -0.3034, -0.5672,  1.0305, -0.4905,  1.2484]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "\n",
      "Positional encoded target embeddings:\n",
      "tensor([[[-0.8383,  1.0009, -0.7504,  1.1854,  0.6211,  1.6382],\n",
      "         [ 0.4713, -0.6700,  1.1867,  0.9090,  0.7320, -0.8453]],\n",
      "\n",
      "        [[-0.0203,  0.5628,  1.6459, -0.3602,  0.3446,  1.5199],\n",
      "         [ 0.4759, -0.7621,  0.1458,  1.4407,  0.2491,  1.0769]],\n",
      "\n",
      "        [[ 0.3380,  1.4544, -0.8025, -0.2952, -0.7502, -0.3120],\n",
      "         [ 0.5955,  2.8428, -1.8353,  0.9492,  1.9436,  1.7915]],\n",
      "\n",
      "        [[-0.0250,  2.3694,  2.6570,  1.9851,  0.3772,  2.1012],\n",
      "         [-0.2841,  0.2233, -1.0461,  0.9137, -0.0912,  1.6870]],\n",
      "\n",
      "        [[-0.2188, -1.4351, -0.0729,  0.9660,  0.7969,  0.8152],\n",
      "         [ 0.0032,  0.5412, -0.7040,  1.1843,  0.6233,  1.6382]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "\n",
      "query =  tensor([[[-1.1258, -0.1524, -0.2506,  0.5661,  0.8487,  1.6920],\n",
      "         [ 0.9613,  1.7780,  1.1632,  0.7516, -1.3505, -0.6959]],\n",
      "\n",
      "        [[-0.3160, -1.1152,  0.3223, -0.2633,  0.3500,  1.3081],\n",
      "         [-0.2844, -0.6121, -0.2042,  0.5650,  0.8509,  1.6920]],\n",
      "\n",
      "        [[ 0.1198,  2.2377,  1.1168,  0.7527, -1.3527, -0.6959],\n",
      "         [ 0.9613,  1.7780,  1.1632,  0.7516, -1.3505, -0.6959]],\n",
      "\n",
      "        [[ 0.5667,  1.7935,  0.5988, -0.5551, -0.3414,  2.8530],\n",
      "         [ 1.7878, -0.3034, -0.5672,  1.0305, -0.4905,  1.2484]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "query shape =  torch.Size([4, 2, 6])\n",
      "\n",
      " _in_projection_packed \n",
      "\n",
      "Q :-  tensor([[[-0.1929,  1.2673,  1.2277, -0.8873,  0.1377, -1.1826],\n",
      "         [ 0.7191, -0.1656, -0.8970,  0.1979,  0.1143,  1.1358]],\n",
      "\n",
      "        [[-0.1973,  0.2204,  0.7046, -1.0105,  0.2393, -0.3701],\n",
      "         [ 0.0721,  0.9433,  0.8903, -0.9088, -0.1743, -0.7039]],\n",
      "\n",
      "        [[ 0.4541,  0.1583, -0.5595,  0.2194,  0.4263,  0.6572],\n",
      "         [ 0.7191, -0.1656, -0.8970,  0.1979,  0.1143,  1.1358]],\n",
      "\n",
      "        [[ 1.4683,  1.0174,  0.7612, -0.8920, -0.3286, -1.3039],\n",
      "         [ 1.1037,  0.6604, -0.7935, -0.6033, -0.9457,  0.4983]]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "\n",
      "K :-  tensor([[[ 0.7118, -0.1985,  0.4590,  0.4204, -0.8352, -1.2161],\n",
      "         [-0.4659, -0.2948, -0.0826, -0.4436,  0.7290,  1.1934]],\n",
      "\n",
      "        [[ 0.4985, -0.2911, -0.1758,  0.6379, -0.1607, -0.2691],\n",
      "         [ 0.7961, -0.1309,  0.1925,  0.6170, -0.5085, -0.8176]],\n",
      "\n",
      "        [[-0.5501, -0.3624,  0.1840, -0.6402,  0.4023,  0.7949],\n",
      "         [-0.4659, -0.2948, -0.0826, -0.4436,  0.7290,  1.1934]],\n",
      "\n",
      "        [[-0.0511, -1.8880, -0.6759, -0.1434,  0.6434,  0.6594],\n",
      "         [ 1.2620, -0.4012, -0.1311,  0.1593,  0.3674,  0.4732]]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "\n",
      "V :-  tensor([[[ 0.4744, -0.6517,  0.3037,  0.1926, -0.8864, -0.1177],\n",
      "         [-1.0499,  1.1658, -0.2202, -0.4648,  1.0806, -0.0142]],\n",
      "\n",
      "        [[ 0.4421, -0.3142,  0.0510, -0.2190, -0.2891,  0.2229],\n",
      "         [ 0.4526, -0.5796, -0.1562, -0.2852, -0.7636, -0.1177]],\n",
      "\n",
      "        [[-1.0281,  1.0936,  0.2398,  0.0131,  0.9578, -0.0142],\n",
      "         [-1.0499,  1.1658, -0.2202, -0.4648,  1.0806, -0.0142]],\n",
      "\n",
      "        [[ 0.1670,  0.9115,  0.7823, -0.1252, -0.7950, -0.6469],\n",
      "         [-0.1894, -0.1627, -1.2114, -1.1383, -0.2602,  0.0351]]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "\n",
      "tgt_len =  4\n",
      "bsz =  2\n",
      "num_heads =  2\n",
      "head_dim =  3\n",
      "\n",
      "tensor([[[-0.1929,  1.2673,  1.2277, -0.8873,  0.1377, -1.1826],\n",
      "         [ 0.7191, -0.1656, -0.8970,  0.1979,  0.1143,  1.1358]],\n",
      "\n",
      "        [[-0.1973,  0.2204,  0.7046, -1.0105,  0.2393, -0.3701],\n",
      "         [ 0.0721,  0.9433,  0.8903, -0.9088, -0.1743, -0.7039]],\n",
      "\n",
      "        [[ 0.4541,  0.1583, -0.5595,  0.2194,  0.4263,  0.6572],\n",
      "         [ 0.7191, -0.1656, -0.8970,  0.1979,  0.1143,  1.1358]],\n",
      "\n",
      "        [[ 1.4683,  1.0174,  0.7612, -0.8920, -0.3286, -1.3039],\n",
      "         [ 1.1037,  0.6604, -0.7935, -0.6033, -0.9457,  0.4983]]],\n",
      "       grad_fn=<SelectBackward0>) torch.Size([4, 2, 6])\n",
      "Q reshaped =  tensor([[[-0.1929,  1.2673,  1.2277],\n",
      "         [-0.1973,  0.2204,  0.7046],\n",
      "         [ 0.4541,  0.1583, -0.5595],\n",
      "         [ 1.4683,  1.0174,  0.7612]],\n",
      "\n",
      "        [[-0.8873,  0.1377, -1.1826],\n",
      "         [-1.0105,  0.2393, -0.3701],\n",
      "         [ 0.2194,  0.4263,  0.6572],\n",
      "         [-0.8920, -0.3286, -1.3039]],\n",
      "\n",
      "        [[ 0.7191, -0.1656, -0.8970],\n",
      "         [ 0.0721,  0.9433,  0.8903],\n",
      "         [ 0.7191, -0.1656, -0.8970],\n",
      "         [ 1.1037,  0.6604, -0.7935]],\n",
      "\n",
      "        [[ 0.1979,  0.1143,  1.1358],\n",
      "         [-0.9088, -0.1743, -0.7039],\n",
      "         [ 0.1979,  0.1143,  1.1358],\n",
      "         [-0.6033, -0.9457,  0.4983]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "K reshaped =  tensor([[[ 0.7118, -0.1985,  0.4590],\n",
      "         [ 0.4985, -0.2911, -0.1758],\n",
      "         [-0.5501, -0.3624,  0.1840],\n",
      "         [-0.0511, -1.8880, -0.6759]],\n",
      "\n",
      "        [[ 0.4204, -0.8352, -1.2161],\n",
      "         [ 0.6379, -0.1607, -0.2691],\n",
      "         [-0.6402,  0.4023,  0.7949],\n",
      "         [-0.1434,  0.6434,  0.6594]],\n",
      "\n",
      "        [[-0.4659, -0.2948, -0.0826],\n",
      "         [ 0.7961, -0.1309,  0.1925],\n",
      "         [-0.4659, -0.2948, -0.0826],\n",
      "         [ 1.2620, -0.4012, -0.1311]],\n",
      "\n",
      "        [[-0.4436,  0.7290,  1.1934],\n",
      "         [ 0.6170, -0.5085, -0.8176],\n",
      "         [-0.4436,  0.7290,  1.1934],\n",
      "         [ 0.1593,  0.3674,  0.4732]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "V reshaped =  tensor([[[ 0.4744, -0.6517,  0.3037],\n",
      "         [ 0.4421, -0.3142,  0.0510],\n",
      "         [-1.0281,  1.0936,  0.2398],\n",
      "         [ 0.1670,  0.9115,  0.7823]],\n",
      "\n",
      "        [[ 0.1926, -0.8864, -0.1177],\n",
      "         [-0.2190, -0.2891,  0.2229],\n",
      "         [ 0.0131,  0.9578, -0.0142],\n",
      "         [-0.1252, -0.7950, -0.6469]],\n",
      "\n",
      "        [[-1.0499,  1.1658, -0.2202],\n",
      "         [ 0.4526, -0.5796, -0.1562],\n",
      "         [-1.0499,  1.1658, -0.2202],\n",
      "         [-0.1894, -0.1627, -1.2114]],\n",
      "\n",
      "        [[-0.4648,  1.0806, -0.0142],\n",
      "         [-0.2852, -0.7636, -0.1177],\n",
      "         [-0.4648,  1.0806, -0.0142],\n",
      "         [-1.1383, -0.2602,  0.0351]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "torch.Size([2, 2, 4, 3]) torch.Size([2, 2, 4, 3]) torch.Size([2, 2, 4, 3])\n",
      "\n",
      "scaled_dot_product_attention\n",
      "attn_output = \n",
      "tensor([[-0.0370,  0.0788,  0.2496,  0.0161, -0.3762, -0.1187],\n",
      "        [-0.3324,  0.2068, -0.5817, -0.5977,  0.6070, -0.0131],\n",
      "        [-0.0378,  0.2132,  0.3046, -0.0239, -0.1488, -0.1605],\n",
      "        [-0.3907,  0.3247, -0.4276, -0.5581,  0.1729, -0.0370],\n",
      "        [ 0.0907,  0.1918,  0.3403, -0.0620, -0.1402, -0.1912],\n",
      "        [-0.3324,  0.2068, -0.5817, -0.5977,  0.6070, -0.0131],\n",
      "        [ 0.2382, -0.2159,  0.2473,  0.0421, -0.4688, -0.1010],\n",
      "        [-0.2425,  0.0902, -0.6163, -0.5781,  0.3846, -0.0252]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "\n",
      "x + self_atten(x) = \n",
      "tensor([[[-1.2611, -0.3947,  0.0760,  0.7907,  0.5982,  1.8951],\n",
      "         [ 1.0436,  1.8441,  0.4541,  0.1472, -1.1841, -1.6121]],\n",
      "\n",
      "        [[-0.6328, -1.3257,  0.4833, -0.0856,  0.1302,  1.2661],\n",
      "         [-0.2418, -0.8332, -0.6045,  0.1475,  0.7715,  1.0000]],\n",
      "\n",
      "        [[-0.1865,  1.9630,  1.2256,  1.0227, -1.5779, -0.7386],\n",
      "         [ 1.0436,  1.8441,  0.4541,  0.1472, -1.1841, -1.6121]],\n",
      "\n",
      "        [[ 0.6685,  1.5604,  0.9773, -0.1630, -0.5240,  3.3362],\n",
      "         [ 2.0348, -0.3258, -1.1741,  0.5612, -0.3756,  0.5597]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "\n",
      "norm1(x + self_atten(x)) = \n",
      "tensor([[[-1.5658e+00, -6.8785e-01, -2.1085e-01,  5.1347e-01,  3.1836e-01,\n",
      "           1.6327e+00],\n",
      "         [ 7.7351e-01,  1.4406e+00,  2.8224e-01,  2.6458e-02, -1.0831e+00,\n",
      "          -1.4397e+00]],\n",
      "\n",
      "        [[-7.4007e-01, -1.5873e+00,  6.2437e-01, -7.1148e-02,  1.9273e-01,\n",
      "           1.5814e+00],\n",
      "         [-4.1795e-01, -1.2954e+00, -9.5617e-01,  1.5965e-01,  1.0855e+00,\n",
      "           1.4244e+00]],\n",
      "\n",
      "        [[-3.8507e-01,  1.3715e+00,  7.6886e-01,  6.0305e-01, -1.5221e+00,\n",
      "          -8.3626e-01],\n",
      "         [ 7.7351e-01,  1.4406e+00,  2.8224e-01,  2.6458e-02, -1.0831e+00,\n",
      "          -1.4397e+00]],\n",
      "\n",
      "        [[-2.4361e-01,  4.6319e-01,  1.1178e-03, -9.0254e-01, -1.1886e+00,\n",
      "           1.8705e+00],\n",
      "         [ 1.8033e+00, -5.3380e-01, -1.3736e+00,  3.4436e-01, -5.8310e-01,\n",
      "           3.4285e-01]]], grad_fn=<NativeLayerNormBackward0>)\n",
      "\n",
      "feed_fwd_op =  tensor([[[-0.0149, -0.2301,  0.0090,  0.3208, -0.4565, -0.3145],\n",
      "         [-0.0718, -0.1622,  0.1729,  0.3502, -0.2727,  0.0289]],\n",
      "\n",
      "        [[-0.0369, -0.3024,  0.0466,  0.3035, -0.4301, -0.2498],\n",
      "         [-0.0881, -0.2616,  0.0914,  0.4350, -0.4415, -0.2755]],\n",
      "\n",
      "        [[ 0.0079, -0.1752,  0.0490,  0.2593, -0.3628, -0.0216],\n",
      "         [-0.0718, -0.1622,  0.1729,  0.3502, -0.2727,  0.0289]],\n",
      "\n",
      "        [[-0.0344, -0.1814,  0.1314,  0.2699, -0.3881, -0.1070],\n",
      "         [-0.1121, -0.2091,  0.2908,  0.4365, -0.3428, -0.0882]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "\n",
      "norm2(norm1(x + self_atten(x)) + feed_fwd_op) = \n",
      "tensor([[[-1.4971, -0.8204, -0.0893,  0.9685, -0.0243,  1.4625],\n",
      "         [ 0.6772,  1.2398,  0.4367,  0.3601, -1.3301, -1.3838]],\n",
      "\n",
      "        [[-0.6427, -1.7172,  0.7557,  0.3321, -0.1216,  1.3936],\n",
      "         [-0.4358, -1.5367, -0.8115,  0.7172,  0.7689,  1.2979]],\n",
      "\n",
      "        [[-0.3059,  1.1240,  0.7802,  0.8206, -1.6761, -0.7427],\n",
      "         [ 0.6772,  1.2398,  0.4367,  0.3601, -1.3301, -1.3838]],\n",
      "\n",
      "        [[-0.2236,  0.3292,  0.1818, -0.5737, -1.5059,  1.7921],\n",
      "         [ 1.6791, -0.7317, -1.0684,  0.7775, -0.9129,  0.2563]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "\n",
      "##################################\n",
      "\n",
      "################### Transformer Decoder start #####################\n",
      "\n",
      "query =  tensor([[[-0.8383,  1.0009, -0.7504,  1.1854,  0.6211,  1.6382],\n",
      "         [ 0.4713, -0.6700,  1.1867,  0.9090,  0.7320, -0.8453]],\n",
      "\n",
      "        [[-0.0203,  0.5628,  1.6459, -0.3602,  0.3446,  1.5199],\n",
      "         [ 0.4759, -0.7621,  0.1458,  1.4407,  0.2491,  1.0769]],\n",
      "\n",
      "        [[ 0.3380,  1.4544, -0.8025, -0.2952, -0.7502, -0.3120],\n",
      "         [ 0.5955,  2.8428, -1.8353,  0.9492,  1.9436,  1.7915]],\n",
      "\n",
      "        [[-0.0250,  2.3694,  2.6570,  1.9851,  0.3772,  2.1012],\n",
      "         [-0.2841,  0.2233, -1.0461,  0.9137, -0.0912,  1.6870]],\n",
      "\n",
      "        [[-0.2188, -1.4351, -0.0729,  0.9660,  0.7969,  0.8152],\n",
      "         [ 0.0032,  0.5412, -0.7040,  1.1843,  0.6233,  1.6382]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "query shape =  torch.Size([5, 2, 6])\n",
      "\n",
      " _in_projection_packed \n",
      "\n",
      "Q :-  tensor([[[ 0.7030, -0.8076,  0.3689, -1.1457,  1.3671, -1.2293],\n",
      "         [-0.5888,  0.3034, -0.0136,  1.1584, -0.1576,  0.6310]],\n",
      "\n",
      "        [[-0.6962, -0.8609, -0.2664,  0.3132,  0.1273, -0.3309],\n",
      "         [ 0.3244, -0.7439, -0.1468,  0.4388,  0.9813,  0.3833]],\n",
      "\n",
      "        [[-0.3426,  0.1588,  0.7452, -1.0411, -0.0952, -0.4006],\n",
      "         [-0.2403,  0.2895, -0.0071, -2.4030,  2.8845, -2.7083]],\n",
      "\n",
      "        [[-1.3988, -1.9553,  1.4398, -0.0275,  0.6348, -0.6489],\n",
      "         [ 0.8990, -0.9110,  0.0977, -0.8612,  1.2382, -0.5424]],\n",
      "\n",
      "        [[ 0.8151, -0.3381, -0.6950,  0.5771,  0.8648,  0.1867],\n",
      "         [ 0.4716, -0.6661,  0.0042, -0.7776,  1.5754, -0.8309]]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "\n",
      "K :-  tensor([[[-0.2597,  0.2710,  1.8737,  0.8988, -0.2196,  0.6986],\n",
      "         [-0.2938, -0.0835, -0.0128, -0.3557,  0.1166, -0.9753]],\n",
      "\n",
      "        [[-0.4699, -0.5533,  0.2744, -0.5533, -1.0508,  0.2207],\n",
      "         [-0.6888,  0.3321,  0.9391,  0.5024, -0.2295, -0.1317]],\n",
      "\n",
      "        [[-0.3356, -0.5741,  0.3522,  0.6865, -0.5667,  0.1726],\n",
      "         [-0.8714, -1.0114,  3.8872,  2.2398, -0.3988,  1.3277]],\n",
      "\n",
      "        [[-1.9068, -1.0680,  2.1575,  0.2491, -2.5790, -0.5539],\n",
      "         [-0.3441,  0.4285,  1.3090,  0.9403, -0.2172,  0.7252]],\n",
      "\n",
      "        [[ 0.1249,  0.7970,  0.4589,  0.0342,  0.6598,  0.0955],\n",
      "         [-0.5588,  0.1010,  1.8689,  1.0367, -0.2734,  0.5928]]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "\n",
      "V :-  tensor([[[ 0.6449, -0.2764, -0.1715, -0.6778, -0.7072,  0.5175],\n",
      "         [-0.0945, -0.6817, -0.3549,  0.0200,  0.4624,  0.9861]],\n",
      "\n",
      "        [[ 0.0028,  0.2022, -0.9116,  0.4309,  0.0052,  0.2838],\n",
      "         [-0.1005, -0.6565, -0.4895, -0.9714, -0.5258,  0.4239]],\n",
      "\n",
      "        [[-0.5572,  0.0586,  0.5353,  0.2075, -0.9310, -0.7120],\n",
      "         [ 0.4689, -0.5180,  0.4793, -0.9286, -1.9786,  0.3029]],\n",
      "\n",
      "        [[-0.4723, -0.9974, -1.5139,  0.2720, -1.1805,  1.4021],\n",
      "         [ 0.2226, -0.1604, -0.0936, -0.9533, -0.9000, -0.1379]],\n",
      "\n",
      "        [[ 0.6013, -0.3083, -0.3781, -0.8708,  0.3526,  0.6405],\n",
      "         [ 0.2900, -0.4193, -0.1846, -0.9342, -0.9466,  0.3053]]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "\n",
      "tgt_len =  5\n",
      "bsz =  2\n",
      "num_heads =  2\n",
      "head_dim =  3\n",
      "\n",
      "tensor([[[ 0.7030, -0.8076,  0.3689, -1.1457,  1.3671, -1.2293],\n",
      "         [-0.5888,  0.3034, -0.0136,  1.1584, -0.1576,  0.6310]],\n",
      "\n",
      "        [[-0.6962, -0.8609, -0.2664,  0.3132,  0.1273, -0.3309],\n",
      "         [ 0.3244, -0.7439, -0.1468,  0.4388,  0.9813,  0.3833]],\n",
      "\n",
      "        [[-0.3426,  0.1588,  0.7452, -1.0411, -0.0952, -0.4006],\n",
      "         [-0.2403,  0.2895, -0.0071, -2.4030,  2.8845, -2.7083]],\n",
      "\n",
      "        [[-1.3988, -1.9553,  1.4398, -0.0275,  0.6348, -0.6489],\n",
      "         [ 0.8990, -0.9110,  0.0977, -0.8612,  1.2382, -0.5424]],\n",
      "\n",
      "        [[ 0.8151, -0.3381, -0.6950,  0.5771,  0.8648,  0.1867],\n",
      "         [ 0.4716, -0.6661,  0.0042, -0.7776,  1.5754, -0.8309]]],\n",
      "       grad_fn=<SelectBackward0>) torch.Size([5, 2, 6])\n",
      "Q reshaped =  tensor([[[ 0.7030, -0.8076,  0.3689],\n",
      "         [-0.6962, -0.8609, -0.2664],\n",
      "         [-0.3426,  0.1588,  0.7452],\n",
      "         [-1.3988, -1.9553,  1.4398],\n",
      "         [ 0.8151, -0.3381, -0.6950]],\n",
      "\n",
      "        [[-1.1457,  1.3671, -1.2293],\n",
      "         [ 0.3132,  0.1273, -0.3309],\n",
      "         [-1.0411, -0.0952, -0.4006],\n",
      "         [-0.0275,  0.6348, -0.6489],\n",
      "         [ 0.5771,  0.8648,  0.1867]],\n",
      "\n",
      "        [[-0.5888,  0.3034, -0.0136],\n",
      "         [ 0.3244, -0.7439, -0.1468],\n",
      "         [-0.2403,  0.2895, -0.0071],\n",
      "         [ 0.8990, -0.9110,  0.0977],\n",
      "         [ 0.4716, -0.6661,  0.0042]],\n",
      "\n",
      "        [[ 1.1584, -0.1576,  0.6310],\n",
      "         [ 0.4388,  0.9813,  0.3833],\n",
      "         [-2.4030,  2.8845, -2.7083],\n",
      "         [-0.8612,  1.2382, -0.5424],\n",
      "         [-0.7776,  1.5754, -0.8309]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "K reshaped =  tensor([[[-0.2597,  0.2710,  1.8737],\n",
      "         [-0.4699, -0.5533,  0.2744],\n",
      "         [-0.3356, -0.5741,  0.3522],\n",
      "         [-1.9068, -1.0680,  2.1575],\n",
      "         [ 0.1249,  0.7970,  0.4589]],\n",
      "\n",
      "        [[ 0.8988, -0.2196,  0.6986],\n",
      "         [-0.5533, -1.0508,  0.2207],\n",
      "         [ 0.6865, -0.5667,  0.1726],\n",
      "         [ 0.2491, -2.5790, -0.5539],\n",
      "         [ 0.0342,  0.6598,  0.0955]],\n",
      "\n",
      "        [[-0.2938, -0.0835, -0.0128],\n",
      "         [-0.6888,  0.3321,  0.9391],\n",
      "         [-0.8714, -1.0114,  3.8872],\n",
      "         [-0.3441,  0.4285,  1.3090],\n",
      "         [-0.5588,  0.1010,  1.8689]],\n",
      "\n",
      "        [[-0.3557,  0.1166, -0.9753],\n",
      "         [ 0.5024, -0.2295, -0.1317],\n",
      "         [ 2.2398, -0.3988,  1.3277],\n",
      "         [ 0.9403, -0.2172,  0.7252],\n",
      "         [ 1.0367, -0.2734,  0.5928]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "V reshaped =  tensor([[[ 0.6449, -0.2764, -0.1715],\n",
      "         [ 0.0028,  0.2022, -0.9116],\n",
      "         [-0.5572,  0.0586,  0.5353],\n",
      "         [-0.4723, -0.9974, -1.5139],\n",
      "         [ 0.6013, -0.3083, -0.3781]],\n",
      "\n",
      "        [[-0.6778, -0.7072,  0.5175],\n",
      "         [ 0.4309,  0.0052,  0.2838],\n",
      "         [ 0.2075, -0.9310, -0.7120],\n",
      "         [ 0.2720, -1.1805,  1.4021],\n",
      "         [-0.8708,  0.3526,  0.6405]],\n",
      "\n",
      "        [[-0.0945, -0.6817, -0.3549],\n",
      "         [-0.1005, -0.6565, -0.4895],\n",
      "         [ 0.4689, -0.5180,  0.4793],\n",
      "         [ 0.2226, -0.1604, -0.0936],\n",
      "         [ 0.2900, -0.4193, -0.1846]],\n",
      "\n",
      "        [[ 0.0200,  0.4624,  0.9861],\n",
      "         [-0.9714, -0.5258,  0.4239],\n",
      "         [-0.9286, -1.9786,  0.3029],\n",
      "         [-0.9533, -0.9000, -0.1379],\n",
      "         [-0.9342, -0.9466,  0.3053]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "torch.Size([2, 2, 5, 3]) torch.Size([2, 2, 5, 3]) torch.Size([2, 2, 5, 3])\n",
      "\n",
      "scaled_dot_product_attention\n",
      "attn_output = \n",
      "tensor([[-1.0603e-03, -2.6510e-01, -4.8671e-01, -4.0940e-01, -6.3514e-02,\n",
      "          4.3662e-01],\n",
      "        [ 1.5099e-01, -4.8659e-01, -1.4348e-01, -9.0200e-01, -1.3823e+00,\n",
      "          2.6461e-01],\n",
      "        [-1.7522e-01, -3.7755e-01, -7.1475e-01, -1.5703e-01, -5.0569e-01,\n",
      "          4.1455e-01],\n",
      "        [ 1.5939e-01, -5.0225e-01, -1.0879e-01, -8.2183e-01, -1.0061e+00,\n",
      "          3.1526e-01],\n",
      "        [ 9.9488e-03, -4.3564e-01, -6.8138e-01, -2.1017e-02, -4.2565e-01,\n",
      "          5.1238e-01],\n",
      "        [ 1.4844e-01, -4.8432e-01, -1.4779e-01, -3.5000e-02,  4.0183e-01,\n",
      "          9.5043e-01],\n",
      "        [-4.0825e-01, -8.9156e-01, -1.3681e+00, -2.6017e-01, -3.4895e-01,\n",
      "          3.8516e-01],\n",
      "        [ 1.9993e-01, -4.9459e-01, -4.2963e-02, -4.5648e-01, -1.8603e-01,\n",
      "          6.2159e-01],\n",
      "        [ 5.0006e-02, -1.1453e-01, -3.2041e-01, -3.6344e-01, -3.6871e-01,\n",
      "          3.2093e-01],\n",
      "        [ 1.7931e-01, -4.9525e-01, -8.0396e-02, -3.9936e-01, -9.7611e-02,\n",
      "          6.6941e-01]], grad_fn=<ViewBackward0>)\n",
      "\n",
      "x =  tensor([[[-0.8383,  1.0009, -0.7504,  1.1854,  0.6211,  1.6382],\n",
      "         [ 0.4713, -0.6700,  1.1867,  0.9090,  0.7320, -0.8453]],\n",
      "\n",
      "        [[-0.0203,  0.5628,  1.6459, -0.3602,  0.3446,  1.5199],\n",
      "         [ 0.4759, -0.7621,  0.1458,  1.4407,  0.2491,  1.0769]],\n",
      "\n",
      "        [[ 0.3380,  1.4544, -0.8025, -0.2952, -0.7502, -0.3120],\n",
      "         [ 0.5955,  2.8428, -1.8353,  0.9492,  1.9436,  1.7915]],\n",
      "\n",
      "        [[-0.0250,  2.3694,  2.6570,  1.9851,  0.3772,  2.1012],\n",
      "         [-0.2841,  0.2233, -1.0461,  0.9137, -0.0912,  1.6870]],\n",
      "\n",
      "        [[-0.2188, -1.4351, -0.0729,  0.9660,  0.7969,  0.8152],\n",
      "         [ 0.0032,  0.5412, -0.7040,  1.1843,  0.6233,  1.6382]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "\n",
      "sa_op =  tensor([[[ 0.2572,  0.0169, -0.2472,  0.6397,  0.3312,  0.2136],\n",
      "         [ 0.3283,  0.7148, -0.9729,  0.7399,  0.6407, -0.2077]],\n",
      "\n",
      "        [[ 0.2581, -0.0156, -0.2736,  0.9474,  0.3807,  0.1914],\n",
      "         [ 0.3769,  0.5179, -0.8612,  0.6676,  0.5279, -0.1483]],\n",
      "\n",
      "        [[ 0.4009, -0.0706, -0.2101,  1.0242,  0.2440,  0.1878],\n",
      "         [ 0.6117, -0.4366, -0.4381,  0.7823,  0.0391, -0.0629]],\n",
      "\n",
      "        [[ 0.5340, -0.3637, -0.1184,  1.4839,  0.4896,  0.7024],\n",
      "         [ 0.5136,  0.0206, -0.6401,  0.6380,  0.2400, -0.1107]],\n",
      "\n",
      "        [[ 0.1315,  0.2273, -0.2956,  0.4926,  0.3270,  0.0492],\n",
      "         [ 0.5247, -0.0511, -0.6052,  0.6707,  0.2199, -0.0904]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "\n",
      "temop =  tensor([[[-0.5811,  1.0178, -0.9976,  1.8251,  0.9523,  1.8518],\n",
      "         [ 0.7997,  0.0448,  0.2139,  1.6489,  1.3727, -1.0530]],\n",
      "\n",
      "        [[ 0.2379,  0.5472,  1.3722,  0.5873,  0.7252,  1.7113],\n",
      "         [ 0.8527, -0.2442, -0.7154,  2.1083,  0.7769,  0.9286]],\n",
      "\n",
      "        [[ 0.7389,  1.3838, -1.0125,  0.7290, -0.5062, -0.1242],\n",
      "         [ 1.2072,  2.4062, -2.2734,  1.7314,  1.9827,  1.7286]],\n",
      "\n",
      "        [[ 0.5089,  2.0057,  2.5386,  3.4691,  0.8668,  2.8037],\n",
      "         [ 0.2295,  0.2439, -1.6862,  1.5517,  0.1488,  1.5763]],\n",
      "\n",
      "        [[-0.0874, -1.2077, -0.3685,  1.4586,  1.1238,  0.8644],\n",
      "         [ 0.5278,  0.4901, -1.3093,  1.8550,  0.8432,  1.5477]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "\n",
      "Decoder norm1(x + sa(x)) = \n",
      "tensor([[[-1.1433,  0.3085, -1.5215,  1.0415,  0.2490,  1.0658],\n",
      "         [ 0.3276, -0.5101, -0.3225,  1.2700,  0.9634, -1.7284]],\n",
      "\n",
      "        [[-1.2253, -0.6194,  0.9963, -0.5410, -0.2708,  1.6602],\n",
      "         [ 0.2594, -0.9520, -1.4724,  1.6461,  0.1757,  0.3432]],\n",
      "\n",
      "        [[ 0.6545,  1.4398, -1.4784,  0.6425, -0.8618, -0.3966],\n",
      "         [ 0.0491,  0.8160, -2.1772,  0.3844,  0.5451,  0.3826]],\n",
      "\n",
      "        [[-1.4528, -0.0252,  0.4831,  1.3705, -1.1115,  0.7359],\n",
      "         [-0.1049, -0.0916, -1.8585,  1.1055, -0.1787,  1.1281]],\n",
      "\n",
      "        [[-0.4127, -1.6150, -0.7144,  1.2464,  0.8871,  0.6087],\n",
      "         [-0.1294, -0.1666, -1.9404,  1.1789,  0.1815,  0.8760]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "\n",
      "Memory = \n",
      "tensor([[[-1.4971, -0.8204, -0.0893,  0.9685, -0.0243,  1.4625],\n",
      "         [ 0.6772,  1.2398,  0.4367,  0.3601, -1.3301, -1.3838]],\n",
      "\n",
      "        [[-0.6427, -1.7172,  0.7557,  0.3321, -0.1216,  1.3936],\n",
      "         [-0.4358, -1.5367, -0.8115,  0.7172,  0.7689,  1.2979]],\n",
      "\n",
      "        [[-0.3059,  1.1240,  0.7802,  0.8206, -1.6761, -0.7427],\n",
      "         [ 0.6772,  1.2398,  0.4367,  0.3601, -1.3301, -1.3838]],\n",
      "\n",
      "        [[-0.2236,  0.3292,  0.1818, -0.5737, -1.5059,  1.7921],\n",
      "         [ 1.6791, -0.7317, -1.0684,  0.7775, -0.9129,  0.2563]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "\n",
      "query =  tensor([[[-1.1433,  0.3085, -1.5215,  1.0415,  0.2490,  1.0658],\n",
      "         [ 0.3276, -0.5101, -0.3225,  1.2700,  0.9634, -1.7284]],\n",
      "\n",
      "        [[-1.2253, -0.6194,  0.9963, -0.5410, -0.2708,  1.6602],\n",
      "         [ 0.2594, -0.9520, -1.4724,  1.6461,  0.1757,  0.3432]],\n",
      "\n",
      "        [[ 0.6545,  1.4398, -1.4784,  0.6425, -0.8618, -0.3966],\n",
      "         [ 0.0491,  0.8160, -2.1772,  0.3844,  0.5451,  0.3826]],\n",
      "\n",
      "        [[-1.4528, -0.0252,  0.4831,  1.3705, -1.1115,  0.7359],\n",
      "         [-0.1049, -0.0916, -1.8585,  1.1055, -0.1787,  1.1281]],\n",
      "\n",
      "        [[-0.4127, -1.6150, -0.7144,  1.2464,  0.8871,  0.6087],\n",
      "         [-0.1294, -0.1666, -1.9404,  1.1789,  0.1815,  0.8760]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "query shape =  torch.Size([5, 2, 6])\n",
      "\n",
      " _in_projection_packed \n",
      "\n",
      "Q :-  tensor([[[ 0.8053, -0.0713,  0.7679, -0.1601,  0.3679,  0.6959],\n",
      "         [-0.0825, -0.5636, -0.3997, -0.5890, -0.5484, -0.5096]],\n",
      "\n",
      "        [[-0.8762,  0.1282,  0.2041,  0.1825,  0.3493, -0.3748],\n",
      "         [ 0.6957, -0.7197,  0.2492, -1.0535, -0.1210,  0.3782]],\n",
      "\n",
      "        [[ 1.8555,  0.2675,  0.3083,  0.3822,  0.4362,  1.0671],\n",
      "         [ 1.1379,  0.0230,  0.7246, -0.2853, -0.0195,  1.2119]],\n",
      "\n",
      "        [[ 0.4550,  0.0176,  0.1286,  0.4914,  0.8859, -0.3512],\n",
      "         [ 1.1513, -0.2869,  0.6664, -0.5751,  0.2492,  0.9650]],\n",
      "\n",
      "        [[-0.4132, -0.8115,  0.1700, -1.1962, -0.3612, -0.2075],\n",
      "         [ 1.0152, -0.3636,  0.6390, -0.6918,  0.0941,  0.8875]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "\n",
      "K :-  tensor([[[-0.7485,  1.1345,  0.6806,  0.0087,  1.0130, -1.4948],\n",
      "         [-0.2423, -0.7448,  0.3559, -1.0010, -0.1218, -0.0587]],\n",
      "\n",
      "        [[-1.0826,  1.7761,  0.2926,  1.0281,  0.5696, -0.4293],\n",
      "         [-0.6347,  0.5470,  0.0896,  0.6914,  0.8278, -0.6580]],\n",
      "\n",
      "        [[-0.6303,  0.0104,  0.8447, -1.2027,  0.3345, -0.9261],\n",
      "         [-0.2423, -0.7448,  0.3559, -1.0010, -0.1218, -0.0587]],\n",
      "\n",
      "        [[-0.3431,  1.4132,  0.9496,  0.2795,  0.1991, -0.9360],\n",
      "         [-1.1947, -0.3792,  0.4218,  0.3515,  0.6886, -0.3362]]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "\n",
      "V :-  tensor([[[-0.0250, -0.5084, -0.5107,  0.7614,  0.9535, -1.5061],\n",
      "         [-0.4400,  0.3982,  0.3126,  0.0151, -0.0878,  0.9698]],\n",
      "\n",
      "        [[ 0.0814, -0.2902, -0.0315, -0.1786,  1.3551, -1.4120],\n",
      "         [ 0.8036, -0.8642, -0.4237,  0.2013,  0.2095, -1.3266]],\n",
      "\n",
      "        [[-0.7897,  0.3449,  0.1140,  0.4739,  0.6440,  0.2836],\n",
      "         [-0.4400,  0.3982,  0.3126,  0.0151, -0.0878,  0.9698]],\n",
      "\n",
      "        [[-0.1839,  1.2599, -0.4281,  0.7636,  1.1720,  0.0077],\n",
      "         [ 1.2200, -0.2009, -0.1498, -0.0384, -0.1307,  0.0661]]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "\n",
      "tgt_len =  5\n",
      "bsz =  2\n",
      "num_heads =  2\n",
      "head_dim =  3\n",
      "\n",
      "tensor([[[ 0.8053, -0.0713,  0.7679, -0.1601,  0.3679,  0.6959],\n",
      "         [-0.0825, -0.5636, -0.3997, -0.5890, -0.5484, -0.5096]],\n",
      "\n",
      "        [[-0.8762,  0.1282,  0.2041,  0.1825,  0.3493, -0.3748],\n",
      "         [ 0.6957, -0.7197,  0.2492, -1.0535, -0.1210,  0.3782]],\n",
      "\n",
      "        [[ 1.8555,  0.2675,  0.3083,  0.3822,  0.4362,  1.0671],\n",
      "         [ 1.1379,  0.0230,  0.7246, -0.2853, -0.0195,  1.2119]],\n",
      "\n",
      "        [[ 0.4550,  0.0176,  0.1286,  0.4914,  0.8859, -0.3512],\n",
      "         [ 1.1513, -0.2869,  0.6664, -0.5751,  0.2492,  0.9650]],\n",
      "\n",
      "        [[-0.4132, -0.8115,  0.1700, -1.1962, -0.3612, -0.2075],\n",
      "         [ 1.0152, -0.3636,  0.6390, -0.6918,  0.0941,  0.8875]]],\n",
      "       grad_fn=<ViewBackward0>) torch.Size([5, 2, 6])\n",
      "Q reshaped =  tensor([[[ 0.8053, -0.0713,  0.7679],\n",
      "         [-0.8762,  0.1282,  0.2041],\n",
      "         [ 1.8555,  0.2675,  0.3083],\n",
      "         [ 0.4550,  0.0176,  0.1286],\n",
      "         [-0.4132, -0.8115,  0.1700]],\n",
      "\n",
      "        [[-0.1601,  0.3679,  0.6959],\n",
      "         [ 0.1825,  0.3493, -0.3748],\n",
      "         [ 0.3822,  0.4362,  1.0671],\n",
      "         [ 0.4914,  0.8859, -0.3512],\n",
      "         [-1.1962, -0.3612, -0.2075]],\n",
      "\n",
      "        [[-0.0825, -0.5636, -0.3997],\n",
      "         [ 0.6957, -0.7197,  0.2492],\n",
      "         [ 1.1379,  0.0230,  0.7246],\n",
      "         [ 1.1513, -0.2869,  0.6664],\n",
      "         [ 1.0152, -0.3636,  0.6390]],\n",
      "\n",
      "        [[-0.5890, -0.5484, -0.5096],\n",
      "         [-1.0535, -0.1210,  0.3782],\n",
      "         [-0.2853, -0.0195,  1.2119],\n",
      "         [-0.5751,  0.2492,  0.9650],\n",
      "         [-0.6918,  0.0941,  0.8875]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "K reshaped =  tensor([[[-0.7485,  1.1345,  0.6806],\n",
      "         [-1.0826,  1.7761,  0.2926],\n",
      "         [-0.6303,  0.0104,  0.8447],\n",
      "         [-0.3431,  1.4132,  0.9496]],\n",
      "\n",
      "        [[ 0.0087,  1.0130, -1.4948],\n",
      "         [ 1.0281,  0.5696, -0.4293],\n",
      "         [-1.2027,  0.3345, -0.9261],\n",
      "         [ 0.2795,  0.1991, -0.9360]],\n",
      "\n",
      "        [[-0.2423, -0.7448,  0.3559],\n",
      "         [-0.6347,  0.5470,  0.0896],\n",
      "         [-0.2423, -0.7448,  0.3559],\n",
      "         [-1.1947, -0.3792,  0.4218]],\n",
      "\n",
      "        [[-1.0010, -0.1218, -0.0587],\n",
      "         [ 0.6914,  0.8278, -0.6580],\n",
      "         [-1.0010, -0.1218, -0.0587],\n",
      "         [ 0.3515,  0.6886, -0.3362]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "V reshaped =  tensor([[[-0.0250, -0.5084, -0.5107],\n",
      "         [ 0.0814, -0.2902, -0.0315],\n",
      "         [-0.7897,  0.3449,  0.1140],\n",
      "         [-0.1839,  1.2599, -0.4281]],\n",
      "\n",
      "        [[ 0.7614,  0.9535, -1.5061],\n",
      "         [-0.1786,  1.3551, -1.4120],\n",
      "         [ 0.4739,  0.6440,  0.2836],\n",
      "         [ 0.7636,  1.1720,  0.0077]],\n",
      "\n",
      "        [[-0.4400,  0.3982,  0.3126],\n",
      "         [ 0.8036, -0.8642, -0.4237],\n",
      "         [-0.4400,  0.3982,  0.3126],\n",
      "         [ 1.2200, -0.2009, -0.1498]],\n",
      "\n",
      "        [[ 0.0151, -0.0878,  0.9698],\n",
      "         [ 0.2013,  0.2095, -1.3266],\n",
      "         [ 0.0151, -0.0878,  0.9698],\n",
      "         [-0.0384, -0.1307,  0.0661]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "torch.Size([2, 2, 5, 3]) torch.Size([2, 2, 4, 3]) torch.Size([2, 2, 4, 3])\n",
      "\n",
      "scaled_dot_product_attention\n",
      "attn_output = \n",
      "tensor([[-2.7258e-01,  3.2869e-01, -2.2960e-01,  4.1981e-01,  1.0329e+00,\n",
      "         -6.5288e-01],\n",
      "        [ 2.2287e-01, -6.7315e-04,  5.1480e-02,  3.6304e-02, -4.6525e-02,\n",
      "          4.3470e-01],\n",
      "        [-1.9962e-01,  1.3583e-01, -2.0740e-01,  4.6903e-01,  1.0405e+00,\n",
      "         -7.5109e-01],\n",
      "        [ 7.4227e-02,  8.5335e-02,  1.0839e-01,  2.7680e-02, -6.1556e-02,\n",
      "          5.8731e-01],\n",
      "        [-2.3968e-01,  3.9867e-01, -2.6064e-01,  3.2891e-01,  1.1141e+00,\n",
      "         -8.0934e-01],\n",
      "        [ 1.1036e-01,  2.3830e-02,  7.5102e-02,  3.3148e-02, -5.0182e-02,\n",
      "          4.2276e-01],\n",
      "        [-2.4017e-01,  2.5122e-01, -2.2340e-01,  4.4401e-01,  1.0709e+00,\n",
      "         -8.7940e-01],\n",
      "        [ 6.3985e-02,  6.7704e-02,  1.0103e-01,  3.2725e-02, -5.1211e-02,\n",
      "          4.4098e-01],\n",
      "        [-3.3631e-01,  2.0532e-01, -1.6291e-01,  5.2353e-01,  8.7643e-01,\n",
      "         -3.0087e-01],\n",
      "        [ 7.2396e-02,  6.8782e-02,  1.0065e-01,  3.0553e-02, -5.5471e-02,\n",
      "          4.9684e-01]], grad_fn=<ViewBackward0>)\n",
      "\n",
      "Decoder norm2(x + mha(x, mem)) = \n",
      "tensor([[[-1.4574,  0.1825, -1.1540,  0.7673,  0.3103,  1.3513],\n",
      "         [ 0.3962, -0.3919, -0.4246,  1.3567,  0.8027, -1.7390]],\n",
      "\n",
      "        [[-1.4126, -0.4391,  0.9497, -0.5774, -0.1213,  1.6006],\n",
      "         [ 0.4802, -0.9630, -1.4157,  1.6747,  0.0487,  0.1751]],\n",
      "\n",
      "        [[ 0.1056,  1.6139, -1.4920,  0.5495, -0.9149,  0.1379],\n",
      "         [ 0.1936,  0.8049, -2.1928,  0.4950,  0.4415,  0.2577]],\n",
      "\n",
      "        [[-1.7268, -0.0143,  0.6227,  0.9632, -0.8418,  0.9969],\n",
      "         [ 0.0874, -0.1348, -1.8618,  1.2062, -0.2832,  0.9862]],\n",
      "\n",
      "        [[-0.6382, -1.7092, -0.3937,  0.9758,  0.9360,  0.8294],\n",
      "         [ 0.0811, -0.2151, -1.9421,  1.2829,  0.0689,  0.7244]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "\n",
      "Decoder norm3(x + ff(x)) = \n",
      "tensor([[[-1.1004,  0.1155, -1.5022,  0.5930,  0.5146,  1.3794],\n",
      "         [ 0.5376, -0.1928, -0.8114,  1.2325,  0.8826, -1.6484]],\n",
      "\n",
      "        [[-1.1626, -0.5178,  0.4735, -0.7798,  0.1061,  1.8806],\n",
      "         [ 0.6701, -0.8974, -1.6259,  1.4162,  0.2418,  0.1952]],\n",
      "\n",
      "        [[ 0.2895,  1.4591, -1.7827,  0.4505, -0.6079,  0.1915],\n",
      "         [ 0.3270,  0.6511, -2.2177,  0.3642,  0.5701,  0.3053]],\n",
      "\n",
      "        [[-1.6698,  0.0060,  0.1573,  0.9773, -0.7582,  1.2873],\n",
      "         [ 0.3110, -0.2023, -2.0099,  0.9705, -0.0078,  0.9385]],\n",
      "\n",
      "        [[-0.3763, -1.6380, -0.7528,  0.7531,  1.0899,  0.9239],\n",
      "         [ 0.2881, -0.2559, -2.0520,  1.0337,  0.2858,  0.7004]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "\n",
      "tensor([[[-6.6667e-01,  3.2296e-01, -4.0249e-01, -3.8964e-01, -2.7701e-01,\n",
      "          -1.1482e-01,  3.5930e-01,  1.2512e+00,  2.0429e-01,  5.5085e-01],\n",
      "         [ 7.3171e-02, -3.3670e-01,  2.7040e-01, -2.0084e-01,  2.5609e-01,\n",
      "           6.8348e-02, -4.1680e-01, -5.0879e-01, -1.9309e-01,  9.6282e-01]],\n",
      "\n",
      "        [[-1.2892e+00,  1.1669e+00, -1.3184e+00, -3.9436e-01,  1.2273e-01,\n",
      "          -1.4213e-01, -4.2031e-02,  5.8542e-01,  2.4001e-01,  9.7504e-02],\n",
      "         [-3.1304e-01,  9.1864e-02, -1.9785e-01, -6.2571e-01, -1.6641e-01,\n",
      "          -2.3585e-01, -1.9337e-01,  3.2745e-01,  3.4554e-02,  1.0395e-01]],\n",
      "\n",
      "        [[ 6.3494e-01, -8.2564e-01,  6.4853e-01, -4.0874e-01, -1.2193e+00,\n",
      "           5.8825e-02,  8.0115e-01,  4.2676e-01,  1.4104e-01,  2.0981e-01],\n",
      "         [ 2.9973e-01, -7.1025e-01,  7.6282e-04,  4.7570e-03, -8.9211e-01,\n",
      "          -2.6281e-01,  5.6628e-01,  7.5355e-01,  3.6228e-01,  7.0990e-01]],\n",
      "\n",
      "        [[-1.0802e+00,  1.1114e+00, -7.8310e-02, -1.1748e+00,  2.2006e-01,\n",
      "           3.6831e-01, -6.7761e-03,  5.8125e-01, -3.0546e-01, -1.6947e-01],\n",
      "         [-1.7524e-01, -8.3427e-02, -1.6040e-01, -5.5476e-01, -6.5517e-01,\n",
      "          -2.5749e-01,  2.9586e-01,  8.1952e-01,  2.1126e-01,  5.2639e-02]],\n",
      "\n",
      "        [[-1.2419e+00,  9.3706e-01, -1.1620e+00, -3.8737e-01,  5.3889e-01,\n",
      "          -3.4226e-01, -5.2016e-01,  6.2436e-01,  1.4019e-01,  4.6169e-01],\n",
      "         [-1.7737e-01, -1.2942e-01, -1.6874e-01, -4.5114e-01, -5.3793e-01,\n",
      "          -2.6594e-01,  2.2634e-01,  7.9536e-01,  2.0525e-01,  2.5652e-01]]],\n",
      "       grad_fn=<ViewBackward0>) torch.Size([5, 2, 10])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "torch.manual_seed(0)\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=512, dropout=0):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.encoding = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1).float()\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(torch.log(torch.tensor(10000.0)) / d_model))\n",
    "        self.encoding[:, 0::2] = torch.sin(position * div_term)\n",
    "        self.encoding[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.encoding = self.encoding.unsqueeze(0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.encoding[:, :x.size(1)].detach()\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, src_vocab_size, tgt_vocab_size,max_seq_len, d_model=4, nhead=2, num_encoder_layers=1, num_decoder_layers=1):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.src_embedding = nn.Embedding(src_vocab_size, d_model)\n",
    "        self.tgt_embedding = nn.Embedding(tgt_vocab_size, d_model)\n",
    "        self.positional_encoding = PositionalEncoding(d_model, dropout=0, max_len=max_seq_len)\n",
    "        self.transformer = nn.Transformer(\n",
    "            d_model=d_model,\n",
    "            nhead=nhead,\n",
    "            num_encoder_layers=num_encoder_layers,\n",
    "            num_decoder_layers=num_decoder_layers,\n",
    "            dropout=0\n",
    "        )\n",
    "        self.fc = nn.Linear(d_model, tgt_vocab_size)\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "\n",
    "        print(\"Source embeddings :- \\n\")\n",
    "        print(self.src_embedding(src))\n",
    "        print()\n",
    "\n",
    "        print(\"Target embeddings :- \\n\")\n",
    "        print(self.tgt_embedding(tgt))\n",
    "        print()\n",
    "\n",
    "\n",
    "        src = self.src_embedding(src) + self.positional_encoding(src)\n",
    "        tgt = self.tgt_embedding(tgt) + self.positional_encoding(tgt)\n",
    "\n",
    "        print(\"Positional encoded source embeddings:\")\n",
    "        print(src)\n",
    "        print()\n",
    "\n",
    "        print(\"Positional encoded target embeddings:\")\n",
    "        print(tgt)\n",
    "        print()\n",
    "\n",
    "        output = self.transformer(src, tgt)\n",
    "        output = self.fc(output)\n",
    "        \n",
    "        return output\n",
    "\n",
    "    \n",
    "\n",
    "src_vocab_size = 10  # Source language vocabulary size\n",
    "tgt_vocab_size = 10  # Target language vocabulary size\n",
    "d_model = 6  # Dimension of the model\n",
    "num_heads = 2\n",
    "# max_seq_len = 4\n",
    "max_seq_len = 5\n",
    "\n",
    "model = TransformerModel(src_vocab_size, tgt_vocab_size, d_model=d_model, max_seq_len = max_seq_len, nhead=num_heads)\n",
    "\n",
    "# Source sentence in the source language\n",
    "# Source token indexes from src vocabulary\n",
    "# src_sentence = torch.tensor([[0], [1], [2], [3]]) \n",
    "\n",
    "\n",
    "\n",
    "# Target sentence in the target language (translation of the source sentence)\n",
    "# Target token indexes from tgt vocabulary\n",
    "# tgt_sentence = torch.tensor([[1], [0], [3], [3]])\n",
    "\n",
    "\n",
    "src_sentence = torch.tensor([[0, 2], [1, 0], [2, 2], [3, 5]])\n",
    "tgt_sentence = torch.tensor([[1, 7], [3, 4], [5, 2], [8, 0], [6, 1]])  # Target sequence\n",
    "max_seq_len = 5\n",
    "\n",
    "\n",
    "\n",
    "# Forward pass\n",
    "output = model(src_sentence, tgt_sentence)\n",
    "print(output, output.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions to get the manual calulations for each component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import functional as F\n",
    "\n",
    "# Using the state dictionary to get the intermediate outputs\n",
    "state_dict = model.state_dict()\n",
    "\n",
    "need_weights = False\n",
    "\n",
    "src_mask = None\n",
    "\n",
    "tgt_mask = None\n",
    "\n",
    "memory_mask = None\n",
    "\n",
    "embed_dim = 6\n",
    "\n",
    "num_heads = 2\n",
    "\n",
    "max_seq_len = 5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "def look_up_table(sentence, vocab_embeds, embedding):\n",
    "\n",
    "    for i in range(sentence.size(0)):\n",
    "        for j in range(sentence.size(1)):\n",
    "            # Get the index for the current word token index in the sequence\n",
    "            word_index = sentence[i, j].item()\n",
    "\n",
    "            if word_index < 0 or word_index >= vocab_embeds.size(0):\n",
    "                raise ValueError(f\"Invalid word index: {word_index}\")\n",
    "\n",
    "            # Lookup the corresponding embedding vector for the word\n",
    "            embedding[i, j, :] = vocab_embeds[word_index, :]\n",
    "\n",
    "            print(f\"Word index: {word_index}, Embedding: {vocab_embeds[word_index, :]}\")\n",
    "    print()\n",
    "    \n",
    "\n",
    "    return embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embeddings and Positional encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding_outputs(src_sentence,  tgt_sentence, max_seq_len, state_dict, d_model):\n",
    "\n",
    "    src_vocab_embeds = state_dict[\"src_embedding.weight\"]\n",
    "\n",
    "    src_embedding = torch.zeros(src_sentence.size(0), src_sentence.size(1), d_model)\n",
    "    print(\"Source sentence embedding\")\n",
    "    src_embedding =  look_up_table(src_sentence, src_vocab_embeds, src_embedding)\n",
    "    print(src_embedding.shape)\n",
    "\n",
    "    tgt_vocab_embeds = state_dict[\"tgt_embedding.weight\"]\n",
    "\n",
    "    tgt_embedding = torch.zeros(tgt_sentence.size(0), tgt_sentence.size(1), d_model)\n",
    "\n",
    "\n",
    "    print(\"Target sentence embedding\")\n",
    "    tgt_embedding =  look_up_table(tgt_sentence, tgt_vocab_embeds, tgt_embedding)\n",
    "\n",
    "    pe = PositionalEncoding(d_model = d_model, dropout=0, max_len=max_seq_len)\n",
    "\n",
    "    print(\"PE of src :\")\n",
    "    print(pe(src_embedding))\n",
    "    print()\n",
    "    print(\"PE of tgt :\")\n",
    "    print(pe(tgt_embedding))\n",
    "    print()\n",
    "\n",
    "    pe_src_embeds = src_embedding + pe(src_embedding)\n",
    "\n",
    "    pe_tgt_embeds = tgt_embedding + pe(tgt_embedding)\n",
    "\n",
    "    print(\"PE source embeddings : \\n\")\n",
    "    print(pe_src_embeds)\n",
    "    print()\n",
    "\n",
    "    print(\"PE target embeddings : \\n\")\n",
    "    print(pe_tgt_embeds)\n",
    "    print()\n",
    "\n",
    "    return pe_src_embeds, pe_tgt_embeds\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source sentence embedding\n",
      "Word index: 0, Embedding: tensor([-1.1258, -1.1524, -0.2506, -0.4339,  0.8487,  0.6920])\n",
      "Word index: 2, Embedding: tensor([ 0.1198,  1.2377,  1.1168, -0.2473, -1.3527, -1.6959])\n",
      "Word index: 1, Embedding: tensor([-0.3160, -2.1152,  0.3223, -1.2633,  0.3500,  0.3081])\n",
      "Word index: 0, Embedding: tensor([-1.1258, -1.1524, -0.2506, -0.4339,  0.8487,  0.6920])\n",
      "Word index: 2, Embedding: tensor([ 0.1198,  1.2377,  1.1168, -0.2473, -1.3527, -1.6959])\n",
      "Word index: 2, Embedding: tensor([ 0.1198,  1.2377,  1.1168, -0.2473, -1.3527, -1.6959])\n",
      "Word index: 3, Embedding: tensor([ 0.5667,  0.7935,  0.5988, -1.5551, -0.3414,  1.8530])\n",
      "Word index: 5, Embedding: tensor([ 0.9463, -0.8437, -0.6136,  0.0316, -0.4927,  0.2484])\n",
      "\n",
      "torch.Size([4, 2, 6])\n",
      "Target sentence embedding\n",
      "Word index: 1, Embedding: tensor([-0.8383,  0.0009, -0.7504,  0.1854,  0.6211,  0.6382])\n",
      "Word index: 7, Embedding: tensor([-0.3701, -1.2103,  1.1404, -0.0899,  0.7298, -1.8453])\n",
      "Word index: 3, Embedding: tensor([-0.0203, -0.4372,  1.6459, -1.3602,  0.3446,  0.5199])\n",
      "Word index: 4, Embedding: tensor([-0.3656, -1.3024,  0.0994,  0.4418,  0.2469,  0.0769])\n",
      "Word index: 5, Embedding: tensor([ 0.3380,  0.4544, -0.8025, -1.2952, -0.7502, -1.3120])\n",
      "Word index: 2, Embedding: tensor([-0.2460,  2.3025, -1.8817, -0.0497,  1.9415,  0.7915])\n",
      "Word index: 8, Embedding: tensor([-0.0250,  1.3694,  2.6570,  0.9851,  0.3772,  1.1012])\n",
      "Word index: 0, Embedding: tensor([-1.1256, -0.3170, -1.0925, -0.0852, -0.0933,  0.6871])\n",
      "Word index: 6, Embedding: tensor([-0.2188, -2.4351, -0.0729, -0.0340,  0.7969, -0.1848])\n",
      "Word index: 1, Embedding: tensor([-0.8383,  0.0009, -0.7504,  0.1854,  0.6211,  0.6382])\n",
      "\n",
      "PE of src :\n",
      "tensor([[[0.0000, 1.0000, 0.0000, 1.0000, 0.0000, 1.0000],\n",
      "         [0.8415, 0.5403, 0.0464, 0.9989, 0.0022, 1.0000]]])\n",
      "\n",
      "PE of tgt :\n",
      "tensor([[[0.0000, 1.0000, 0.0000, 1.0000, 0.0000, 1.0000],\n",
      "         [0.8415, 0.5403, 0.0464, 0.9989, 0.0022, 1.0000]]])\n",
      "\n",
      "PE source embeddings : \n",
      "\n",
      "tensor([[[-1.1258, -0.1524, -0.2506,  0.5661,  0.8487,  1.6920],\n",
      "         [ 0.9613,  1.7780,  1.1632,  0.7516, -1.3505, -0.6959]],\n",
      "\n",
      "        [[-0.3160, -1.1152,  0.3223, -0.2633,  0.3500,  1.3081],\n",
      "         [-0.2844, -0.6121, -0.2042,  0.5650,  0.8509,  1.6920]],\n",
      "\n",
      "        [[ 0.1198,  2.2377,  1.1168,  0.7527, -1.3527, -0.6959],\n",
      "         [ 0.9613,  1.7780,  1.1632,  0.7516, -1.3505, -0.6959]],\n",
      "\n",
      "        [[ 0.5667,  1.7935,  0.5988, -0.5551, -0.3414,  2.8530],\n",
      "         [ 1.7878, -0.3034, -0.5672,  1.0305, -0.4905,  1.2484]]])\n",
      "\n",
      "PE target embeddings : \n",
      "\n",
      "tensor([[[-0.8383,  1.0009, -0.7504,  1.1854,  0.6211,  1.6382],\n",
      "         [ 0.4713, -0.6700,  1.1867,  0.9090,  0.7320, -0.8453]],\n",
      "\n",
      "        [[-0.0203,  0.5628,  1.6459, -0.3602,  0.3446,  1.5199],\n",
      "         [ 0.4759, -0.7621,  0.1458,  1.4407,  0.2491,  1.0769]],\n",
      "\n",
      "        [[ 0.3380,  1.4544, -0.8025, -0.2952, -0.7502, -0.3120],\n",
      "         [ 0.5955,  2.8428, -1.8353,  0.9492,  1.9436,  1.7915]],\n",
      "\n",
      "        [[-0.0250,  2.3694,  2.6570,  1.9851,  0.3772,  2.1012],\n",
      "         [-0.2841,  0.2233, -1.0461,  0.9137, -0.0912,  1.6870]],\n",
      "\n",
      "        [[-0.2188, -1.4351, -0.0729,  0.9660,  0.7969,  0.8152],\n",
      "         [ 0.0032,  0.5412, -0.7040,  1.1843,  0.6233,  1.6382]]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pe_src_embeds, pe_tgt_embeds = get_embedding_outputs(src_sentence=src_sentence,  tgt_sentence=tgt_sentence, state_dict=state_dict, max_seq_len=max_seq_len, d_model = d_model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder function to display the intermediate outputs and get the final outputs from the encoder\n",
    "\n",
    "### Self attention "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "def atten_product_needs_wts_false(Q, V, K, bsz, head_dim, src_len, tgt_len, attn_mask):\n",
    "\n",
    "\n",
    "    # *** For multi-head attention ***\n",
    "    #  (bsz*num_heads, src_len , head_dim) -> (bsz, num_heads, tgt_len, head_dim)\n",
    "    Q1 = Q.view(bsz, num_heads, tgt_len, head_dim)\n",
    "    K1 = K.view(bsz, num_heads, src_len, head_dim)\n",
    "    V1 = V.view(bsz, num_heads, src_len, head_dim)\n",
    "\n",
    "    L, S = Q1.size(-2), K1.size(-2)\n",
    "\n",
    "    scale_factor = 1 / math.sqrt(Q1.size(-1)) \n",
    "    # scale_factor = 1\n",
    "    attn_bias = torch.zeros(L, S, dtype=Q1.dtype)\n",
    "\n",
    "    if attn_mask is not None:\n",
    "        if attn_mask.dtype == torch.bool:\n",
    "            attn_mask.masked_fill_(attn_mask.logical_not(), float(\"-inf\"))\n",
    "        else:\n",
    "            attn_bias += attn_mask\n",
    "\n",
    "    # (bsz, num_heads, tgt_len, head_dim) @ (bsz, num_heads, head_dim, tgt_len) -> (bsz, num_heads, tgt_len, tgt_len) \n",
    "    attn_weight = Q1 @ K1.transpose(-2, -1) * scale_factor\n",
    "    attn_weight += attn_bias\n",
    "\n",
    "    # (bsz, num_heads, tgt_len, tgt_len) \n",
    "    attn_weight = torch.softmax(attn_weight, dim=-1)\n",
    "\n",
    "    print(\"Attention weights = \")\n",
    "    print(attn_weight.shape, attn_weight)\n",
    "\n",
    "    sum_last_dim = attn_weight.sum(dim=-1)\n",
    "\n",
    "    tolerance = 1e-6  \n",
    "    assert torch.allclose(sum_last_dim, torch.ones_like(sum_last_dim), atol=tolerance), \"Attention weights sum is not approximately equal to 1\"\n",
    "\n",
    "    # (bsz, num_heads, tgt_len, tgt_len) @ (bsz, num_heads, tgt_len, head_dim) -> (bsz, num_heads, tgt_len, head_dim) \n",
    "    attn_output = attn_weight @ V1\n",
    "\n",
    "    # (bsz*tgt_len, embed_dim)\n",
    "    attn_output = attn_output.permute(2, 0, 1, 3).contiguous().view(bsz * tgt_len, embed_dim)\n",
    "\n",
    "    return attn_output\n",
    "\n",
    "\n",
    "\n",
    "def atten_product_needs_wts_true(Q, K, V, bsz, tgt_len, attn_mask):\n",
    "\n",
    "    # *** For multi-head attention ***\n",
    "    #  (bsz*num_heads, src_len , head_dim)\n",
    "    B, Nt, E = Q.shape\n",
    "\n",
    "    Q_scaled = Q / math.sqrt(E)\n",
    "\n",
    "    if attn_mask is not None:\n",
    "        temp_pdt_matrix = torch.baddbmm(attn_mask, Q_scaled, K.transpose(-2, -1))\n",
    "    else:\n",
    "        temp_pdt_matrix = torch.bmm(Q_scaled, K.transpose(-2, -1))\n",
    "\n",
    "    attn_wt_matrix = torch.nn.functional.softmax(temp_pdt_matrix, dim=-1)\n",
    "\n",
    "    attn_output = torch.bmm(attn_wt_matrix, V)\n",
    "\n",
    "    attn_output = attn_output.transpose(0, 1).contiguous().view(tgt_len * bsz, embed_dim)\n",
    "\n",
    "\n",
    "    sum_last_dim = attn_wt_matrix.sum(dim=-1)\n",
    "\n",
    "    tolerance = 1e-6  \n",
    "    assert torch.allclose(sum_last_dim, torch.ones_like(sum_last_dim), atol=tolerance), \"Attention weights sum is not approximately equal to 1\"\n",
    "\n",
    "\n",
    "    print(\"Encoder Attention output = \")\n",
    "    print(attn_output)\n",
    "    print()\n",
    "\n",
    "    return attn_output, attn_wt_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_qkv(query, key, value ,W, b):\n",
    "\n",
    "    # embed_dim\n",
    "    E = query.size(-1)\n",
    "\n",
    "    if key is value:\n",
    "        if query is key:\n",
    "            \n",
    "            # (src_len, bsz, embed_dim) @ (embed_dim*num_heads, embed_dim).T -> (src_len, bsz, embed_dim*num_heads)\n",
    "            tempop1 = query@W.T\n",
    "\n",
    "            # reshape to 3, E and not E, 3 is deliberate for better memory coalescing and keeping same order as chunk()\n",
    "            tempop1 = tempop1.unflatten(-1, (3, E)).unsqueeze(0).transpose(0, -2).squeeze(-2).contiguous()\n",
    "\n",
    "            # (src_len, bsz, embed_dim)\n",
    "            return tempop1[0], tempop1[1], tempop1[2]\n",
    "        \n",
    "\n",
    "        else:\n",
    "\n",
    "            # (embed_dim*1, embed_dim)\n",
    "            # (embed_dim*2, embed_dim)\n",
    "            W_q, W_kv = W.split([E, E * 2])\n",
    "\n",
    "            if b is None:\n",
    "                b_q = b_kv = None\n",
    "            else:\n",
    "                b_q, b_kv = b.split([E, E * 2])\n",
    "\n",
    "            # (src_len, bsz, embed_dim) @ (embed_dim*1, embed_dim).T -> (src_len, bsz, embed_dim)\n",
    "            q_matmul = query@W_q.T\n",
    "\n",
    "            # (src_len, bsz, embed_dim) @ (embed_dim*2, embed_dim).T -> (src_len, bsz, embed_dim*2)\n",
    "            kv_matmul = key@W_kv.T\n",
    "\n",
    "            kv_matmul = kv_matmul.unflatten(-1, (2, E)).unsqueeze(0).transpose(0, -2).squeeze(-2).contiguous()\n",
    "\n",
    "            # (src_len, bsz, embed_dim)\n",
    "            return q_matmul, kv_matmul[0], kv_matmul[1]\n",
    "\n",
    "    else:\n",
    "\n",
    "        W_q, W_k, W_v = W.chunk(3)\n",
    "        if b is None:\n",
    "            b_q = b_k = b_v = None\n",
    "        else:\n",
    "            b_q, b_k, b_v = b.chunk(3)\n",
    "\n",
    "\n",
    "        q_matmul = query@W_q.T\n",
    "        k_matmul = key@W_k.T\n",
    "        v_matmul = value@W_v.T\n",
    "\n",
    "        return q_matmul, k_matmul, v_matmul\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder_block_attn_output(x, state_dict, layer_num, embed_dim, num_heads, need_weights = False, src_mask = None):\n",
    "\n",
    "    # (src_len, bsz, embed_dim)\n",
    "    query_enc = key_enc = value_enc = x\n",
    "\n",
    "    tgt_len, bsz, embed_dim = x.shape\n",
    "\n",
    "    # print(x.shape)\n",
    "    \n",
    "    # (embed_dim*num_heads, embed_dim)\n",
    "    W_enc = state_dict[\"transformer.encoder.layers.{}.self_attn.in_proj_weight\".format(layer_num)]\n",
    "    b_enc = state_dict[\"transformer.encoder.layers.{}.self_attn.in_proj_bias\".format(layer_num)]\n",
    "\n",
    "\n",
    "    head_dim = embed_dim//num_heads\n",
    "    \n",
    "    # (src_len, bsz, embed_dim)\n",
    "    Q_enc,K_enc,V_enc = get_qkv(query_enc, key_enc, value_enc ,W_enc, b_enc)\n",
    "\n",
    "    # (1, src_len, bsz, embed_dim)\n",
    "    # Q_enc = Q_enc.unsqueeze(0)\n",
    "    # K_enc = K_enc.unsqueeze(0)\n",
    "    # V_enc = V_enc.unsqueeze(0)\n",
    "\n",
    "    # (1, src_len, bsz, embed_dim) -> ( bsz*num_heads, src_len , head_dim)\n",
    "    Q_enc = Q_enc.reshape(Q_enc.shape[0], bsz * num_heads, head_dim).transpose(0, 1)\n",
    "    K_enc = K_enc.reshape(K_enc.shape[0], bsz * num_heads, head_dim).transpose(0, 1)\n",
    "    V_enc = V_enc.reshape(V_enc.shape[0], bsz * num_heads, head_dim).transpose(0, 1)\n",
    "\n",
    "    print(\"Q_enc_{} = \".format(layer_num))\n",
    "    print(Q_enc)\n",
    "    print()\n",
    "\n",
    "    print(\"K_enc_{} = \".format(layer_num))\n",
    "    print(K_enc)\n",
    "    print()\n",
    "\n",
    "    print(\"V_enc_{} = \".format(layer_num))\n",
    "    print(V_enc)\n",
    "    print()\n",
    "\n",
    "\n",
    "    src_len = K_enc.size(1)\n",
    "\n",
    "    attn_mask = src_mask\n",
    "    if attn_mask is not None:\n",
    "\n",
    "        # Ensuring attn_mask's dim is 3\n",
    "        if attn_mask.dim() == 2:\n",
    "            correct_2d_size = (tgt_len, src_len)\n",
    "            if attn_mask.shape != correct_2d_size:\n",
    "                raise RuntimeError(f\"The shape of the 2D attn_mask is {attn_mask.shape}, but should be {correct_2d_size}.\")\n",
    "            attn_mask = attn_mask.unsqueeze(0)\n",
    "        elif attn_mask.dim() == 3:\n",
    "            correct_3d_size = (bsz * num_heads, tgt_len, src_len)\n",
    "            if attn_mask.shape != correct_3d_size:\n",
    "                raise RuntimeError(f\"The shape of the 3D attn_mask is {attn_mask.shape}, but should be {correct_3d_size}.\")\n",
    "        else:\n",
    "            raise RuntimeError(f\"attn_mask's dimension {attn_mask.dim()} is not supported\")\n",
    "\n",
    "    # attn_mask can be either (L,S) or (N*num_heads, L, S)\n",
    "    # if attn_mask's shape is (1, L, S) we need to unsqueeze to (1, 1, L, S)\n",
    "    # in order to match the input for SDPA of (N, num_heads, L, S)\n",
    "    if attn_mask is not None:\n",
    "        if attn_mask.size(0) == 1 and attn_mask.dim() == 3:\n",
    "            attn_mask = attn_mask.unsqueeze(0)\n",
    "        else:\n",
    "            attn_mask = attn_mask.view(bsz, num_heads, -1, src_len)\n",
    "\n",
    "\n",
    "    if need_weights is False:\n",
    "\n",
    "        attn_output = atten_product_needs_wts_false(Q_enc, V_enc, K_enc, bsz, head_dim, src_len, tgt_len, attn_mask)\n",
    "\n",
    "        return attn_output, src_len, head_dim, None\n",
    "    \n",
    "    else:\n",
    "\n",
    "        attn_enc_output,attn_wt_matrix_enc = atten_product_needs_wts_true(Q_enc, K_enc, V_enc, bsz, tgt_len, attn_mask)\n",
    "\n",
    "        return attn_enc_output, src_len, head_dim, attn_wt_matrix_enc\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q_enc_0 = \n",
      "tensor([[[-0.1929,  1.2673,  1.2277],\n",
      "         [-0.1973,  0.2204,  0.7046],\n",
      "         [ 0.4541,  0.1583, -0.5595],\n",
      "         [ 1.4683,  1.0174,  0.7612]],\n",
      "\n",
      "        [[-0.8873,  0.1377, -1.1826],\n",
      "         [-1.0105,  0.2393, -0.3701],\n",
      "         [ 0.2194,  0.4263,  0.6572],\n",
      "         [-0.8920, -0.3286, -1.3039]],\n",
      "\n",
      "        [[ 0.7191, -0.1656, -0.8970],\n",
      "         [ 0.0721,  0.9433,  0.8903],\n",
      "         [ 0.7191, -0.1656, -0.8970],\n",
      "         [ 1.1037,  0.6604, -0.7935]],\n",
      "\n",
      "        [[ 0.1979,  0.1143,  1.1358],\n",
      "         [-0.9088, -0.1743, -0.7039],\n",
      "         [ 0.1979,  0.1143,  1.1358],\n",
      "         [-0.6033, -0.9457,  0.4983]]])\n",
      "\n",
      "K_enc_0 = \n",
      "tensor([[[ 0.7118, -0.1985,  0.4590],\n",
      "         [ 0.4985, -0.2911, -0.1758],\n",
      "         [-0.5501, -0.3624,  0.1840],\n",
      "         [-0.0511, -1.8880, -0.6759]],\n",
      "\n",
      "        [[ 0.4204, -0.8352, -1.2161],\n",
      "         [ 0.6379, -0.1607, -0.2691],\n",
      "         [-0.6402,  0.4023,  0.7949],\n",
      "         [-0.1434,  0.6434,  0.6594]],\n",
      "\n",
      "        [[-0.4659, -0.2948, -0.0826],\n",
      "         [ 0.7961, -0.1309,  0.1925],\n",
      "         [-0.4659, -0.2948, -0.0826],\n",
      "         [ 1.2620, -0.4012, -0.1311]],\n",
      "\n",
      "        [[-0.4436,  0.7290,  1.1934],\n",
      "         [ 0.6170, -0.5085, -0.8176],\n",
      "         [-0.4436,  0.7290,  1.1934],\n",
      "         [ 0.1593,  0.3674,  0.4732]]])\n",
      "\n",
      "V_enc_0 = \n",
      "tensor([[[ 0.4744, -0.6517,  0.3037],\n",
      "         [ 0.4421, -0.3142,  0.0510],\n",
      "         [-1.0281,  1.0936,  0.2398],\n",
      "         [ 0.1670,  0.9115,  0.7823]],\n",
      "\n",
      "        [[ 0.1926, -0.8864, -0.1177],\n",
      "         [-0.2190, -0.2891,  0.2229],\n",
      "         [ 0.0131,  0.9578, -0.0142],\n",
      "         [-0.1252, -0.7950, -0.6469]],\n",
      "\n",
      "        [[-1.0499,  1.1658, -0.2202],\n",
      "         [ 0.4526, -0.5796, -0.1562],\n",
      "         [-1.0499,  1.1658, -0.2202],\n",
      "         [-0.1894, -0.1627, -1.2114]],\n",
      "\n",
      "        [[-0.4648,  1.0806, -0.0142],\n",
      "         [-0.2852, -0.7636, -0.1177],\n",
      "         [-0.4648,  1.0806, -0.0142],\n",
      "         [-1.1383, -0.2602,  0.0351]]])\n",
      "\n",
      "Attention weights = \n",
      "torch.Size([2, 2, 4, 4]) tensor([[[[0.3858, 0.2354, 0.3241, 0.0546],\n",
      "          [0.2987, 0.2336, 0.3020, 0.1656],\n",
      "          [0.2540, 0.2924, 0.1965, 0.2571],\n",
      "          [0.5020, 0.3002, 0.1386, 0.0592]],\n",
      "\n",
      "         [[0.4179, 0.2066, 0.2011, 0.1744],\n",
      "          [0.2291, 0.1809, 0.3284, 0.2616],\n",
      "          [0.1248, 0.2170, 0.3174, 0.3407],\n",
      "          [0.5176, 0.1996, 0.1555, 0.1274]]],\n",
      "\n",
      "\n",
      "        [[[0.1797, 0.2591, 0.1797, 0.3815],\n",
      "          [0.2317, 0.3075, 0.2317, 0.2291],\n",
      "          [0.1797, 0.2591, 0.1797, 0.3815],\n",
      "          [0.1419, 0.2975, 0.1419, 0.4188]],\n",
      "\n",
      "         [[0.3412, 0.0949, 0.3412, 0.2226],\n",
      "          [0.2231, 0.3279, 0.2231, 0.2259],\n",
      "          [0.3412, 0.0949, 0.3412, 0.2226],\n",
      "          [0.2806, 0.2137, 0.2806, 0.2252]]]])\n"
     ]
    }
   ],
   "source": [
    "# need_weights = True\n",
    "\n",
    "attn_enc_output, src_len, head_dim, attn_weights = encoder_block_attn_output(pe_src_embeds, state_dict, layer_num = 0, need_weights = need_weights, embed_dim=embed_dim, num_heads=num_heads)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Post self attention in encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder_block_post_attn_output(x, attn_enc_output, state_dict, layer_num, bsz, tgt_len):\n",
    "\n",
    "    # (bsz*src_len , embed_dim) @ (embed_dim , embed_dim).T -> (bsz*src_len , embed_dim)\n",
    "    op_enc_1 = torch.matmul(attn_enc_output, state_dict[\"transformer.encoder.layers.{}.self_attn.out_proj.weight\".format(layer_num)].t()) + state_dict[\"transformer.encoder.layers.{}.self_attn.out_proj.bias\".format(layer_num)]\n",
    "\n",
    "\n",
    "\n",
    "    # (bsz*src_len , embed_dim) -> (src_len, bsz, embed_dim)\n",
    "    attn_enc_output = op_enc_1.view(tgt_len, bsz, attn_enc_output.size(1))\n",
    "\n",
    "\n",
    "    # Here src is the original passed inputs to the 1st transformer encoder layer which \n",
    "    # are pe_src_embeds \n",
    "\n",
    "    # (src_len, bsz, embed_dim)\n",
    "    output_enc_1 = attn_enc_output + x\n",
    "\n",
    "    #  (src_len, bsz, embed_dim) @ (embed_dim) -> (src_len, bsz, embed_dim) \n",
    "    linear_result_enc_1 = output_enc_1*state_dict[\"transformer.encoder.layers.{}.norm1.weight\".format(layer_num)] + state_dict[\"transformer.encoder.layers.{}.norm1.bias\".format(layer_num)]\n",
    "\n",
    "    # Layer normalization from Torch's implementation\n",
    "    layernorm_enc_1 = torch.nn.LayerNorm(normalized_shape=linear_result_enc_1.shape[2:])\n",
    "    linear_op_enc_1 = layernorm_enc_1(linear_result_enc_1)\n",
    "\n",
    "\n",
    "    # Manual Layer Normalization\n",
    "    x = linear_result_enc_1\n",
    "\n",
    "    # Obtained layer norm weights and biases (learnable)\n",
    "    w = layernorm_enc_1.weight\n",
    "    b = layernorm_enc_1.bias\n",
    "\n",
    "    linear_result_enc_1f = w*x + b\n",
    "\n",
    "    epsilon = 1e-5  \n",
    "    mean = linear_result_enc_1f.mean(dim=-1, keepdim=True)\n",
    "    std = linear_result_enc_1f.std(dim=-1, unbiased=False, keepdim=True)\n",
    "    normalized_result_enc_1 = (linear_result_enc_1f - mean) / (std + epsilon) * w + b\n",
    "\n",
    "\n",
    "    op_enc_1 = torch.matmul(normalized_result_enc_1, state_dict[\"transformer.encoder.layers.{}.linear1.weight\".format(layer_num)].t()) + state_dict[\"transformer.encoder.layers.{}.linear1.bias\".format(layer_num)]\n",
    "    op_enc_1_relu = torch.nn.functional.relu(op_enc_1)\n",
    "    op_enc_2 = torch.matmul(op_enc_1_relu, state_dict[\"transformer.encoder.layers.{}.linear2.weight\".format(layer_num)].t()) + state_dict[\"transformer.encoder.layers.{}.linear2.bias\".format(layer_num)]\n",
    "\n",
    "\n",
    "    output_enc_2 = op_enc_2 + linear_op_enc_1\n",
    "    output_enc_2_norm = output_enc_2*state_dict[\"transformer.encoder.layers.{}.norm2.weight\".format(layer_num)] + state_dict[\"transformer.encoder.layers.{}.norm2.bias\".format(layer_num)]\n",
    "\n",
    "    # Layer normalization from Torch's implementation\n",
    "    layernorm_enc_final = torch.nn.LayerNorm(normalized_shape=output_enc_2_norm.shape[2:])\n",
    "    output_enc_final = layernorm_enc_final(output_enc_2_norm)\n",
    "\n",
    "\n",
    "    # Manual Layer Normalization \n",
    "    x = output_enc_2_norm\n",
    "\n",
    "    # Obtained layer norm weights and biases (learnable)\n",
    "    w = layernorm_enc_final.weight\n",
    "    b = layernorm_enc_final.bias\n",
    "\n",
    "    linear_result_enc_2 = w*x + b\n",
    "\n",
    "    epsilon = 1e-5  \n",
    "    mean = linear_result_enc_2.mean(dim=-1, keepdim=True)\n",
    "    std = linear_result_enc_2.std(dim=-1, unbiased=False, keepdim=True)\n",
    "    output_enc_final = (linear_result_enc_2 - mean) / (std + epsilon) * w + b\n",
    "\n",
    "    print(\"Final Encoder {} Output :\".format(layer_num))\n",
    "    print(\"norm2(norm1(x + self_atten(x)) + feed_fwd_op)\\n\")\n",
    "    print(output_enc_final)\n",
    "    print()\n",
    "\n",
    "    # (src_len, bsz, embed_dim) \n",
    "    return output_enc_final\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Encoder 0 Output :\n",
      "norm2(norm1(x + self_atten(x)) + feed_fwd_op)\n",
      "\n",
      "tensor([[[-1.4971, -0.8204, -0.0893,  0.9685, -0.0243,  1.4625],\n",
      "         [ 0.6772,  1.2398,  0.4367,  0.3601, -1.3300, -1.3837]],\n",
      "\n",
      "        [[-0.6427, -1.7172,  0.7557,  0.3321, -0.1216,  1.3936],\n",
      "         [-0.4358, -1.5367, -0.8115,  0.7172,  0.7689,  1.2979]],\n",
      "\n",
      "        [[-0.3059,  1.1240,  0.7801,  0.8206, -1.6761, -0.7427],\n",
      "         [ 0.6772,  1.2398,  0.4367,  0.3601, -1.3300, -1.3837]],\n",
      "\n",
      "        [[-0.2236,  0.3292,  0.1818, -0.5737, -1.5059,  1.7921],\n",
      "         [ 1.6791, -0.7317, -1.0684,  0.7775, -0.9129,  0.2563]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tgt_len, bsz, embed_dim = pe_src_embeds.shape\n",
    "output_enc_final = encoder_block_post_attn_output(pe_src_embeds, attn_enc_output, state_dict, layer_num = 0 , bsz = bsz, tgt_len = tgt_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder functions to display the intermediate outputs and get the final outputs from the decoder\n",
    "\n",
    "\n",
    "### Self attention outputs from a decoder block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoder_block_self_attn_output(x, state_dict, layer_num, tgt_mask = None,need_weights = False):\n",
    "\n",
    "    # (tgt_len, bsz, embed_dim)\n",
    "    query_dec = key_dec = value_dec = x\n",
    "\n",
    "    tgt_len, bsz, embed_dim = x.shape\n",
    " \n",
    "    # (embed_dim*num_heads, embed_dim)\n",
    "    W_dec = state_dict[\"transformer.decoder.layers.{}.self_attn.in_proj_weight\".format(layer_num)]\n",
    "    b_dec = state_dict[\"transformer.decoder.layers.{}.self_attn.in_proj_bias\".format(layer_num)]\n",
    "\n",
    "\n",
    "    head_dim = embed_dim//num_heads\n",
    "\n",
    "    # (tgt_len, bsz, embed_dim)\n",
    "    Q_dec,K_dec,V_dec = get_qkv(query_dec, key_dec, value_dec ,W_dec, b_dec)\n",
    "    \n",
    "    # Q_dec = Q_dec.unsqueeze(0)\n",
    "    # K_dec = K_dec.unsqueeze(0)\n",
    "    # V_dec = V_dec.unsqueeze(0)\n",
    "\n",
    "    # (1, tgt_len, bsz, embed_dim)\n",
    "    print(Q_dec.shape, K_dec.shape , V_dec.shape)\n",
    "    print(tgt_len, bsz * num_heads, head_dim)\n",
    "\n",
    "    # (1, tgt_len, bsz, embed_dim) -> ( bsz*num_heads, tgt_len , head_dim )\n",
    "    Q_dec = Q_dec.reshape(Q_dec.shape[0], bsz * num_heads, head_dim).transpose(0, 1)\n",
    "    K_dec = K_dec.reshape(K_dec.shape[0], bsz * num_heads, head_dim).transpose(0, 1)\n",
    "    V_dec = V_dec.reshape(V_dec.shape[0], bsz * num_heads, head_dim).transpose(0, 1)\n",
    "\n",
    "    print(\"Q_dec_{} = \".format(layer_num))\n",
    "    print(Q_dec)\n",
    "    print()\n",
    "\n",
    "    print(\"K_dec_{} = \".format(layer_num))\n",
    "    print(K_dec)\n",
    "    print()\n",
    "\n",
    "    print(\"V_dec_{} = \".format(layer_num))\n",
    "    print(V_dec)\n",
    "    print()\n",
    "\n",
    "    src_len = K_dec.size(1)\n",
    "\n",
    "\n",
    "    attn_mask = tgt_mask\n",
    "    if attn_mask is not None:\n",
    "\n",
    "        # Ensuring attn_mask's dim is 3\n",
    "        if attn_mask.dim() == 2:\n",
    "            correct_2d_size = (tgt_len, src_len)\n",
    "            if attn_mask.shape != correct_2d_size:\n",
    "                raise RuntimeError(f\"The shape of the 2D attn_mask is {attn_mask.shape}, but should be {correct_2d_size}.\")\n",
    "            attn_mask = attn_mask.unsqueeze(0)\n",
    "        elif attn_mask.dim() == 3:\n",
    "            correct_3d_size = (bsz * num_heads, tgt_len, src_len)\n",
    "            if attn_mask.shape != correct_3d_size:\n",
    "                raise RuntimeError(f\"The shape of the 3D attn_mask is {attn_mask.shape}, but should be {correct_3d_size}.\")\n",
    "        else:\n",
    "            raise RuntimeError(f\"attn_mask's dimension {attn_mask.dim()} is not supported\")\n",
    "\n",
    "    # attn_mask can be either (L,S) or (N*num_heads, L, S)\n",
    "    # if attn_mask's shape is (1, L, S) we need to unsqueeze to (1, 1, L, S)\n",
    "    # in order to match the input for SDPA of (N, num_heads, L, S)\n",
    "    if attn_mask is not None:\n",
    "        if attn_mask.size(0) == 1 and attn_mask.dim() == 3:\n",
    "            attn_mask = attn_mask.unsqueeze(0)\n",
    "        else:\n",
    "            attn_mask = attn_mask.view(bsz, num_heads, -1, src_len)\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    if need_weights is False:\n",
    "        attn_output = atten_product_needs_wts_false(Q_dec, V_dec, K_dec, bsz, head_dim, src_len, tgt_len, attn_mask)\n",
    "\n",
    "        print(\"Decoder Self Attention = \")\n",
    "        print(attn_output)\n",
    "        print()\n",
    "\n",
    "        op_dec_1 = torch.matmul(attn_output, state_dict[\"transformer.decoder.layers.{}.self_attn.out_proj.weight\".format(layer_num)].t()) + state_dict[\"transformer.decoder.layers.{}.self_attn.out_proj.bias\".format(layer_num)]\n",
    "        attn_dec_output = op_dec_1.view(tgt_len, bsz, attn_output.size(1))\n",
    "\n",
    "        return attn_dec_output, None\n",
    "    \n",
    "    else:\n",
    "\n",
    "        attn_dec_output,attn_wt_matrix_dec = atten_product_needs_wts_true(Q_dec, K_dec, V_dec, bsz, tgt_len, attn_mask)\n",
    "\n",
    "        print(\"Decoder Attention output = \")\n",
    "        print(attn_wt_matrix_dec)\n",
    "        print()\n",
    "\n",
    "        op_dec_1 = torch.matmul(attn_dec_output, state_dict[\"transformer.decoder.layers.{}.self_attn.out_proj.weight\".format(layer_num)].t()) + state_dict[\"transformer.decoder.layers.{}.self_attn.out_proj.bias\".format(layer_num)]\n",
    "        attn_dec_output = op_dec_1.view(tgt_len, bsz, attn_dec_output.size(1))\n",
    "    \n",
    "\n",
    "        return attn_dec_output, attn_wt_matrix_dec\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "need_weights "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 2, 6]) torch.Size([5, 2, 6]) torch.Size([5, 2, 6])\n",
      "5 4 3\n",
      "Q_dec_0 = \n",
      "tensor([[[ 0.7030, -0.8076,  0.3689],\n",
      "         [-0.6962, -0.8609, -0.2664],\n",
      "         [-0.3426,  0.1588,  0.7452],\n",
      "         [-1.3988, -1.9553,  1.4398],\n",
      "         [ 0.8151, -0.3381, -0.6950]],\n",
      "\n",
      "        [[-1.1457,  1.3671, -1.2293],\n",
      "         [ 0.3132,  0.1273, -0.3309],\n",
      "         [-1.0411, -0.0952, -0.4006],\n",
      "         [-0.0275,  0.6348, -0.6489],\n",
      "         [ 0.5771,  0.8648,  0.1867]],\n",
      "\n",
      "        [[-0.5888,  0.3034, -0.0136],\n",
      "         [ 0.3244, -0.7439, -0.1468],\n",
      "         [-0.2403,  0.2895, -0.0071],\n",
      "         [ 0.8990, -0.9110,  0.0977],\n",
      "         [ 0.4716, -0.6661,  0.0042]],\n",
      "\n",
      "        [[ 1.1584, -0.1576,  0.6310],\n",
      "         [ 0.4388,  0.9813,  0.3833],\n",
      "         [-2.4030,  2.8845, -2.7083],\n",
      "         [-0.8612,  1.2382, -0.5424],\n",
      "         [-0.7776,  1.5754, -0.8309]]])\n",
      "\n",
      "K_dec_0 = \n",
      "tensor([[[-0.2597,  0.2710,  1.8737],\n",
      "         [-0.4699, -0.5533,  0.2744],\n",
      "         [-0.3356, -0.5741,  0.3522],\n",
      "         [-1.9068, -1.0680,  2.1575],\n",
      "         [ 0.1249,  0.7970,  0.4589]],\n",
      "\n",
      "        [[ 0.8988, -0.2196,  0.6986],\n",
      "         [-0.5533, -1.0508,  0.2207],\n",
      "         [ 0.6865, -0.5667,  0.1726],\n",
      "         [ 0.2491, -2.5790, -0.5539],\n",
      "         [ 0.0342,  0.6598,  0.0955]],\n",
      "\n",
      "        [[-0.2938, -0.0835, -0.0128],\n",
      "         [-0.6888,  0.3321,  0.9391],\n",
      "         [-0.8714, -1.0114,  3.8872],\n",
      "         [-0.3441,  0.4285,  1.3090],\n",
      "         [-0.5588,  0.1010,  1.8689]],\n",
      "\n",
      "        [[-0.3557,  0.1166, -0.9753],\n",
      "         [ 0.5024, -0.2295, -0.1317],\n",
      "         [ 2.2398, -0.3988,  1.3277],\n",
      "         [ 0.9403, -0.2172,  0.7252],\n",
      "         [ 1.0367, -0.2734,  0.5928]]])\n",
      "\n",
      "V_dec_0 = \n",
      "tensor([[[ 0.6449, -0.2764, -0.1715],\n",
      "         [ 0.0028,  0.2022, -0.9116],\n",
      "         [-0.5572,  0.0586,  0.5353],\n",
      "         [-0.4723, -0.9974, -1.5139],\n",
      "         [ 0.6013, -0.3083, -0.3781]],\n",
      "\n",
      "        [[-0.6778, -0.7072,  0.5175],\n",
      "         [ 0.4309,  0.0052,  0.2838],\n",
      "         [ 0.2075, -0.9310, -0.7120],\n",
      "         [ 0.2720, -1.1805,  1.4021],\n",
      "         [-0.8708,  0.3526,  0.6405]],\n",
      "\n",
      "        [[-0.0945, -0.6817, -0.3549],\n",
      "         [-0.1005, -0.6565, -0.4895],\n",
      "         [ 0.4689, -0.5180,  0.4793],\n",
      "         [ 0.2226, -0.1604, -0.0936],\n",
      "         [ 0.2900, -0.4193, -0.1846]],\n",
      "\n",
      "        [[ 0.0200,  0.4624,  0.9861],\n",
      "         [-0.9714, -0.5258,  0.4239],\n",
      "         [-0.9286, -1.9786,  0.3029],\n",
      "         [-0.9533, -0.9000, -0.1379],\n",
      "         [-0.9342, -0.9466,  0.3053]]])\n",
      "\n",
      "Attention weights = \n",
      "torch.Size([2, 2, 5, 5]) tensor([[[[2.1312e-01, 2.0444e-01, 2.2163e-01, 2.1660e-01, 1.4422e-01],\n",
      "          [1.0515e-01, 2.2044e-01, 2.0852e-01, 3.7967e-01, 8.6230e-02],\n",
      "          [2.5725e-01, 1.2495e-01, 1.2558e-01, 3.5610e-01, 1.3612e-01],\n",
      "          [4.0966e-02, 3.2580e-02, 3.1924e-02, 8.8941e-01, 5.1157e-03],\n",
      "          [1.3231e-01, 2.6742e-01, 2.7723e-01, 7.0630e-02, 2.5241e-01]],\n",
      "\n",
      "         [[9.8077e-02, 1.8668e-01, 1.2465e-01, 5.6953e-02, 5.3364e-01],\n",
      "          [2.0818e-01, 1.6502e-01, 2.1594e-01, 1.9773e-01, 2.1313e-01],\n",
      "          [1.0874e-01, 3.0432e-01, 1.4221e-01, 2.4443e-01, 2.0029e-01],\n",
      "          [1.8475e-01, 1.6673e-01, 1.9878e-01, 1.2568e-01, 3.2406e-01],\n",
      "          [2.9126e-01, 1.1260e-01, 2.1561e-01, 6.3097e-02, 3.1743e-01]]],\n",
      "\n",
      "\n",
      "        [[[1.8386e-01, 2.2448e-01, 1.8445e-01, 2.0247e-01, 2.0475e-01],\n",
      "          [2.4168e-01, 1.7320e-01, 2.3218e-01, 1.7179e-01, 1.8115e-01],\n",
      "          [1.9255e-01, 2.1717e-01, 1.7579e-01, 2.1007e-01, 2.0442e-01],\n",
      "          [2.0507e-01, 1.4166e-01, 3.0847e-01, 1.6444e-01, 1.8036e-01],\n",
      "          [2.1354e-01, 1.6382e-01, 2.6318e-01, 1.7355e-01, 1.8591e-01]],\n",
      "\n",
      "         [[3.7789e-02, 9.4137e-02, 5.1997e-01, 1.7220e-01, 1.7591e-01],\n",
      "          [1.2627e-01, 1.5547e-01, 3.0299e-01, 2.1145e-01, 2.0383e-01],\n",
      "          [9.4414e-01, 4.3129e-02, 2.9815e-04, 6.2794e-03, 6.1543e-03],\n",
      "          [5.1063e-01, 1.9981e-01, 4.7252e-02, 1.2398e-01, 1.1833e-01],\n",
      "          [5.6971e-01, 1.8873e-01, 3.6825e-02, 1.0394e-01, 1.0079e-01]]]])\n",
      "Decoder Self Attention = \n",
      "tensor([[-1.0603e-03, -2.6510e-01, -4.8671e-01, -4.0940e-01, -6.3514e-02,\n",
      "          4.3662e-01],\n",
      "        [ 1.5099e-01, -4.8659e-01, -1.4348e-01, -9.0200e-01, -1.3823e+00,\n",
      "          2.6461e-01],\n",
      "        [-1.7522e-01, -3.7755e-01, -7.1475e-01, -1.5703e-01, -5.0569e-01,\n",
      "          4.1455e-01],\n",
      "        [ 1.5939e-01, -5.0225e-01, -1.0879e-01, -8.2183e-01, -1.0061e+00,\n",
      "          3.1526e-01],\n",
      "        [ 9.9488e-03, -4.3564e-01, -6.8138e-01, -2.1017e-02, -4.2565e-01,\n",
      "          5.1238e-01],\n",
      "        [ 1.4844e-01, -4.8432e-01, -1.4779e-01, -3.5000e-02,  4.0183e-01,\n",
      "          9.5043e-01],\n",
      "        [-4.0825e-01, -8.9156e-01, -1.3681e+00, -2.6017e-01, -3.4895e-01,\n",
      "          3.8516e-01],\n",
      "        [ 1.9993e-01, -4.9459e-01, -4.2963e-02, -4.5648e-01, -1.8603e-01,\n",
      "          6.2159e-01],\n",
      "        [ 5.0006e-02, -1.1453e-01, -3.2041e-01, -3.6344e-01, -3.6871e-01,\n",
      "          3.2093e-01],\n",
      "        [ 1.7931e-01, -4.9525e-01, -8.0396e-02, -3.9936e-01, -9.7611e-02,\n",
      "          6.6941e-01]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# need_weights = True\n",
    "\n",
    "self_attn_dec, dec_sa_wts = decoder_block_self_attn_output(pe_tgt_embeds, state_dict, layer_num = 0, need_weights = need_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.2572,  0.0169, -0.2472,  0.6397,  0.3312,  0.2136],\n",
       "         [ 0.3283,  0.7148, -0.9729,  0.7399,  0.6407, -0.2077]],\n",
       "\n",
       "        [[ 0.2581, -0.0156, -0.2736,  0.9474,  0.3807,  0.1914],\n",
       "         [ 0.3769,  0.5179, -0.8612,  0.6676,  0.5279, -0.1483]],\n",
       "\n",
       "        [[ 0.4009, -0.0706, -0.2101,  1.0242,  0.2440,  0.1878],\n",
       "         [ 0.6117, -0.4366, -0.4381,  0.7823,  0.0391, -0.0629]],\n",
       "\n",
       "        [[ 0.5340, -0.3637, -0.1184,  1.4839,  0.4896,  0.7024],\n",
       "         [ 0.5136,  0.0206, -0.6401,  0.6380,  0.2400, -0.1107]],\n",
       "\n",
       "        [[ 0.1315,  0.2273, -0.2956,  0.4926,  0.3270,  0.0492],\n",
       "         [ 0.5247, -0.0511, -0.6052,  0.6707,  0.2199, -0.0904]]])"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "self_attn_dec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dec_post_self_attn(self_attn_dec, x, state_dict, layer_num):\n",
    "\n",
    "    # (bsz*tgt_len , embed_dim) @ (embed_dim , embed_dim).T -> (bsz*tgt_len , embed_dim)\n",
    "    output_dec_1 = self_attn_dec + x\n",
    "\n",
    "    # (bsz*tgt_len , embed_dim) @ (embed_dim , embed_dim).T -> (bsz*tgt_len , embed_dim)\n",
    "    linear_result_dec_1 = output_dec_1*state_dict[\"transformer.decoder.layers.{}.norm1.weight\".format(layer_num)] + state_dict[\"transformer.decoder.layers.{}.norm1.bias\".format(layer_num)]\n",
    "\n",
    "    layernorm_dec_1 = torch.nn.LayerNorm(normalized_shape=linear_result_dec_1.shape[2:])\n",
    "    linear_op_dec_1 = layernorm_dec_1(linear_result_dec_1)\n",
    "\n",
    "    # From torch's implementation\n",
    "    # print(linear_op_dec_1)\n",
    "\n",
    "    x = linear_result_dec_1\n",
    "\n",
    "    # Obtained layer norm weights and biases (learnable)\n",
    "    w = layernorm_dec_1.weight\n",
    "    b = layernorm_dec_1.bias\n",
    "\n",
    "    linear_result_dec_1f = w*x + b\n",
    "\n",
    "    epsilon = 1e-5  \n",
    "    mean = linear_result_dec_1f.mean(dim=-1, keepdim=True)\n",
    "    std = linear_result_dec_1f.std(dim=-1, unbiased=False, keepdim=True)\n",
    "    normalized_result_dec_1 = (linear_result_dec_1f - mean) / (std + epsilon) * w + b\n",
    "\n",
    "\n",
    "    print(\"Decoder_{} norm1(x + sa(x))\".format(layer_num))\n",
    "    print(normalized_result_dec_1)\n",
    "    print()\n",
    "\n",
    "    # (tgt_len, bsz, embed_dim)\n",
    "    return normalized_result_dec_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoder_0 norm1(x + sa(x))\n",
      "tensor([[[-1.1433,  0.3085, -1.5215,  1.0415,  0.2490,  1.0658],\n",
      "         [ 0.3276, -0.5101, -0.3225,  1.2700,  0.9634, -1.7284]],\n",
      "\n",
      "        [[-1.2253, -0.6194,  0.9963, -0.5410, -0.2708,  1.6602],\n",
      "         [ 0.2594, -0.9520, -1.4724,  1.6461,  0.1757,  0.3432]],\n",
      "\n",
      "        [[ 0.6545,  1.4398, -1.4784,  0.6425, -0.8618, -0.3966],\n",
      "         [ 0.0491,  0.8160, -2.1772,  0.3844,  0.5451,  0.3826]],\n",
      "\n",
      "        [[-1.4528, -0.0252,  0.4831,  1.3705, -1.1115,  0.7359],\n",
      "         [-0.1049, -0.0916, -1.8584,  1.1055, -0.1787,  1.1281]],\n",
      "\n",
      "        [[-0.4127, -1.6150, -0.7144,  1.2463,  0.8871,  0.6087],\n",
      "         [-0.1294, -0.1666, -1.9404,  1.1789,  0.1815,  0.8760]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x_dec = dec_post_self_attn(self_attn_dec, pe_tgt_embeds, state_dict, layer_num = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross attention in decoder \n",
    "### query, key, value =  x, mem, mem \n",
    "\n",
    "### Cross attention between the encoder's final output and the decoder layer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.1433,  0.3085, -1.5215,  1.0415,  0.2490,  1.0658],\n",
       "         [ 0.3276, -0.5101, -0.3225,  1.2700,  0.9634, -1.7284]],\n",
       "\n",
       "        [[-1.2253, -0.6194,  0.9963, -0.5410, -0.2708,  1.6602],\n",
       "         [ 0.2594, -0.9520, -1.4724,  1.6461,  0.1757,  0.3432]],\n",
       "\n",
       "        [[ 0.6545,  1.4398, -1.4784,  0.6425, -0.8618, -0.3966],\n",
       "         [ 0.0491,  0.8160, -2.1772,  0.3844,  0.5451,  0.3826]],\n",
       "\n",
       "        [[-1.4528, -0.0252,  0.4831,  1.3705, -1.1115,  0.7359],\n",
       "         [-0.1049, -0.0916, -1.8584,  1.1055, -0.1787,  1.1281]],\n",
       "\n",
       "        [[-0.4127, -1.6150, -0.7144,  1.2463,  0.8871,  0.6087],\n",
       "         [-0.1294, -0.1666, -1.9404,  1.1789,  0.1815,  0.8760]]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "memory = output_enc_final\n",
    "x_dec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 2, 6])"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoder_block_cross_attn_output(x_dec, memory, state_dict, layer_num, tgt_len, src_len, head_dim, memory_mask = None,need_weights = False):\n",
    "\n",
    "    # (tgt_len, bsz, embed_dim)\n",
    "    query_dec_mha = x_dec\n",
    "\n",
    "    # (src_len, bsz, embed_dim)\n",
    "    key_dec_mha, value_dec_mha = memory, memory\n",
    "\n",
    "\n",
    "    tgt_len, bsz, embed_dim = query_dec_mha.shape\n",
    "\n",
    "\n",
    "    W_dec_mha = state_dict[\"transformer.decoder.layers.{}.multihead_attn.in_proj_weight\".format(layer_num)]\n",
    "    b_dec_mha = state_dict[\"transformer.decoder.layers.{}.multihead_attn.in_proj_bias\".format(layer_num)]\n",
    "\n",
    "\n",
    "    ########## CHANGE FROM SELF ATTENTION ############\n",
    "\n",
    "    Q_dec_mha,K_dec_mha,V_dec_mha = get_qkv(query_dec_mha, key_dec_mha, value_dec_mha ,W_dec_mha, b_dec_mha)\n",
    "\n",
    "\n",
    "    #################################################\n",
    "\n",
    "    # K_dec_mha = K_dec_mha.unsqueeze(0)\n",
    "    # V_dec_mha = V_dec_mha.unsqueeze(0)\n",
    "    # Q_dec_mha = Q_dec_mha.unsqueeze(0)\n",
    "\n",
    "\n",
    "    Q_dec_mha = Q_dec_mha.reshape(Q_dec_mha.shape[0], bsz * num_heads, head_dim).transpose(0, 1)\n",
    "    K_dec_mha = K_dec_mha.reshape(K_dec_mha.shape[0], bsz * num_heads, head_dim).transpose(0, 1)\n",
    "    V_dec_mha = V_dec_mha.reshape(V_dec_mha.shape[0], bsz * num_heads, head_dim).transpose(0, 1)\n",
    "\n",
    "    print(\"Q_dec_{} = \".format(layer_num))\n",
    "    print(Q_dec_mha)\n",
    "    print()\n",
    "\n",
    "    print(\"K_dec_{} = \".format(layer_num))\n",
    "    print(K_dec_mha)\n",
    "    print()\n",
    "\n",
    "    print(\"V_dec_{} = \".format(layer_num))\n",
    "    print(V_dec_mha)\n",
    "    print()\n",
    "\n",
    "\n",
    "    attn_mask = memory_mask\n",
    "    if attn_mask is not None:\n",
    "\n",
    "        # Ensuring attn_mask's dim is 3\n",
    "        if attn_mask.dim() == 2:\n",
    "            correct_2d_size = (tgt_len, src_len)\n",
    "            if attn_mask.shape != correct_2d_size:\n",
    "                raise RuntimeError(f\"The shape of the 2D attn_mask is {attn_mask.shape}, but should be {correct_2d_size}.\")\n",
    "            attn_mask = attn_mask.unsqueeze(0)\n",
    "        elif attn_mask.dim() == 3:\n",
    "            correct_3d_size = (bsz * num_heads, tgt_len, src_len)\n",
    "            if attn_mask.shape != correct_3d_size:\n",
    "                raise RuntimeError(f\"The shape of the 3D attn_mask is {attn_mask.shape}, but should be {correct_3d_size}.\")\n",
    "        else:\n",
    "            raise RuntimeError(f\"attn_mask's dimension {attn_mask.dim()} is not supported\")\n",
    "\n",
    "    # attn_mask can be either (L,S) or (N*num_heads, L, S)\n",
    "    # if attn_mask's shape is (1, L, S) we need to unsqueeze to (1, 1, L, S)\n",
    "    # in order to match the input for SDPA of (N, num_heads, L, S)\n",
    "    if attn_mask is not None:\n",
    "        if attn_mask.size(0) == 1 and attn_mask.dim() == 3:\n",
    "            attn_mask = attn_mask.unsqueeze(0)\n",
    "        else:\n",
    "            attn_mask = attn_mask.view(bsz, num_heads, -1, src_len)\n",
    "\n",
    "\n",
    "\n",
    "    if need_weights is False:\n",
    "\n",
    "        attn_output_dec_mha = atten_product_needs_wts_false(Q_dec_mha, V_dec_mha, K_dec_mha, bsz, head_dim, src_len, tgt_len, attn_mask)\n",
    "        \n",
    "        print(\"Cross attention in decoder_{}\".format(layer_num))\n",
    "        print(attn_output_dec_mha)\n",
    "        print()\n",
    "\n",
    "        op_dec_mha_1 = torch.matmul(attn_output_dec_mha, state_dict[\"transformer.decoder.layers.{}.multihead_attn.out_proj.weight\".format(layer_num)].t()) + state_dict[\"transformer.decoder.layers.{}.multihead_attn.out_proj.bias\".format(layer_num)]\n",
    "\n",
    "        attn_dec_mha_output = op_dec_mha_1.view(tgt_len, bsz, attn_output_dec_mha.size(1))\n",
    "\n",
    "        return attn_dec_mha_output, None\n",
    "    \n",
    "    else: \n",
    "\n",
    "    \n",
    "        attn_dec_mha_output ,attn_wt_matrix_dec_mha = atten_product_needs_wts_true(Q_dec_mha, K_dec_mha, V_dec_mha, bsz, tgt_len, attn_mask)\n",
    "\n",
    "        print(\"Decoder mha Attention output = \")\n",
    "        print(attn_dec_mha_output)\n",
    "        print()\n",
    "\n",
    "\n",
    "        op_dec_mha = torch.matmul(attn_dec_mha_output, state_dict[\"transformer.decoder.layers.{}.multihead_attn.out_proj.weight\".format(layer_num)].t()) + state_dict[\"transformer.decoder.layers.{}.multihead_attn.out_proj.bias\".format(layer_num)]\n",
    "        attn_dec_output_mha = op_dec_mha.view(tgt_len, bsz, attn_dec_mha_output.size(1))\n",
    "\n",
    "\n",
    "        return attn_dec_output_mha , attn_wt_matrix_dec_mha\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q_dec_0 = \n",
      "tensor([[[ 0.8053, -0.0713,  0.7679],\n",
      "         [-0.8762,  0.1282,  0.2041],\n",
      "         [ 1.8555,  0.2675,  0.3083],\n",
      "         [ 0.4550,  0.0176,  0.1286],\n",
      "         [-0.4132, -0.8115,  0.1700]],\n",
      "\n",
      "        [[-0.1601,  0.3679,  0.6959],\n",
      "         [ 0.1825,  0.3493, -0.3748],\n",
      "         [ 0.3822,  0.4362,  1.0671],\n",
      "         [ 0.4914,  0.8859, -0.3512],\n",
      "         [-1.1962, -0.3612, -0.2075]],\n",
      "\n",
      "        [[-0.0825, -0.5636, -0.3997],\n",
      "         [ 0.6957, -0.7197,  0.2492],\n",
      "         [ 1.1379,  0.0230,  0.7246],\n",
      "         [ 1.1512, -0.2868,  0.6664],\n",
      "         [ 1.0151, -0.3636,  0.6390]],\n",
      "\n",
      "        [[-0.5889, -0.5484, -0.5096],\n",
      "         [-1.0535, -0.1210,  0.3782],\n",
      "         [-0.2853, -0.0195,  1.2119],\n",
      "         [-0.5751,  0.2492,  0.9650],\n",
      "         [-0.6918,  0.0941,  0.8875]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "K_dec_0 = \n",
      "tensor([[[-0.7485,  1.1345,  0.6806],\n",
      "         [-1.0826,  1.7761,  0.2926],\n",
      "         [-0.6303,  0.0104,  0.8447],\n",
      "         [-0.3431,  1.4132,  0.9495]],\n",
      "\n",
      "        [[ 0.0087,  1.0130, -1.4948],\n",
      "         [ 1.0281,  0.5696, -0.4293],\n",
      "         [-1.2027,  0.3345, -0.9261],\n",
      "         [ 0.2795,  0.1991, -0.9360]],\n",
      "\n",
      "        [[-0.2423, -0.7448,  0.3559],\n",
      "         [-0.6347,  0.5470,  0.0896],\n",
      "         [-0.2423, -0.7448,  0.3559],\n",
      "         [-1.1947, -0.3792,  0.4218]],\n",
      "\n",
      "        [[-1.0010, -0.1218, -0.0587],\n",
      "         [ 0.6914,  0.8278, -0.6580],\n",
      "         [-1.0010, -0.1218, -0.0587],\n",
      "         [ 0.3515,  0.6886, -0.3362]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "V_dec_0 = \n",
      "tensor([[[-0.0250, -0.5084, -0.5107],\n",
      "         [ 0.0814, -0.2902, -0.0315],\n",
      "         [-0.7897,  0.3449,  0.1140],\n",
      "         [-0.1839,  1.2598, -0.4280]],\n",
      "\n",
      "        [[ 0.7614,  0.9535, -1.5061],\n",
      "         [-0.1786,  1.3551, -1.4120],\n",
      "         [ 0.4739,  0.6440,  0.2836],\n",
      "         [ 0.7636,  1.1720,  0.0077]],\n",
      "\n",
      "        [[-0.4400,  0.3982,  0.3126],\n",
      "         [ 0.8036, -0.8642, -0.4237],\n",
      "         [-0.4400,  0.3982,  0.3126],\n",
      "         [ 1.2200, -0.2009, -0.1498]],\n",
      "\n",
      "        [[ 0.0151, -0.0878,  0.9698],\n",
      "         [ 0.2013,  0.2095, -1.3266],\n",
      "         [ 0.0151, -0.0878,  0.9698],\n",
      "         [-0.0384, -0.1307,  0.0661]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "Attention weights = \n",
      "torch.Size([2, 2, 5, 4]) tensor([[[[0.2360, 0.1657, 0.2809, 0.3174],\n",
      "          [0.2546, 0.3020, 0.2250, 0.2185],\n",
      "          [0.2276, 0.1640, 0.2236, 0.3848],\n",
      "          [0.2459, 0.2203, 0.2539, 0.2799],\n",
      "          [0.2346, 0.1811, 0.3924, 0.1919]],\n",
      "\n",
      "         [[0.2215, 0.2815, 0.2695, 0.2275],\n",
      "          [0.3067, 0.2479, 0.2081, 0.2373],\n",
      "          [0.1888, 0.4077, 0.1730, 0.2304],\n",
      "          [0.3401, 0.2917, 0.1519, 0.2163],\n",
      "          [0.2049, 0.0978, 0.5090, 0.1883]]],\n",
      "\n",
      "\n",
      "        [[[0.2757, 0.1962, 0.2757, 0.2523],\n",
      "          [0.3255, 0.1564, 0.3255, 0.1926],\n",
      "          [0.3071, 0.2160, 0.3071, 0.1697],\n",
      "          [0.3253, 0.1827, 0.3253, 0.1668],\n",
      "          [0.3234, 0.1776, 0.3234, 0.1756]],\n",
      "\n",
      "         [[0.3304, 0.1641, 0.3304, 0.1751],\n",
      "          [0.3726, 0.1093, 0.3726, 0.1456],\n",
      "          [0.3179, 0.1565, 0.3179, 0.2076],\n",
      "          [0.3244, 0.1518, 0.3244, 0.1993],\n",
      "          [0.3422, 0.1348, 0.3422, 0.1807]]]], grad_fn=<SoftmaxBackward0>)\n",
      "Cross attention in decoder_0\n",
      "tensor([[-2.7258e-01,  3.2869e-01, -2.2960e-01,  4.1981e-01,  1.0328e+00,\n",
      "         -6.5287e-01],\n",
      "        [ 2.2287e-01, -6.7371e-04,  5.1479e-02,  3.6303e-02, -4.6525e-02,\n",
      "          4.3469e-01],\n",
      "        [-1.9962e-01,  1.3583e-01, -2.0739e-01,  4.6903e-01,  1.0405e+00,\n",
      "         -7.5108e-01],\n",
      "        [ 7.4229e-02,  8.5333e-02,  1.0839e-01,  2.7680e-02, -6.1556e-02,\n",
      "          5.8730e-01],\n",
      "        [-2.3968e-01,  3.9866e-01, -2.6063e-01,  3.2891e-01,  1.1141e+00,\n",
      "         -8.0933e-01],\n",
      "        [ 1.1036e-01,  2.3829e-02,  7.5101e-02,  3.3148e-02, -5.0182e-02,\n",
      "          4.2276e-01],\n",
      "        [-2.4016e-01,  2.5122e-01, -2.2339e-01,  4.4401e-01,  1.0709e+00,\n",
      "         -8.7940e-01],\n",
      "        [ 6.3986e-02,  6.7703e-02,  1.0102e-01,  3.2725e-02, -5.1210e-02,\n",
      "          4.4097e-01],\n",
      "        [-3.3631e-01,  2.0532e-01, -1.6291e-01,  5.2353e-01,  8.7643e-01,\n",
      "         -3.0087e-01],\n",
      "        [ 7.2398e-02,  6.8780e-02,  1.0065e-01,  3.0553e-02, -5.5471e-02,\n",
      "          4.9684e-01]], grad_fn=<ViewBackward0>)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# need_weights = True\n",
    "\n",
    "attn_dec_mha_output, attn_dec_mha_wts = decoder_block_cross_attn_output(x_dec, memory, state_dict, layer_num = 0, tgt_len = tgt_len, src_len = src_len, head_dim = head_dim, need_weights = need_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoder_block_post_attn_output(x_dec, attn_dec_mha_output, state_dict, layer_num):\n",
    "\n",
    "    # (tgt_len, bsz, embed_dim)\n",
    "    output_dec_2 = attn_dec_mha_output + x_dec\n",
    "\n",
    "    linear_result_dec_2 = output_dec_2*state_dict[\"transformer.decoder.layers.{}.norm2.weight\".format(layer_num)] + state_dict[\"transformer.decoder.layers.{}.norm2.bias\".format(layer_num)]\n",
    "\n",
    "    # Layer normalization from Torch's implementation \n",
    "    layernorm_dec_2 = torch.nn.LayerNorm(normalized_shape=linear_result_dec_2.shape[2:])\n",
    "    linear_op_dec_2 = layernorm_dec_2(linear_result_dec_2)\n",
    "\n",
    "    x = linear_result_dec_2\n",
    "    w = layernorm_dec_2.weight\n",
    "    b = layernorm_dec_2.bias\n",
    "\n",
    "    linear_result_dec_2f = w*x + b\n",
    "\n",
    "    epsilon = 1e-5  \n",
    "    mean = linear_result_dec_2f.mean(dim=-1, keepdim=True)\n",
    "    std = linear_result_dec_2f.std(dim=-1, unbiased=False, keepdim=True)\n",
    "    normalized_result_dec_2 = (linear_result_dec_2f - mean) / (std + epsilon) * w + b\n",
    "\n",
    "    print(\"norm2(x' + mha(x', mem)), \\n where x' = Decoder_curr_layer norm1(x + sa(x))\")\n",
    "    print(normalized_result_dec_2)\n",
    "    print(\"\\n\\n\")\n",
    "\n",
    "    x_dec2_norm = normalized_result_dec_2\n",
    "\n",
    "    op_dec_1 = torch.matmul(x_dec2_norm, state_dict[\"transformer.decoder.layers.{}.linear1.weight\".format(layer_num)].t()) + state_dict[\"transformer.decoder.layers.{}.linear1.bias\".format(layer_num)]\n",
    "    op_dec_1_relu = torch.nn.functional.relu(op_dec_1)\n",
    "    op_dec_2 = torch.matmul(op_dec_1_relu, state_dict[\"transformer.decoder.layers.{}.linear2.weight\".format(layer_num)].t()) + state_dict[\"transformer.decoder.layers.{}.linear2.bias\".format(layer_num)]\n",
    "\n",
    "    ff_dec = op_dec_2\n",
    "\n",
    "    x_dec3_unorm = x_dec2_norm + ff_dec\n",
    "\n",
    "    linear_result_dec_3 = x_dec3_unorm*state_dict[\"transformer.decoder.layers.0.norm3.weight\"] + state_dict[\"transformer.decoder.layers.0.norm3.bias\"]\n",
    "\n",
    "    # Layer normalization from Torch's implementation \n",
    "    layernorm_dec_3 = torch.nn.LayerNorm(normalized_shape=linear_result_dec_3.shape[2:])\n",
    "    linear_op_dec_3 = layernorm_dec_3(linear_result_dec_3)\n",
    "\n",
    "    # From torch's implementation\n",
    "    x = linear_result_dec_3\n",
    "\n",
    "    # Obtained layer norm weights and biases (learnable)\n",
    "    w = layernorm_dec_3.weight\n",
    "    b = layernorm_dec_3.bias\n",
    "\n",
    "    linear_result_dec_3f = w*x + b\n",
    "\n",
    "    epsilon = 1e-5  \n",
    "    mean = linear_result_dec_3f.mean(dim=-1, keepdim=True)\n",
    "    std = linear_result_dec_3f.std(dim=-1, unbiased=False, keepdim=True)\n",
    "    normalized_result_dec_3 = (linear_result_dec_3f - mean) / (std + epsilon) * w + b\n",
    "\n",
    "    print(\"norm3(x'' + ff(x'')) \\n where, x'' = Decoder_curr_layer norm2(x' + mha(x'))\")\n",
    "    print(normalized_result_dec_3)\n",
    "    print()\n",
    "\n",
    "    return normalized_result_dec_3\n",
    "\n",
    "def feef_fwd_transformer(dec_output_final, state_dict):\n",
    "\n",
    "    # (tgt_len, bsz, embed_dim) @ (vocab_size, embed_dim).T -> (tgt_len, bsz, vocab_size)\n",
    "    final_op = dec_output_final@state_dict[\"fc.weight\"].T + state_dict[\"fc.bias\"]\n",
    "\n",
    "    return final_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "norm2(x' + mha(x', mem)), \n",
      " where x' = Decoder_curr_layer norm1(x + sa(x))\n",
      "tensor([[[-1.4574,  0.1825, -1.1540,  0.7673,  0.3103,  1.3513],\n",
      "         [ 0.3961, -0.3919, -0.4246,  1.3567,  0.8027, -1.7390]],\n",
      "\n",
      "        [[-1.4126, -0.4391,  0.9497, -0.5774, -0.1213,  1.6006],\n",
      "         [ 0.4802, -0.9630, -1.4157,  1.6747,  0.0487,  0.1751]],\n",
      "\n",
      "        [[ 0.1056,  1.6139, -1.4920,  0.5495, -0.9149,  0.1379],\n",
      "         [ 0.1936,  0.8049, -2.1927,  0.4950,  0.4415,  0.2577]],\n",
      "\n",
      "        [[-1.7267, -0.0143,  0.6227,  0.9632, -0.8418,  0.9969],\n",
      "         [ 0.0873, -0.1348, -1.8618,  1.2062, -0.2832,  0.9862]],\n",
      "\n",
      "        [[-0.6382, -1.7092, -0.3937,  0.9758,  0.9360,  0.8294],\n",
      "         [ 0.0811, -0.2151, -1.9421,  1.2829,  0.0689,  0.7244]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "\n",
      "norm3(x'' + ff(x'')) \n",
      " where, x'' = Decoder_curr_layer norm2(x' + mha(x'))\n",
      "tensor([[[-1.1004,  0.1155, -1.5022,  0.5930,  0.5146,  1.3794],\n",
      "         [ 0.5376, -0.1928, -0.8114,  1.2325,  0.8826, -1.6484]],\n",
      "\n",
      "        [[-1.1626, -0.5178,  0.4735, -0.7798,  0.1061,  1.8806],\n",
      "         [ 0.6701, -0.8974, -1.6259,  1.4162,  0.2418,  0.1952]],\n",
      "\n",
      "        [[ 0.2895,  1.4591, -1.7827,  0.4505, -0.6079,  0.1915],\n",
      "         [ 0.3270,  0.6511, -2.2177,  0.3642,  0.5701,  0.3053]],\n",
      "\n",
      "        [[-1.6697,  0.0060,  0.1573,  0.9773, -0.7582,  1.2873],\n",
      "         [ 0.3110, -0.2023, -2.0099,  0.9705, -0.0078,  0.9385]],\n",
      "\n",
      "        [[-0.3763, -1.6379, -0.7528,  0.7531,  1.0899,  0.9239],\n",
      "         [ 0.2881, -0.2559, -2.0520,  1.0337,  0.2858,  0.7004]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final_attn_op_decoder = decoder_block_post_attn_output(x_dec, attn_dec_mha_output, state_dict, layer_num = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (tgt_len, bsz, vocab_dim)\n",
    "final_op = feef_fwd_transformer(final_attn_op_decoder, state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-6.6667e-01,  3.2296e-01, -4.0249e-01, -3.8964e-01, -2.7701e-01,\n",
       "          -1.1482e-01,  3.5929e-01,  1.2512e+00,  2.0429e-01,  5.5085e-01],\n",
       "         [ 7.3170e-02, -3.3670e-01,  2.7040e-01, -2.0084e-01,  2.5609e-01,\n",
       "           6.8348e-02, -4.1680e-01, -5.0878e-01, -1.9309e-01,  9.6282e-01]],\n",
       "\n",
       "        [[-1.2892e+00,  1.1669e+00, -1.3184e+00, -3.9436e-01,  1.2273e-01,\n",
       "          -1.4213e-01, -4.2032e-02,  5.8542e-01,  2.4001e-01,  9.7504e-02],\n",
       "         [-3.1303e-01,  9.1863e-02, -1.9785e-01, -6.2570e-01, -1.6641e-01,\n",
       "          -2.3585e-01, -1.9337e-01,  3.2745e-01,  3.4555e-02,  1.0395e-01]],\n",
       "\n",
       "        [[ 6.3494e-01, -8.2564e-01,  6.4853e-01, -4.0874e-01, -1.2193e+00,\n",
       "           5.8824e-02,  8.0115e-01,  4.2675e-01,  1.4104e-01,  2.0981e-01],\n",
       "         [ 2.9972e-01, -7.1025e-01,  7.6157e-04,  4.7561e-03, -8.9210e-01,\n",
       "          -2.6281e-01,  5.6628e-01,  7.5355e-01,  3.6227e-01,  7.0990e-01]],\n",
       "\n",
       "        [[-1.0802e+00,  1.1114e+00, -7.8312e-02, -1.1748e+00,  2.2005e-01,\n",
       "           3.6830e-01, -6.7762e-03,  5.8124e-01, -3.0546e-01, -1.6947e-01],\n",
       "         [-1.7524e-01, -8.3427e-02, -1.6040e-01, -5.5476e-01, -6.5517e-01,\n",
       "          -2.5749e-01,  2.9586e-01,  8.1952e-01,  2.1126e-01,  5.2641e-02]],\n",
       "\n",
       "        [[-1.2419e+00,  9.3705e-01, -1.1620e+00, -3.8737e-01,  5.3889e-01,\n",
       "          -3.4226e-01, -5.2016e-01,  6.2436e-01,  1.4019e-01,  4.6169e-01],\n",
       "         [-1.7737e-01, -1.2942e-01, -1.6874e-01, -4.5114e-01, -5.3793e-01,\n",
       "          -2.6593e-01,  2.2634e-01,  7.9535e-01,  2.0525e-01,  2.5652e-01]]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 2, 10])"
      ]
     },
     "execution_count": 270,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_op.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "########################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examaple :- \n",
    "\n",
    "Transformer with 3 encoders and 3 decoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 2])\n",
      "torch.Size([5, 2])\n",
      "Source embeddings :- \n",
      "\n",
      "tensor([[[ 0.3485,  0.9371,  0.6244, -0.0230, -0.1803,  1.6200],\n",
      "         [-1.2299,  0.8020,  0.4715,  0.4010, -0.5315, -1.7415]],\n",
      "\n",
      "        [[-0.1163,  0.3765, -0.3016, -0.2952, -0.5298, -0.6820],\n",
      "         [ 0.3485,  0.9371,  0.6244, -0.0230, -0.1803,  1.6200]],\n",
      "\n",
      "        [[-1.2299,  0.8020,  0.4715,  0.4010, -0.5315, -1.7415],\n",
      "         [-1.2299,  0.8020,  0.4715,  0.4010, -0.5315, -1.7415]],\n",
      "\n",
      "        [[-0.2243,  1.6587, -0.3522, -0.6067, -0.2162, -0.6181],\n",
      "         [-0.4952, -2.1157,  1.7508, -0.6661, -0.6780,  0.7846]]],\n",
      "       grad_fn=<EmbeddingBackward0>)\n",
      "\n",
      "Target embeddings :- \n",
      "\n",
      "tensor([[[ 0.1129, -1.0142, -0.9221,  0.8812, -1.6048,  0.2050],\n",
      "         [ 1.6339, -0.6463, -0.1945,  0.6870,  0.3138,  0.4030]],\n",
      "\n",
      "        [[-0.7208,  0.4244, -1.1285,  1.7011,  2.0456,  0.1133],\n",
      "         [ 0.7506, -0.2308,  1.0457, -0.1141, -0.8790, -0.7974]],\n",
      "\n",
      "        [[-0.1423,  0.0984, -1.4589,  1.0058, -0.5254,  0.3244],\n",
      "         [ 0.1265, -0.2915, -0.4858,  0.6941,  0.3983, -0.5563]],\n",
      "\n",
      "        [[-0.3389,  1.8031, -0.6392,  0.0925,  0.3354, -0.2486],\n",
      "         [-0.3784, -0.2966,  0.0895,  1.3754,  2.1131,  0.1386]],\n",
      "\n",
      "        [[-0.3823,  0.4868,  0.8619,  0.4363, -0.8054,  0.7608],\n",
      "         [ 0.1129, -1.0142, -0.9221,  0.8812, -1.6048,  0.2050]]],\n",
      "       grad_fn=<EmbeddingBackward0>)\n",
      "\n",
      "Positional encoded source embeddings:\n",
      "tensor([[[ 0.3485,  1.9371,  0.6244,  0.9770, -0.1803,  2.6200],\n",
      "         [-0.3884,  1.3423,  0.5179,  1.4000, -0.5294, -0.7415]],\n",
      "\n",
      "        [[-0.1163,  1.3765, -0.3016,  0.7048, -0.5298,  0.3180],\n",
      "         [ 1.1900,  1.4774,  0.6708,  0.9760, -0.1782,  2.6200]],\n",
      "\n",
      "        [[-1.2299,  1.8020,  0.4715,  1.4010, -0.5315, -0.7415],\n",
      "         [-0.3884,  1.3423,  0.5179,  1.4000, -0.5294, -0.7415]],\n",
      "\n",
      "        [[-0.2243,  2.6587, -0.3522,  0.3933, -0.2162,  0.3819],\n",
      "         [ 0.3463, -1.5754,  1.7972,  0.3328, -0.6758,  1.7845]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "\n",
      "Positional encoded target embeddings:\n",
      "tensor([[[ 0.1129, -0.0142, -0.9221,  1.8812, -1.6048,  1.2050],\n",
      "         [ 2.4754, -0.1060, -0.1481,  1.6859,  0.3160,  1.4030]],\n",
      "\n",
      "        [[-0.7208,  1.4244, -1.1285,  2.7011,  2.0456,  1.1133],\n",
      "         [ 1.5921,  0.3095,  1.0921,  0.8848, -0.8769,  0.2026]],\n",
      "\n",
      "        [[-0.1423,  1.0984, -1.4589,  2.0058, -0.5254,  1.3244],\n",
      "         [ 0.9680,  0.2488, -0.4394,  1.6930,  0.4004,  0.4437]],\n",
      "\n",
      "        [[-0.3389,  2.8031, -0.6392,  1.0925,  0.3354,  0.7514],\n",
      "         [ 0.4631,  0.2437,  0.1359,  2.3743,  2.1152,  1.1386]],\n",
      "\n",
      "        [[-0.3823,  1.4868,  0.8619,  1.4363, -0.8054,  1.7608],\n",
      "         [ 0.9544, -0.4739, -0.8757,  1.8801, -1.6027,  1.2050]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "\n",
      "query =  tensor([[[ 0.3485,  1.9371,  0.6244,  0.9770, -0.1803,  2.6200],\n",
      "         [-0.3884,  1.3423,  0.5179,  1.4000, -0.5294, -0.7415]],\n",
      "\n",
      "        [[-0.1163,  1.3765, -0.3016,  0.7048, -0.5298,  0.3180],\n",
      "         [ 1.1900,  1.4774,  0.6708,  0.9760, -0.1782,  2.6200]],\n",
      "\n",
      "        [[-1.2299,  1.8020,  0.4715,  1.4010, -0.5315, -0.7415],\n",
      "         [-0.3884,  1.3423,  0.5179,  1.4000, -0.5294, -0.7415]],\n",
      "\n",
      "        [[-0.2243,  2.6587, -0.3522,  0.3933, -0.2162,  0.3819],\n",
      "         [ 0.3463, -1.5754,  1.7972,  0.3328, -0.6758,  1.7845]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "query shape =  torch.Size([4, 2, 6])\n",
      "\n",
      " _in_projection_packed \n",
      "\n",
      "Q :-  tensor([[[ 1.3542,  0.7988, -1.9542,  0.3685, -0.6178,  0.1329],\n",
      "         [ 0.4360,  0.1708, -1.1372,  1.5151, -0.2740, -0.6128]],\n",
      "\n",
      "        [[ 0.6537, -0.1274, -1.0933,  0.8181, -0.1809, -0.3052],\n",
      "         [ 1.0151,  0.7766, -1.3345,  0.0141, -0.4347, -0.1055]],\n",
      "\n",
      "        [[ 0.7752,  0.1929, -1.7568,  1.8695, -0.4570, -0.3744],\n",
      "         [ 0.4360,  0.1708, -1.1372,  1.5151, -0.2740, -0.6128]],\n",
      "\n",
      "        [[ 1.2694, -0.1703, -1.5901,  1.2231,  0.0914, -0.5582],\n",
      "         [-0.2332,  0.7542,  0.1986, -0.5066, -0.7136,  0.6494]]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "\n",
      "K :-  tensor([[[-1.4025,  0.2942, -0.8988,  0.2983,  0.1654,  1.2723],\n",
      "         [-0.2456,  0.5762, -0.0164, -0.2161,  0.3141,  1.6730]],\n",
      "\n",
      "        [[-0.7072,  0.1992, -0.0046, -0.3592, -0.0803,  1.0749],\n",
      "         [-0.8842,  0.3547, -1.1704,  0.5738,  0.0392,  0.7456]],\n",
      "\n",
      "        [[-0.7639,  0.5158,  0.2552, -0.4916,  0.4404,  2.1996],\n",
      "         [-0.2456,  0.5762, -0.0164, -0.2161,  0.3141,  1.6730]],\n",
      "\n",
      "        [[-1.2674,  0.3434,  0.5734, -1.1030, -0.0657,  1.4841],\n",
      "         [ 0.9147,  0.4109, -1.5272,  1.7960,  0.5259, -0.2905]]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "\n",
      "V :-  tensor([[[-1.9645, -0.0160, -0.5482, -0.9240,  0.0568, -1.0505],\n",
      "         [-0.9230,  0.6382, -1.4058, -0.0558,  0.5399, -0.9999]],\n",
      "\n",
      "        [[-0.5377,  0.0031, -1.1717, -0.0226,  0.3606, -1.2210],\n",
      "         [-1.7408, -0.3671, -0.0161, -0.9842, -0.1416, -0.9719]],\n",
      "\n",
      "        [[-1.1467,  0.9893, -1.9379,  0.0044,  0.7383, -1.0785],\n",
      "         [-0.9230,  0.6382, -1.4058, -0.0558,  0.5399, -0.9999]],\n",
      "\n",
      "        [[-0.5897, -0.0174, -1.6497,  0.0139,  0.5846, -1.5888],\n",
      "         [-1.3913,  0.2779,  1.7576, -0.5571, -0.9032,  1.0796]]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "\n",
      "tgt_len =  4\n",
      "bsz =  2\n",
      "num_heads =  2\n",
      "head_dim =  3\n",
      "\n",
      "tensor([[[ 1.3542,  0.7988, -1.9542,  0.3685, -0.6178,  0.1329],\n",
      "         [ 0.4360,  0.1708, -1.1372,  1.5151, -0.2740, -0.6128]],\n",
      "\n",
      "        [[ 0.6537, -0.1274, -1.0933,  0.8181, -0.1809, -0.3052],\n",
      "         [ 1.0151,  0.7766, -1.3345,  0.0141, -0.4347, -0.1055]],\n",
      "\n",
      "        [[ 0.7752,  0.1929, -1.7568,  1.8695, -0.4570, -0.3744],\n",
      "         [ 0.4360,  0.1708, -1.1372,  1.5151, -0.2740, -0.6128]],\n",
      "\n",
      "        [[ 1.2694, -0.1703, -1.5901,  1.2231,  0.0914, -0.5582],\n",
      "         [-0.2332,  0.7542,  0.1986, -0.5066, -0.7136,  0.6494]]],\n",
      "       grad_fn=<SelectBackward0>) torch.Size([4, 2, 6])\n",
      "Q reshaped =  tensor([[[ 1.3542,  0.7988, -1.9542],\n",
      "         [ 0.6537, -0.1274, -1.0933],\n",
      "         [ 0.7752,  0.1929, -1.7568],\n",
      "         [ 1.2694, -0.1703, -1.5901]],\n",
      "\n",
      "        [[ 0.3685, -0.6178,  0.1329],\n",
      "         [ 0.8181, -0.1809, -0.3052],\n",
      "         [ 1.8695, -0.4570, -0.3744],\n",
      "         [ 1.2231,  0.0914, -0.5582]],\n",
      "\n",
      "        [[ 0.4360,  0.1708, -1.1372],\n",
      "         [ 1.0151,  0.7766, -1.3345],\n",
      "         [ 0.4360,  0.1708, -1.1372],\n",
      "         [-0.2332,  0.7542,  0.1986]],\n",
      "\n",
      "        [[ 1.5151, -0.2740, -0.6128],\n",
      "         [ 0.0141, -0.4347, -0.1055],\n",
      "         [ 1.5151, -0.2740, -0.6128],\n",
      "         [-0.5066, -0.7136,  0.6494]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "K reshaped =  tensor([[[-1.4025,  0.2942, -0.8988],\n",
      "         [-0.7072,  0.1992, -0.0046],\n",
      "         [-0.7639,  0.5158,  0.2552],\n",
      "         [-1.2674,  0.3434,  0.5734]],\n",
      "\n",
      "        [[ 0.2983,  0.1654,  1.2723],\n",
      "         [-0.3592, -0.0803,  1.0749],\n",
      "         [-0.4916,  0.4404,  2.1996],\n",
      "         [-1.1030, -0.0657,  1.4841]],\n",
      "\n",
      "        [[-0.2456,  0.5762, -0.0164],\n",
      "         [-0.8842,  0.3547, -1.1704],\n",
      "         [-0.2456,  0.5762, -0.0164],\n",
      "         [ 0.9147,  0.4109, -1.5272]],\n",
      "\n",
      "        [[-0.2161,  0.3141,  1.6730],\n",
      "         [ 0.5738,  0.0392,  0.7456],\n",
      "         [-0.2161,  0.3141,  1.6730],\n",
      "         [ 1.7960,  0.5259, -0.2905]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "V reshaped =  tensor([[[-1.9645, -0.0160, -0.5482],\n",
      "         [-0.5377,  0.0031, -1.1717],\n",
      "         [-1.1467,  0.9893, -1.9379],\n",
      "         [-0.5897, -0.0174, -1.6497]],\n",
      "\n",
      "        [[-0.9240,  0.0568, -1.0505],\n",
      "         [-0.0226,  0.3606, -1.2210],\n",
      "         [ 0.0044,  0.7383, -1.0785],\n",
      "         [ 0.0139,  0.5846, -1.5888]],\n",
      "\n",
      "        [[-0.9230,  0.6382, -1.4058],\n",
      "         [-1.7408, -0.3671, -0.0161],\n",
      "         [-0.9230,  0.6382, -1.4058],\n",
      "         [-1.3913,  0.2779,  1.7576]],\n",
      "\n",
      "        [[-0.0558,  0.5399, -0.9999],\n",
      "         [-0.9842, -0.1416, -0.9719],\n",
      "         [-0.0558,  0.5399, -0.9999],\n",
      "         [-0.5571, -0.9032,  1.0796]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "torch.Size([2, 2, 4, 3]) torch.Size([2, 2, 4, 3]) torch.Size([2, 2, 4, 3])\n",
      "\n",
      "scaled_dot_product_attention\n",
      "attn_output = \n",
      "tensor([[-1.2900,  0.2045, -1.1112, -0.2600,  0.4139, -1.2248],\n",
      "        [-1.3482,  0.2198,  0.4644, -0.5716, -0.5881,  0.4547],\n",
      "        [-1.1951,  0.2088, -1.1852, -0.3263,  0.3693, -1.1985],\n",
      "        [-1.3479,  0.2506,  0.7972, -0.4352, -0.0149, -0.4579],\n",
      "        [-1.3061,  0.1812, -1.0881, -0.4446,  0.2980, -1.1568],\n",
      "        [-1.3482,  0.2198,  0.4644, -0.5716, -0.5881,  0.4547],\n",
      "        [-1.2282,  0.2016, -1.1434, -0.3828,  0.3342, -1.1776],\n",
      "        [-1.2121,  0.3235, -0.4664, -0.3013,  0.2684, -0.8174]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "\n",
      "x + self_atten(x) = \n",
      "tensor([[[ 0.6489,  3.6362,  1.8769,  2.6125,  0.0066,  2.7768],\n",
      "         [-1.6001,  2.1445,  1.7840,  1.7893, -1.0323, -0.6002]],\n",
      "\n",
      "        [[ 0.2744,  3.0595,  0.9569,  2.3650, -0.2901,  0.5281],\n",
      "         [ 0.4454,  2.5847,  1.5764,  1.4890, -1.0819,  2.6001]],\n",
      "\n",
      "        [[-0.9314,  3.6126,  1.8337,  3.0705, -0.3862, -0.5261],\n",
      "         [-1.6001,  2.1445,  1.7840,  1.7893, -1.0323, -0.6002]],\n",
      "\n",
      "        [[ 0.1362,  4.3873,  0.9469,  2.0558, -0.0155,  0.6010],\n",
      "         [ 0.2672, -0.3065,  2.9525,  1.5091, -0.7616,  1.9805]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "\n",
      "norm1(x + self_atten(x)) = \n",
      "tensor([[[-1.0186,  1.3633, -0.0394,  0.5471, -1.5307,  0.6782],\n",
      "         [-1.3215,  1.1352,  0.8987,  0.9021, -0.9490, -0.6655]],\n",
      "\n",
      "        [[-0.7397,  1.6159, -0.1625,  1.0285, -1.2172, -0.5251],\n",
      "         [-0.6428,  1.0268,  0.2400,  0.1718, -1.8347,  1.0389]],\n",
      "\n",
      "        [[-1.1274,  1.3795,  0.3981,  1.0804, -0.8266, -0.9038],\n",
      "         [-1.3215,  1.1352,  0.8987,  0.9021, -0.9490, -0.6655]],\n",
      "\n",
      "        [[-0.8021,  2.0026, -0.2672,  0.4644, -0.9022, -0.4955],\n",
      "         [-0.5122, -0.9488,  1.5315,  0.4330, -1.2952,  0.7917]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "\n",
      "feed_fwd_op =  tensor([[[ 0.3886, -0.0750, -0.0224,  0.2044, -0.1062, -0.3945],\n",
      "         [ 0.4732, -0.0126,  0.0294,  0.1484, -0.0232, -0.4397]],\n",
      "\n",
      "        [[ 0.4580, -0.0639, -0.0321,  0.2092, -0.0240, -0.4272],\n",
      "         [ 0.3535, -0.0761, -0.0339,  0.1911, -0.1792, -0.4306]],\n",
      "\n",
      "        [[ 0.4833, -0.0262,  0.0119,  0.1755, -0.0026, -0.4256],\n",
      "         [ 0.4732, -0.0126,  0.0294,  0.1484, -0.0232, -0.4397]],\n",
      "\n",
      "        [[ 0.4139, -0.0641, -0.0225,  0.1952, -0.0414, -0.4009],\n",
      "         [ 0.3795,  0.0045,  0.0250,  0.1091, -0.2230, -0.4632]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "\n",
      "norm2(norm1(x + self_atten(x)) + feed_fwd_op) = \n",
      "tensor([[[-0.6640,  1.3608, -0.0644,  0.7941, -1.7269,  0.3003],\n",
      "         [-0.8699,  1.0838,  0.8909,  1.0124, -0.9927, -1.1245]],\n",
      "\n",
      "        [[-0.2898,  1.4716, -0.2061,  1.1697, -1.2114, -0.9339],\n",
      "         [-0.2696,  1.0159,  0.2440,  0.4065, -2.0576,  0.6609]],\n",
      "\n",
      "        [[-0.6566,  1.2715,  0.3610,  1.1775, -0.8353, -1.3181],\n",
      "         [-0.8699,  1.0838,  0.8909,  1.0124, -0.9927, -1.1245]],\n",
      "\n",
      "        [[-0.3976,  1.9061, -0.3001,  0.6398, -0.9475, -0.9007],\n",
      "         [-0.1044, -0.9137,  1.5801,  0.5685, -1.4859,  0.3555]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "\n",
      "##################################\n",
      "query =  tensor([[[-0.6640,  1.3608, -0.0644,  0.7941, -1.7269,  0.3003],\n",
      "         [-0.8699,  1.0838,  0.8909,  1.0124, -0.9927, -1.1245]],\n",
      "\n",
      "        [[-0.2898,  1.4716, -0.2061,  1.1697, -1.2114, -0.9339],\n",
      "         [-0.2696,  1.0159,  0.2440,  0.4065, -2.0576,  0.6609]],\n",
      "\n",
      "        [[-0.6566,  1.2715,  0.3610,  1.1775, -0.8353, -1.3181],\n",
      "         [-0.8699,  1.0838,  0.8909,  1.0124, -0.9927, -1.1245]],\n",
      "\n",
      "        [[-0.3976,  1.9061, -0.3001,  0.6398, -0.9475, -0.9007],\n",
      "         [-0.1044, -0.9137,  1.5801,  0.5685, -1.4859,  0.3555]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "query shape =  torch.Size([4, 2, 6])\n",
      "\n",
      " _in_projection_packed \n",
      "\n",
      "Q :-  tensor([[[ 0.1062, -1.3256,  0.8023,  0.4596,  0.5511,  0.6325],\n",
      "         [ 0.0107, -0.6982, -0.0194,  0.8653,  0.8188,  0.7955]],\n",
      "\n",
      "        [[ 0.0720, -0.6842,  0.5549,  0.6075,  0.8223,  0.7524],\n",
      "         [ 0.3022, -1.4428,  0.7871,  0.6386,  0.5101,  0.1899]],\n",
      "\n",
      "        [[-0.0249, -0.5067,  0.1385,  0.7091,  0.8366,  0.8730],\n",
      "         [ 0.0107, -0.6982, -0.0194,  0.8653,  0.8188,  0.7955]],\n",
      "\n",
      "        [[ 0.0754, -0.3350,  0.6414,  0.0795,  0.7293,  0.6773],\n",
      "         [ 0.2625, -1.3457, -0.3344,  1.5853,  0.3659, -0.1566]]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "\n",
      "K :-  tensor([[[ 0.1612, -0.0076,  0.4991,  0.8756,  1.5937, -0.7307],\n",
      "         [-0.5119, -0.7061,  0.2347,  0.1476,  1.1747, -0.6728]],\n",
      "\n",
      "        [[-0.0512,  0.1495,  0.4237,  0.3883,  1.2711, -0.7333],\n",
      "         [ 0.2638, -0.1448,  0.8370,  0.6155,  1.2894, -0.5124]],\n",
      "\n",
      "        [[-0.3917, -0.3114,  0.1881,  0.1986,  1.1641, -0.7149],\n",
      "         [-0.5119, -0.7061,  0.2347,  0.1476,  1.1747, -0.6728]],\n",
      "\n",
      "        [[-0.0999,  0.0048,  0.4946,  0.2660,  1.2394, -0.8099],\n",
      "         [-0.1415, -0.7540,  0.4165, -0.0033,  0.2692,  0.1448]]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "\n",
      "V :-  tensor([[[ 0.2575,  0.4791, -0.6375, -0.4460,  1.3066,  0.6505],\n",
      "         [ 0.9274,  0.9059, -0.0759, -0.4483,  0.9797,  0.2257]],\n",
      "\n",
      "        [[ 0.8317,  1.0239, -0.3566, -0.9145,  1.2622,  0.3598],\n",
      "         [ 0.2356,  0.3226, -0.5502, -0.3706,  1.3958,  0.7039]],\n",
      "\n",
      "        [[ 0.9557,  1.0392, -0.1398, -0.6914,  0.9932,  0.1945],\n",
      "         [ 0.9274,  0.9059, -0.0759, -0.4483,  0.9797,  0.2257]],\n",
      "\n",
      "        [[ 0.6605,  1.0671, -0.4351, -1.0283,  1.0360,  0.5689],\n",
      "         [ 0.4907, -0.1479,  0.2277,  0.4950,  0.8483, -0.1000]]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "\n",
      "tgt_len =  4\n",
      "bsz =  2\n",
      "num_heads =  2\n",
      "head_dim =  3\n",
      "\n",
      "tensor([[[ 0.1062, -1.3256,  0.8023,  0.4596,  0.5511,  0.6325],\n",
      "         [ 0.0107, -0.6982, -0.0194,  0.8653,  0.8188,  0.7955]],\n",
      "\n",
      "        [[ 0.0720, -0.6842,  0.5549,  0.6075,  0.8223,  0.7524],\n",
      "         [ 0.3022, -1.4428,  0.7871,  0.6386,  0.5101,  0.1899]],\n",
      "\n",
      "        [[-0.0249, -0.5067,  0.1385,  0.7091,  0.8366,  0.8730],\n",
      "         [ 0.0107, -0.6982, -0.0194,  0.8653,  0.8188,  0.7955]],\n",
      "\n",
      "        [[ 0.0754, -0.3350,  0.6414,  0.0795,  0.7293,  0.6773],\n",
      "         [ 0.2625, -1.3457, -0.3344,  1.5853,  0.3659, -0.1566]]],\n",
      "       grad_fn=<SelectBackward0>) torch.Size([4, 2, 6])\n",
      "Q reshaped =  tensor([[[ 0.1062, -1.3256,  0.8023],\n",
      "         [ 0.0720, -0.6842,  0.5549],\n",
      "         [-0.0249, -0.5067,  0.1385],\n",
      "         [ 0.0754, -0.3350,  0.6414]],\n",
      "\n",
      "        [[ 0.4596,  0.5511,  0.6325],\n",
      "         [ 0.6075,  0.8223,  0.7524],\n",
      "         [ 0.7091,  0.8366,  0.8730],\n",
      "         [ 0.0795,  0.7293,  0.6773]],\n",
      "\n",
      "        [[ 0.0107, -0.6982, -0.0194],\n",
      "         [ 0.3022, -1.4428,  0.7871],\n",
      "         [ 0.0107, -0.6982, -0.0194],\n",
      "         [ 0.2625, -1.3457, -0.3344]],\n",
      "\n",
      "        [[ 0.8653,  0.8188,  0.7955],\n",
      "         [ 0.6386,  0.5101,  0.1899],\n",
      "         [ 0.8653,  0.8188,  0.7955],\n",
      "         [ 1.5853,  0.3659, -0.1566]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "K reshaped =  tensor([[[ 0.1612, -0.0076,  0.4991],\n",
      "         [-0.0512,  0.1495,  0.4237],\n",
      "         [-0.3917, -0.3114,  0.1881],\n",
      "         [-0.0999,  0.0048,  0.4946]],\n",
      "\n",
      "        [[ 0.8756,  1.5937, -0.7307],\n",
      "         [ 0.3883,  1.2711, -0.7333],\n",
      "         [ 0.1986,  1.1641, -0.7149],\n",
      "         [ 0.2660,  1.2394, -0.8099]],\n",
      "\n",
      "        [[-0.5119, -0.7061,  0.2347],\n",
      "         [ 0.2638, -0.1448,  0.8370],\n",
      "         [-0.5119, -0.7061,  0.2347],\n",
      "         [-0.1415, -0.7540,  0.4165]],\n",
      "\n",
      "        [[ 0.1476,  1.1747, -0.6728],\n",
      "         [ 0.6155,  1.2894, -0.5124],\n",
      "         [ 0.1476,  1.1747, -0.6728],\n",
      "         [-0.0033,  0.2692,  0.1448]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "V reshaped =  tensor([[[ 0.2575,  0.4791, -0.6375],\n",
      "         [ 0.8317,  1.0239, -0.3566],\n",
      "         [ 0.9557,  1.0392, -0.1398],\n",
      "         [ 0.6605,  1.0671, -0.4351]],\n",
      "\n",
      "        [[-0.4460,  1.3066,  0.6505],\n",
      "         [-0.9145,  1.2622,  0.3598],\n",
      "         [-0.6914,  0.9932,  0.1945],\n",
      "         [-1.0283,  1.0360,  0.5689]],\n",
      "\n",
      "        [[ 0.9274,  0.9059, -0.0759],\n",
      "         [ 0.2356,  0.3226, -0.5502],\n",
      "         [ 0.9274,  0.9059, -0.0759],\n",
      "         [ 0.4907, -0.1479,  0.2277]],\n",
      "\n",
      "        [[-0.4483,  0.9797,  0.2257],\n",
      "         [-0.3706,  1.3958,  0.7039],\n",
      "         [-0.4483,  0.9797,  0.2257],\n",
      "         [ 0.4950,  0.8483, -0.1000]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "torch.Size([2, 2, 4, 3]) torch.Size([2, 2, 4, 3]) torch.Size([2, 2, 4, 3])\n",
      "\n",
      "scaled_dot_product_attention\n",
      "attn_output = \n",
      "tensor([[ 0.6743,  0.8983, -0.3897, -0.7466,  1.1642,  0.4591],\n",
      "        [ 0.6664,  0.5026, -0.0937, -0.2301,  1.0913,  0.3184],\n",
      "        [ 0.6726,  0.8987, -0.3930, -0.7369,  1.1701,  0.4659],\n",
      "        [ 0.6432,  0.4671, -0.0955, -0.2394,  1.0827,  0.3098],\n",
      "        [ 0.6797,  0.9034, -0.3880, -0.7332,  1.1723,  0.4682],\n",
      "        [ 0.6664,  0.5026, -0.0937, -0.2301,  1.0913,  0.3184],\n",
      "        [ 0.6682,  0.8969, -0.3978, -0.7542,  1.1592,  0.4535],\n",
      "        [ 0.6817,  0.5028, -0.0719, -0.2703,  1.1118,  0.3498]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "\n",
      "x + self_atten(x) = \n",
      "tensor([[[-1.1603,  0.4283,  0.3842,  0.5657, -0.9085, -0.4692],\n",
      "         [-1.1528,  0.4599,  1.5018,  1.0690, -0.3363, -1.6160]],\n",
      "\n",
      "        [[-0.7873,  0.5365,  0.2549,  0.9403, -0.3941, -1.7097],\n",
      "         [-0.5301,  0.3761,  0.8348,  0.4585, -1.4007,  0.1908]],\n",
      "\n",
      "        [[-1.1586,  0.3415,  0.8272,  0.9503, -0.0176, -2.0947],\n",
      "         [-1.1528,  0.4599,  1.5018,  1.0690, -0.3363, -1.6160]],\n",
      "\n",
      "        [[-0.8898,  0.9698,  0.1395,  0.4106, -0.1293, -1.6710],\n",
      "         [-0.4045, -1.5697,  2.1862,  0.5943, -0.8030, -0.1197]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "\n",
      "norm1(x + self_atten(x)) = \n",
      "tensor([[[-1.4108,  0.9069,  0.8426,  1.1074, -1.0435, -0.4025],\n",
      "         [-1.0092,  0.4179,  1.3399,  0.9570, -0.2866, -1.4190]],\n",
      "\n",
      "        [[-0.6704,  0.8236,  0.5057,  1.2792, -0.2267, -1.7114],\n",
      "         [-0.6960,  0.5208,  1.1369,  0.6315, -1.8652,  0.2720]],\n",
      "\n",
      "        [[-0.8816,  0.4865,  0.9296,  1.0418,  0.1590, -1.7353],\n",
      "         [-1.0092,  0.4179,  1.3399,  0.9570, -0.2866, -1.4190]],\n",
      "\n",
      "        [[-0.8021,  1.3449,  0.3862,  0.6992,  0.0759, -1.7041],\n",
      "         [-0.3251, -1.3089,  1.8622,  0.5182, -0.6616, -0.0847]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "\n",
      "feed_fwd_op =  tensor([[[-0.1909,  0.1154,  0.0457, -0.2918,  0.5686,  0.1561],\n",
      "         [-0.2542,  0.0531,  0.0634, -0.2474,  0.5450,  0.0633]],\n",
      "\n",
      "        [[-0.2410,  0.0619,  0.1066, -0.2622,  0.5184,  0.0583],\n",
      "         [-0.1494,  0.0817, -0.0463, -0.4018,  0.5950,  0.1286]],\n",
      "\n",
      "        [[-0.2675,  0.0472,  0.0965, -0.2247,  0.5287,  0.0494],\n",
      "         [-0.2542,  0.0531,  0.0634, -0.2474,  0.5450,  0.0633]],\n",
      "\n",
      "        [[-0.2653,  0.0910,  0.1020, -0.2177,  0.5005,  0.1015],\n",
      "         [-0.2152, -0.0376, -0.0307, -0.3682,  0.5895, -0.0179]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "\n",
      "norm2(norm1(x + self_atten(x)) + feed_fwd_op) = \n",
      "tensor([[[-1.7717,  1.0139,  0.8717,  0.7944, -0.5755, -0.3329],\n",
      "         [-1.2808,  0.4273,  1.3453,  0.6620,  0.2178, -1.3717]],\n",
      "\n",
      "        [[-0.9654,  0.8572,  0.5801,  0.9906,  0.2550, -1.7176],\n",
      "         [-1.0663,  0.6882,  1.2795,  0.2363, -1.5811,  0.4435]],\n",
      "\n",
      "        [[-1.1293,  0.4712,  0.9395,  0.7408,  0.6177, -1.6400],\n",
      "         [-1.2808,  0.4273,  1.3453,  0.6620,  0.2178, -1.3717]],\n",
      "\n",
      "        [[-1.0709,  1.3239,  0.4174,  0.4109,  0.5017, -1.5829],\n",
      "         [-0.5506, -1.3929,  1.9275,  0.1706, -0.0614, -0.0932]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "\n",
      "##################################\n",
      "query =  tensor([[[-1.7717,  1.0139,  0.8717,  0.7944, -0.5755, -0.3329],\n",
      "         [-1.2808,  0.4273,  1.3453,  0.6620,  0.2178, -1.3717]],\n",
      "\n",
      "        [[-0.9654,  0.8572,  0.5801,  0.9906,  0.2550, -1.7176],\n",
      "         [-1.0663,  0.6882,  1.2795,  0.2363, -1.5811,  0.4435]],\n",
      "\n",
      "        [[-1.1293,  0.4712,  0.9395,  0.7408,  0.6177, -1.6400],\n",
      "         [-1.2808,  0.4273,  1.3453,  0.6620,  0.2178, -1.3717]],\n",
      "\n",
      "        [[-1.0709,  1.3239,  0.4174,  0.4109,  0.5017, -1.5829],\n",
      "         [-0.5506, -1.3929,  1.9275,  0.1706, -0.0614, -0.0932]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "query shape =  torch.Size([4, 2, 6])\n",
      "\n",
      " _in_projection_packed \n",
      "\n",
      "Q :-  tensor([[[ 0.2289, -0.5269,  0.0316, -0.9560,  1.6078, -0.9373],\n",
      "         [ 0.4271, -0.4071, -0.6207, -0.7901,  1.4355, -0.7204]],\n",
      "\n",
      "        [[-0.1144, -0.5508, -1.1111, -0.2186,  1.2584, -0.7177],\n",
      "         [ 0.4600,  0.3031,  1.1440, -0.9863,  1.0246, -1.0800]],\n",
      "\n",
      "        [[ 0.2202, -0.5950, -1.0567, -0.5085,  1.3105, -0.5441],\n",
      "         [ 0.4271, -0.4071, -0.6207, -0.7901,  1.4355, -0.7204]],\n",
      "\n",
      "        [[ 0.0420, -0.7964, -1.0580, -0.2844,  1.2172, -0.8302],\n",
      "         [ 0.9201,  0.4826,  0.4964, -1.0440,  0.5866,  0.0615]]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "\n",
      "K :-  tensor([[[ 0.5091, -0.6628, -0.9111, -1.0284, -0.7130, -0.7506],\n",
      "         [ 0.6125, -0.4998, -0.7126, -1.1584, -0.0672, -0.4524]],\n",
      "\n",
      "        [[ 1.0458, -1.0039, -0.2222, -1.1035, -0.5919, -0.6427],\n",
      "         [ 0.2681, -0.0638, -1.2828, -0.3837, -0.4667, -0.1655]],\n",
      "\n",
      "        [[ 0.6880, -0.6849, -0.3371, -1.1733, -0.1343, -0.5221],\n",
      "         [ 0.6125, -0.4998, -0.7126, -1.1584, -0.0672, -0.4524]],\n",
      "\n",
      "        [[ 0.6396, -1.1779,  0.2599, -1.0854, -0.6629, -0.3711],\n",
      "         [-0.1664,  1.0490, -1.4975, -0.3789,  1.1908,  0.0197]]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "\n",
      "V :-  tensor([[[ 1.3087, -0.0618,  0.1470,  0.3940, -0.3864, -0.8816],\n",
      "         [ 0.9345,  0.4384, -0.5373,  0.1685,  0.0879, -0.6273]],\n",
      "\n",
      "        [[ 1.2569,  0.6041, -0.1589,  0.0177, -0.0824, -1.0547],\n",
      "         [ 0.6879, -0.7909, -0.2930, -0.0168, -0.3705, -0.1634]],\n",
      "\n",
      "        [[ 0.9768,  0.6941, -0.3513,  0.2080,  0.1214, -0.7919],\n",
      "         [ 0.9345,  0.4384, -0.5373,  0.1685,  0.0879, -0.6273]],\n",
      "\n",
      "        [[ 1.3072,  0.5238,  0.3304,  0.6100,  0.0360, -1.2280],\n",
      "         [-0.5623,  0.0331, -1.5234, -0.4351,  0.3204,  0.9106]]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "\n",
      "tgt_len =  4\n",
      "bsz =  2\n",
      "num_heads =  2\n",
      "head_dim =  3\n",
      "\n",
      "tensor([[[ 0.2289, -0.5269,  0.0316, -0.9560,  1.6078, -0.9373],\n",
      "         [ 0.4271, -0.4071, -0.6207, -0.7901,  1.4355, -0.7204]],\n",
      "\n",
      "        [[-0.1144, -0.5508, -1.1111, -0.2186,  1.2584, -0.7177],\n",
      "         [ 0.4600,  0.3031,  1.1440, -0.9863,  1.0246, -1.0800]],\n",
      "\n",
      "        [[ 0.2202, -0.5950, -1.0567, -0.5085,  1.3105, -0.5441],\n",
      "         [ 0.4271, -0.4071, -0.6207, -0.7901,  1.4355, -0.7204]],\n",
      "\n",
      "        [[ 0.0420, -0.7964, -1.0580, -0.2844,  1.2172, -0.8302],\n",
      "         [ 0.9201,  0.4826,  0.4964, -1.0440,  0.5866,  0.0615]]],\n",
      "       grad_fn=<SelectBackward0>) torch.Size([4, 2, 6])\n",
      "Q reshaped =  tensor([[[ 0.2289, -0.5269,  0.0316],\n",
      "         [-0.1144, -0.5508, -1.1111],\n",
      "         [ 0.2202, -0.5950, -1.0567],\n",
      "         [ 0.0420, -0.7964, -1.0580]],\n",
      "\n",
      "        [[-0.9560,  1.6078, -0.9373],\n",
      "         [-0.2186,  1.2584, -0.7177],\n",
      "         [-0.5085,  1.3105, -0.5441],\n",
      "         [-0.2844,  1.2172, -0.8302]],\n",
      "\n",
      "        [[ 0.4271, -0.4071, -0.6207],\n",
      "         [ 0.4600,  0.3031,  1.1440],\n",
      "         [ 0.4271, -0.4071, -0.6207],\n",
      "         [ 0.9201,  0.4826,  0.4964]],\n",
      "\n",
      "        [[-0.7901,  1.4355, -0.7204],\n",
      "         [-0.9863,  1.0246, -1.0800],\n",
      "         [-0.7901,  1.4355, -0.7204],\n",
      "         [-1.0440,  0.5866,  0.0615]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "K reshaped =  tensor([[[ 0.5091, -0.6628, -0.9111],\n",
      "         [ 1.0458, -1.0039, -0.2222],\n",
      "         [ 0.6880, -0.6849, -0.3371],\n",
      "         [ 0.6396, -1.1779,  0.2599]],\n",
      "\n",
      "        [[-1.0284, -0.7130, -0.7506],\n",
      "         [-1.1035, -0.5919, -0.6427],\n",
      "         [-1.1733, -0.1343, -0.5221],\n",
      "         [-1.0854, -0.6629, -0.3711]],\n",
      "\n",
      "        [[ 0.6125, -0.4998, -0.7126],\n",
      "         [ 0.2681, -0.0638, -1.2828],\n",
      "         [ 0.6125, -0.4998, -0.7126],\n",
      "         [-0.1664,  1.0490, -1.4975]],\n",
      "\n",
      "        [[-1.1584, -0.0672, -0.4524],\n",
      "         [-0.3837, -0.4667, -0.1655],\n",
      "         [-1.1584, -0.0672, -0.4524],\n",
      "         [-0.3789,  1.1908,  0.0197]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "V reshaped =  tensor([[[ 1.3087, -0.0618,  0.1470],\n",
      "         [ 1.2569,  0.6041, -0.1589],\n",
      "         [ 0.9768,  0.6941, -0.3513],\n",
      "         [ 1.3072,  0.5238,  0.3304]],\n",
      "\n",
      "        [[ 0.3940, -0.3864, -0.8816],\n",
      "         [ 0.0177, -0.0824, -1.0547],\n",
      "         [ 0.2080,  0.1214, -0.7919],\n",
      "         [ 0.6100,  0.0360, -1.2280]],\n",
      "\n",
      "        [[ 0.9345,  0.4384, -0.5373],\n",
      "         [ 0.6879, -0.7909, -0.2930],\n",
      "         [ 0.9345,  0.4384, -0.5373],\n",
      "         [-0.5623,  0.0331, -1.5234]],\n",
      "\n",
      "        [[ 0.1685,  0.0879, -0.6273],\n",
      "         [-0.0168, -0.3705, -0.1634],\n",
      "         [ 0.1685,  0.0879, -0.6273],\n",
      "         [-0.4351,  0.3204,  0.9106]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "torch.Size([2, 2, 4, 3]) torch.Size([2, 2, 4, 3]) torch.Size([2, 2, 4, 3])\n",
      "\n",
      "scaled_dot_product_attention\n",
      "attn_output = \n",
      "tensor([[ 1.2168,  0.4540, -0.0020,  0.2795, -0.0534, -0.9570],\n",
      "        [ 0.5670,  0.0258, -0.6699, -0.0934,  0.1307,  0.0391],\n",
      "        [ 1.2182,  0.3830, -0.0077,  0.2870, -0.0619, -0.9654],\n",
      "        [ 0.5971,  0.1097, -0.6765, -0.0314,  0.0993, -0.1189],\n",
      "        [ 1.2178,  0.3957, -0.0120,  0.2868, -0.0573, -0.9650],\n",
      "        [ 0.5670,  0.0258, -0.6699, -0.0934,  0.1307,  0.0391],\n",
      "        [ 1.2193,  0.3949, -0.0071,  0.2865, -0.0638, -0.9653],\n",
      "        [ 0.5431,  0.0753, -0.7052, -0.0271,  0.0802, -0.1301]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "\n",
      "x + self_atten(x) = \n",
      "tensor([[[-2.7295,  1.9743,  1.0621,  1.6781, -0.9712,  0.0366],\n",
      "         [-1.2818,  0.4971,  1.1215,  0.3729,  0.2047, -1.4432]],\n",
      "\n",
      "        [[-1.8767,  1.7948,  0.7438,  1.8354, -0.1953, -1.3429],\n",
      "         [-1.2260,  0.8700,  1.1641,  0.0838, -1.5842,  0.4423]],\n",
      "\n",
      "        [[-2.0519,  1.4113,  1.1071,  1.5885,  0.1795, -1.2667],\n",
      "         [-1.2818,  0.4971,  1.1215,  0.3729,  0.2047, -1.4432]],\n",
      "\n",
      "        [[-1.9888,  2.2679,  0.5877,  1.2649,  0.0584, -1.2078],\n",
      "         [-0.6831, -1.2369,  1.8303, -0.0091, -0.0647, -0.0834]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "\n",
      "norm1(x + self_atten(x)) = \n",
      "tensor([[[-1.7758,  1.1000,  0.5423,  0.9189, -0.7008, -0.0846],\n",
      "         [-1.2623,  0.6189,  1.2792,  0.4876,  0.3097, -1.4330]],\n",
      "\n",
      "        [[-1.4193,  1.1394,  0.4070,  1.1677, -0.2475, -1.0473],\n",
      "         [-1.1543,  0.8885,  1.1751,  0.1223, -1.5033,  0.4717]],\n",
      "\n",
      "        [[-1.6034,  0.9056,  0.6852,  1.0340,  0.0132, -1.0345],\n",
      "         [-1.2623,  0.6189,  1.2792,  0.4876,  0.3097, -1.4330]],\n",
      "\n",
      "        [[-1.5006,  1.4669,  0.2956,  0.7677, -0.0734, -0.9562],\n",
      "         [-0.6800, -1.2666,  1.9823,  0.0339, -0.0249, -0.0448]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "\n",
      "feed_fwd_op =  tensor([[[-0.0593, -0.1121, -0.1809,  0.2050,  0.1565, -0.1392],\n",
      "         [ 0.0470, -0.0915, -0.0840,  0.1675,  0.1555, -0.1007]],\n",
      "\n",
      "        [[ 0.0269, -0.1052, -0.1613,  0.1744,  0.1919, -0.0630],\n",
      "         [-0.1488, -0.1342, -0.1626,  0.2690,  0.1068, -0.2276]],\n",
      "\n",
      "        [[ 0.0308, -0.1002, -0.1380,  0.1627,  0.1723, -0.0791],\n",
      "         [ 0.0470, -0.0915, -0.0840,  0.1675,  0.1555, -0.1007]],\n",
      "\n",
      "        [[ 0.0295, -0.1098, -0.1862,  0.1660,  0.2287, -0.0780],\n",
      "         [-0.0569, -0.0998,  0.0266,  0.2527, -0.0309, -0.1916]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "\n",
      "norm2(norm1(x + self_atten(x)) + feed_fwd_op) = \n",
      "tensor([[[-1.8013,  1.0029,  0.3805,  1.1379, -0.5191, -0.2008],\n",
      "         [-1.2129,  0.5043,  1.1623,  0.6301,  0.4429, -1.5266]],\n",
      "\n",
      "        [[-1.3909,  1.0147,  0.2330,  1.3200, -0.0657, -1.1112],\n",
      "         [-1.3164,  0.8442,  1.1154,  0.4630, -1.4145,  0.3084]],\n",
      "\n",
      "        [[-1.5642,  0.7890,  0.5335,  1.1761,  0.1755, -1.1100],\n",
      "         [-1.2129,  0.5043,  1.1623,  0.6301,  0.4429, -1.5266]],\n",
      "\n",
      "        [[-1.4816,  1.3507,  0.1011,  0.9266,  0.1472, -1.0440],\n",
      "         [-0.6874, -1.2884,  1.9335,  0.2895, -0.0374, -0.2098]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "\n",
      "##################################\n",
      "\n",
      "################### Transformer Decoder start #####################\n",
      "\n",
      "query =  tensor([[[ 0.1129, -0.0142, -0.9221,  1.8812, -1.6048,  1.2050],\n",
      "         [ 2.4754, -0.1060, -0.1481,  1.6859,  0.3160,  1.4030]],\n",
      "\n",
      "        [[-0.7208,  1.4244, -1.1285,  2.7011,  2.0456,  1.1133],\n",
      "         [ 1.5921,  0.3095,  1.0921,  0.8848, -0.8769,  0.2026]],\n",
      "\n",
      "        [[-0.1423,  1.0984, -1.4589,  2.0058, -0.5254,  1.3244],\n",
      "         [ 0.9680,  0.2488, -0.4394,  1.6930,  0.4004,  0.4437]],\n",
      "\n",
      "        [[-0.3389,  2.8031, -0.6392,  1.0925,  0.3354,  0.7514],\n",
      "         [ 0.4631,  0.2437,  0.1359,  2.3743,  2.1152,  1.1386]],\n",
      "\n",
      "        [[-0.3823,  1.4868,  0.8619,  1.4363, -0.8054,  1.7608],\n",
      "         [ 0.9544, -0.4739, -0.8757,  1.8801, -1.6027,  1.2050]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "query shape =  torch.Size([5, 2, 6])\n",
      "\n",
      " _in_projection_packed \n",
      "\n",
      "Q :-  tensor([[[ 0.4612,  0.7150, -0.1970,  0.3807,  0.8797,  1.1237],\n",
      "         [-0.6600,  1.3372,  0.1437, -1.3862,  1.5751, -0.4920]],\n",
      "\n",
      "        [[ 1.7537,  1.5231, -1.1108, -1.7384, -0.6668,  0.1902],\n",
      "         [-1.0110,  0.8553, -0.2146, -0.6178,  1.8102, -0.4565]],\n",
      "\n",
      "        [[ 1.1574,  1.0876, -0.3296, -0.4216,  0.2208,  0.9168],\n",
      "         [ 0.1844,  0.8557, -0.3885, -0.9176,  0.6901,  0.0786]],\n",
      "\n",
      "        [[ 1.1634,  1.3648, -0.4997, -1.3201, -0.1142, -0.0148],\n",
      "         [ 0.4514,  1.4858, -0.8963, -1.8558,  0.2442, -0.5875]],\n",
      "\n",
      "        [[ 0.1803,  1.6181, -0.5739, -0.6772,  0.9390, -0.1867],\n",
      "         [ 0.0115,  0.6962,  0.0163,  0.2698,  1.2986,  0.9597]]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "\n",
      "K :-  tensor([[[ 1.8757, -0.6419,  0.5213,  0.8497,  1.5520,  0.1447],\n",
      "         [ 2.0420, -0.5998,  0.3753, -0.1979,  1.0160,  0.3778]],\n",
      "\n",
      "        [[ 1.3558, -0.3728, -0.2499, -0.6737,  1.6369,  0.0997],\n",
      "         [ 0.8788,  0.0173,  0.4889,  0.4692,  0.2791, -0.5299]],\n",
      "\n",
      "        [[ 2.2385, -0.6531,  0.3003,  0.4285,  1.7092, -0.0621],\n",
      "         [ 1.2026, -0.5522,  0.4932, -0.4256,  0.7922, -0.0192]],\n",
      "\n",
      "        [[ 1.7762,  0.0192, -0.1201,  0.3476,  1.0604, -0.9397],\n",
      "         [ 0.7588, -0.1102, -0.3465, -0.6824,  1.1521,  0.4684]],\n",
      "\n",
      "        [[ 1.4401,  0.5857, -0.6356,  1.6144,  1.4940, -0.3410],\n",
      "         [ 2.0290, -0.8308,  0.7390,  0.6302,  1.4267,  0.2994]]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "\n",
      "V :-  tensor([[[ 0.1211, -2.3318, -1.2496,  1.9385, -0.2112, -0.9854],\n",
      "         [-1.1081, -2.4076, -0.1160,  0.0640,  0.0498, -0.5188]],\n",
      "\n",
      "        [[-0.8642, -0.9783, -1.0363,  0.9860,  2.0659, -1.6061],\n",
      "         [-1.0105, -1.1840,  0.3056,  0.3156, -0.7080,  0.0466]],\n",
      "\n",
      "        [[-0.3346, -2.1076, -1.4744,  1.6842,  0.8099, -1.6885],\n",
      "         [-0.5781, -1.4158, -0.2908,  0.3486,  0.5931, -0.7173]],\n",
      "\n",
      "        [[-1.3006, -0.7710, -0.9757,  0.9040,  1.2912, -1.7897],\n",
      "         [-1.0825, -0.8964, -0.1283,  0.2918,  1.0922, -0.4472]],\n",
      "\n",
      "        [[-1.2371, -1.1791, -0.8307,  1.9020, -0.3485, -0.6405],\n",
      "         [ 0.0726, -2.7021, -1.0014,  1.5656, -0.4102, -0.7978]]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "\n",
      "tgt_len =  5\n",
      "bsz =  2\n",
      "num_heads =  2\n",
      "head_dim =  3\n",
      "\n",
      "tensor([[[ 0.4612,  0.7150, -0.1970,  0.3807,  0.8797,  1.1237],\n",
      "         [-0.6600,  1.3372,  0.1437, -1.3862,  1.5751, -0.4920]],\n",
      "\n",
      "        [[ 1.7537,  1.5231, -1.1108, -1.7384, -0.6668,  0.1902],\n",
      "         [-1.0110,  0.8553, -0.2146, -0.6178,  1.8102, -0.4565]],\n",
      "\n",
      "        [[ 1.1574,  1.0876, -0.3296, -0.4216,  0.2208,  0.9168],\n",
      "         [ 0.1844,  0.8557, -0.3885, -0.9176,  0.6901,  0.0786]],\n",
      "\n",
      "        [[ 1.1634,  1.3648, -0.4997, -1.3201, -0.1142, -0.0148],\n",
      "         [ 0.4514,  1.4858, -0.8963, -1.8558,  0.2442, -0.5875]],\n",
      "\n",
      "        [[ 0.1803,  1.6181, -0.5739, -0.6772,  0.9390, -0.1867],\n",
      "         [ 0.0115,  0.6962,  0.0163,  0.2698,  1.2986,  0.9597]]],\n",
      "       grad_fn=<SelectBackward0>) torch.Size([5, 2, 6])\n",
      "Q reshaped =  tensor([[[ 0.4612,  0.7150, -0.1970],\n",
      "         [ 1.7537,  1.5231, -1.1108],\n",
      "         [ 1.1574,  1.0876, -0.3296],\n",
      "         [ 1.1634,  1.3648, -0.4997],\n",
      "         [ 0.1803,  1.6181, -0.5739]],\n",
      "\n",
      "        [[ 0.3807,  0.8797,  1.1237],\n",
      "         [-1.7384, -0.6668,  0.1902],\n",
      "         [-0.4216,  0.2208,  0.9168],\n",
      "         [-1.3201, -0.1142, -0.0148],\n",
      "         [-0.6772,  0.9390, -0.1867]],\n",
      "\n",
      "        [[-0.6600,  1.3372,  0.1437],\n",
      "         [-1.0110,  0.8553, -0.2146],\n",
      "         [ 0.1844,  0.8557, -0.3885],\n",
      "         [ 0.4514,  1.4858, -0.8963],\n",
      "         [ 0.0115,  0.6962,  0.0163]],\n",
      "\n",
      "        [[-1.3862,  1.5751, -0.4920],\n",
      "         [-0.6178,  1.8102, -0.4565],\n",
      "         [-0.9176,  0.6901,  0.0786],\n",
      "         [-1.8558,  0.2442, -0.5875],\n",
      "         [ 0.2698,  1.2986,  0.9597]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "K reshaped =  tensor([[[ 1.8757, -0.6419,  0.5213],\n",
      "         [ 1.3558, -0.3728, -0.2499],\n",
      "         [ 2.2385, -0.6531,  0.3003],\n",
      "         [ 1.7762,  0.0192, -0.1201],\n",
      "         [ 1.4401,  0.5857, -0.6356]],\n",
      "\n",
      "        [[ 0.8497,  1.5520,  0.1447],\n",
      "         [-0.6737,  1.6369,  0.0997],\n",
      "         [ 0.4285,  1.7092, -0.0621],\n",
      "         [ 0.3476,  1.0604, -0.9397],\n",
      "         [ 1.6144,  1.4940, -0.3410]],\n",
      "\n",
      "        [[ 2.0420, -0.5998,  0.3753],\n",
      "         [ 0.8788,  0.0173,  0.4889],\n",
      "         [ 1.2026, -0.5522,  0.4932],\n",
      "         [ 0.7588, -0.1102, -0.3465],\n",
      "         [ 2.0290, -0.8308,  0.7390]],\n",
      "\n",
      "        [[-0.1979,  1.0160,  0.3778],\n",
      "         [ 0.4692,  0.2791, -0.5299],\n",
      "         [-0.4256,  0.7922, -0.0192],\n",
      "         [-0.6824,  1.1521,  0.4684],\n",
      "         [ 0.6302,  1.4267,  0.2994]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "V reshaped =  tensor([[[ 0.1211, -2.3318, -1.2496],\n",
      "         [-0.8642, -0.9783, -1.0363],\n",
      "         [-0.3346, -2.1076, -1.4744],\n",
      "         [-1.3006, -0.7710, -0.9757],\n",
      "         [-1.2371, -1.1791, -0.8307]],\n",
      "\n",
      "        [[ 1.9385, -0.2112, -0.9854],\n",
      "         [ 0.9860,  2.0659, -1.6061],\n",
      "         [ 1.6842,  0.8099, -1.6885],\n",
      "         [ 0.9040,  1.2912, -1.7897],\n",
      "         [ 1.9020, -0.3485, -0.6405]],\n",
      "\n",
      "        [[-1.1081, -2.4076, -0.1160],\n",
      "         [-1.0105, -1.1840,  0.3056],\n",
      "         [-0.5781, -1.4158, -0.2908],\n",
      "         [-1.0825, -0.8964, -0.1283],\n",
      "         [ 0.0726, -2.7021, -1.0014]],\n",
      "\n",
      "        [[ 0.0640,  0.0498, -0.5188],\n",
      "         [ 0.3156, -0.7080,  0.0466],\n",
      "         [ 0.3486,  0.5931, -0.7173],\n",
      "         [ 0.2918,  1.0922, -0.4472],\n",
      "         [ 1.5656, -0.4102, -0.7978]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "torch.Size([2, 2, 5, 3]) torch.Size([2, 2, 5, 3]) torch.Size([2, 2, 5, 3])\n",
      "\n",
      "scaled_dot_product_attention\n",
      "attn_output = \n",
      "tensor([[-0.8080, -1.4066, -1.0804,  1.5943,  0.5674, -1.2627],\n",
      "        [-0.8492, -1.4598, -0.1135,  0.4598,  0.3869, -0.5383],\n",
      "        [-0.9312, -1.3190, -1.0333,  1.2285,  1.3492, -1.5385],\n",
      "        [-0.8638, -1.4110, -0.1131,  0.5608,  0.2579, -0.5507],\n",
      "        [-0.8302, -1.4007, -1.0769,  1.4551,  0.8793, -1.3832],\n",
      "        [-0.8008, -1.6268, -0.1947,  0.4653,  0.3190, -0.5253],\n",
      "        [-0.8828, -1.3540, -1.0518,  1.2869,  1.2097, -1.5051],\n",
      "        [-0.8456, -1.5624, -0.1654,  0.3772,  0.4088, -0.5086],\n",
      "        [-0.9670, -1.2695, -1.0028,  1.3970,  0.9764, -1.4317],\n",
      "        [-0.7799, -1.6473, -0.2021,  0.6625,  0.1599, -0.5726]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "\n",
      "x =  tensor([[[ 0.1129, -0.0142, -0.9221,  1.8812, -1.6048,  1.2050],\n",
      "         [ 2.4754, -0.1060, -0.1481,  1.6859,  0.3160,  1.4030]],\n",
      "\n",
      "        [[-0.7208,  1.4244, -1.1285,  2.7011,  2.0456,  1.1133],\n",
      "         [ 1.5921,  0.3095,  1.0921,  0.8848, -0.8769,  0.2026]],\n",
      "\n",
      "        [[-0.1423,  1.0984, -1.4589,  2.0058, -0.5254,  1.3244],\n",
      "         [ 0.9680,  0.2488, -0.4394,  1.6930,  0.4004,  0.4437]],\n",
      "\n",
      "        [[-0.3389,  2.8031, -0.6392,  1.0925,  0.3354,  0.7514],\n",
      "         [ 0.4631,  0.2437,  0.1359,  2.3743,  2.1152,  1.1386]],\n",
      "\n",
      "        [[-0.3823,  1.4868,  0.8619,  1.4363, -0.8054,  1.7608],\n",
      "         [ 0.9544, -0.4739, -0.8757,  1.8801, -1.6027,  1.2050]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "\n",
      "sa_op =  tensor([[[-3.1013, -0.0255,  0.6184, -0.6706, -0.0364,  1.0810],\n",
      "         [-1.8163, -0.3504,  1.3062, -0.6329, -0.3251,  0.1342]],\n",
      "\n",
      "        [[-3.3672, -0.2941,  0.7508, -0.7519,  0.1644,  1.5081],\n",
      "         [-1.8100, -0.2304,  1.2675, -0.6262, -0.3014,  0.1404]],\n",
      "\n",
      "        [[-3.2240, -0.1414,  0.6655, -0.7006,  0.0067,  1.2211],\n",
      "         [-1.8586, -0.4376,  1.3595, -0.6738, -0.5004, -0.0239]],\n",
      "\n",
      "        [[-3.3253, -0.2521,  0.7245, -0.7387,  0.0882,  1.3983],\n",
      "         [-1.8145, -0.4844,  1.3787, -0.6829, -0.4246,  0.0548]],\n",
      "\n",
      "        [[-3.2325, -0.0995,  0.7205, -0.7373,  0.1711,  1.4055],\n",
      "         [-1.9412, -0.2687,  1.3068, -0.6459, -0.5198, -0.0562]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "\n",
      "temop =  tensor([[[-2.9884, -0.0397, -0.3037,  1.2106, -1.6412,  2.2860],\n",
      "         [ 0.6591, -0.4564,  1.1581,  1.0530, -0.0091,  1.5373]],\n",
      "\n",
      "        [[-4.0880,  1.1302, -0.3777,  1.9492,  2.2100,  2.6214],\n",
      "         [-0.2179,  0.0791,  2.3596,  0.2586, -1.1782,  0.3430]],\n",
      "\n",
      "        [[-3.3664,  0.9570, -0.7934,  1.3052, -0.5188,  2.5455],\n",
      "         [-0.8905, -0.1888,  0.9201,  1.0192, -0.0999,  0.4198]],\n",
      "\n",
      "        [[-3.6642,  2.5511,  0.0853,  0.3538,  0.4237,  2.1497],\n",
      "         [-1.3514, -0.2407,  1.5146,  1.6913,  1.6907,  1.1934]],\n",
      "\n",
      "        [[-3.6148,  1.3873,  1.5824,  0.6989, -0.6343,  3.1662],\n",
      "         [-0.9868, -0.7426,  0.4312,  1.2342, -2.1224,  1.1488]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "\n",
      "Decoder norm1(x + sa(x)) = \n",
      "tensor([[[-1.5813,  0.1190, -0.0332,  0.8399, -0.8044,  1.4600],\n",
      "         [ 0.0030, -1.6109,  0.7250,  0.5730, -0.9638,  1.2736]],\n",
      "\n",
      "        [[-2.0280,  0.2419, -0.4141,  0.5981,  0.7116,  0.8905],\n",
      "         [-0.4640, -0.1838,  1.9672, -0.0146, -1.3698,  0.0651]],\n",
      "\n",
      "        [[-1.7970,  0.4962, -0.4323,  0.6809, -0.2866,  1.3387],\n",
      "         [-1.6303, -0.5779,  1.0848,  1.2335, -0.4447,  0.3346]],\n",
      "\n",
      "        [[-1.9794,  1.1111, -0.1150,  0.0185,  0.0532,  0.9115],\n",
      "         [-1.8253, -0.8604,  0.6645,  0.8181,  0.8175,  0.3855]],\n",
      "\n",
      "        [[-1.8968,  0.4484,  0.5398,  0.1256, -0.4994,  1.2824],\n",
      "         [-0.6690, -0.4682,  0.4966,  1.1567, -1.6025,  1.0865]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "\n",
      "Memory = \n",
      "tensor([[[-1.8013,  1.0029,  0.3805,  1.1379, -0.5191, -0.2008],\n",
      "         [-1.2129,  0.5043,  1.1623,  0.6301,  0.4429, -1.5266]],\n",
      "\n",
      "        [[-1.3909,  1.0147,  0.2330,  1.3200, -0.0657, -1.1112],\n",
      "         [-1.3164,  0.8442,  1.1154,  0.4630, -1.4145,  0.3084]],\n",
      "\n",
      "        [[-1.5642,  0.7890,  0.5335,  1.1761,  0.1755, -1.1100],\n",
      "         [-1.2129,  0.5043,  1.1623,  0.6301,  0.4429, -1.5266]],\n",
      "\n",
      "        [[-1.4816,  1.3507,  0.1011,  0.9266,  0.1472, -1.0440],\n",
      "         [-0.6874, -1.2884,  1.9335,  0.2895, -0.0374, -0.2098]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "\n",
      "query =  tensor([[[-1.5813,  0.1190, -0.0332,  0.8399, -0.8044,  1.4600],\n",
      "         [ 0.0030, -1.6109,  0.7250,  0.5730, -0.9638,  1.2736]],\n",
      "\n",
      "        [[-2.0280,  0.2419, -0.4141,  0.5981,  0.7116,  0.8905],\n",
      "         [-0.4640, -0.1838,  1.9672, -0.0146, -1.3698,  0.0651]],\n",
      "\n",
      "        [[-1.7970,  0.4962, -0.4323,  0.6809, -0.2866,  1.3387],\n",
      "         [-1.6303, -0.5779,  1.0848,  1.2335, -0.4447,  0.3346]],\n",
      "\n",
      "        [[-1.9794,  1.1111, -0.1150,  0.0185,  0.0532,  0.9115],\n",
      "         [-1.8253, -0.8604,  0.6645,  0.8181,  0.8175,  0.3855]],\n",
      "\n",
      "        [[-1.8968,  0.4484,  0.5398,  0.1256, -0.4994,  1.2824],\n",
      "         [-0.6690, -0.4682,  0.4966,  1.1567, -1.6025,  1.0865]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "query shape =  torch.Size([5, 2, 6])\n",
      "\n",
      " _in_projection_packed \n",
      "\n",
      "Q :-  tensor([[[-1.5483, -0.7983, -1.2635, -0.2318, -0.9707, -0.6952],\n",
      "         [-0.7208, -1.2926, -0.3300,  0.3141, -0.7120,  0.3669]],\n",
      "\n",
      "        [[-0.8117, -0.3117, -0.5359, -1.0906, -0.3877, -0.8562],\n",
      "         [-1.5277, -0.5413, -1.6074,  1.1116, -1.4285, -0.2818]],\n",
      "\n",
      "        [[-1.3457, -0.4852, -1.0727, -0.5315, -0.6356, -0.8392],\n",
      "         [-1.1579, -0.9279, -1.1007, -0.4737, -1.6768, -0.6648]],\n",
      "\n",
      "        [[-1.4904,  0.0178, -1.3222, -0.2879, -0.4508, -1.0513],\n",
      "         [-0.5798, -0.7233, -0.3360, -1.0750, -0.9036, -0.5824]],\n",
      "\n",
      "        [[-1.9093, -0.4975, -1.6296,  0.0852, -0.8805, -0.8878],\n",
      "         [-1.3160, -1.0952, -1.1789,  0.2480, -1.3860, -0.2596]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "\n",
      "K :-  tensor([[[-0.0186, -0.5460,  0.2203,  0.6764,  0.9028, -0.1567],\n",
      "         [-0.4881,  0.1675,  0.2365, -0.1162,  0.2022, -0.6936]],\n",
      "\n",
      "        [[-0.1492, -0.3401,  0.5566,  0.3858,  0.5791, -0.1208],\n",
      "         [-0.3853, -0.7232,  0.1445,  0.0860,  0.8641, -0.1104]],\n",
      "\n",
      "        [[-0.1565, -0.1457,  0.3307,  0.4139,  0.5679, -0.3611],\n",
      "         [-0.4881,  0.1675,  0.2365, -0.1162,  0.2022, -0.6936]],\n",
      "\n",
      "        [[-0.2247, -0.3982,  0.5040,  0.3441,  0.3737, -0.3500],\n",
      "         [-0.1405,  0.6472, -0.6238, -0.0167,  0.5454, -0.4865]]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "\n",
      "V :-  tensor([[[ 0.2656,  0.3090,  0.0818, -0.9749,  0.1841,  0.9269],\n",
      "         [ 0.1486, -0.1291,  0.2575, -0.7274,  1.0670,  0.3328]],\n",
      "\n",
      "        [[ 0.0234,  0.3141,  0.2552, -0.6353,  0.4146,  0.3636],\n",
      "         [ 0.7640, -0.0023, -0.4907, -1.2351, -0.1445,  1.1570]],\n",
      "\n",
      "        [[ 0.0679,  0.2161,  0.3214, -0.6979,  0.7098,  0.5145],\n",
      "         [ 0.1486, -0.1291,  0.2575, -0.7274,  1.0670,  0.3328]],\n",
      "\n",
      "        [[-0.1433,  0.1568,  0.3225, -0.9209,  0.3016,  0.2045],\n",
      "         [ 0.8041, -0.1381, -0.1243, -0.0426,  1.3181,  1.1637]]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "\n",
      "tgt_len =  5\n",
      "bsz =  2\n",
      "num_heads =  2\n",
      "head_dim =  3\n",
      "\n",
      "tensor([[[-1.5483, -0.7983, -1.2635, -0.2318, -0.9707, -0.6952],\n",
      "         [-0.7208, -1.2926, -0.3300,  0.3141, -0.7120,  0.3669]],\n",
      "\n",
      "        [[-0.8117, -0.3117, -0.5359, -1.0906, -0.3877, -0.8562],\n",
      "         [-1.5277, -0.5413, -1.6074,  1.1116, -1.4285, -0.2818]],\n",
      "\n",
      "        [[-1.3457, -0.4852, -1.0727, -0.5315, -0.6356, -0.8392],\n",
      "         [-1.1579, -0.9279, -1.1007, -0.4737, -1.6768, -0.6648]],\n",
      "\n",
      "        [[-1.4904,  0.0178, -1.3222, -0.2879, -0.4508, -1.0513],\n",
      "         [-0.5798, -0.7233, -0.3360, -1.0750, -0.9036, -0.5824]],\n",
      "\n",
      "        [[-1.9093, -0.4975, -1.6296,  0.0852, -0.8805, -0.8878],\n",
      "         [-1.3160, -1.0952, -1.1789,  0.2480, -1.3860, -0.2596]]],\n",
      "       grad_fn=<ViewBackward0>) torch.Size([5, 2, 6])\n",
      "Q reshaped =  tensor([[[-1.5483, -0.7983, -1.2635],\n",
      "         [-0.8117, -0.3117, -0.5359],\n",
      "         [-1.3457, -0.4852, -1.0727],\n",
      "         [-1.4904,  0.0178, -1.3222],\n",
      "         [-1.9093, -0.4975, -1.6296]],\n",
      "\n",
      "        [[-0.2318, -0.9707, -0.6952],\n",
      "         [-1.0906, -0.3877, -0.8562],\n",
      "         [-0.5315, -0.6356, -0.8392],\n",
      "         [-0.2879, -0.4508, -1.0513],\n",
      "         [ 0.0852, -0.8805, -0.8878]],\n",
      "\n",
      "        [[-0.7208, -1.2926, -0.3300],\n",
      "         [-1.5277, -0.5413, -1.6074],\n",
      "         [-1.1579, -0.9279, -1.1007],\n",
      "         [-0.5798, -0.7233, -0.3360],\n",
      "         [-1.3160, -1.0952, -1.1789]],\n",
      "\n",
      "        [[ 0.3141, -0.7120,  0.3669],\n",
      "         [ 1.1116, -1.4285, -0.2818],\n",
      "         [-0.4737, -1.6768, -0.6648],\n",
      "         [-1.0750, -0.9036, -0.5824],\n",
      "         [ 0.2480, -1.3860, -0.2596]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "K reshaped =  tensor([[[-0.0186, -0.5460,  0.2203],\n",
      "         [-0.1492, -0.3401,  0.5566],\n",
      "         [-0.1565, -0.1457,  0.3307],\n",
      "         [-0.2247, -0.3982,  0.5040]],\n",
      "\n",
      "        [[ 0.6764,  0.9028, -0.1567],\n",
      "         [ 0.3858,  0.5791, -0.1208],\n",
      "         [ 0.4139,  0.5679, -0.3611],\n",
      "         [ 0.3441,  0.3737, -0.3500]],\n",
      "\n",
      "        [[-0.4881,  0.1675,  0.2365],\n",
      "         [-0.3853, -0.7232,  0.1445],\n",
      "         [-0.4881,  0.1675,  0.2365],\n",
      "         [-0.1405,  0.6472, -0.6238]],\n",
      "\n",
      "        [[-0.1162,  0.2022, -0.6936],\n",
      "         [ 0.0860,  0.8641, -0.1104],\n",
      "         [-0.1162,  0.2022, -0.6936],\n",
      "         [-0.0167,  0.5454, -0.4865]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "V reshaped =  tensor([[[ 0.2656,  0.3090,  0.0818],\n",
      "         [ 0.0234,  0.3141,  0.2552],\n",
      "         [ 0.0679,  0.2161,  0.3214],\n",
      "         [-0.1433,  0.1568,  0.3225]],\n",
      "\n",
      "        [[-0.9749,  0.1841,  0.9269],\n",
      "         [-0.6353,  0.4146,  0.3636],\n",
      "         [-0.6979,  0.7098,  0.5145],\n",
      "         [-0.9209,  0.3016,  0.2045]],\n",
      "\n",
      "        [[ 0.1486, -0.1291,  0.2575],\n",
      "         [ 0.7640, -0.0023, -0.4907],\n",
      "         [ 0.1486, -0.1291,  0.2575],\n",
      "         [ 0.8041, -0.1381, -0.1243]],\n",
      "\n",
      "        [[-0.7274,  1.0670,  0.3328],\n",
      "         [-1.2351, -0.1445,  1.1570],\n",
      "         [-0.7274,  1.0670,  0.3328],\n",
      "         [-0.0426,  1.3181,  1.1637]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "torch.Size([2, 2, 5, 3]) torch.Size([2, 2, 4, 3]) torch.Size([2, 2, 4, 3])\n",
      "\n",
      "scaled_dot_product_attention\n",
      "attn_output = \n",
      "tensor([[ 0.0593,  0.2488,  0.2400, -0.8035,  0.4142,  0.4667],\n",
      "        [ 0.5031, -0.0783, -0.1093, -0.6806,  0.8441,  0.7269],\n",
      "        [ 0.0547,  0.2485,  0.2439, -0.8014,  0.4170,  0.4660],\n",
      "        [ 0.5153, -0.0965, -0.0646, -0.6601,  0.9095,  0.6703],\n",
      "        [ 0.0573,  0.2483,  0.2422, -0.8032,  0.4150,  0.4693],\n",
      "        [ 0.5086, -0.0877, -0.0858, -0.6488,  0.9616,  0.6167],\n",
      "        [ 0.0565,  0.2473,  0.2449, -0.8051,  0.4138,  0.4770],\n",
      "        [ 0.4850, -0.0887, -0.0681, -0.6530,  0.9322,  0.6507],\n",
      "        [ 0.0591,  0.2479,  0.2416, -0.8054,  0.4131,  0.4733],\n",
      "        [ 0.5133, -0.0848, -0.0969, -0.6578,  0.9201,  0.6594]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "\n",
      "Decoder norm2(x + mha(x, mem)) = \n",
      "tensor([[[-1.7184,  0.2625,  0.6900,  0.6839, -0.9667,  1.0486],\n",
      "         [ 0.0114, -1.2258,  1.0978, -0.1957, -1.1097,  1.4221]],\n",
      "\n",
      "        [[-2.2195,  0.3816,  0.4199,  0.5303,  0.2482,  0.6396],\n",
      "         [-0.3012, -0.0654,  1.9101, -0.6203, -1.3118,  0.3886]],\n",
      "\n",
      "        [[-1.9638,  0.5798,  0.3919,  0.5807, -0.5790,  0.9906],\n",
      "         [-1.4108, -0.4309,  1.5861,  0.3590, -0.8179,  0.7144]],\n",
      "\n",
      "        [[-2.0206,  1.0327,  0.6262,  0.0342, -0.2854,  0.6129],\n",
      "         [-1.7159, -0.7621,  1.2955, -0.0034,  0.3766,  0.8092]],\n",
      "\n",
      "        [[-1.8460,  0.4876,  1.0732,  0.1115, -0.6797,  0.8534],\n",
      "         [-0.5101, -0.3086,  0.9582,  0.2669, -1.7152,  1.3089]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "\n",
      "Decoder norm3(x + ff(x)) = \n",
      "tensor([[[-1.8376,  0.2066,  0.8120,  0.3597, -0.6886,  1.1479],\n",
      "         [-0.3593, -1.1350,  1.1816, -0.4914, -0.7429,  1.5470]],\n",
      "\n",
      "        [[-2.1931,  0.3093,  0.6172,  0.2265,  0.2733,  0.7667],\n",
      "         [-0.7067, -0.0661,  1.8546, -0.8206, -0.9519,  0.6907]],\n",
      "\n",
      "        [[-2.0057,  0.4771,  0.5809,  0.2709, -0.4064,  1.0832],\n",
      "         [-1.6020, -0.3706,  1.5085,  0.0534, -0.4778,  0.8885]],\n",
      "\n",
      "        [[-2.0027,  0.8513,  0.7694, -0.1987, -0.1916,  0.7723],\n",
      "         [-1.7140, -0.6242,  1.2498, -0.2820,  0.4710,  0.8994]],\n",
      "\n",
      "        [[-1.8676,  0.3952,  1.0919, -0.1513, -0.4409,  0.9727],\n",
      "         [-0.8788, -0.3061,  1.0722, -0.0086, -1.3494,  1.4708]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "\n",
      "\n",
      "################### Transformer Decoder start #####################\n",
      "\n",
      "query =  tensor([[[-1.8376,  0.2066,  0.8120,  0.3597, -0.6886,  1.1479],\n",
      "         [-0.3593, -1.1350,  1.1816, -0.4914, -0.7429,  1.5470]],\n",
      "\n",
      "        [[-2.1931,  0.3093,  0.6172,  0.2265,  0.2733,  0.7667],\n",
      "         [-0.7067, -0.0661,  1.8546, -0.8206, -0.9519,  0.6907]],\n",
      "\n",
      "        [[-2.0057,  0.4771,  0.5809,  0.2709, -0.4064,  1.0832],\n",
      "         [-1.6020, -0.3706,  1.5085,  0.0534, -0.4778,  0.8885]],\n",
      "\n",
      "        [[-2.0027,  0.8513,  0.7694, -0.1987, -0.1916,  0.7723],\n",
      "         [-1.7140, -0.6242,  1.2498, -0.2820,  0.4710,  0.8994]],\n",
      "\n",
      "        [[-1.8676,  0.3952,  1.0919, -0.1513, -0.4409,  0.9727],\n",
      "         [-0.8788, -0.3061,  1.0722, -0.0086, -1.3494,  1.4708]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "query shape =  torch.Size([5, 2, 6])\n",
      "\n",
      " _in_projection_packed \n",
      "\n",
      "Q :-  tensor([[[-0.3478,  0.2786, -1.0721,  1.4186, -0.3654,  0.1426],\n",
      "         [ 0.6535,  0.8112, -0.2569,  0.9452,  0.2540, -0.4830]],\n",
      "\n",
      "        [[ 0.1248, -0.1658, -1.3525,  1.3630, -0.6031,  0.4038],\n",
      "         [ 0.4043,  0.3611, -0.6321,  1.1292,  0.3858,  0.1274]],\n",
      "\n",
      "        [[-0.3311, -0.0108, -1.2459,  1.3252, -0.3824,  0.3720],\n",
      "         [ 0.3170,  0.5157, -0.9179,  1.5812, -0.3431, -0.0938]],\n",
      "\n",
      "        [[-0.1205, -0.3663, -1.4137,  1.2297, -0.1969,  0.7153],\n",
      "         [ 1.0118,  0.1882, -1.0864,  1.4362, -0.4261,  0.0270]],\n",
      "\n",
      "        [[ 0.0043,  0.0022, -1.2550,  1.4060, -0.1895,  0.4129],\n",
      "         [-0.2559,  0.7118, -0.5207,  1.1559,  0.1304, -0.2180]]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "\n",
      "K :-  tensor([[[-0.3217,  0.1830,  0.0944,  0.1322,  0.1059,  0.8411],\n",
      "         [-0.2391, -1.0596,  0.5661, -0.0041,  0.0509,  0.7262]],\n",
      "\n",
      "        [[-0.2553,  0.9009,  0.0822, -0.3813,  0.5637,  0.4291],\n",
      "         [-0.6948, -0.1852,  0.9850, -0.0380,  0.2270,  0.7053]],\n",
      "\n",
      "        [[-0.2668,  0.4474,  0.0538,  0.0729,  0.1125,  0.5715],\n",
      "         [-0.5111,  0.1938,  0.4657, -0.2930,  0.5855,  1.0287]],\n",
      "\n",
      "        [[-0.3970,  0.8009,  0.3342, -0.0709,  0.2176,  0.2650],\n",
      "         [-0.3480,  0.4931,  0.5029, -0.7766,  0.9277,  0.5393]],\n",
      "\n",
      "        [[-0.4534,  0.4555,  0.4167, -0.0754,  0.2723,  0.5854],\n",
      "         [-0.3414, -0.7848,  0.3403,  0.4403, -0.2648,  0.9690]]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "\n",
      "V :-  tensor([[[-0.4277, -0.9453, -0.5253, -0.0719,  0.4101,  0.2127],\n",
      "         [-0.8145, -1.0011, -0.8979, -0.5119,  0.1173,  0.0495]],\n",
      "\n",
      "        [[-0.7016, -0.9931, -0.4352,  0.0152,  0.4791, -0.0197],\n",
      "         [ 0.0040, -1.2409, -0.2573, -0.4151,  0.6063,  0.3816]],\n",
      "\n",
      "        [[-0.4234, -0.8310, -0.4350,  0.0860,  0.3326,  0.1743],\n",
      "         [-0.6269, -1.4087, -0.6050, -0.4431,  0.7253,  0.1297]],\n",
      "\n",
      "        [[-0.2043, -0.8783, -0.1826,  0.1643,  0.4084,  0.2276],\n",
      "         [-1.1349, -1.4296, -0.7276, -0.4018,  0.5964, -0.1695]],\n",
      "\n",
      "        [[-0.3598, -1.0998, -0.3859, -0.0601,  0.4966,  0.2286],\n",
      "         [-0.2932, -0.8668, -0.6497, -0.2730,  0.2174,  0.3255]]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "\n",
      "tgt_len =  5\n",
      "bsz =  2\n",
      "num_heads =  2\n",
      "head_dim =  3\n",
      "\n",
      "tensor([[[-0.3478,  0.2786, -1.0721,  1.4186, -0.3654,  0.1426],\n",
      "         [ 0.6535,  0.8112, -0.2569,  0.9452,  0.2540, -0.4830]],\n",
      "\n",
      "        [[ 0.1248, -0.1658, -1.3525,  1.3630, -0.6031,  0.4038],\n",
      "         [ 0.4043,  0.3611, -0.6321,  1.1292,  0.3858,  0.1274]],\n",
      "\n",
      "        [[-0.3311, -0.0108, -1.2459,  1.3252, -0.3824,  0.3720],\n",
      "         [ 0.3170,  0.5157, -0.9179,  1.5812, -0.3431, -0.0938]],\n",
      "\n",
      "        [[-0.1205, -0.3663, -1.4137,  1.2297, -0.1969,  0.7153],\n",
      "         [ 1.0118,  0.1882, -1.0864,  1.4362, -0.4261,  0.0270]],\n",
      "\n",
      "        [[ 0.0043,  0.0022, -1.2550,  1.4060, -0.1895,  0.4129],\n",
      "         [-0.2559,  0.7118, -0.5207,  1.1559,  0.1304, -0.2180]]],\n",
      "       grad_fn=<SelectBackward0>) torch.Size([5, 2, 6])\n",
      "Q reshaped =  tensor([[[-0.3478,  0.2786, -1.0721],\n",
      "         [ 0.1248, -0.1658, -1.3525],\n",
      "         [-0.3311, -0.0108, -1.2459],\n",
      "         [-0.1205, -0.3663, -1.4137],\n",
      "         [ 0.0043,  0.0022, -1.2550]],\n",
      "\n",
      "        [[ 1.4186, -0.3654,  0.1426],\n",
      "         [ 1.3630, -0.6031,  0.4038],\n",
      "         [ 1.3252, -0.3824,  0.3720],\n",
      "         [ 1.2297, -0.1969,  0.7153],\n",
      "         [ 1.4060, -0.1895,  0.4129]],\n",
      "\n",
      "        [[ 0.6535,  0.8112, -0.2569],\n",
      "         [ 0.4043,  0.3611, -0.6321],\n",
      "         [ 0.3170,  0.5157, -0.9179],\n",
      "         [ 1.0118,  0.1882, -1.0864],\n",
      "         [-0.2559,  0.7118, -0.5207]],\n",
      "\n",
      "        [[ 0.9452,  0.2540, -0.4830],\n",
      "         [ 1.1292,  0.3858,  0.1274],\n",
      "         [ 1.5812, -0.3431, -0.0938],\n",
      "         [ 1.4362, -0.4261,  0.0270],\n",
      "         [ 1.1559,  0.1304, -0.2180]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "K reshaped =  tensor([[[-0.3217,  0.1830,  0.0944],\n",
      "         [-0.2553,  0.9009,  0.0822],\n",
      "         [-0.2668,  0.4474,  0.0538],\n",
      "         [-0.3970,  0.8009,  0.3342],\n",
      "         [-0.4534,  0.4555,  0.4167]],\n",
      "\n",
      "        [[ 0.1322,  0.1059,  0.8411],\n",
      "         [-0.3813,  0.5637,  0.4291],\n",
      "         [ 0.0729,  0.1125,  0.5715],\n",
      "         [-0.0709,  0.2176,  0.2650],\n",
      "         [-0.0754,  0.2723,  0.5854]],\n",
      "\n",
      "        [[-0.2391, -1.0596,  0.5661],\n",
      "         [-0.6948, -0.1852,  0.9850],\n",
      "         [-0.5111,  0.1938,  0.4657],\n",
      "         [-0.3480,  0.4931,  0.5029],\n",
      "         [-0.3414, -0.7848,  0.3403]],\n",
      "\n",
      "        [[-0.0041,  0.0509,  0.7262],\n",
      "         [-0.0380,  0.2270,  0.7053],\n",
      "         [-0.2930,  0.5855,  1.0287],\n",
      "         [-0.7766,  0.9277,  0.5393],\n",
      "         [ 0.4403, -0.2648,  0.9690]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "V reshaped =  tensor([[[-0.4277, -0.9453, -0.5253],\n",
      "         [-0.7016, -0.9931, -0.4352],\n",
      "         [-0.4234, -0.8310, -0.4350],\n",
      "         [-0.2043, -0.8783, -0.1826],\n",
      "         [-0.3598, -1.0998, -0.3859]],\n",
      "\n",
      "        [[-0.0719,  0.4101,  0.2127],\n",
      "         [ 0.0152,  0.4791, -0.0197],\n",
      "         [ 0.0860,  0.3326,  0.1743],\n",
      "         [ 0.1643,  0.4084,  0.2276],\n",
      "         [-0.0601,  0.4966,  0.2286]],\n",
      "\n",
      "        [[-0.8145, -1.0011, -0.8979],\n",
      "         [ 0.0040, -1.2409, -0.2573],\n",
      "         [-0.6269, -1.4087, -0.6050],\n",
      "         [-1.1349, -1.4296, -0.7276],\n",
      "         [-0.2932, -0.8668, -0.6497]],\n",
      "\n",
      "        [[-0.5119,  0.1173,  0.0495],\n",
      "         [-0.4151,  0.6063,  0.3816],\n",
      "         [-0.4431,  0.7253,  0.1297],\n",
      "         [-0.4018,  0.5964, -0.1695],\n",
      "         [-0.2730,  0.2174,  0.3255]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "torch.Size([2, 2, 5, 3]) torch.Size([2, 2, 5, 3]) torch.Size([2, 2, 5, 3])\n",
      "\n",
      "scaled_dot_product_attention\n",
      "attn_output = \n",
      "tensor([[-0.4328, -0.9460, -0.3958,  0.0241,  0.4187,  0.1772],\n",
      "        [-0.6379, -1.2397, -0.6300, -0.4044,  0.4312,  0.1636],\n",
      "        [-0.4345, -0.9439, -0.4032,  0.0220,  0.4179,  0.1787],\n",
      "        [-0.6145, -1.2080, -0.6384, -0.4018,  0.4298,  0.1714],\n",
      "        [-0.4324, -0.9452, -0.3999,  0.0223,  0.4189,  0.1772],\n",
      "        [-0.6272, -1.2183, -0.6390, -0.3905,  0.3840,  0.2032],\n",
      "        [-0.4325, -0.9440, -0.4051,  0.0195,  0.4196,  0.1763],\n",
      "        [-0.6255, -1.1875, -0.6572, -0.3908,  0.3852,  0.2022],\n",
      "        [-0.4342, -0.9445, -0.4006,  0.0218,  0.4192,  0.1767],\n",
      "        [-0.6167, -1.2377, -0.6211, -0.4004,  0.4183,  0.1769]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "\n",
      "x =  tensor([[[-1.8376,  0.2066,  0.8120,  0.3597, -0.6886,  1.1479],\n",
      "         [-0.3593, -1.1350,  1.1816, -0.4914, -0.7429,  1.5470]],\n",
      "\n",
      "        [[-2.1931,  0.3093,  0.6172,  0.2265,  0.2733,  0.7667],\n",
      "         [-0.7067, -0.0661,  1.8546, -0.8206, -0.9519,  0.6907]],\n",
      "\n",
      "        [[-2.0057,  0.4771,  0.5809,  0.2709, -0.4064,  1.0832],\n",
      "         [-1.6020, -0.3706,  1.5085,  0.0534, -0.4778,  0.8885]],\n",
      "\n",
      "        [[-2.0027,  0.8513,  0.7694, -0.1987, -0.1916,  0.7723],\n",
      "         [-1.7140, -0.6242,  1.2498, -0.2820,  0.4710,  0.8994]],\n",
      "\n",
      "        [[-1.8676,  0.3952,  1.0919, -0.1513, -0.4409,  0.9727],\n",
      "         [-0.8788, -0.3061,  1.0722, -0.0086, -1.3494,  1.4708]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "\n",
      "sa_op =  tensor([[[-0.7885, -0.0402, -0.4081, -0.0645, -0.7562,  0.1027],\n",
      "         [-0.9512,  0.2274, -0.7572, -0.2511, -1.0914,  0.3876]],\n",
      "\n",
      "        [[-0.7904, -0.0378, -0.4106, -0.0687, -0.7603,  0.1048],\n",
      "         [-0.9350,  0.2187, -0.7617, -0.2587, -1.0923,  0.3841]],\n",
      "\n",
      "        [[-0.7900, -0.0387, -0.4107, -0.0670, -0.7589,  0.1049],\n",
      "         [-0.9313,  0.2154, -0.7577, -0.2502, -1.0798,  0.3760]],\n",
      "\n",
      "        [[-0.7921, -0.0361, -0.4136, -0.0704, -0.7623,  0.1079],\n",
      "         [-0.9234,  0.2193, -0.7509, -0.2630, -1.0803,  0.3747]],\n",
      "\n",
      "        [[-0.7900, -0.0377, -0.4096, -0.0676, -0.7589,  0.1047],\n",
      "         [-0.9411,  0.2158, -0.7679, -0.2453, -1.0878,  0.3856]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "\n",
      "temop =  tensor([[[-2.6261,  0.1664,  0.4039,  0.2952, -1.4449,  1.2506],\n",
      "         [-1.3104, -0.9076,  0.4244, -0.7425, -1.8342,  1.9346]],\n",
      "\n",
      "        [[-2.9834,  0.2715,  0.2066,  0.1579, -0.4870,  0.8715],\n",
      "         [-1.6417,  0.1526,  1.0929, -1.0793, -2.0442,  1.0748]],\n",
      "\n",
      "        [[-2.7958,  0.4384,  0.1702,  0.2039, -1.1654,  1.1881],\n",
      "         [-2.5333, -0.1552,  0.7507, -0.1968, -1.5576,  1.2646]],\n",
      "\n",
      "        [[-2.7948,  0.8152,  0.3558, -0.2691, -0.9539,  0.8802],\n",
      "         [-2.6375, -0.4048,  0.4989, -0.5451, -0.6093,  1.2741]],\n",
      "\n",
      "        [[-2.6576,  0.3575,  0.6823, -0.2189, -1.1998,  1.0775],\n",
      "         [-1.8200, -0.0903,  0.3043, -0.2539, -2.4372,  1.8564]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "\n",
      "Decoder norm1(x + sa(x)) = \n",
      "tensor([[[-1.7652e+00,  3.7769e-01,  5.5994e-01,  4.7656e-01, -8.5874e-01,\n",
      "           1.2097e+00],\n",
      "         [-7.2298e-01, -4.0095e-01,  6.6372e-01, -2.6902e-01, -1.1417e+00,\n",
      "           1.8709e+00]],\n",
      "\n",
      "        [[-2.1226e+00,  4.7836e-01,  4.2653e-01,  3.8757e-01, -1.2769e-01,\n",
      "           9.5779e-01],\n",
      "         [-9.8525e-01,  4.4707e-01,  1.1978e+00, -5.3629e-01, -1.3065e+00,\n",
      "           1.1833e+00]],\n",
      "\n",
      "        [[-1.8929e+00,  5.8660e-01,  3.8100e-01,  4.0685e-01, -6.4293e-01,\n",
      "           1.1614e+00],\n",
      "         [-1.6417e+00,  1.9233e-01,  8.9105e-01,  1.6024e-01, -8.8923e-01,\n",
      "           1.2873e+00]],\n",
      "\n",
      "        [[-1.9388e+00,  8.9826e-01,  5.3720e-01,  4.6108e-02, -4.9206e-01,\n",
      "           9.4932e-01],\n",
      "         [-1.8568e+00, -7.4154e-04,  7.5057e-01, -1.1734e-01, -1.7073e-01,\n",
      "           1.3950e+00]],\n",
      "\n",
      "        [[-1.8367e+00,  5.3891e-01,  7.9485e-01,  8.4788e-02, -6.8804e-01,\n",
      "           1.1062e+00],\n",
      "         [-1.0047e+00,  2.2500e-01,  5.0556e-01,  1.0867e-01, -1.4435e+00,\n",
      "           1.6090e+00]]], grad_fn=<NativeLayerNormBackward0>)\n",
      "\n",
      "Memory = \n",
      "tensor([[[-1.8013,  1.0029,  0.3805,  1.1379, -0.5191, -0.2008],\n",
      "         [-1.2129,  0.5043,  1.1623,  0.6301,  0.4429, -1.5266]],\n",
      "\n",
      "        [[-1.3909,  1.0147,  0.2330,  1.3200, -0.0657, -1.1112],\n",
      "         [-1.3164,  0.8442,  1.1154,  0.4630, -1.4145,  0.3084]],\n",
      "\n",
      "        [[-1.5642,  0.7890,  0.5335,  1.1761,  0.1755, -1.1100],\n",
      "         [-1.2129,  0.5043,  1.1623,  0.6301,  0.4429, -1.5266]],\n",
      "\n",
      "        [[-1.4816,  1.3507,  0.1011,  0.9266,  0.1472, -1.0440],\n",
      "         [-0.6874, -1.2884,  1.9335,  0.2895, -0.0374, -0.2098]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "\n",
      "query =  tensor([[[-1.7652e+00,  3.7769e-01,  5.5994e-01,  4.7656e-01, -8.5874e-01,\n",
      "           1.2097e+00],\n",
      "         [-7.2298e-01, -4.0095e-01,  6.6372e-01, -2.6902e-01, -1.1417e+00,\n",
      "           1.8709e+00]],\n",
      "\n",
      "        [[-2.1226e+00,  4.7836e-01,  4.2653e-01,  3.8757e-01, -1.2769e-01,\n",
      "           9.5779e-01],\n",
      "         [-9.8525e-01,  4.4707e-01,  1.1978e+00, -5.3629e-01, -1.3065e+00,\n",
      "           1.1833e+00]],\n",
      "\n",
      "        [[-1.8929e+00,  5.8660e-01,  3.8100e-01,  4.0685e-01, -6.4293e-01,\n",
      "           1.1614e+00],\n",
      "         [-1.6417e+00,  1.9233e-01,  8.9105e-01,  1.6024e-01, -8.8923e-01,\n",
      "           1.2873e+00]],\n",
      "\n",
      "        [[-1.9388e+00,  8.9826e-01,  5.3720e-01,  4.6108e-02, -4.9206e-01,\n",
      "           9.4932e-01],\n",
      "         [-1.8568e+00, -7.4154e-04,  7.5057e-01, -1.1734e-01, -1.7073e-01,\n",
      "           1.3950e+00]],\n",
      "\n",
      "        [[-1.8367e+00,  5.3891e-01,  7.9485e-01,  8.4788e-02, -6.8804e-01,\n",
      "           1.1062e+00],\n",
      "         [-1.0047e+00,  2.2500e-01,  5.0556e-01,  1.0867e-01, -1.4435e+00,\n",
      "           1.6090e+00]]], grad_fn=<NativeLayerNormBackward0>)\n",
      "query shape =  torch.Size([5, 2, 6])\n",
      "\n",
      " _in_projection_packed \n",
      "\n",
      "Q :-  tensor([[[ 1.8945, -0.8742,  0.7459,  1.0035,  0.9831,  0.1729],\n",
      "         [ 1.7493, -0.8848,  0.7593,  0.5973,  1.2324,  0.8940]],\n",
      "\n",
      "        [[ 1.6718, -0.6849,  0.6293,  1.0737,  0.7489,  0.1074],\n",
      "         [ 2.1463, -1.1228,  0.5644,  0.2173,  1.4744,  0.8025]],\n",
      "\n",
      "        [[ 1.9135, -0.8638,  0.6521,  0.9876,  0.9952,  0.0410],\n",
      "         [ 1.9608, -0.9215,  0.7820,  0.8706,  1.0898,  0.5375]],\n",
      "\n",
      "        [[ 2.0325, -0.9291,  0.5051,  0.7449,  1.1226,  0.1096],\n",
      "         [ 1.7776, -0.7533,  0.7360,  0.9552,  0.9570,  0.7185]],\n",
      "\n",
      "        [[ 2.0290, -0.9388,  0.6615,  0.8139,  1.1172,  0.3713],\n",
      "         [ 2.0108, -1.0326,  0.7011,  0.6523,  1.3201,  0.3726]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "\n",
      "K :-  tensor([[[-0.7890, -0.7910,  0.3627,  0.9334, -0.8464, -0.4720],\n",
      "         [ 0.3582, -0.3951,  0.8824,  0.8271, -1.3125, -0.3293]],\n",
      "\n",
      "        [[-0.6541, -0.5877,  0.8497,  1.1986, -0.9939, -0.2692],\n",
      "         [-0.6886, -0.4561, -0.5103,  0.7059, -1.0208, -1.0201]],\n",
      "\n",
      "        [[-0.2970, -0.6666,  0.9204,  1.0127, -1.1084, -0.2972],\n",
      "         [ 0.3582, -0.3951,  0.8824,  0.8271, -1.3125, -0.3293]],\n",
      "\n",
      "        [[-0.6124, -0.5058,  0.7181,  1.1560, -0.5249,  0.1625],\n",
      "         [ 1.1311, -0.4773,  0.3183, -0.3564, -1.8216, -1.3830]]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "\n",
      "V :-  tensor([[[ 0.2817, -0.2321,  0.3553, -0.4018, -0.5195, -0.1440],\n",
      "         [-0.0805,  0.5336,  0.7379,  0.6010, -0.1542,  0.4101]],\n",
      "\n",
      "        [[ 0.2458,  0.1228,  0.2894, -0.1521, -0.5553,  0.1613],\n",
      "         [ 0.3168,  0.0436,  0.2456, -0.4081, -0.2836, -0.2422]],\n",
      "\n",
      "        [[ 0.1086,  0.1445,  0.5631,  0.0885, -0.4423,  0.2221],\n",
      "         [-0.0805,  0.5336,  0.7379,  0.6010, -0.1542,  0.4101]],\n",
      "\n",
      "        [[ 0.2637, -0.1023,  0.0586, -0.1636, -0.4475, -0.1032],\n",
      "         [-0.4616,  0.7337,  1.5562,  0.8964,  0.1685,  0.7602]]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "\n",
      "tgt_len =  5\n",
      "bsz =  2\n",
      "num_heads =  2\n",
      "head_dim =  3\n",
      "\n",
      "tensor([[[ 1.8945, -0.8742,  0.7459,  1.0035,  0.9831,  0.1729],\n",
      "         [ 1.7493, -0.8848,  0.7593,  0.5973,  1.2324,  0.8940]],\n",
      "\n",
      "        [[ 1.6718, -0.6849,  0.6293,  1.0737,  0.7489,  0.1074],\n",
      "         [ 2.1463, -1.1228,  0.5644,  0.2173,  1.4744,  0.8025]],\n",
      "\n",
      "        [[ 1.9135, -0.8638,  0.6521,  0.9876,  0.9952,  0.0410],\n",
      "         [ 1.9608, -0.9215,  0.7820,  0.8706,  1.0898,  0.5375]],\n",
      "\n",
      "        [[ 2.0325, -0.9291,  0.5051,  0.7449,  1.1226,  0.1096],\n",
      "         [ 1.7776, -0.7533,  0.7360,  0.9552,  0.9570,  0.7185]],\n",
      "\n",
      "        [[ 2.0290, -0.9388,  0.6615,  0.8139,  1.1172,  0.3713],\n",
      "         [ 2.0108, -1.0326,  0.7011,  0.6523,  1.3201,  0.3726]]],\n",
      "       grad_fn=<ViewBackward0>) torch.Size([5, 2, 6])\n",
      "Q reshaped =  tensor([[[ 1.8945, -0.8742,  0.7459],\n",
      "         [ 1.6718, -0.6849,  0.6293],\n",
      "         [ 1.9135, -0.8638,  0.6521],\n",
      "         [ 2.0325, -0.9291,  0.5051],\n",
      "         [ 2.0290, -0.9388,  0.6615]],\n",
      "\n",
      "        [[ 1.0035,  0.9831,  0.1729],\n",
      "         [ 1.0737,  0.7489,  0.1074],\n",
      "         [ 0.9876,  0.9952,  0.0410],\n",
      "         [ 0.7449,  1.1226,  0.1096],\n",
      "         [ 0.8139,  1.1172,  0.3713]],\n",
      "\n",
      "        [[ 1.7493, -0.8848,  0.7593],\n",
      "         [ 2.1463, -1.1228,  0.5644],\n",
      "         [ 1.9608, -0.9215,  0.7820],\n",
      "         [ 1.7776, -0.7533,  0.7360],\n",
      "         [ 2.0108, -1.0326,  0.7011]],\n",
      "\n",
      "        [[ 0.5973,  1.2324,  0.8940],\n",
      "         [ 0.2173,  1.4744,  0.8025],\n",
      "         [ 0.8706,  1.0898,  0.5375],\n",
      "         [ 0.9552,  0.9570,  0.7185],\n",
      "         [ 0.6523,  1.3201,  0.3726]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "K reshaped =  tensor([[[-0.7890, -0.7910,  0.3627],\n",
      "         [-0.6541, -0.5877,  0.8497],\n",
      "         [-0.2970, -0.6666,  0.9204],\n",
      "         [-0.6124, -0.5058,  0.7181]],\n",
      "\n",
      "        [[ 0.9334, -0.8464, -0.4720],\n",
      "         [ 1.1986, -0.9939, -0.2692],\n",
      "         [ 1.0127, -1.1084, -0.2972],\n",
      "         [ 1.1560, -0.5249,  0.1625]],\n",
      "\n",
      "        [[ 0.3582, -0.3951,  0.8824],\n",
      "         [-0.6886, -0.4561, -0.5103],\n",
      "         [ 0.3582, -0.3951,  0.8824],\n",
      "         [ 1.1311, -0.4773,  0.3183]],\n",
      "\n",
      "        [[ 0.8271, -1.3125, -0.3293],\n",
      "         [ 0.7059, -1.0208, -1.0201],\n",
      "         [ 0.8271, -1.3125, -0.3293],\n",
      "         [-0.3564, -1.8216, -1.3830]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "V reshaped =  tensor([[[ 0.2817, -0.2321,  0.3553],\n",
      "         [ 0.2458,  0.1228,  0.2894],\n",
      "         [ 0.1086,  0.1445,  0.5631],\n",
      "         [ 0.2637, -0.1023,  0.0586]],\n",
      "\n",
      "        [[-0.4018, -0.5195, -0.1440],\n",
      "         [-0.1521, -0.5553,  0.1613],\n",
      "         [ 0.0885, -0.4423,  0.2221],\n",
      "         [-0.1636, -0.4475, -0.1032]],\n",
      "\n",
      "        [[-0.0805,  0.5336,  0.7379],\n",
      "         [ 0.3168,  0.0436,  0.2456],\n",
      "         [-0.0805,  0.5336,  0.7379],\n",
      "         [-0.4616,  0.7337,  1.5562]],\n",
      "\n",
      "        [[ 0.6010, -0.1542,  0.4101],\n",
      "         [-0.4081, -0.2836, -0.2422],\n",
      "         [ 0.6010, -0.1542,  0.4101],\n",
      "         [ 0.8964,  0.1685,  0.7602]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "torch.Size([2, 2, 5, 3]) torch.Size([2, 2, 4, 3]) torch.Size([2, 2, 4, 3])\n",
      "\n",
      "scaled_dot_product_attention\n",
      "attn_output = \n",
      "tensor([[ 0.2057,  0.0174,  0.3511, -0.1623, -0.4889,  0.0193],\n",
      "        [-0.2316,  0.5991,  1.0800,  0.3572, -0.1607,  0.2663],\n",
      "        [ 0.2084,  0.0130,  0.3458, -0.1605, -0.4898,  0.0238],\n",
      "        [-0.2596,  0.6162,  1.1369,  0.3483, -0.1566,  0.2635],\n",
      "        [ 0.2062,  0.0161,  0.3506, -0.1631, -0.4894,  0.0199],\n",
      "        [-0.2443,  0.6081,  1.1044,  0.3400, -0.1620,  0.2558],\n",
      "        [ 0.2061,  0.0150,  0.3516, -0.1645, -0.4888,  0.0169],\n",
      "        [-0.2331,  0.6000,  1.0831,  0.3549, -0.1623,  0.2642],\n",
      "        [ 0.2051,  0.0175,  0.3528, -0.1628, -0.4878,  0.0154],\n",
      "        [-0.2490,  0.6104,  1.1146,  0.3238, -0.1605,  0.2472]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "\n",
      "Decoder norm2(x + mha(x, mem)) = \n",
      "tensor([[[-1.8856,  0.7937,  0.5414, -0.0978, -0.4827,  1.1309],\n",
      "         [-1.4081,  0.0361,  0.9449, -0.9894, -0.0471,  1.4636]],\n",
      "\n",
      "        [[-2.0912,  0.8375,  0.3829, -0.1699,  0.1975,  0.8432],\n",
      "         [-1.4379,  0.8009,  1.3086, -1.1021, -0.1528,  0.5834]],\n",
      "\n",
      "        [[-1.9378,  0.9593,  0.3575, -0.1590, -0.2685,  1.0485],\n",
      "         [-1.9880,  0.5594,  1.0117, -0.4587,  0.1953,  0.6803]],\n",
      "\n",
      "        [[-1.8730,  1.1773,  0.4751, -0.4633, -0.1227,  0.8065],\n",
      "         [-1.9574,  0.3466,  0.7788, -0.6210,  0.7400,  0.7131]],\n",
      "\n",
      "        [[-1.8515,  0.8953,  0.7269, -0.4445, -0.3024,  0.9763],\n",
      "         [-1.7201,  0.7133,  0.8301, -0.6209, -0.3562,  1.1537]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "\n",
      "Decoder norm3(x + ff(x)) = \n",
      "tensor([[[-2.0163,  0.4393,  0.4481,  0.0260, -0.1310,  1.2340],\n",
      "         [-1.4961, -0.2380,  0.8343, -0.8610,  0.2816,  1.4793]],\n",
      "\n",
      "        [[-2.1504,  0.5226,  0.2917, -0.0362,  0.4928,  0.8795],\n",
      "         [-1.5732,  0.5013,  1.3057, -1.0526,  0.1590,  0.6598]],\n",
      "\n",
      "        [[-2.0610,  0.6251,  0.2683, -0.0339,  0.0687,  1.1329],\n",
      "         [-2.0548,  0.2524,  0.9216, -0.3266,  0.4837,  0.7238]],\n",
      "\n",
      "        [[-2.0256,  0.8791,  0.4115, -0.3525,  0.1977,  0.8897],\n",
      "         [-1.9745,  0.0934,  0.6804, -0.4637,  0.9533,  0.7111]],\n",
      "\n",
      "        [[-1.9958,  0.5713,  0.6566, -0.3328,  0.0293,  1.0713],\n",
      "         [-1.8569,  0.3893,  0.7567, -0.5198, -0.0172,  1.2479]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "\n",
      "\n",
      "################### Transformer Decoder start #####################\n",
      "\n",
      "query =  tensor([[[-2.0163,  0.4393,  0.4481,  0.0260, -0.1310,  1.2340],\n",
      "         [-1.4961, -0.2380,  0.8343, -0.8610,  0.2816,  1.4793]],\n",
      "\n",
      "        [[-2.1504,  0.5226,  0.2917, -0.0362,  0.4928,  0.8795],\n",
      "         [-1.5732,  0.5013,  1.3057, -1.0526,  0.1590,  0.6598]],\n",
      "\n",
      "        [[-2.0610,  0.6251,  0.2683, -0.0339,  0.0687,  1.1329],\n",
      "         [-2.0548,  0.2524,  0.9216, -0.3266,  0.4837,  0.7238]],\n",
      "\n",
      "        [[-2.0256,  0.8791,  0.4115, -0.3525,  0.1977,  0.8897],\n",
      "         [-1.9745,  0.0934,  0.6804, -0.4637,  0.9533,  0.7111]],\n",
      "\n",
      "        [[-1.9958,  0.5713,  0.6566, -0.3328,  0.0293,  1.0713],\n",
      "         [-1.8569,  0.3893,  0.7567, -0.5198, -0.0172,  1.2479]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "query shape =  torch.Size([5, 2, 6])\n",
      "\n",
      " _in_projection_packed \n",
      "\n",
      "Q :-  tensor([[[-0.0340, -0.2817, -0.1928, -1.5353, -0.3455, -1.1560],\n",
      "         [-0.1311,  0.1617, -0.2290, -1.8189, -0.7019, -1.0975]],\n",
      "\n",
      "        [[ 0.3832, -0.1907, -0.1228, -1.5665, -0.1099, -0.9450],\n",
      "         [ 0.4658,  0.4414,  0.1194, -1.9698, -0.0729, -0.6746]],\n",
      "\n",
      "        [[ 0.0655, -0.2777, -0.2202, -1.5342, -0.2805, -1.1257],\n",
      "         [ 0.5858,  0.0994,  0.0851, -1.8037, -0.0172, -0.7619]],\n",
      "\n",
      "        [[ 0.2792, -0.0878, -0.1404, -1.6820, -0.1264, -0.9888],\n",
      "         [ 0.6592,  0.1440,  0.0163, -1.7659, -0.0746, -0.7106]],\n",
      "\n",
      "        [[ 0.1595, -0.0709, -0.1178, -1.7430, -0.2483, -1.0512],\n",
      "         [ 0.0166, -0.0038, -0.1604, -1.8060, -0.4098, -1.1128]]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "\n",
      "K :-  tensor([[[-0.3675,  0.5522, -0.5522, -0.4097,  0.6536,  0.3218],\n",
      "         [-0.4935,  0.7820, -1.3030, -0.6707,  0.3962,  0.8260]],\n",
      "\n",
      "        [[-0.2497,  0.7782, -0.5343, -0.3929,  0.3036,  0.6453],\n",
      "         [ 0.2609,  0.9656, -1.1049, -0.9366,  0.1653,  0.8494]],\n",
      "\n",
      "        [[-0.2778,  0.5042, -0.4718, -0.3343,  0.5737,  0.4225],\n",
      "         [-0.1387,  1.1645, -0.8971, -0.7516,  0.1390,  0.7972]],\n",
      "\n",
      "        [[ 0.0118,  0.5652, -0.5267, -0.4402,  0.4265,  0.5949],\n",
      "         [-0.2318,  1.2048, -0.9823, -0.6610, -0.0569,  1.0450]],\n",
      "\n",
      "        [[-0.1463,  0.6793, -0.7097, -0.5587,  0.5041,  0.5391],\n",
      "         [-0.2259,  0.6660, -0.8904, -0.6099,  0.5417,  0.5814]]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "\n",
      "V :-  tensor([[[-0.8205, -1.3382, -0.4877,  0.9973, -0.1155,  0.1120],\n",
      "         [-0.5446, -1.1736,  0.5781,  0.6683, -0.5586, -0.5735]],\n",
      "\n",
      "        [[-0.7713, -1.3616, -0.4134,  1.2136,  0.3519, -0.2628],\n",
      "         [-0.1364, -0.8122,  0.2697,  0.9825, -0.4153, -0.6802]],\n",
      "\n",
      "        [[-0.8657, -1.2872, -0.5563,  1.0259,  0.0674,  0.0073],\n",
      "         [-0.4299, -1.3329, -0.0038,  1.3113,  0.1060, -0.5105]],\n",
      "\n",
      "        [[-0.7155, -1.1019, -0.4909,  1.0748,  0.0969, -0.2250],\n",
      "         [-0.4664, -1.3360,  0.2353,  1.2617,  0.3120, -0.8026]],\n",
      "\n",
      "        [[-0.6688, -1.2134, -0.3151,  1.0516, -0.1376, -0.1671],\n",
      "         [-0.6423, -1.1863, -0.0887,  0.9183, -0.3506, -0.2321]]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "\n",
      "tgt_len =  5\n",
      "bsz =  2\n",
      "num_heads =  2\n",
      "head_dim =  3\n",
      "\n",
      "tensor([[[-0.0340, -0.2817, -0.1928, -1.5353, -0.3455, -1.1560],\n",
      "         [-0.1311,  0.1617, -0.2290, -1.8189, -0.7019, -1.0975]],\n",
      "\n",
      "        [[ 0.3832, -0.1907, -0.1228, -1.5665, -0.1099, -0.9450],\n",
      "         [ 0.4658,  0.4414,  0.1194, -1.9698, -0.0729, -0.6746]],\n",
      "\n",
      "        [[ 0.0655, -0.2777, -0.2202, -1.5342, -0.2805, -1.1257],\n",
      "         [ 0.5858,  0.0994,  0.0851, -1.8037, -0.0172, -0.7619]],\n",
      "\n",
      "        [[ 0.2792, -0.0878, -0.1404, -1.6820, -0.1264, -0.9888],\n",
      "         [ 0.6592,  0.1440,  0.0163, -1.7659, -0.0746, -0.7106]],\n",
      "\n",
      "        [[ 0.1595, -0.0709, -0.1178, -1.7430, -0.2483, -1.0512],\n",
      "         [ 0.0166, -0.0038, -0.1604, -1.8060, -0.4098, -1.1128]]],\n",
      "       grad_fn=<SelectBackward0>) torch.Size([5, 2, 6])\n",
      "Q reshaped =  tensor([[[-0.0340, -0.2817, -0.1928],\n",
      "         [ 0.3832, -0.1907, -0.1228],\n",
      "         [ 0.0655, -0.2777, -0.2202],\n",
      "         [ 0.2792, -0.0878, -0.1404],\n",
      "         [ 0.1595, -0.0709, -0.1178]],\n",
      "\n",
      "        [[-1.5353, -0.3455, -1.1560],\n",
      "         [-1.5665, -0.1099, -0.9450],\n",
      "         [-1.5342, -0.2805, -1.1257],\n",
      "         [-1.6820, -0.1264, -0.9888],\n",
      "         [-1.7430, -0.2483, -1.0512]],\n",
      "\n",
      "        [[-0.1311,  0.1617, -0.2290],\n",
      "         [ 0.4658,  0.4414,  0.1194],\n",
      "         [ 0.5858,  0.0994,  0.0851],\n",
      "         [ 0.6592,  0.1440,  0.0163],\n",
      "         [ 0.0166, -0.0038, -0.1604]],\n",
      "\n",
      "        [[-1.8189, -0.7019, -1.0975],\n",
      "         [-1.9698, -0.0729, -0.6746],\n",
      "         [-1.8037, -0.0172, -0.7619],\n",
      "         [-1.7659, -0.0746, -0.7106],\n",
      "         [-1.8060, -0.4098, -1.1128]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "K reshaped =  tensor([[[-0.3675,  0.5522, -0.5522],\n",
      "         [-0.2497,  0.7782, -0.5343],\n",
      "         [-0.2778,  0.5042, -0.4718],\n",
      "         [ 0.0118,  0.5652, -0.5267],\n",
      "         [-0.1463,  0.6793, -0.7097]],\n",
      "\n",
      "        [[-0.4097,  0.6536,  0.3218],\n",
      "         [-0.3929,  0.3036,  0.6453],\n",
      "         [-0.3343,  0.5737,  0.4225],\n",
      "         [-0.4402,  0.4265,  0.5949],\n",
      "         [-0.5587,  0.5041,  0.5391]],\n",
      "\n",
      "        [[-0.4935,  0.7820, -1.3030],\n",
      "         [ 0.2609,  0.9656, -1.1049],\n",
      "         [-0.1387,  1.1645, -0.8971],\n",
      "         [-0.2318,  1.2048, -0.9823],\n",
      "         [-0.2259,  0.6660, -0.8904]],\n",
      "\n",
      "        [[-0.6707,  0.3962,  0.8260],\n",
      "         [-0.9366,  0.1653,  0.8494],\n",
      "         [-0.7516,  0.1390,  0.7972],\n",
      "         [-0.6610, -0.0569,  1.0450],\n",
      "         [-0.6099,  0.5417,  0.5814]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "V reshaped =  tensor([[[-0.8205, -1.3382, -0.4877],\n",
      "         [-0.7713, -1.3616, -0.4134],\n",
      "         [-0.8657, -1.2872, -0.5563],\n",
      "         [-0.7155, -1.1019, -0.4909],\n",
      "         [-0.6688, -1.2134, -0.3151]],\n",
      "\n",
      "        [[ 0.9973, -0.1155,  0.1120],\n",
      "         [ 1.2136,  0.3519, -0.2628],\n",
      "         [ 1.0259,  0.0674,  0.0073],\n",
      "         [ 1.0748,  0.0969, -0.2250],\n",
      "         [ 1.0516, -0.1376, -0.1671]],\n",
      "\n",
      "        [[-0.5446, -1.1736,  0.5781],\n",
      "         [-0.1364, -0.8122,  0.2697],\n",
      "         [-0.4299, -1.3329, -0.0038],\n",
      "         [-0.4664, -1.3360,  0.2353],\n",
      "         [-0.6423, -1.1863, -0.0887]],\n",
      "\n",
      "        [[ 0.6683, -0.5586, -0.5735],\n",
      "         [ 0.9825, -0.4153, -0.6802],\n",
      "         [ 1.3113,  0.1060, -0.5105],\n",
      "         [ 1.2617,  0.3120, -0.8026],\n",
      "         [ 0.9183, -0.3506, -0.2321]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "torch.Size([2, 2, 5, 3]) torch.Size([2, 2, 5, 3]) torch.Size([2, 2, 5, 3])\n",
      "\n",
      "scaled_dot_product_attention\n",
      "attn_output = \n",
      "tensor([[-0.7686, -1.2601, -0.4531,  1.0691,  0.0410, -0.1025],\n",
      "        [-0.4446, -1.1704,  0.2037,  1.0372, -0.1853, -0.5665],\n",
      "        [-0.7670, -1.2576, -0.4524,  1.0689,  0.0402, -0.1024],\n",
      "        [-0.4308, -1.1634,  0.1911,  1.0281, -0.1976, -0.5620],\n",
      "        [-0.7681, -1.2594, -0.4528,  1.0690,  0.0406, -0.1022],\n",
      "        [-0.4308, -1.1579,  0.1913,  1.0262, -0.1995, -0.5587],\n",
      "        [-0.7671, -1.2584, -0.4522,  1.0688,  0.0396, -0.1024],\n",
      "        [-0.4286, -1.1562,  0.1920,  1.0276, -0.1971, -0.5603],\n",
      "        [-0.7676, -1.2592, -0.4523,  1.0691,  0.0401, -0.1031],\n",
      "        [-0.4432, -1.1668,  0.2012,  1.0314, -0.1935, -0.5608]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "\n",
      "x =  tensor([[[-2.0163,  0.4393,  0.4481,  0.0260, -0.1310,  1.2340],\n",
      "         [-1.4961, -0.2380,  0.8343, -0.8610,  0.2816,  1.4793]],\n",
      "\n",
      "        [[-2.1504,  0.5226,  0.2917, -0.0362,  0.4928,  0.8795],\n",
      "         [-1.5732,  0.5013,  1.3057, -1.0526,  0.1590,  0.6598]],\n",
      "\n",
      "        [[-2.0610,  0.6251,  0.2683, -0.0339,  0.0687,  1.1329],\n",
      "         [-2.0548,  0.2524,  0.9216, -0.3266,  0.4837,  0.7238]],\n",
      "\n",
      "        [[-2.0256,  0.8791,  0.4115, -0.3525,  0.1977,  0.8897],\n",
      "         [-1.9745,  0.0934,  0.6804, -0.4637,  0.9533,  0.7111]],\n",
      "\n",
      "        [[-1.9958,  0.5713,  0.6566, -0.3328,  0.0293,  1.0713],\n",
      "         [-1.8569,  0.3893,  0.7567, -0.5198, -0.0172,  1.2479]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "\n",
      "sa_op =  tensor([[[ 0.0670,  0.1445,  0.4266,  0.0500, -0.1689, -1.0734],\n",
      "         [ 0.2827,  0.6705,  0.3970, -0.3661,  0.0368, -0.9344]],\n",
      "\n",
      "        [[ 0.0669,  0.1456,  0.4266,  0.0504, -0.1688, -1.0723],\n",
      "         [ 0.2872,  0.6748,  0.3961, -0.3507,  0.0442, -0.9350]],\n",
      "\n",
      "        [[ 0.0671,  0.1447,  0.4265,  0.0503, -0.1690, -1.0731],\n",
      "         [ 0.2860,  0.6732,  0.3971, -0.3485,  0.0421, -0.9334]],\n",
      "\n",
      "        [[ 0.0675,  0.1457,  0.4266,  0.0504, -0.1687, -1.0728],\n",
      "         [ 0.2854,  0.6747,  0.3958, -0.3500,  0.0429, -0.9311]],\n",
      "\n",
      "        [[ 0.0673,  0.1457,  0.4268,  0.0499, -0.1685, -1.0733],\n",
      "         [ 0.2846,  0.6690,  0.3985, -0.3589,  0.0363, -0.9358]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "\n",
      "temop =  tensor([[[-1.9493e+00,  5.8380e-01,  8.7472e-01,  7.6026e-02, -2.9993e-01,\n",
      "           1.6052e-01],\n",
      "         [-1.2135e+00,  4.3254e-01,  1.2313e+00, -1.2271e+00,  3.1838e-01,\n",
      "           5.4491e-01]],\n",
      "\n",
      "        [[-2.0834e+00,  6.6813e-01,  7.1832e-01,  1.4208e-02,  3.2407e-01,\n",
      "          -1.9286e-01],\n",
      "         [-1.2860e+00,  1.1761e+00,  1.7018e+00, -1.4034e+00,  2.0318e-01,\n",
      "          -2.7518e-01]],\n",
      "\n",
      "        [[-1.9939e+00,  7.6979e-01,  6.9476e-01,  1.6364e-02, -1.0029e-01,\n",
      "           5.9789e-02],\n",
      "         [-1.7688e+00,  9.2562e-01,  1.3188e+00, -6.7510e-01,  5.2577e-01,\n",
      "          -2.0967e-01]],\n",
      "\n",
      "        [[-1.9581e+00,  1.0248e+00,  8.3817e-01, -3.0204e-01,  2.8994e-02,\n",
      "          -1.8314e-01],\n",
      "         [-1.6892e+00,  7.6816e-01,  1.0763e+00, -8.1370e-01,  9.9615e-01,\n",
      "          -2.2003e-01]],\n",
      "\n",
      "        [[-1.9284e+00,  7.1701e-01,  1.0835e+00, -2.8282e-01, -1.3916e-01,\n",
      "          -1.9479e-03],\n",
      "         [-1.5723e+00,  1.0583e+00,  1.1553e+00, -8.7871e-01,  1.9047e-02,\n",
      "           3.1212e-01]]], grad_fn=<AddBackward0>)\n",
      "\n",
      "Decoder norm1(x + sa(x)) = \n",
      "tensor([[[-2.0391,  0.7425,  1.0619,  0.1849, -0.2279,  0.2777],\n",
      "         [-1.3346,  0.4545,  1.3226, -1.3495,  0.3304,  0.5766]],\n",
      "\n",
      "        [[-2.1002,  0.8016,  0.8545,  0.1119,  0.4387, -0.1064],\n",
      "         [-1.1293,  1.0006,  1.4554, -1.2308,  0.1590, -0.2548]],\n",
      "\n",
      "        [[-2.0803,  0.9430,  0.8609,  0.1188, -0.0088,  0.1663],\n",
      "         [-1.7183,  0.8707,  1.2485, -0.6674,  0.4865, -0.2201]],\n",
      "\n",
      "        [[-1.9211,  1.1495,  0.9574, -0.2163,  0.1244, -0.0939],\n",
      "         [-1.6673,  0.7304,  1.0310, -0.8131,  0.9528, -0.2338]],\n",
      "\n",
      "        [[-1.9259,  0.8484,  1.2327, -0.2001, -0.0495,  0.0944],\n",
      "         [-1.6157,  1.0609,  1.1596, -0.9100,  0.0035,  0.3017]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "\n",
      "Memory = \n",
      "tensor([[[-1.8013,  1.0029,  0.3805,  1.1379, -0.5191, -0.2008],\n",
      "         [-1.2129,  0.5043,  1.1623,  0.6301,  0.4429, -1.5266]],\n",
      "\n",
      "        [[-1.3909,  1.0147,  0.2330,  1.3200, -0.0657, -1.1112],\n",
      "         [-1.3164,  0.8442,  1.1154,  0.4630, -1.4145,  0.3084]],\n",
      "\n",
      "        [[-1.5642,  0.7890,  0.5335,  1.1761,  0.1755, -1.1100],\n",
      "         [-1.2129,  0.5043,  1.1623,  0.6301,  0.4429, -1.5266]],\n",
      "\n",
      "        [[-1.4816,  1.3507,  0.1011,  0.9266,  0.1472, -1.0440],\n",
      "         [-0.6874, -1.2884,  1.9335,  0.2895, -0.0374, -0.2098]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "\n",
      "query =  tensor([[[-2.0391,  0.7425,  1.0619,  0.1849, -0.2279,  0.2777],\n",
      "         [-1.3346,  0.4545,  1.3226, -1.3495,  0.3304,  0.5766]],\n",
      "\n",
      "        [[-2.1002,  0.8016,  0.8545,  0.1119,  0.4387, -0.1064],\n",
      "         [-1.1293,  1.0006,  1.4554, -1.2308,  0.1590, -0.2548]],\n",
      "\n",
      "        [[-2.0803,  0.9430,  0.8609,  0.1188, -0.0088,  0.1663],\n",
      "         [-1.7183,  0.8707,  1.2485, -0.6674,  0.4865, -0.2201]],\n",
      "\n",
      "        [[-1.9211,  1.1495,  0.9574, -0.2163,  0.1244, -0.0939],\n",
      "         [-1.6673,  0.7304,  1.0310, -0.8131,  0.9528, -0.2338]],\n",
      "\n",
      "        [[-1.9259,  0.8484,  1.2327, -0.2001, -0.0495,  0.0944],\n",
      "         [-1.6157,  1.0609,  1.1596, -0.9100,  0.0035,  0.3017]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "query shape =  torch.Size([5, 2, 6])\n",
      "\n",
      " _in_projection_packed \n",
      "\n",
      "Q :-  tensor([[[ 0.0090,  0.4867,  0.6530, -0.9873,  0.7099,  0.0631],\n",
      "         [ 0.4424,  0.7532, -0.0021, -0.2667,  1.0012,  0.3442]],\n",
      "\n",
      "        [[-0.0074,  0.7634,  0.3587, -0.8563,  0.8962, -0.1300],\n",
      "         [ 0.1452,  1.1584, -0.0829, -0.0715,  1.0652,  0.4572]],\n",
      "\n",
      "        [[-0.0684,  0.5642,  0.5266, -1.0728,  0.6944, -0.0572],\n",
      "         [ 0.1304,  1.0800,  0.0773, -0.4050,  1.1255,  0.1626]],\n",
      "\n",
      "        [[-0.0917,  0.8202,  0.3255, -0.8803,  0.8151,  0.0202],\n",
      "         [ 0.2058,  1.1189, -0.1379, -0.3084,  1.1814,  0.0228]],\n",
      "\n",
      "        [[ 0.0542,  0.7442,  0.4573, -0.7637,  0.8901,  0.1596],\n",
      "         [ 0.0975,  0.7798,  0.1912, -0.7012,  0.8247,  0.2357]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "\n",
      "K :-  tensor([[[ 0.2996,  0.2977,  0.7836, -0.6700, -0.3637, -0.9730],\n",
      "         [ 0.0092,  0.5145, -0.1354, -1.0598, -0.1247, -1.1359]],\n",
      "\n",
      "        [[-0.0363,  0.6465,  0.3285, -1.4401, -0.1514, -1.4035],\n",
      "         [ 0.6587,  0.5903,  1.0065,  0.5453, -0.4586, -0.7660]],\n",
      "\n",
      "        [[-0.0193,  0.4305,  0.2183, -1.2863, -0.1913, -1.2202],\n",
      "         [ 0.0092,  0.5145, -0.1354, -1.0598, -0.1247, -1.1359]],\n",
      "\n",
      "        [[ 0.2289,  0.4600,  0.4609, -1.5475, -0.1572, -1.2230],\n",
      "         [-0.2660, -0.0730, -0.4458,  1.0256, -0.2511, -0.0107]]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "\n",
      "V :-  tensor([[[-0.1937, -0.0298,  0.7223,  0.8744, -0.4382, -0.7092],\n",
      "         [-0.7418, -0.5217,  0.4601,  0.5126, -0.6385,  0.3217]],\n",
      "\n",
      "        [[-0.7177, -0.3821,  0.3268,  0.6252, -0.7886,  0.0821],\n",
      "         [ 0.5024,  0.1558,  0.3832,  0.8222,  0.3879, -1.3130]],\n",
      "\n",
      "        [[-0.7481, -0.3514,  0.6030,  0.6682, -0.7998,  0.0293],\n",
      "         [-0.7418, -0.5217,  0.4601,  0.5126, -0.6385,  0.3217]],\n",
      "\n",
      "        [[-0.4517, -0.4936,  0.3106,  0.7026, -0.7816,  0.2585],\n",
      "         [-0.4881,  0.3204,  1.0185,  0.1975,  0.1538, -0.8837]]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "\n",
      "tgt_len =  5\n",
      "bsz =  2\n",
      "num_heads =  2\n",
      "head_dim =  3\n",
      "\n",
      "tensor([[[ 0.0090,  0.4867,  0.6530, -0.9873,  0.7099,  0.0631],\n",
      "         [ 0.4424,  0.7532, -0.0021, -0.2667,  1.0012,  0.3442]],\n",
      "\n",
      "        [[-0.0074,  0.7634,  0.3587, -0.8563,  0.8962, -0.1300],\n",
      "         [ 0.1452,  1.1584, -0.0829, -0.0715,  1.0652,  0.4572]],\n",
      "\n",
      "        [[-0.0684,  0.5642,  0.5266, -1.0728,  0.6944, -0.0572],\n",
      "         [ 0.1304,  1.0800,  0.0773, -0.4050,  1.1255,  0.1626]],\n",
      "\n",
      "        [[-0.0917,  0.8202,  0.3255, -0.8803,  0.8151,  0.0202],\n",
      "         [ 0.2058,  1.1189, -0.1379, -0.3084,  1.1814,  0.0228]],\n",
      "\n",
      "        [[ 0.0542,  0.7442,  0.4573, -0.7637,  0.8901,  0.1596],\n",
      "         [ 0.0975,  0.7798,  0.1912, -0.7012,  0.8247,  0.2357]]],\n",
      "       grad_fn=<ViewBackward0>) torch.Size([5, 2, 6])\n",
      "Q reshaped =  tensor([[[ 0.0090,  0.4867,  0.6530],\n",
      "         [-0.0074,  0.7634,  0.3587],\n",
      "         [-0.0684,  0.5642,  0.5266],\n",
      "         [-0.0917,  0.8202,  0.3255],\n",
      "         [ 0.0542,  0.7442,  0.4573]],\n",
      "\n",
      "        [[-0.9873,  0.7099,  0.0631],\n",
      "         [-0.8563,  0.8962, -0.1300],\n",
      "         [-1.0728,  0.6944, -0.0572],\n",
      "         [-0.8803,  0.8151,  0.0202],\n",
      "         [-0.7637,  0.8901,  0.1596]],\n",
      "\n",
      "        [[ 0.4424,  0.7532, -0.0021],\n",
      "         [ 0.1452,  1.1584, -0.0829],\n",
      "         [ 0.1304,  1.0800,  0.0773],\n",
      "         [ 0.2058,  1.1189, -0.1379],\n",
      "         [ 0.0975,  0.7798,  0.1912]],\n",
      "\n",
      "        [[-0.2667,  1.0012,  0.3442],\n",
      "         [-0.0715,  1.0652,  0.4572],\n",
      "         [-0.4050,  1.1255,  0.1626],\n",
      "         [-0.3084,  1.1814,  0.0228],\n",
      "         [-0.7012,  0.8247,  0.2357]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "K reshaped =  tensor([[[ 0.2996,  0.2977,  0.7836],\n",
      "         [-0.0363,  0.6465,  0.3285],\n",
      "         [-0.0193,  0.4305,  0.2183],\n",
      "         [ 0.2289,  0.4600,  0.4609]],\n",
      "\n",
      "        [[-0.6700, -0.3637, -0.9730],\n",
      "         [-1.4401, -0.1514, -1.4035],\n",
      "         [-1.2863, -0.1913, -1.2202],\n",
      "         [-1.5475, -0.1572, -1.2230]],\n",
      "\n",
      "        [[ 0.0092,  0.5145, -0.1354],\n",
      "         [ 0.6587,  0.5903,  1.0065],\n",
      "         [ 0.0092,  0.5145, -0.1354],\n",
      "         [-0.2660, -0.0730, -0.4458]],\n",
      "\n",
      "        [[-1.0598, -0.1247, -1.1359],\n",
      "         [ 0.5453, -0.4586, -0.7660],\n",
      "         [-1.0598, -0.1247, -1.1359],\n",
      "         [ 1.0256, -0.2511, -0.0107]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "V reshaped =  tensor([[[-0.1937, -0.0298,  0.7223],\n",
      "         [-0.7177, -0.3821,  0.3268],\n",
      "         [-0.7481, -0.3514,  0.6030],\n",
      "         [-0.4517, -0.4936,  0.3106]],\n",
      "\n",
      "        [[ 0.8744, -0.4382, -0.7092],\n",
      "         [ 0.6252, -0.7886,  0.0821],\n",
      "         [ 0.6682, -0.7998,  0.0293],\n",
      "         [ 0.7026, -0.7816,  0.2585]],\n",
      "\n",
      "        [[-0.7418, -0.5217,  0.4601],\n",
      "         [ 0.5024,  0.1558,  0.3832],\n",
      "         [-0.7418, -0.5217,  0.4601],\n",
      "         [-0.4881,  0.3204,  1.0185]],\n",
      "\n",
      "        [[ 0.5126, -0.6385,  0.3217],\n",
      "         [ 0.8222,  0.3879, -1.3130],\n",
      "         [ 0.5126, -0.6385,  0.3217],\n",
      "         [ 0.1975,  0.1538, -0.8837]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "torch.Size([2, 2, 5, 3]) torch.Size([2, 2, 4, 3]) torch.Size([2, 2, 4, 3])\n",
      "\n",
      "scaled_dot_product_attention\n",
      "attn_output = \n",
      "tensor([[-0.5159, -0.3077,  0.4925,  0.7011, -0.7305, -0.0115],\n",
      "        [-0.3106, -0.1579,  0.5387,  0.4982, -0.2484, -0.2861],\n",
      "        [-0.5277, -0.3150,  0.4863,  0.7008, -0.7305, -0.0125],\n",
      "        [-0.3445, -0.1787,  0.5388,  0.4876, -0.1947, -0.3690],\n",
      "        [-0.5218, -0.3110,  0.4899,  0.6993, -0.7333, -0.0047],\n",
      "        [-0.3209, -0.1694,  0.5345,  0.5061, -0.2993, -0.2074],\n",
      "        [-0.5308, -0.3166,  0.4850,  0.7018, -0.7292, -0.0153],\n",
      "        [-0.3485, -0.1789,  0.5407,  0.5076, -0.2913, -0.2200],\n",
      "        [-0.5236, -0.3128,  0.4879,  0.7039, -0.7258, -0.0238],\n",
      "        [-0.3112, -0.1570,  0.5397,  0.5114, -0.3479, -0.1321]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "\n",
      "Decoder norm2(x + mha(x, mem)) = \n",
      "tensor([[[-2.1477,  0.2547,  0.9863,  0.5233,  0.2253,  0.1581],\n",
      "         [-1.5385,  0.1645,  1.2393, -1.0824,  0.8894,  0.3278]],\n",
      "\n",
      "        [[-2.1261,  0.2797,  0.8009,  0.4606,  0.7264, -0.1415],\n",
      "         [-1.3729,  0.5857,  1.3947, -0.9985,  0.8093, -0.4182]],\n",
      "\n",
      "        [[-2.1808,  0.4097,  0.8303,  0.4769,  0.3930,  0.0710],\n",
      "         [-1.8065,  0.5114,  1.1356, -0.4535,  0.9358, -0.3228]],\n",
      "\n",
      "        [[-2.1111,  0.5760,  0.9352,  0.2240,  0.5168, -0.1408],\n",
      "         [-1.7064,  0.3592,  0.9364, -0.5481,  1.2925, -0.3337]],\n",
      "\n",
      "        [[-2.0942,  0.3319,  1.1440,  0.2284,  0.3784,  0.0115],\n",
      "         [-1.8439,  0.7497,  1.1253, -0.6902,  0.5268,  0.1324]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "\n",
      "Decoder norm3(x + ff(x)) = \n",
      "tensor([[[-2.0880, -0.0851,  1.0847,  0.5607,  0.3007,  0.2271],\n",
      "         [-1.4874, -0.1941,  1.3059, -0.9802,  0.9648,  0.3910]],\n",
      "\n",
      "        [[-2.0665, -0.0639,  0.9127,  0.5283,  0.7781, -0.0887],\n",
      "         [-1.3848,  0.1682,  1.5226, -0.8750,  0.9203, -0.3512]],\n",
      "\n",
      "        [[-2.1389,  0.0537,  0.9503,  0.5363,  0.4613,  0.1374],\n",
      "         [-1.7837,  0.1121,  1.2571, -0.3480,  1.0191, -0.2566]],\n",
      "\n",
      "        [[-2.0898,  0.1936,  1.0671,  0.3093,  0.5957, -0.0759],\n",
      "         [-1.6731, -0.0233,  1.0620, -0.4374,  1.3441, -0.2722]],\n",
      "\n",
      "        [[-2.0405, -0.0294,  1.2378,  0.2863,  0.4630,  0.0828],\n",
      "         [-1.8531,  0.3335,  1.2730, -0.6143,  0.6379,  0.2230]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "\n",
      "tensor([[[ 0.0351, -1.1913, -0.3768, -0.1533, -0.6715,  0.2967,  0.9567,\n",
      "          -0.4592, -0.7431,  0.5005],\n",
      "         [ 0.5240, -1.0788, -0.4459, -0.1862, -0.0582,  0.5289,  0.4782,\n",
      "          -0.9111, -0.0584,  0.9501]],\n",
      "\n",
      "        [[ 0.3253, -1.2071, -0.4222, -0.2448, -0.6172,  0.4023,  0.9607,\n",
      "          -0.6148, -0.7525,  0.4649],\n",
      "         [ 0.6851, -1.1969, -0.5047,  0.0477, -0.4231,  0.6040,  0.7750,\n",
      "          -0.5608, -0.2766,  0.7577]],\n",
      "\n",
      "        [[ 0.1552, -1.1610, -0.4760, -0.2129, -0.6705,  0.3657,  1.0074,\n",
      "          -0.4979, -0.7873,  0.5562],\n",
      "         [ 0.6300, -1.2311, -0.5301, -0.1492, -0.4834,  0.5986,  0.8887,\n",
      "          -0.6590, -0.5067,  0.7288]],\n",
      "\n",
      "        [[ 0.3160, -1.2016, -0.5434, -0.1745, -0.7006,  0.4669,  1.0648,\n",
      "          -0.4734, -0.7825,  0.6207],\n",
      "         [ 0.7733, -1.1772, -0.5043, -0.2409, -0.3084,  0.6118,  0.7447,\n",
      "          -0.8627, -0.3858,  0.7044]],\n",
      "\n",
      "        [[ 0.1892, -1.2463, -0.4136, -0.1336, -0.6515,  0.3959,  0.9682,\n",
      "          -0.4872, -0.6962,  0.5847],\n",
      "         [ 0.4427, -1.1010, -0.6811, -0.1530, -0.4359,  0.5963,  0.8850,\n",
      "          -0.5812, -0.4528,  1.0195]]], grad_fn=<ViewBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sreevaatsav/.pyenv/versions/project_env/lib/python3.10/site-packages/torch/nn/modules/transformer.py:282: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=512, dropout=0):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.encoding = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1).float()\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(torch.log(torch.tensor(10000.0)) / d_model))\n",
    "        self.encoding[:, 0::2] = torch.sin(position * div_term)\n",
    "        self.encoding[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.encoding = self.encoding.unsqueeze(0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.encoding[:, :x.size(1)].detach()\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, src_vocab_size, tgt_vocab_size,max_seq_len, num_encoder_layers=1, num_decoder_layers=1, d_model=4, nhead=2):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.src_embedding = nn.Embedding(src_vocab_size, d_model)\n",
    "        self.tgt_embedding = nn.Embedding(tgt_vocab_size, d_model)\n",
    "        self.positional_encoding = PositionalEncoding(d_model, dropout=0, max_len=max_seq_len)\n",
    "        self.transformer = nn.Transformer(\n",
    "            d_model=d_model,\n",
    "            nhead=nhead,\n",
    "            num_encoder_layers=num_encoder_layers,\n",
    "            num_decoder_layers=num_decoder_layers,\n",
    "            dropout=0\n",
    "        )\n",
    "        self.fc = nn.Linear(d_model, tgt_vocab_size)\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "\n",
    "        print(\"Source embeddings :- \\n\")\n",
    "        print(self.src_embedding(src))\n",
    "        print()\n",
    "\n",
    "        print(\"Target embeddings :- \\n\")\n",
    "        print(self.tgt_embedding(tgt))\n",
    "        print()\n",
    "\n",
    "\n",
    "        src = self.src_embedding(src) + self.positional_encoding(src)\n",
    "        tgt = self.tgt_embedding(tgt) + self.positional_encoding(tgt)\n",
    "\n",
    "        print(\"Positional encoded source embeddings:\")\n",
    "        print(src)\n",
    "        print()\n",
    "\n",
    "        print(\"Positional encoded target embeddings:\")\n",
    "        print(tgt)\n",
    "        print()\n",
    "\n",
    "        output = self.transformer(src, tgt)\n",
    "        output = self.fc(output)\n",
    "        \n",
    "        return output\n",
    "    \n",
    "\n",
    "src_vocab_size = 10  # Source language vocabulary size\n",
    "tgt_vocab_size = 10  # Target language vocabulary size\n",
    "d_model = 4  # Dimension of the model\n",
    "num_heads = 2\n",
    "num_encoder_layers = 3\n",
    "num_decoder_layers = 3\n",
    "\n",
    "need_weights = False\n",
    "\n",
    "src_mask = None\n",
    "tgt_mask = None\n",
    "memory_mask = None\n",
    "\n",
    "max_seq_len = 5\n",
    "\n",
    "d_model = 6\n",
    "\n",
    "\n",
    "model = TransformerModel(src_vocab_size, tgt_vocab_size, num_encoder_layers=num_encoder_layers, num_decoder_layers=num_decoder_layers, d_model=d_model, max_seq_len = max_seq_len, nhead=num_heads)\n",
    "# Source sentence in the source language\n",
    "src_sentence = torch.tensor([[0, 2], [1, 0], [2, 2], [3, 5]])\n",
    "print(src_sentence.shape)\n",
    "# Target sentence in the target language (translation of the source sentence)\n",
    "tgt_sentence = torch.tensor([[1, 7], [3, 4], [5, 2], [8, 0], [6, 1]])  # Target sequence\n",
    "print(tgt_sentence.shape)\n",
    "\n",
    "# Forward pass\n",
    "output = model(src_sentence, tgt_sentence)\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_intermediate_outputs(src_sentence, tgt_sentence,d_model, model, num_encoder_layers , num_decoder_layers):\n",
    "\n",
    "    state_dict = model.state_dict()\n",
    "\n",
    "    pe_src_embeds, pe_tgt_embeds = get_embedding_outputs(src_sentence=src_sentence,  tgt_sentence=tgt_sentence, state_dict=state_dict, max_seq_len=max_seq_len, d_model = d_model)\n",
    "    print(\"###\"*25)\n",
    "    print(\"### Encoder Start ###\")\n",
    "    print()\n",
    "\n",
    "    x_enc = pe_src_embeds\n",
    "\n",
    "    for lno in range(num_encoder_layers):\n",
    "        attn_enc_output, src_len, head_dim, attn_weights = encoder_block_attn_output(x_enc, state_dict, layer_num = lno, need_weights = False, embed_dim = d_model, num_heads= num_heads, src_mask=None)\n",
    "\n",
    "        if lno == 0:\n",
    "            tgt_len, bsz, embed_dim = x_enc.shape\n",
    "\n",
    "        output_enc_final = encoder_block_post_attn_output(x_enc, attn_enc_output, state_dict, layer_num = lno , bsz = bsz, tgt_len = tgt_len)\n",
    "\n",
    "        x_enc = output_enc_final\n",
    "\n",
    "    \n",
    "    print(\"### Encoder Done ###\")\n",
    "    print(\"\\n\\n\\n\")\n",
    "    print(\"### Decoder Start ###\")\n",
    "\n",
    "    x_dec = pe_tgt_embeds\n",
    "    memory = x_enc\n",
    "\n",
    "    for lno in range(num_decoder_layers):\n",
    "\n",
    "        self_attn_dec, dec_sa_wts = decoder_block_self_attn_output(x_dec, state_dict, layer_num = lno, need_weights = False, tgt_mask=None)\n",
    "        x_dec = dec_post_self_attn(self_attn_dec, x_dec, state_dict, layer_num = lno)\n",
    "\n",
    "        attn_dec_mha_output, attn_dec_mha_wts = decoder_block_cross_attn_output(x_dec, memory, state_dict, layer_num = lno, tgt_len = tgt_len, src_len = src_len, head_dim = head_dim, need_weights = False, memory_mask=None)\n",
    "        final_op = decoder_block_post_attn_output(x_dec, attn_dec_mha_output, state_dict, layer_num = lno)\n",
    "\n",
    "        print(pe_tgt_embeds.shape, final_op.shape)\n",
    "        x_dec = final_op\n",
    "\n",
    "    print(\"### Decoder Done ###\")\n",
    "\n",
    "\n",
    "    final_op = feef_fwd_transformer(final_op, state_dict)\n",
    "\n",
    "\n",
    "    \n",
    "    return final_op\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source sentence embedding\n",
      "Word index: 0, Embedding: tensor([ 0.3485,  0.9371,  0.6244, -0.0230, -0.1803,  1.6200])\n",
      "Word index: 2, Embedding: tensor([-1.2299,  0.8020,  0.4715,  0.4010, -0.5315, -1.7415])\n",
      "Word index: 1, Embedding: tensor([-0.1163,  0.3765, -0.3016, -0.2952, -0.5298, -0.6820])\n",
      "Word index: 0, Embedding: tensor([ 0.3485,  0.9371,  0.6244, -0.0230, -0.1803,  1.6200])\n",
      "Word index: 2, Embedding: tensor([-1.2299,  0.8020,  0.4715,  0.4010, -0.5315, -1.7415])\n",
      "Word index: 2, Embedding: tensor([-1.2299,  0.8020,  0.4715,  0.4010, -0.5315, -1.7415])\n",
      "Word index: 3, Embedding: tensor([-0.2243,  1.6587, -0.3522, -0.6067, -0.2162, -0.6181])\n",
      "Word index: 5, Embedding: tensor([-0.4952, -2.1157,  1.7508, -0.6661, -0.6780,  0.7846])\n",
      "\n",
      "torch.Size([4, 2, 6])\n",
      "Target sentence embedding\n",
      "Word index: 1, Embedding: tensor([ 0.1129, -1.0142, -0.9221,  0.8812, -1.6048,  0.2050])\n",
      "Word index: 7, Embedding: tensor([ 1.6339, -0.6463, -0.1945,  0.6870,  0.3138,  0.4030])\n",
      "Word index: 3, Embedding: tensor([-0.7208,  0.4244, -1.1285,  1.7011,  2.0456,  0.1133])\n",
      "Word index: 4, Embedding: tensor([ 0.7506, -0.2308,  1.0457, -0.1141, -0.8790, -0.7974])\n",
      "Word index: 5, Embedding: tensor([-0.1423,  0.0984, -1.4589,  1.0058, -0.5254,  0.3244])\n",
      "Word index: 2, Embedding: tensor([ 0.1265, -0.2915, -0.4858,  0.6941,  0.3983, -0.5563])\n",
      "Word index: 8, Embedding: tensor([-0.3389,  1.8031, -0.6392,  0.0925,  0.3354, -0.2486])\n",
      "Word index: 0, Embedding: tensor([-0.3784, -0.2966,  0.0895,  1.3754,  2.1131,  0.1386])\n",
      "Word index: 6, Embedding: tensor([-0.3823,  0.4868,  0.8619,  0.4363, -0.8054,  0.7608])\n",
      "Word index: 1, Embedding: tensor([ 0.1129, -1.0142, -0.9221,  0.8812, -1.6048,  0.2050])\n",
      "\n",
      "PE of src :\n",
      "tensor([[[0.0000, 1.0000, 0.0000, 1.0000, 0.0000, 1.0000],\n",
      "         [0.8415, 0.5403, 0.0464, 0.9989, 0.0022, 1.0000]]])\n",
      "\n",
      "PE of tgt :\n",
      "tensor([[[0.0000, 1.0000, 0.0000, 1.0000, 0.0000, 1.0000],\n",
      "         [0.8415, 0.5403, 0.0464, 0.9989, 0.0022, 1.0000]]])\n",
      "\n",
      "PE source embeddings : \n",
      "\n",
      "tensor([[[ 0.3485,  1.9371,  0.6244,  0.9770, -0.1803,  2.6200],\n",
      "         [-0.3884,  1.3423,  0.5179,  1.4000, -0.5294, -0.7415]],\n",
      "\n",
      "        [[-0.1163,  1.3765, -0.3016,  0.7048, -0.5298,  0.3180],\n",
      "         [ 1.1900,  1.4774,  0.6708,  0.9760, -0.1782,  2.6200]],\n",
      "\n",
      "        [[-1.2299,  1.8020,  0.4715,  1.4010, -0.5315, -0.7415],\n",
      "         [-0.3884,  1.3423,  0.5179,  1.4000, -0.5294, -0.7415]],\n",
      "\n",
      "        [[-0.2243,  2.6587, -0.3522,  0.3933, -0.2162,  0.3819],\n",
      "         [ 0.3463, -1.5754,  1.7972,  0.3328, -0.6758,  1.7845]]])\n",
      "\n",
      "PE target embeddings : \n",
      "\n",
      "tensor([[[ 0.1129, -0.0142, -0.9221,  1.8812, -1.6048,  1.2050],\n",
      "         [ 2.4754, -0.1060, -0.1481,  1.6859,  0.3160,  1.4030]],\n",
      "\n",
      "        [[-0.7208,  1.4244, -1.1285,  2.7011,  2.0456,  1.1133],\n",
      "         [ 1.5921,  0.3095,  1.0921,  0.8848, -0.8769,  0.2026]],\n",
      "\n",
      "        [[-0.1423,  1.0984, -1.4589,  2.0058, -0.5254,  1.3244],\n",
      "         [ 0.9680,  0.2488, -0.4394,  1.6930,  0.4004,  0.4437]],\n",
      "\n",
      "        [[-0.3389,  2.8031, -0.6392,  1.0925,  0.3354,  0.7514],\n",
      "         [ 0.4631,  0.2437,  0.1359,  2.3743,  2.1152,  1.1386]],\n",
      "\n",
      "        [[-0.3823,  1.4868,  0.8619,  1.4363, -0.8054,  1.7608],\n",
      "         [ 0.9544, -0.4739, -0.8757,  1.8801, -1.6027,  1.2050]]])\n",
      "\n",
      "###########################################################################\n",
      "### Encoder Start ###\n",
      "\n",
      "Q_enc_0 = \n",
      "tensor([[[ 1.3542,  0.7988, -1.9542],\n",
      "         [ 0.6537, -0.1274, -1.0933],\n",
      "         [ 0.7752,  0.1929, -1.7568],\n",
      "         [ 1.2694, -0.1703, -1.5901]],\n",
      "\n",
      "        [[ 0.3685, -0.6178,  0.1329],\n",
      "         [ 0.8181, -0.1809, -0.3052],\n",
      "         [ 1.8695, -0.4570, -0.3744],\n",
      "         [ 1.2231,  0.0914, -0.5582]],\n",
      "\n",
      "        [[ 0.4360,  0.1708, -1.1372],\n",
      "         [ 1.0151,  0.7766, -1.3345],\n",
      "         [ 0.4360,  0.1708, -1.1372],\n",
      "         [-0.2332,  0.7542,  0.1986]],\n",
      "\n",
      "        [[ 1.5151, -0.2740, -0.6128],\n",
      "         [ 0.0141, -0.4347, -0.1055],\n",
      "         [ 1.5151, -0.2740, -0.6128],\n",
      "         [-0.5066, -0.7136,  0.6494]]])\n",
      "\n",
      "K_enc_0 = \n",
      "tensor([[[-1.4025,  0.2942, -0.8988],\n",
      "         [-0.7072,  0.1992, -0.0046],\n",
      "         [-0.7639,  0.5158,  0.2552],\n",
      "         [-1.2674,  0.3434,  0.5734]],\n",
      "\n",
      "        [[ 0.2983,  0.1654,  1.2723],\n",
      "         [-0.3592, -0.0803,  1.0749],\n",
      "         [-0.4916,  0.4404,  2.1996],\n",
      "         [-1.1030, -0.0657,  1.4841]],\n",
      "\n",
      "        [[-0.2456,  0.5762, -0.0164],\n",
      "         [-0.8842,  0.3547, -1.1704],\n",
      "         [-0.2456,  0.5762, -0.0164],\n",
      "         [ 0.9147,  0.4109, -1.5272]],\n",
      "\n",
      "        [[-0.2161,  0.3141,  1.6730],\n",
      "         [ 0.5738,  0.0392,  0.7456],\n",
      "         [-0.2161,  0.3141,  1.6730],\n",
      "         [ 1.7960,  0.5259, -0.2905]]])\n",
      "\n",
      "V_enc_0 = \n",
      "tensor([[[-1.9645, -0.0160, -0.5482],\n",
      "         [-0.5377,  0.0031, -1.1717],\n",
      "         [-1.1467,  0.9893, -1.9379],\n",
      "         [-0.5897, -0.0174, -1.6497]],\n",
      "\n",
      "        [[-0.9240,  0.0568, -1.0505],\n",
      "         [-0.0226,  0.3606, -1.2210],\n",
      "         [ 0.0044,  0.7383, -1.0785],\n",
      "         [ 0.0139,  0.5846, -1.5888]],\n",
      "\n",
      "        [[-0.9230,  0.6382, -1.4058],\n",
      "         [-1.7408, -0.3671, -0.0161],\n",
      "         [-0.9230,  0.6382, -1.4058],\n",
      "         [-1.3913,  0.2779,  1.7576]],\n",
      "\n",
      "        [[-0.0558,  0.5399, -0.9999],\n",
      "         [-0.9842, -0.1416, -0.9719],\n",
      "         [-0.0558,  0.5399, -0.9999],\n",
      "         [-0.5571, -0.9032,  1.0796]]])\n",
      "\n",
      "Attention weights = \n",
      "torch.Size([2, 2, 4, 4]) tensor([[[[0.4323, 0.2598, 0.2145, 0.0934],\n",
      "          [0.3619, 0.2695, 0.2187, 0.1498],\n",
      "          [0.4528, 0.2469, 0.1916, 0.1087],\n",
      "          [0.3898, 0.2882, 0.2111, 0.1109]],\n",
      "\n",
      "         [[0.2796, 0.2614, 0.2300, 0.2290],\n",
      "          [0.3501, 0.2726, 0.1990, 0.1783],\n",
      "          [0.4772, 0.2613, 0.1548, 0.1068],\n",
      "          [0.4107, 0.2715, 0.1769, 0.1409]]],\n",
      "\n",
      "\n",
      "        [[[0.1364, 0.2425, 0.1364, 0.4847],\n",
      "          [0.1065, 0.1614, 0.1065, 0.6255],\n",
      "          [0.1364, 0.2425, 0.1364, 0.4847],\n",
      "          [0.2828, 0.2451, 0.2828, 0.1893]],\n",
      "\n",
      "         [[0.0619, 0.1791, 0.0619, 0.6971],\n",
      "          [0.2365, 0.2699, 0.2365, 0.2570],\n",
      "          [0.0619, 0.1791, 0.0619, 0.6971],\n",
      "          [0.3483, 0.2186, 0.3483, 0.0849]]]])\n",
      "Final Encoder 0 Output :\n",
      "norm2(norm1(x + self_atten(x)) + feed_fwd_op)\n",
      "\n",
      "tensor([[[-0.6640,  1.3608, -0.0643,  0.7941, -1.7269,  0.3003],\n",
      "         [-0.8699,  1.0838,  0.8909,  1.0124, -0.9927, -1.1245]],\n",
      "\n",
      "        [[-0.2898,  1.4716, -0.2061,  1.1697, -1.2114, -0.9339],\n",
      "         [-0.2696,  1.0159,  0.2440,  0.4065, -2.0576,  0.6609]],\n",
      "\n",
      "        [[-0.6566,  1.2715,  0.3610,  1.1775, -0.8353, -1.3181],\n",
      "         [-0.8699,  1.0838,  0.8909,  1.0124, -0.9927, -1.1245]],\n",
      "\n",
      "        [[-0.3976,  1.9061, -0.3001,  0.6398, -0.9475, -0.9007],\n",
      "         [-0.1044, -0.9137,  1.5801,  0.5685, -1.4859,  0.3555]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "\n",
      "Q_enc_1 = \n",
      "tensor([[[ 0.1062, -1.3256,  0.8023],\n",
      "         [ 0.0720, -0.6842,  0.5549],\n",
      "         [-0.0249, -0.5067,  0.1385],\n",
      "         [ 0.0754, -0.3350,  0.6414]],\n",
      "\n",
      "        [[ 0.4596,  0.5511,  0.6325],\n",
      "         [ 0.6075,  0.8223,  0.7524],\n",
      "         [ 0.7091,  0.8366,  0.8730],\n",
      "         [ 0.0795,  0.7293,  0.6773]],\n",
      "\n",
      "        [[ 0.0107, -0.6982, -0.0194],\n",
      "         [ 0.3022, -1.4428,  0.7870],\n",
      "         [ 0.0107, -0.6982, -0.0194],\n",
      "         [ 0.2625, -1.3457, -0.3344]],\n",
      "\n",
      "        [[ 0.8653,  0.8187,  0.7955],\n",
      "         [ 0.6386,  0.5101,  0.1899],\n",
      "         [ 0.8653,  0.8187,  0.7955],\n",
      "         [ 1.5853,  0.3659, -0.1566]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "K_enc_1 = \n",
      "tensor([[[ 0.1612, -0.0076,  0.4991],\n",
      "         [-0.0512,  0.1495,  0.4237],\n",
      "         [-0.3917, -0.3114,  0.1881],\n",
      "         [-0.0999,  0.0048,  0.4946]],\n",
      "\n",
      "        [[ 0.8756,  1.5937, -0.7307],\n",
      "         [ 0.3883,  1.2711, -0.7333],\n",
      "         [ 0.1986,  1.1641, -0.7149],\n",
      "         [ 0.2660,  1.2394, -0.8099]],\n",
      "\n",
      "        [[-0.5119, -0.7061,  0.2347],\n",
      "         [ 0.2638, -0.1448,  0.8370],\n",
      "         [-0.5119, -0.7061,  0.2347],\n",
      "         [-0.1415, -0.7540,  0.4165]],\n",
      "\n",
      "        [[ 0.1476,  1.1747, -0.6728],\n",
      "         [ 0.6155,  1.2894, -0.5124],\n",
      "         [ 0.1476,  1.1747, -0.6728],\n",
      "         [-0.0033,  0.2691,  0.1448]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "V_enc_1 = \n",
      "tensor([[[ 0.2575,  0.4791, -0.6375],\n",
      "         [ 0.8317,  1.0239, -0.3566],\n",
      "         [ 0.9557,  1.0392, -0.1398],\n",
      "         [ 0.6605,  1.0671, -0.4351]],\n",
      "\n",
      "        [[-0.4460,  1.3066,  0.6505],\n",
      "         [-0.9145,  1.2622,  0.3598],\n",
      "         [-0.6914,  0.9932,  0.1945],\n",
      "         [-1.0283,  1.0360,  0.5688]],\n",
      "\n",
      "        [[ 0.9274,  0.9059, -0.0759],\n",
      "         [ 0.2356,  0.3226, -0.5502],\n",
      "         [ 0.9274,  0.9059, -0.0759],\n",
      "         [ 0.4907, -0.1479,  0.2277]],\n",
      "\n",
      "        [[-0.4483,  0.9797,  0.2257],\n",
      "         [-0.3706,  1.3958,  0.7039],\n",
      "         [-0.4483,  0.9797,  0.2257],\n",
      "         [ 0.4950,  0.8483, -0.1000]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "Attention weights = \n",
      "torch.Size([2, 2, 4, 4]) tensor([[[[0.2581, 0.2182, 0.2726, 0.2511],\n",
      "          [0.2571, 0.2338, 0.2564, 0.2527],\n",
      "          [0.2483, 0.2365, 0.2669, 0.2483],\n",
      "          [0.2603, 0.2433, 0.2402, 0.2563]],\n",
      "\n",
      "         [[0.3064, 0.2427, 0.2246, 0.2262],\n",
      "          [0.3301, 0.2385, 0.2138, 0.2177],\n",
      "          [0.3388, 0.2372, 0.2103, 0.2137],\n",
      "          [0.2874, 0.2451, 0.2340, 0.2334]]],\n",
      "\n",
      "\n",
      "        [[[0.2621, 0.2086, 0.2621, 0.2673],\n",
      "          [0.2410, 0.2273, 0.2410, 0.2906],\n",
      "          [0.2621, 0.2086, 0.2621, 0.2673],\n",
      "          [0.2697, 0.1746, 0.2697, 0.2859]],\n",
      "\n",
      "         [[0.2317, 0.3327, 0.2317, 0.2039],\n",
      "          [0.2473, 0.3094, 0.2473, 0.1960],\n",
      "          [0.2317, 0.3327, 0.2317, 0.2039],\n",
      "          [0.2371, 0.3674, 0.2371, 0.1584]]]], grad_fn=<SoftmaxBackward0>)\n",
      "Final Encoder 1 Output :\n",
      "norm2(norm1(x + self_atten(x)) + feed_fwd_op)\n",
      "\n",
      "tensor([[[-1.7717,  1.0139,  0.8717,  0.7944, -0.5755, -0.3329],\n",
      "         [-1.2808,  0.4273,  1.3453,  0.6620,  0.2178, -1.3717]],\n",
      "\n",
      "        [[-0.9654,  0.8572,  0.5801,  0.9906,  0.2550, -1.7176],\n",
      "         [-1.0663,  0.6882,  1.2795,  0.2363, -1.5811,  0.4435]],\n",
      "\n",
      "        [[-1.1293,  0.4712,  0.9395,  0.7408,  0.6177, -1.6399],\n",
      "         [-1.2808,  0.4273,  1.3453,  0.6620,  0.2178, -1.3717]],\n",
      "\n",
      "        [[-1.0709,  1.3239,  0.4174,  0.4109,  0.5017, -1.5829],\n",
      "         [-0.5506, -1.3929,  1.9274,  0.1706, -0.0614, -0.0932]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "\n",
      "Q_enc_2 = \n",
      "tensor([[[ 0.2289, -0.5269,  0.0316],\n",
      "         [-0.1144, -0.5508, -1.1111],\n",
      "         [ 0.2202, -0.5950, -1.0567],\n",
      "         [ 0.0420, -0.7964, -1.0580]],\n",
      "\n",
      "        [[-0.9560,  1.6078, -0.9373],\n",
      "         [-0.2186,  1.2584, -0.7177],\n",
      "         [-0.5085,  1.3105, -0.5441],\n",
      "         [-0.2844,  1.2172, -0.8302]],\n",
      "\n",
      "        [[ 0.4271, -0.4071, -0.6207],\n",
      "         [ 0.4600,  0.3031,  1.1440],\n",
      "         [ 0.4271, -0.4071, -0.6207],\n",
      "         [ 0.9201,  0.4825,  0.4964]],\n",
      "\n",
      "        [[-0.7901,  1.4355, -0.7203],\n",
      "         [-0.9863,  1.0246, -1.0800],\n",
      "         [-0.7901,  1.4355, -0.7203],\n",
      "         [-1.0440,  0.5866,  0.0615]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "K_enc_2 = \n",
      "tensor([[[ 0.5091, -0.6628, -0.9111],\n",
      "         [ 1.0458, -1.0039, -0.2222],\n",
      "         [ 0.6880, -0.6849, -0.3371],\n",
      "         [ 0.6396, -1.1779,  0.2599]],\n",
      "\n",
      "        [[-1.0284, -0.7130, -0.7505],\n",
      "         [-1.1035, -0.5919, -0.6427],\n",
      "         [-1.1733, -0.1343, -0.5221],\n",
      "         [-1.0854, -0.6629, -0.3711]],\n",
      "\n",
      "        [[ 0.6125, -0.4998, -0.7126],\n",
      "         [ 0.2681, -0.0638, -1.2828],\n",
      "         [ 0.6125, -0.4998, -0.7126],\n",
      "         [-0.1664,  1.0490, -1.4975]],\n",
      "\n",
      "        [[-1.1584, -0.0672, -0.4524],\n",
      "         [-0.3837, -0.4667, -0.1655],\n",
      "         [-1.1584, -0.0672, -0.4524],\n",
      "         [-0.3789,  1.1908,  0.0197]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "V_enc_2 = \n",
      "tensor([[[ 1.3087, -0.0618,  0.1470],\n",
      "         [ 1.2569,  0.6041, -0.1589],\n",
      "         [ 0.9768,  0.6941, -0.3513],\n",
      "         [ 1.3072,  0.5238,  0.3304]],\n",
      "\n",
      "        [[ 0.3940, -0.3864, -0.8816],\n",
      "         [ 0.0177, -0.0824, -1.0547],\n",
      "         [ 0.2080,  0.1214, -0.7919],\n",
      "         [ 0.6100,  0.0360, -1.2280]],\n",
      "\n",
      "        [[ 0.9345,  0.4384, -0.5372],\n",
      "         [ 0.6878, -0.7909, -0.2930],\n",
      "         [ 0.9345,  0.4384, -0.5372],\n",
      "         [-0.5623,  0.0331, -1.5234]],\n",
      "\n",
      "        [[ 0.1685,  0.0879, -0.6273],\n",
      "         [-0.0168, -0.3705, -0.1634],\n",
      "         [ 0.1685,  0.0879, -0.6273],\n",
      "         [-0.4351,  0.3204,  0.9106]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "Attention weights = \n",
      "torch.Size([2, 2, 4, 4]) tensor([[[[0.2240, 0.2702, 0.2334, 0.2724],\n",
      "          [0.3412, 0.2359, 0.2349, 0.1880],\n",
      "          [0.3214, 0.2542, 0.2335, 0.1909],\n",
      "          [0.3211, 0.2498, 0.2294, 0.1996]],\n",
      "\n",
      "         [[0.2165, 0.2382, 0.3547, 0.1906],\n",
      "          [0.2295, 0.2419, 0.3237, 0.2048],\n",
      "          [0.2210, 0.2393, 0.3325, 0.2072],\n",
      "          [0.2327, 0.2436, 0.3208, 0.2029]]],\n",
      "\n",
      "\n",
      "        [[[0.2648, 0.2693, 0.2648, 0.2012],\n",
      "          [0.3020, 0.2041, 0.3020, 0.1918],\n",
      "          [0.2648, 0.2693, 0.2648, 0.2012],\n",
      "          [0.2769, 0.2211, 0.2769, 0.2251]],\n",
      "\n",
      "         [[0.2450, 0.1097, 0.2450, 0.4002],\n",
      "          [0.2915, 0.1238, 0.2915, 0.2932],\n",
      "          [0.2450, 0.1097, 0.2450, 0.4002],\n",
      "          [0.2836, 0.1569, 0.2836, 0.2760]]]], grad_fn=<SoftmaxBackward0>)\n",
      "Final Encoder 2 Output :\n",
      "norm2(norm1(x + self_atten(x)) + feed_fwd_op)\n",
      "\n",
      "tensor([[[-1.8013,  1.0029,  0.3805,  1.1379, -0.5191, -0.2008],\n",
      "         [-1.2129,  0.5042,  1.1623,  0.6301,  0.4429, -1.5266]],\n",
      "\n",
      "        [[-1.3909,  1.0147,  0.2330,  1.3200, -0.0657, -1.1112],\n",
      "         [-1.3164,  0.8442,  1.1154,  0.4629, -1.4145,  0.3084]],\n",
      "\n",
      "        [[-1.5642,  0.7890,  0.5335,  1.1761,  0.1755, -1.1100],\n",
      "         [-1.2129,  0.5042,  1.1623,  0.6301,  0.4429, -1.5266]],\n",
      "\n",
      "        [[-1.4816,  1.3507,  0.1011,  0.9266,  0.1472, -1.0440],\n",
      "         [-0.6874, -1.2884,  1.9334,  0.2895, -0.0374, -0.2098]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "\n",
      "### Encoder Done ###\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "### Decoder Start ###\n",
      "torch.Size([5, 2, 6]) torch.Size([5, 2, 6]) torch.Size([5, 2, 6])\n",
      "5 4 3\n",
      "Q_dec_0 = \n",
      "tensor([[[ 0.4612,  0.7150, -0.1970],\n",
      "         [ 1.7537,  1.5231, -1.1108],\n",
      "         [ 1.1574,  1.0876, -0.3296],\n",
      "         [ 1.1634,  1.3648, -0.4997],\n",
      "         [ 0.1803,  1.6181, -0.5739]],\n",
      "\n",
      "        [[ 0.3807,  0.8797,  1.1237],\n",
      "         [-1.7384, -0.6668,  0.1902],\n",
      "         [-0.4216,  0.2208,  0.9168],\n",
      "         [-1.3201, -0.1142, -0.0148],\n",
      "         [-0.6772,  0.9390, -0.1867]],\n",
      "\n",
      "        [[-0.6600,  1.3372,  0.1437],\n",
      "         [-1.0110,  0.8553, -0.2146],\n",
      "         [ 0.1844,  0.8557, -0.3885],\n",
      "         [ 0.4514,  1.4858, -0.8963],\n",
      "         [ 0.0115,  0.6962,  0.0163]],\n",
      "\n",
      "        [[-1.3862,  1.5751, -0.4920],\n",
      "         [-0.6178,  1.8102, -0.4565],\n",
      "         [-0.9176,  0.6901,  0.0786],\n",
      "         [-1.8558,  0.2442, -0.5875],\n",
      "         [ 0.2698,  1.2986,  0.9597]]])\n",
      "\n",
      "K_dec_0 = \n",
      "tensor([[[ 1.8757, -0.6419,  0.5213],\n",
      "         [ 1.3558, -0.3728, -0.2499],\n",
      "         [ 2.2385, -0.6531,  0.3003],\n",
      "         [ 1.7762,  0.0192, -0.1201],\n",
      "         [ 1.4401,  0.5857, -0.6356]],\n",
      "\n",
      "        [[ 0.8497,  1.5520,  0.1447],\n",
      "         [-0.6737,  1.6369,  0.0997],\n",
      "         [ 0.4285,  1.7092, -0.0621],\n",
      "         [ 0.3476,  1.0604, -0.9397],\n",
      "         [ 1.6144,  1.4940, -0.3410]],\n",
      "\n",
      "        [[ 2.0420, -0.5998,  0.3753],\n",
      "         [ 0.8788,  0.0173,  0.4889],\n",
      "         [ 1.2026, -0.5522,  0.4932],\n",
      "         [ 0.7588, -0.1102, -0.3465],\n",
      "         [ 2.0290, -0.8308,  0.7390]],\n",
      "\n",
      "        [[-0.1979,  1.0160,  0.3778],\n",
      "         [ 0.4692,  0.2791, -0.5299],\n",
      "         [-0.4256,  0.7922, -0.0192],\n",
      "         [-0.6824,  1.1521,  0.4684],\n",
      "         [ 0.6302,  1.4267,  0.2994]]])\n",
      "\n",
      "V_dec_0 = \n",
      "tensor([[[ 0.1211, -2.3318, -1.2496],\n",
      "         [-0.8642, -0.9783, -1.0363],\n",
      "         [-0.3346, -2.1076, -1.4744],\n",
      "         [-1.3006, -0.7710, -0.9757],\n",
      "         [-1.2371, -1.1791, -0.8307]],\n",
      "\n",
      "        [[ 1.9385, -0.2112, -0.9854],\n",
      "         [ 0.9860,  2.0659, -1.6061],\n",
      "         [ 1.6842,  0.8099, -1.6885],\n",
      "         [ 0.9040,  1.2912, -1.7897],\n",
      "         [ 1.9020, -0.3485, -0.6405]],\n",
      "\n",
      "        [[-1.1081, -2.4076, -0.1160],\n",
      "         [-1.0105, -1.1840,  0.3056],\n",
      "         [-0.5781, -1.4158, -0.2908],\n",
      "         [-1.0825, -0.8964, -0.1283],\n",
      "         [ 0.0726, -2.7021, -1.0014]],\n",
      "\n",
      "        [[ 0.0640,  0.0498, -0.5188],\n",
      "         [ 0.3156, -0.7080,  0.0466],\n",
      "         [ 0.3486,  0.5931, -0.7173],\n",
      "         [ 0.2918,  1.0922, -0.4472],\n",
      "         [ 1.5656, -0.4102, -0.7978]]])\n",
      "\n",
      "Attention weights = \n",
      "torch.Size([2, 2, 5, 5]) tensor([[[[0.1600, 0.1700, 0.1799, 0.2202, 0.2698],\n",
      "          [0.0972, 0.1193, 0.1601, 0.2371, 0.3864],\n",
      "          [0.1449, 0.1403, 0.1912, 0.2319, 0.2917],\n",
      "          [0.1239, 0.1349, 0.1670, 0.2347, 0.3396],\n",
      "          [0.0965, 0.1518, 0.1067, 0.2191, 0.4260]],\n",
      "\n",
      "         [[0.2651, 0.1923, 0.2289, 0.0915, 0.2222],\n",
      "          [0.1102, 0.4897, 0.1548, 0.1957, 0.0496],\n",
      "          [0.2138, 0.3057, 0.2166, 0.1278, 0.1362],\n",
      "          [0.1310, 0.4161, 0.1790, 0.2002, 0.0737],\n",
      "          [0.1659, 0.3167, 0.2178, 0.1739, 0.1257]]],\n",
      "\n",
      "\n",
      "        [[[0.1237, 0.3133, 0.1785, 0.2773, 0.1072],\n",
      "          [0.1100, 0.2901, 0.1812, 0.3241, 0.0946],\n",
      "          [0.1920, 0.2243, 0.1751, 0.2508, 0.1577],\n",
      "          [0.1912, 0.2261, 0.1506, 0.3026, 0.1295],\n",
      "          [0.1851, 0.2357, 0.1879, 0.2220, 0.1693]],\n",
      "\n",
      "         [[0.2051, 0.0796, 0.2248, 0.3334, 0.1571],\n",
      "          [0.2032, 0.0942, 0.1936, 0.2719, 0.2371],\n",
      "          [0.2154, 0.1082, 0.2183, 0.2951, 0.1630],\n",
      "          [0.1952, 0.1171, 0.2761, 0.3242, 0.0874],\n",
      "          [0.2172, 0.0839, 0.1423, 0.2346, 0.3220]]]])\n",
      "Decoder Self Attention = \n",
      "tensor([[-0.8080, -1.4066, -1.0804,  1.5943,  0.5674, -1.2627],\n",
      "        [-0.8492, -1.4598, -0.1135,  0.4598,  0.3869, -0.5383],\n",
      "        [-0.9312, -1.3190, -1.0333,  1.2285,  1.3492, -1.5385],\n",
      "        [-0.8638, -1.4110, -0.1131,  0.5608,  0.2579, -0.5507],\n",
      "        [-0.8302, -1.4007, -1.0769,  1.4551,  0.8793, -1.3832],\n",
      "        [-0.8008, -1.6268, -0.1947,  0.4653,  0.3190, -0.5253],\n",
      "        [-0.8828, -1.3540, -1.0518,  1.2869,  1.2097, -1.5051],\n",
      "        [-0.8456, -1.5624, -0.1654,  0.3772,  0.4088, -0.5086],\n",
      "        [-0.9670, -1.2695, -1.0028,  1.3970,  0.9764, -1.4317],\n",
      "        [-0.7799, -1.6473, -0.2021,  0.6625,  0.1599, -0.5726]])\n",
      "\n",
      "Decoder_0 norm1(x + sa(x))\n",
      "tensor([[[-1.5813,  0.1190, -0.0332,  0.8399, -0.8044,  1.4600],\n",
      "         [ 0.0030, -1.6109,  0.7250,  0.5730, -0.9638,  1.2736]],\n",
      "\n",
      "        [[-2.0280,  0.2419, -0.4141,  0.5981,  0.7116,  0.8905],\n",
      "         [-0.4640, -0.1838,  1.9672, -0.0146, -1.3698,  0.0651]],\n",
      "\n",
      "        [[-1.7969,  0.4962, -0.4323,  0.6809, -0.2866,  1.3387],\n",
      "         [-1.6303, -0.5779,  1.0848,  1.2335, -0.4447,  0.3346]],\n",
      "\n",
      "        [[-1.9794,  1.1111, -0.1150,  0.0185,  0.0532,  0.9115],\n",
      "         [-1.8253, -0.8604,  0.6645,  0.8181,  0.8175,  0.3855]],\n",
      "\n",
      "        [[-1.8968,  0.4484,  0.5398,  0.1256, -0.4994,  1.2824],\n",
      "         [-0.6690, -0.4682,  0.4966,  1.1567, -1.6025,  1.0865]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "\n",
      "Q_dec_0 = \n",
      "tensor([[[-1.5483, -0.7983, -1.2635],\n",
      "         [-0.8117, -0.3117, -0.5359],\n",
      "         [-1.3457, -0.4852, -1.0727],\n",
      "         [-1.4904,  0.0178, -1.3222],\n",
      "         [-1.9093, -0.4975, -1.6295]],\n",
      "\n",
      "        [[-0.2318, -0.9707, -0.6952],\n",
      "         [-1.0906, -0.3877, -0.8562],\n",
      "         [-0.5315, -0.6356, -0.8392],\n",
      "         [-0.2879, -0.4508, -1.0513],\n",
      "         [ 0.0852, -0.8805, -0.8878]],\n",
      "\n",
      "        [[-0.7208, -1.2926, -0.3300],\n",
      "         [-1.5277, -0.5413, -1.6074],\n",
      "         [-1.1579, -0.9279, -1.1007],\n",
      "         [-0.5798, -0.7233, -0.3360],\n",
      "         [-1.3160, -1.0952, -1.1789]],\n",
      "\n",
      "        [[ 0.3141, -0.7120,  0.3669],\n",
      "         [ 1.1116, -1.4285, -0.2818],\n",
      "         [-0.4737, -1.6768, -0.6648],\n",
      "         [-1.0750, -0.9036, -0.5824],\n",
      "         [ 0.2480, -1.3860, -0.2596]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "K_dec_0 = \n",
      "tensor([[[-0.0186, -0.5460,  0.2203],\n",
      "         [-0.1492, -0.3401,  0.5566],\n",
      "         [-0.1565, -0.1457,  0.3307],\n",
      "         [-0.2247, -0.3982,  0.5040]],\n",
      "\n",
      "        [[ 0.6764,  0.9028, -0.1567],\n",
      "         [ 0.3858,  0.5791, -0.1208],\n",
      "         [ 0.4139,  0.5679, -0.3611],\n",
      "         [ 0.3441,  0.3737, -0.3499]],\n",
      "\n",
      "        [[-0.4881,  0.1675,  0.2365],\n",
      "         [-0.3853, -0.7232,  0.1445],\n",
      "         [-0.4881,  0.1675,  0.2365],\n",
      "         [-0.1405,  0.6472, -0.6238]],\n",
      "\n",
      "        [[-0.1162,  0.2022, -0.6936],\n",
      "         [ 0.0860,  0.8641, -0.1104],\n",
      "         [-0.1162,  0.2022, -0.6936],\n",
      "         [-0.0167,  0.5454, -0.4865]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "V_dec_0 = \n",
      "tensor([[[ 0.2656,  0.3090,  0.0818],\n",
      "         [ 0.0234,  0.3141,  0.2552],\n",
      "         [ 0.0679,  0.2161,  0.3214],\n",
      "         [-0.1433,  0.1568,  0.3225]],\n",
      "\n",
      "        [[-0.9749,  0.1841,  0.9268],\n",
      "         [-0.6353,  0.4146,  0.3636],\n",
      "         [-0.6979,  0.7098,  0.5145],\n",
      "         [-0.9209,  0.3016,  0.2045]],\n",
      "\n",
      "        [[ 0.1486, -0.1291,  0.2575],\n",
      "         [ 0.7640, -0.0023, -0.4907],\n",
      "         [ 0.1486, -0.1291,  0.2575],\n",
      "         [ 0.8041, -0.1381, -0.1243]],\n",
      "\n",
      "        [[-0.7274,  1.0670,  0.3328],\n",
      "         [-1.2351, -0.1445,  1.1570],\n",
      "         [-0.7274,  1.0670,  0.3328],\n",
      "         [-0.0426,  1.3181,  1.1637]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "Attention weights = \n",
      "torch.Size([2, 2, 5, 4]) tensor([[[[0.2793, 0.2233, 0.2424, 0.2550],\n",
      "          [0.2587, 0.2389, 0.2482, 0.2542],\n",
      "          [0.2687, 0.2279, 0.2496, 0.2538],\n",
      "          [0.2584, 0.2242, 0.2686, 0.2489],\n",
      "          [0.2740, 0.2174, 0.2563, 0.2524]],\n",
      "\n",
      "         [[0.1959, 0.2407, 0.2657, 0.2977],\n",
      "          [0.1922, 0.2438, 0.2704, 0.2935],\n",
      "          [0.1984, 0.2401, 0.2685, 0.2930],\n",
      "          [0.2096, 0.2341, 0.2704, 0.2858],\n",
      "          [0.2057, 0.2347, 0.2673, 0.2924]]],\n",
      "\n",
      "\n",
      "        [[[0.2170, 0.4113, 0.2170, 0.1546],\n",
      "          [0.2118, 0.2783, 0.2118, 0.2981],\n",
      "          [0.2149, 0.3427, 0.2149, 0.2275],\n",
      "          [0.2332, 0.3328, 0.2332, 0.2008],\n",
      "          [0.2106, 0.3642, 0.2106, 0.2145]],\n",
      "\n",
      "         [[0.2619, 0.2342, 0.2619, 0.2420],\n",
      "          [0.2962, 0.1777, 0.2962, 0.2300],\n",
      "          [0.3286, 0.1310, 0.3286, 0.2119],\n",
      "          [0.3080, 0.1581, 0.3080, 0.2258],\n",
      "          [0.3028, 0.1682, 0.3028, 0.2263]]]], grad_fn=<SoftmaxBackward0>)\n",
      "Cross attention in decoder_0\n",
      "tensor([[ 0.0593,  0.2488,  0.2400, -0.8035,  0.4142,  0.4667],\n",
      "        [ 0.5031, -0.0783, -0.1093, -0.6806,  0.8440,  0.7269],\n",
      "        [ 0.0547,  0.2485,  0.2439, -0.8014,  0.4169,  0.4660],\n",
      "        [ 0.5152, -0.0965, -0.0646, -0.6601,  0.9095,  0.6703],\n",
      "        [ 0.0573,  0.2483,  0.2422, -0.8032,  0.4150,  0.4693],\n",
      "        [ 0.5086, -0.0877, -0.0858, -0.6488,  0.9615,  0.6167],\n",
      "        [ 0.0565,  0.2473,  0.2449, -0.8051,  0.4138,  0.4770],\n",
      "        [ 0.4850, -0.0887, -0.0681, -0.6530,  0.9322,  0.6507],\n",
      "        [ 0.0591,  0.2479,  0.2416, -0.8054,  0.4131,  0.4733],\n",
      "        [ 0.5133, -0.0848, -0.0969, -0.6578,  0.9201,  0.6594]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "\n",
      "norm2(x' + mha(x', mem)), \n",
      " where x' = Decoder_curr_layer norm1(x + sa(x))\n",
      "tensor([[[-1.7184,  0.2625,  0.6900,  0.6839, -0.9667,  1.0486],\n",
      "         [ 0.0114, -1.2258,  1.0978, -0.1957, -1.1097,  1.4221]],\n",
      "\n",
      "        [[-2.2195,  0.3816,  0.4199,  0.5303,  0.2482,  0.6396],\n",
      "         [-0.3012, -0.0654,  1.9101, -0.6203, -1.3118,  0.3886]],\n",
      "\n",
      "        [[-1.9638,  0.5797,  0.3919,  0.5807, -0.5790,  0.9906],\n",
      "         [-1.4108, -0.4309,  1.5861,  0.3590, -0.8179,  0.7144]],\n",
      "\n",
      "        [[-2.0206,  1.0327,  0.6262,  0.0342, -0.2854,  0.6129],\n",
      "         [-1.7159, -0.7621,  1.2955, -0.0034,  0.3766,  0.8092]],\n",
      "\n",
      "        [[-1.8460,  0.4876,  1.0732,  0.1115, -0.6797,  0.8534],\n",
      "         [-0.5101, -0.3086,  0.9582,  0.2669, -1.7152,  1.3089]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "\n",
      "norm3(x'' + ff(x'')) \n",
      " where, x'' = Decoder_curr_layer norm2(x' + mha(x'))\n",
      "tensor([[[-1.8376,  0.2066,  0.8120,  0.3597, -0.6886,  1.1479],\n",
      "         [-0.3593, -1.1350,  1.1816, -0.4914, -0.7429,  1.5470]],\n",
      "\n",
      "        [[-2.1931,  0.3093,  0.6172,  0.2265,  0.2733,  0.7667],\n",
      "         [-0.7067, -0.0661,  1.8546, -0.8206, -0.9519,  0.6907]],\n",
      "\n",
      "        [[-2.0057,  0.4771,  0.5809,  0.2709, -0.4064,  1.0832],\n",
      "         [-1.6020, -0.3706,  1.5085,  0.0534, -0.4778,  0.8885]],\n",
      "\n",
      "        [[-2.0027,  0.8513,  0.7694, -0.1987, -0.1916,  0.7723],\n",
      "         [-1.7140, -0.6242,  1.2498, -0.2820,  0.4710,  0.8994]],\n",
      "\n",
      "        [[-1.8676,  0.3952,  1.0919, -0.1513, -0.4409,  0.9727],\n",
      "         [-0.8788, -0.3061,  1.0722, -0.0086, -1.3494,  1.4708]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "\n",
      "torch.Size([5, 2, 6]) torch.Size([5, 2, 6])\n",
      "torch.Size([5, 2, 6]) torch.Size([5, 2, 6]) torch.Size([5, 2, 6])\n",
      "5 4 3\n",
      "Q_dec_1 = \n",
      "tensor([[[-0.3478,  0.2786, -1.0721],\n",
      "         [ 0.1248, -0.1658, -1.3525],\n",
      "         [-0.3311, -0.0108, -1.2459],\n",
      "         [-0.1205, -0.3663, -1.4137],\n",
      "         [ 0.0043,  0.0022, -1.2550]],\n",
      "\n",
      "        [[ 1.4186, -0.3654,  0.1426],\n",
      "         [ 1.3630, -0.6031,  0.4038],\n",
      "         [ 1.3252, -0.3824,  0.3720],\n",
      "         [ 1.2297, -0.1969,  0.7153],\n",
      "         [ 1.4060, -0.1895,  0.4129]],\n",
      "\n",
      "        [[ 0.6535,  0.8112, -0.2569],\n",
      "         [ 0.4043,  0.3611, -0.6321],\n",
      "         [ 0.3170,  0.5157, -0.9179],\n",
      "         [ 1.0118,  0.1882, -1.0864],\n",
      "         [-0.2559,  0.7118, -0.5207]],\n",
      "\n",
      "        [[ 0.9452,  0.2540, -0.4830],\n",
      "         [ 1.1292,  0.3858,  0.1274],\n",
      "         [ 1.5812, -0.3431, -0.0938],\n",
      "         [ 1.4362, -0.4261,  0.0270],\n",
      "         [ 1.1559,  0.1304, -0.2180]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "K_dec_1 = \n",
      "tensor([[[-0.3217,  0.1830,  0.0944],\n",
      "         [-0.2553,  0.9009,  0.0822],\n",
      "         [-0.2668,  0.4474,  0.0538],\n",
      "         [-0.3970,  0.8009,  0.3342],\n",
      "         [-0.4534,  0.4555,  0.4167]],\n",
      "\n",
      "        [[ 0.1322,  0.1059,  0.8411],\n",
      "         [-0.3813,  0.5637,  0.4291],\n",
      "         [ 0.0729,  0.1125,  0.5715],\n",
      "         [-0.0709,  0.2176,  0.2650],\n",
      "         [-0.0754,  0.2723,  0.5854]],\n",
      "\n",
      "        [[-0.2391, -1.0596,  0.5661],\n",
      "         [-0.6948, -0.1852,  0.9850],\n",
      "         [-0.5111,  0.1938,  0.4657],\n",
      "         [-0.3480,  0.4931,  0.5029],\n",
      "         [-0.3414, -0.7848,  0.3403]],\n",
      "\n",
      "        [[-0.0041,  0.0509,  0.7262],\n",
      "         [-0.0380,  0.2270,  0.7053],\n",
      "         [-0.2930,  0.5855,  1.0287],\n",
      "         [-0.7766,  0.9277,  0.5393],\n",
      "         [ 0.4403, -0.2648,  0.9690]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "V_dec_1 = \n",
      "tensor([[[-0.4277, -0.9453, -0.5253],\n",
      "         [-0.7016, -0.9931, -0.4352],\n",
      "         [-0.4234, -0.8310, -0.4349],\n",
      "         [-0.2043, -0.8783, -0.1826],\n",
      "         [-0.3598, -1.0997, -0.3859]],\n",
      "\n",
      "        [[-0.0719,  0.4101,  0.2127],\n",
      "         [ 0.0152,  0.4791, -0.0197],\n",
      "         [ 0.0860,  0.3326,  0.1743],\n",
      "         [ 0.1643,  0.4083,  0.2276],\n",
      "         [-0.0601,  0.4966,  0.2286]],\n",
      "\n",
      "        [[-0.8145, -1.0011, -0.8979],\n",
      "         [ 0.0040, -1.2409, -0.2573],\n",
      "         [-0.6269, -1.4087, -0.6050],\n",
      "         [-1.1349, -1.4296, -0.7276],\n",
      "         [-0.2932, -0.8668, -0.6497]],\n",
      "\n",
      "        [[-0.5119,  0.1173,  0.0495],\n",
      "         [-0.4151,  0.6063,  0.3816],\n",
      "         [-0.4431,  0.7253,  0.1297],\n",
      "         [-0.4018,  0.5964, -0.1695],\n",
      "         [-0.2730,  0.2174,  0.3255]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "Attention weights = \n",
      "torch.Size([2, 2, 5, 5]) tensor([[[[0.1992, 0.2223, 0.2108, 0.1926, 0.1751],\n",
      "          [0.2230, 0.2111, 0.2253, 0.1733, 0.1673],\n",
      "          [0.2141, 0.2123, 0.2178, 0.1821, 0.1738],\n",
      "          [0.2328, 0.2011, 0.2267, 0.1689, 0.1705],\n",
      "          [0.2140, 0.2161, 0.2205, 0.1800, 0.1694]],\n",
      "\n",
      "         [[0.2443, 0.1408, 0.2273, 0.1927, 0.1949],\n",
      "          [0.2575, 0.1332, 0.2303, 0.1846, 0.1945],\n",
      "          [0.2515, 0.1405, 0.2265, 0.1856, 0.1958],\n",
      "          [0.2598, 0.1445, 0.2227, 0.1751, 0.1980],\n",
      "          [0.2518, 0.1431, 0.2248, 0.1838, 0.1965]]],\n",
      "\n",
      "\n",
      "        [[[0.1434, 0.1708, 0.2362, 0.2874, 0.1622],\n",
      "          [0.1757, 0.1627, 0.2221, 0.2423, 0.1973],\n",
      "          [0.1607, 0.1536, 0.2342, 0.2586, 0.1929],\n",
      "          [0.2015, 0.1306, 0.2098, 0.2329, 0.2253],\n",
      "          [0.1365, 0.1845, 0.2452, 0.2677, 0.1661]],\n",
      "\n",
      "         [[0.2090, 0.2117, 0.1774, 0.1642, 0.2376],\n",
      "          [0.2015, 0.2047, 0.1923, 0.1460, 0.2555],\n",
      "          [0.2168, 0.2032, 0.1474, 0.0909, 0.3417],\n",
      "          [0.2166, 0.2017, 0.1502, 0.0918, 0.3397],\n",
      "          [0.2107, 0.2093, 0.1741, 0.1376, 0.2684]]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Decoder Self Attention = \n",
      "tensor([[-0.4328, -0.9460, -0.3958,  0.0241,  0.4187,  0.1772],\n",
      "        [-0.6379, -1.2397, -0.6300, -0.4044,  0.4312,  0.1636],\n",
      "        [-0.4345, -0.9439, -0.4032,  0.0220,  0.4179,  0.1787],\n",
      "        [-0.6145, -1.2080, -0.6384, -0.4018,  0.4298,  0.1714],\n",
      "        [-0.4324, -0.9452, -0.3999,  0.0223,  0.4188,  0.1772],\n",
      "        [-0.6271, -1.2183, -0.6390, -0.3905,  0.3840,  0.2032],\n",
      "        [-0.4325, -0.9440, -0.4051,  0.0195,  0.4196,  0.1763],\n",
      "        [-0.6255, -1.1875, -0.6572, -0.3908,  0.3852,  0.2022],\n",
      "        [-0.4342, -0.9445, -0.4006,  0.0218,  0.4192,  0.1767],\n",
      "        [-0.6167, -1.2377, -0.6211, -0.4004,  0.4183,  0.1769]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "\n",
      "Decoder_1 norm1(x + sa(x))\n",
      "tensor([[[-1.7652e+00,  3.7769e-01,  5.5994e-01,  4.7656e-01, -8.5873e-01,\n",
      "           1.2097e+00],\n",
      "         [-7.2298e-01, -4.0094e-01,  6.6372e-01, -2.6902e-01, -1.1417e+00,\n",
      "           1.8709e+00]],\n",
      "\n",
      "        [[-2.1226e+00,  4.7836e-01,  4.2653e-01,  3.8757e-01, -1.2768e-01,\n",
      "           9.5779e-01],\n",
      "         [-9.8525e-01,  4.4706e-01,  1.1977e+00, -5.3629e-01, -1.3065e+00,\n",
      "           1.1833e+00]],\n",
      "\n",
      "        [[-1.8929e+00,  5.8660e-01,  3.8100e-01,  4.0684e-01, -6.4293e-01,\n",
      "           1.1614e+00],\n",
      "         [-1.6417e+00,  1.9233e-01,  8.9105e-01,  1.6024e-01, -8.8922e-01,\n",
      "           1.2873e+00]],\n",
      "\n",
      "        [[-1.9388e+00,  8.9826e-01,  5.3719e-01,  4.6107e-02, -4.9205e-01,\n",
      "           9.4931e-01],\n",
      "         [-1.8568e+00, -7.4096e-04,  7.5057e-01, -1.1734e-01, -1.7073e-01,\n",
      "           1.3950e+00]],\n",
      "\n",
      "        [[-1.8367e+00,  5.3891e-01,  7.9485e-01,  8.4786e-02, -6.8804e-01,\n",
      "           1.1062e+00],\n",
      "         [-1.0047e+00,  2.2500e-01,  5.0556e-01,  1.0867e-01, -1.4435e+00,\n",
      "           1.6090e+00]]], grad_fn=<AddBackward0>)\n",
      "\n",
      "Q_dec_1 = \n",
      "tensor([[[ 1.8945, -0.8742,  0.7459],\n",
      "         [ 1.6718, -0.6849,  0.6293],\n",
      "         [ 1.9135, -0.8638,  0.6521],\n",
      "         [ 2.0325, -0.9291,  0.5051],\n",
      "         [ 2.0290, -0.9388,  0.6615]],\n",
      "\n",
      "        [[ 1.0035,  0.9831,  0.1729],\n",
      "         [ 1.0737,  0.7489,  0.1074],\n",
      "         [ 0.9876,  0.9952,  0.0410],\n",
      "         [ 0.7449,  1.1226,  0.1096],\n",
      "         [ 0.8139,  1.1172,  0.3713]],\n",
      "\n",
      "        [[ 1.7493, -0.8848,  0.7593],\n",
      "         [ 2.1463, -1.1228,  0.5644],\n",
      "         [ 1.9608, -0.9215,  0.7820],\n",
      "         [ 1.7776, -0.7533,  0.7360],\n",
      "         [ 2.0108, -1.0326,  0.7011]],\n",
      "\n",
      "        [[ 0.5973,  1.2324,  0.8940],\n",
      "         [ 0.2173,  1.4744,  0.8025],\n",
      "         [ 0.8706,  1.0898,  0.5375],\n",
      "         [ 0.9552,  0.9570,  0.7185],\n",
      "         [ 0.6523,  1.3201,  0.3726]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "K_dec_1 = \n",
      "tensor([[[-0.7890, -0.7910,  0.3627],\n",
      "         [-0.6541, -0.5877,  0.8497],\n",
      "         [-0.2970, -0.6666,  0.9204],\n",
      "         [-0.6124, -0.5058,  0.7181]],\n",
      "\n",
      "        [[ 0.9334, -0.8464, -0.4720],\n",
      "         [ 1.1986, -0.9939, -0.2692],\n",
      "         [ 1.0127, -1.1084, -0.2972],\n",
      "         [ 1.1560, -0.5249,  0.1625]],\n",
      "\n",
      "        [[ 0.3582, -0.3951,  0.8824],\n",
      "         [-0.6886, -0.4561, -0.5103],\n",
      "         [ 0.3582, -0.3951,  0.8824],\n",
      "         [ 1.1311, -0.4773,  0.3183]],\n",
      "\n",
      "        [[ 0.8271, -1.3125, -0.3293],\n",
      "         [ 0.7059, -1.0208, -1.0201],\n",
      "         [ 0.8271, -1.3125, -0.3293],\n",
      "         [-0.3564, -1.8216, -1.3830]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "V_dec_1 = \n",
      "tensor([[[ 0.2817, -0.2321,  0.3553],\n",
      "         [ 0.2458,  0.1228,  0.2894],\n",
      "         [ 0.1086,  0.1445,  0.5631],\n",
      "         [ 0.2637, -0.1023,  0.0586]],\n",
      "\n",
      "        [[-0.4018, -0.5195, -0.1440],\n",
      "         [-0.1521, -0.5553,  0.1613],\n",
      "         [ 0.0885, -0.4423,  0.2221],\n",
      "         [-0.1636, -0.4475, -0.1032]],\n",
      "\n",
      "        [[-0.0805,  0.5336,  0.7379],\n",
      "         [ 0.3168,  0.0436,  0.2456],\n",
      "         [-0.0805,  0.5336,  0.7379],\n",
      "         [-0.4616,  0.7337,  1.5562]],\n",
      "\n",
      "        [[ 0.6010, -0.1542,  0.4101],\n",
      "         [-0.4081, -0.2836, -0.2422],\n",
      "         [ 0.6010, -0.1542,  0.4101],\n",
      "         [ 0.8964,  0.1685,  0.7602]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "Attention weights = \n",
      "torch.Size([2, 2, 5, 4]) tensor([[[[0.1799, 0.2321, 0.3679, 0.2202],\n",
      "          [0.1875, 0.2352, 0.3514, 0.2260],\n",
      "          [0.1831, 0.2307, 0.3656, 0.2207],\n",
      "          [0.1872, 0.2266, 0.3670, 0.2192],\n",
      "          [0.1809, 0.2285, 0.3723, 0.2183]],\n",
      "\n",
      "         [[0.2239, 0.2450, 0.2055, 0.3256],\n",
      "          [0.2252, 0.2522, 0.2135, 0.3090],\n",
      "          [0.2291, 0.2461, 0.2071, 0.3177],\n",
      "          [0.2312, 0.2385, 0.2041, 0.3262],\n",
      "          [0.2197, 0.2364, 0.2000, 0.3439]]],\n",
      "\n",
      "\n",
      "        [[[0.2518, 0.0490, 0.2518, 0.4475],\n",
      "          [0.2238, 0.0404, 0.2238, 0.5119],\n",
      "          [0.2433, 0.0410, 0.2433, 0.4725],\n",
      "          [0.2502, 0.0486, 0.2502, 0.4510],\n",
      "          [0.2367, 0.0414, 0.2367, 0.4852]],\n",
      "\n",
      "         [[0.3231, 0.2670, 0.3231, 0.0868],\n",
      "          [0.3068, 0.2812, 0.3068, 0.1052],\n",
      "          [0.3124, 0.2850, 0.3124, 0.0902],\n",
      "          [0.3248, 0.2680, 0.3248, 0.0824],\n",
      "          [0.2963, 0.3048, 0.2963, 0.1026]]]], grad_fn=<SoftmaxBackward0>)\n",
      "Cross attention in decoder_1\n",
      "tensor([[ 0.2057,  0.0174,  0.3511, -0.1623, -0.4889,  0.0193],\n",
      "        [-0.2316,  0.5991,  1.0800,  0.3572, -0.1607,  0.2663],\n",
      "        [ 0.2084,  0.0130,  0.3458, -0.1605, -0.4898,  0.0238],\n",
      "        [-0.2595,  0.6162,  1.1369,  0.3483, -0.1566,  0.2635],\n",
      "        [ 0.2062,  0.0161,  0.3506, -0.1631, -0.4894,  0.0199],\n",
      "        [-0.2443,  0.6081,  1.1044,  0.3400, -0.1620,  0.2558],\n",
      "        [ 0.2061,  0.0150,  0.3516, -0.1645, -0.4888,  0.0169],\n",
      "        [-0.2331,  0.6000,  1.0831,  0.3549, -0.1623,  0.2642],\n",
      "        [ 0.2051,  0.0175,  0.3528, -0.1628, -0.4878,  0.0154],\n",
      "        [-0.2490,  0.6104,  1.1146,  0.3238, -0.1605,  0.2472]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "\n",
      "norm2(x' + mha(x', mem)), \n",
      " where x' = Decoder_curr_layer norm1(x + sa(x))\n",
      "tensor([[[-1.8855,  0.7937,  0.5414, -0.0978, -0.4827,  1.1309],\n",
      "         [-1.4081,  0.0361,  0.9449, -0.9894, -0.0471,  1.4636]],\n",
      "\n",
      "        [[-2.0912,  0.8375,  0.3829, -0.1699,  0.1975,  0.8432],\n",
      "         [-1.4379,  0.8009,  1.3086, -1.1021, -0.1528,  0.5834]],\n",
      "\n",
      "        [[-1.9378,  0.9593,  0.3575, -0.1590, -0.2685,  1.0485],\n",
      "         [-1.9880,  0.5594,  1.0117, -0.4587,  0.1953,  0.6803]],\n",
      "\n",
      "        [[-1.8730,  1.1773,  0.4751, -0.4633, -0.1227,  0.8065],\n",
      "         [-1.9574,  0.3466,  0.7788, -0.6210,  0.7400,  0.7131]],\n",
      "\n",
      "        [[-1.8515,  0.8952,  0.7269, -0.4445, -0.3024,  0.9763],\n",
      "         [-1.7201,  0.7133,  0.8301, -0.6209, -0.3562,  1.1537]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "\n",
      "norm3(x'' + ff(x'')) \n",
      " where, x'' = Decoder_curr_layer norm2(x' + mha(x'))\n",
      "tensor([[[-2.0163,  0.4393,  0.4481,  0.0260, -0.1310,  1.2340],\n",
      "         [-1.4961, -0.2380,  0.8343, -0.8610,  0.2816,  1.4793]],\n",
      "\n",
      "        [[-2.1504,  0.5226,  0.2917, -0.0362,  0.4928,  0.8794],\n",
      "         [-1.5732,  0.5013,  1.3057, -1.0526,  0.1590,  0.6598]],\n",
      "\n",
      "        [[-2.0610,  0.6251,  0.2683, -0.0339,  0.0687,  1.1329],\n",
      "         [-2.0548,  0.2524,  0.9216, -0.3266,  0.4837,  0.7237]],\n",
      "\n",
      "        [[-2.0255,  0.8791,  0.4115, -0.3525,  0.1977,  0.8897],\n",
      "         [-1.9745,  0.0934,  0.6804, -0.4637,  0.9533,  0.7111]],\n",
      "\n",
      "        [[-1.9958,  0.5713,  0.6566, -0.3328,  0.0293,  1.0713],\n",
      "         [-1.8569,  0.3893,  0.7567, -0.5198, -0.0172,  1.2479]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "\n",
      "torch.Size([5, 2, 6]) torch.Size([5, 2, 6])\n",
      "torch.Size([5, 2, 6]) torch.Size([5, 2, 6]) torch.Size([5, 2, 6])\n",
      "5 4 3\n",
      "Q_dec_2 = \n",
      "tensor([[[-0.0340, -0.2817, -0.1928],\n",
      "         [ 0.3832, -0.1907, -0.1228],\n",
      "         [ 0.0655, -0.2777, -0.2202],\n",
      "         [ 0.2792, -0.0878, -0.1404],\n",
      "         [ 0.1595, -0.0709, -0.1178]],\n",
      "\n",
      "        [[-1.5353, -0.3455, -1.1560],\n",
      "         [-1.5665, -0.1099, -0.9450],\n",
      "         [-1.5342, -0.2805, -1.1256],\n",
      "         [-1.6819, -0.1264, -0.9888],\n",
      "         [-1.7430, -0.2483, -1.0512]],\n",
      "\n",
      "        [[-0.1311,  0.1617, -0.2290],\n",
      "         [ 0.4658,  0.4414,  0.1194],\n",
      "         [ 0.5858,  0.0994,  0.0851],\n",
      "         [ 0.6592,  0.1440,  0.0163],\n",
      "         [ 0.0166, -0.0038, -0.1604]],\n",
      "\n",
      "        [[-1.8189, -0.7019, -1.0974],\n",
      "         [-1.9698, -0.0729, -0.6745],\n",
      "         [-1.8036, -0.0172, -0.7619],\n",
      "         [-1.7659, -0.0746, -0.7106],\n",
      "         [-1.8059, -0.4098, -1.1128]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "K_dec_2 = \n",
      "tensor([[[-0.3675,  0.5522, -0.5522],\n",
      "         [-0.2497,  0.7782, -0.5343],\n",
      "         [-0.2778,  0.5042, -0.4718],\n",
      "         [ 0.0118,  0.5652, -0.5267],\n",
      "         [-0.1463,  0.6793, -0.7097]],\n",
      "\n",
      "        [[-0.4097,  0.6536,  0.3218],\n",
      "         [-0.3929,  0.3036,  0.6453],\n",
      "         [-0.3343,  0.5737,  0.4225],\n",
      "         [-0.4402,  0.4265,  0.5949],\n",
      "         [-0.5587,  0.5041,  0.5391]],\n",
      "\n",
      "        [[-0.4935,  0.7820, -1.3030],\n",
      "         [ 0.2609,  0.9656, -1.1049],\n",
      "         [-0.1387,  1.1645, -0.8971],\n",
      "         [-0.2318,  1.2048, -0.9823],\n",
      "         [-0.2259,  0.6660, -0.8903]],\n",
      "\n",
      "        [[-0.6707,  0.3962,  0.8260],\n",
      "         [-0.9366,  0.1653,  0.8494],\n",
      "         [-0.7516,  0.1390,  0.7972],\n",
      "         [-0.6610, -0.0569,  1.0450],\n",
      "         [-0.6099,  0.5417,  0.5814]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "V_dec_2 = \n",
      "tensor([[[-0.8205, -1.3382, -0.4877],\n",
      "         [-0.7713, -1.3616, -0.4134],\n",
      "         [-0.8657, -1.2872, -0.5563],\n",
      "         [-0.7155, -1.1019, -0.4909],\n",
      "         [-0.6688, -1.2134, -0.3151]],\n",
      "\n",
      "        [[ 0.9973, -0.1155,  0.1120],\n",
      "         [ 1.2136,  0.3519, -0.2628],\n",
      "         [ 1.0259,  0.0674,  0.0073],\n",
      "         [ 1.0748,  0.0969, -0.2250],\n",
      "         [ 1.0516, -0.1376, -0.1671]],\n",
      "\n",
      "        [[-0.5446, -1.1736,  0.5781],\n",
      "         [-0.1364, -0.8122,  0.2697],\n",
      "         [-0.4299, -1.3329, -0.0038],\n",
      "         [-0.4664, -1.3360,  0.2353],\n",
      "         [-0.6423, -1.1863, -0.0887]],\n",
      "\n",
      "        [[ 0.6683, -0.5586, -0.5735],\n",
      "         [ 0.9825, -0.4153, -0.6801],\n",
      "         [ 1.3113,  0.1060, -0.5105],\n",
      "         [ 1.2617,  0.3120, -0.8026],\n",
      "         [ 0.9183, -0.3506, -0.2321]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "Attention weights = \n",
      "torch.Size([2, 2, 5, 5]) tensor([[[[0.2025, 0.1944, 0.2020, 0.2001, 0.2010],\n",
      "          [0.1941, 0.1941, 0.1980, 0.2105, 0.2033],\n",
      "          [0.2006, 0.1939, 0.2008, 0.2024, 0.2022],\n",
      "          [0.1953, 0.1965, 0.1974, 0.2071, 0.2037],\n",
      "          [0.1974, 0.1975, 0.1984, 0.2040, 0.2026]],\n",
      "\n",
      "         [[0.2149, 0.1830, 0.1910, 0.1925, 0.2186],\n",
      "          [0.2147, 0.1812, 0.1908, 0.1929, 0.2203],\n",
      "          [0.2155, 0.1821, 0.1913, 0.1923, 0.2187],\n",
      "          [0.2151, 0.1805, 0.1898, 0.1927, 0.2219],\n",
      "          [0.2139, 0.1817, 0.1887, 0.1931, 0.2225]]],\n",
      "\n",
      "\n",
      "        [[[0.2089, 0.1955, 0.1997, 0.2042, 0.1917],\n",
      "          [0.1711, 0.2227, 0.2134, 0.2091, 0.1837],\n",
      "          [0.1742, 0.2295, 0.2048, 0.1981, 0.1933],\n",
      "          [0.1727, 0.2341, 0.2048, 0.1982, 0.1901],\n",
      "          [0.2044, 0.2021, 0.1974, 0.1988, 0.1973]],\n",
      "\n",
      "         [[0.1746, 0.2497, 0.2148, 0.1807, 0.1803],\n",
      "          [0.1845, 0.2498, 0.2068, 0.1708, 0.1882],\n",
      "          [0.1866, 0.2441, 0.2061, 0.1685, 0.1947],\n",
      "          [0.1859, 0.2439, 0.2066, 0.1716, 0.1920],\n",
      "          [0.1796, 0.2465, 0.2115, 0.1719, 0.1905]]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Decoder Self Attention = \n",
      "tensor([[-0.7686, -1.2601, -0.4531,  1.0691,  0.0410, -0.1025],\n",
      "        [-0.4446, -1.1703,  0.2037,  1.0372, -0.1853, -0.5665],\n",
      "        [-0.7670, -1.2576, -0.4524,  1.0689,  0.0402, -0.1024],\n",
      "        [-0.4308, -1.1634,  0.1911,  1.0281, -0.1976, -0.5620],\n",
      "        [-0.7681, -1.2594, -0.4528,  1.0690,  0.0406, -0.1022],\n",
      "        [-0.4308, -1.1579,  0.1913,  1.0262, -0.1995, -0.5587],\n",
      "        [-0.7671, -1.2584, -0.4521,  1.0688,  0.0396, -0.1024],\n",
      "        [-0.4286, -1.1562,  0.1920,  1.0276, -0.1971, -0.5603],\n",
      "        [-0.7676, -1.2592, -0.4523,  1.0691,  0.0401, -0.1031],\n",
      "        [-0.4432, -1.1668,  0.2012,  1.0314, -0.1935, -0.5608]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "\n",
      "Decoder_2 norm1(x + sa(x))\n",
      "tensor([[[-2.0390,  0.7425,  1.0619,  0.1849, -0.2279,  0.2777],\n",
      "         [-1.3346,  0.4545,  1.3226, -1.3495,  0.3304,  0.5766]],\n",
      "\n",
      "        [[-2.1002,  0.8016,  0.8545,  0.1119,  0.4387, -0.1064],\n",
      "         [-1.1293,  1.0006,  1.4554, -1.2308,  0.1590, -0.2548]],\n",
      "\n",
      "        [[-2.0803,  0.9430,  0.8609,  0.1188, -0.0088,  0.1663],\n",
      "         [-1.7183,  0.8707,  1.2485, -0.6674,  0.4865, -0.2201]],\n",
      "\n",
      "        [[-1.9211,  1.1495,  0.9574, -0.2163,  0.1244, -0.0939],\n",
      "         [-1.6673,  0.7304,  1.0310, -0.8131,  0.9528, -0.2338]],\n",
      "\n",
      "        [[-1.9259,  0.8484,  1.2327, -0.2001, -0.0495,  0.0944],\n",
      "         [-1.6157,  1.0609,  1.1596, -0.9100,  0.0035,  0.3017]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "\n",
      "Q_dec_2 = \n",
      "tensor([[[ 0.0090,  0.4867,  0.6530],\n",
      "         [-0.0074,  0.7634,  0.3587],\n",
      "         [-0.0684,  0.5642,  0.5266],\n",
      "         [-0.0917,  0.8202,  0.3255],\n",
      "         [ 0.0542,  0.7442,  0.4573]],\n",
      "\n",
      "        [[-0.9873,  0.7099,  0.0631],\n",
      "         [-0.8562,  0.8962, -0.1300],\n",
      "         [-1.0728,  0.6944, -0.0572],\n",
      "         [-0.8803,  0.8151,  0.0202],\n",
      "         [-0.7637,  0.8901,  0.1596]],\n",
      "\n",
      "        [[ 0.4424,  0.7532, -0.0021],\n",
      "         [ 0.1452,  1.1584, -0.0829],\n",
      "         [ 0.1304,  1.0800,  0.0773],\n",
      "         [ 0.2058,  1.1189, -0.1379],\n",
      "         [ 0.0975,  0.7798,  0.1912]],\n",
      "\n",
      "        [[-0.2667,  1.0012,  0.3442],\n",
      "         [-0.0715,  1.0652,  0.4572],\n",
      "         [-0.4050,  1.1255,  0.1626],\n",
      "         [-0.3084,  1.1814,  0.0228],\n",
      "         [-0.7012,  0.8247,  0.2357]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "K_dec_2 = \n",
      "tensor([[[ 0.2996,  0.2977,  0.7836],\n",
      "         [-0.0363,  0.6465,  0.3285],\n",
      "         [-0.0193,  0.4305,  0.2183],\n",
      "         [ 0.2289,  0.4600,  0.4609]],\n",
      "\n",
      "        [[-0.6700, -0.3637, -0.9730],\n",
      "         [-1.4401, -0.1514, -1.4035],\n",
      "         [-1.2863, -0.1913, -1.2202],\n",
      "         [-1.5475, -0.1572, -1.2230]],\n",
      "\n",
      "        [[ 0.0092,  0.5145, -0.1354],\n",
      "         [ 0.6587,  0.5903,  1.0065],\n",
      "         [ 0.0092,  0.5145, -0.1354],\n",
      "         [-0.2660, -0.0730, -0.4458]],\n",
      "\n",
      "        [[-1.0598, -0.1247, -1.1359],\n",
      "         [ 0.5453, -0.4586, -0.7660],\n",
      "         [-1.0598, -0.1247, -1.1359],\n",
      "         [ 1.0256, -0.2511, -0.0107]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "V_dec_2 = \n",
      "tensor([[[-0.1937, -0.0298,  0.7223],\n",
      "         [-0.7177, -0.3821,  0.3268],\n",
      "         [-0.7481, -0.3513,  0.6030],\n",
      "         [-0.4517, -0.4936,  0.3106]],\n",
      "\n",
      "        [[ 0.8744, -0.4382, -0.7092],\n",
      "         [ 0.6252, -0.7886,  0.0821],\n",
      "         [ 0.6682, -0.7998,  0.0293],\n",
      "         [ 0.7026, -0.7816,  0.2585]],\n",
      "\n",
      "        [[-0.7418, -0.5217,  0.4601],\n",
      "         [ 0.5024,  0.1557,  0.3832],\n",
      "         [-0.7418, -0.5217,  0.4601],\n",
      "         [-0.4881,  0.3204,  1.0185]],\n",
      "\n",
      "        [[ 0.5126, -0.6385,  0.3217],\n",
      "         [ 0.8222,  0.3879, -1.3130],\n",
      "         [ 0.5126, -0.6385,  0.3217],\n",
      "         [ 0.1975,  0.1538, -0.8837]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "Attention weights = \n",
      "torch.Size([2, 2, 5, 4]) tensor([[[[0.2709, 0.2513, 0.2269, 0.2510],\n",
      "          [0.2492, 0.2649, 0.2354, 0.2505],\n",
      "          [0.2606, 0.2576, 0.2320, 0.2498],\n",
      "          [0.2441, 0.2691, 0.2377, 0.2490],\n",
      "          [0.2561, 0.2611, 0.2312, 0.2516]],\n",
      "\n",
      "         [[0.1679, 0.2797, 0.2538, 0.2986],\n",
      "          [0.1682, 0.2837, 0.2540, 0.2942],\n",
      "          [0.1600, 0.2848, 0.2533, 0.3019],\n",
      "          [0.1718, 0.2793, 0.2541, 0.2948],\n",
      "          [0.1816, 0.2734, 0.2545, 0.2905]]],\n",
      "\n",
      "\n",
      "        [[[0.2538, 0.3092, 0.2538, 0.1833],\n",
      "          [0.2687, 0.2826, 0.2687, 0.1799],\n",
      "          [0.2612, 0.3026, 0.2612, 0.1749],\n",
      "          [0.2692, 0.2789, 0.2692, 0.1827],\n",
      "          [0.2533, 0.3084, 0.2533, 0.1850]],\n",
      "\n",
      "         [[0.2828, 0.1960, 0.2828, 0.2384],\n",
      "          [0.2511, 0.2110, 0.2511, 0.2868],\n",
      "          [0.3124, 0.1789, 0.3124, 0.1964],\n",
      "          [0.3083, 0.1854, 0.3083, 0.1980],\n",
      "          [0.3401, 0.1593, 0.3401, 0.1604]]]], grad_fn=<SoftmaxBackward0>)\n",
      "Cross attention in decoder_2\n",
      "tensor([[-0.5159, -0.3077,  0.4925,  0.7011, -0.7305, -0.0115],\n",
      "        [-0.3106, -0.1579,  0.5387,  0.4982, -0.2484, -0.2861],\n",
      "        [-0.5277, -0.3150,  0.4863,  0.7008, -0.7305, -0.0125],\n",
      "        [-0.3445, -0.1787,  0.5388,  0.4876, -0.1947, -0.3690],\n",
      "        [-0.5218, -0.3110,  0.4899,  0.6993, -0.7332, -0.0047],\n",
      "        [-0.3209, -0.1694,  0.5345,  0.5061, -0.2993, -0.2074],\n",
      "        [-0.5308, -0.3165,  0.4850,  0.7018, -0.7292, -0.0153],\n",
      "        [-0.3485, -0.1789,  0.5407,  0.5076, -0.2913, -0.2200],\n",
      "        [-0.5236, -0.3128,  0.4879,  0.7039, -0.7258, -0.0238],\n",
      "        [-0.3112, -0.1570,  0.5397,  0.5114, -0.3479, -0.1321]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "\n",
      "norm2(x' + mha(x', mem)), \n",
      " where x' = Decoder_curr_layer norm1(x + sa(x))\n",
      "tensor([[[-2.1477,  0.2547,  0.9863,  0.5233,  0.2253,  0.1581],\n",
      "         [-1.5385,  0.1645,  1.2393, -1.0824,  0.8894,  0.3278]],\n",
      "\n",
      "        [[-2.1261,  0.2797,  0.8009,  0.4606,  0.7264, -0.1415],\n",
      "         [-1.3729,  0.5857,  1.3946, -0.9985,  0.8093, -0.4182]],\n",
      "\n",
      "        [[-2.1808,  0.4097,  0.8303,  0.4769,  0.3930,  0.0710],\n",
      "         [-1.8064,  0.5114,  1.1356, -0.4535,  0.9358, -0.3228]],\n",
      "\n",
      "        [[-2.1111,  0.5760,  0.9351,  0.2240,  0.5168, -0.1408],\n",
      "         [-1.7064,  0.3592,  0.9364, -0.5481,  1.2925, -0.3337]],\n",
      "\n",
      "        [[-2.0942,  0.3319,  1.1440,  0.2284,  0.3784,  0.0115],\n",
      "         [-1.8439,  0.7497,  1.1253, -0.6902,  0.5268,  0.1324]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "\n",
      "norm3(x'' + ff(x'')) \n",
      " where, x'' = Decoder_curr_layer norm2(x' + mha(x'))\n",
      "tensor([[[-2.0880, -0.0851,  1.0847,  0.5607,  0.3007,  0.2271],\n",
      "         [-1.4874, -0.1941,  1.3059, -0.9802,  0.9648,  0.3910]],\n",
      "\n",
      "        [[-2.0665, -0.0639,  0.9127,  0.5283,  0.7781, -0.0887],\n",
      "         [-1.3847,  0.1682,  1.5225, -0.8750,  0.9203, -0.3512]],\n",
      "\n",
      "        [[-2.1389,  0.0537,  0.9503,  0.5363,  0.4613,  0.1374],\n",
      "         [-1.7837,  0.1121,  1.2570, -0.3480,  1.0191, -0.2566]],\n",
      "\n",
      "        [[-2.0898,  0.1936,  1.0671,  0.3093,  0.5957, -0.0759],\n",
      "         [-1.6731, -0.0233,  1.0620, -0.4374,  1.3441, -0.2722]],\n",
      "\n",
      "        [[-2.0405, -0.0294,  1.2378,  0.2863,  0.4630,  0.0828],\n",
      "         [-1.8531,  0.3335,  1.2730, -0.6143,  0.6379,  0.2230]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "\n",
      "torch.Size([5, 2, 6]) torch.Size([5, 2, 6])\n",
      "### Decoder Done ###\n"
     ]
    }
   ],
   "source": [
    "d_model = 6\n",
    "num_encoder_layers = 3\n",
    "num_decoder_layers = 3\n",
    "\n",
    "final_op = get_all_intermediate_outputs(src_sentence, tgt_sentence, model = model, num_encoder_layers = num_encoder_layers , num_decoder_layers = num_encoder_layers, d_model=d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5, 2, 10]), torch.Size([5, 2, 10]))"
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape, final_op.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0351, -1.1913, -0.3768, -0.1533, -0.6715,  0.2967,  0.9567,\n",
       "          -0.4592, -0.7431,  0.5005],\n",
       "         [ 0.5240, -1.0788, -0.4459, -0.1862, -0.0582,  0.5289,  0.4782,\n",
       "          -0.9111, -0.0584,  0.9501]],\n",
       "\n",
       "        [[ 0.3253, -1.2071, -0.4222, -0.2448, -0.6172,  0.4023,  0.9607,\n",
       "          -0.6148, -0.7525,  0.4649],\n",
       "         [ 0.6851, -1.1969, -0.5047,  0.0477, -0.4231,  0.6040,  0.7750,\n",
       "          -0.5608, -0.2766,  0.7577]],\n",
       "\n",
       "        [[ 0.1552, -1.1610, -0.4760, -0.2129, -0.6705,  0.3657,  1.0074,\n",
       "          -0.4979, -0.7873,  0.5562],\n",
       "         [ 0.6300, -1.2311, -0.5301, -0.1492, -0.4834,  0.5986,  0.8887,\n",
       "          -0.6590, -0.5067,  0.7288]],\n",
       "\n",
       "        [[ 0.3160, -1.2016, -0.5434, -0.1745, -0.7006,  0.4669,  1.0648,\n",
       "          -0.4734, -0.7825,  0.6207],\n",
       "         [ 0.7733, -1.1772, -0.5043, -0.2409, -0.3084,  0.6118,  0.7447,\n",
       "          -0.8627, -0.3858,  0.7044]],\n",
       "\n",
       "        [[ 0.1892, -1.2463, -0.4136, -0.1336, -0.6515,  0.3959,  0.9682,\n",
       "          -0.4872, -0.6962,  0.5847],\n",
       "         [ 0.4427, -1.1010, -0.6811, -0.1530, -0.4359,  0.5963,  0.8850,\n",
       "          -0.5812, -0.4528,  1.0195]]], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0351, -1.1913, -0.3768, -0.1533, -0.6715,  0.2967,  0.9567,\n",
       "          -0.4592, -0.7431,  0.5005],\n",
       "         [ 0.5240, -1.0788, -0.4459, -0.1862, -0.0582,  0.5289,  0.4782,\n",
       "          -0.9111, -0.0584,  0.9501]],\n",
       "\n",
       "        [[ 0.3253, -1.2071, -0.4222, -0.2448, -0.6172,  0.4023,  0.9607,\n",
       "          -0.6148, -0.7525,  0.4649],\n",
       "         [ 0.6851, -1.1969, -0.5047,  0.0477, -0.4231,  0.6040,  0.7749,\n",
       "          -0.5608, -0.2766,  0.7577]],\n",
       "\n",
       "        [[ 0.1552, -1.1610, -0.4760, -0.2129, -0.6705,  0.3657,  1.0074,\n",
       "          -0.4979, -0.7873,  0.5562],\n",
       "         [ 0.6300, -1.2311, -0.5301, -0.1492, -0.4834,  0.5986,  0.8887,\n",
       "          -0.6590, -0.5067,  0.7288]],\n",
       "\n",
       "        [[ 0.3160, -1.2016, -0.5434, -0.1745, -0.7005,  0.4669,  1.0648,\n",
       "          -0.4734, -0.7824,  0.6207],\n",
       "         [ 0.7733, -1.1772, -0.5043, -0.2409, -0.3084,  0.6118,  0.7447,\n",
       "          -0.8627, -0.3858,  0.7044]],\n",
       "\n",
       "        [[ 0.1892, -1.2463, -0.4136, -0.1336, -0.6515,  0.3959,  0.9682,\n",
       "          -0.4872, -0.6962,  0.5847],\n",
       "         [ 0.4427, -1.1010, -0.6811, -0.1530, -0.4359,  0.5963,  0.8850,\n",
       "          -0.5812, -0.4528,  1.0195]]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 276,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "my_project_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
