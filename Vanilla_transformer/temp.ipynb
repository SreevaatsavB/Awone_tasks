{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import math\n",
    "import copy\n",
    "\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        # Ensure that the model dimension (d_model) is divisible by the number of heads\n",
    "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
    "        \n",
    "        # Initialize dimensions\n",
    "        self.d_model = d_model # Model's dimension\n",
    "        self.num_heads = num_heads # Number of attention heads\n",
    "        self.d_k = d_model // num_heads # Dimension of each head's key, query, and value\n",
    "        \n",
    "        # Linear layers for transforming inputs\n",
    "        self.W_q = nn.Linear(d_model, d_model) # Query transformation\n",
    "        self.W_k = nn.Linear(d_model, d_model) # Key transformation\n",
    "        self.W_v = nn.Linear(d_model, d_model) # Value transformation\n",
    "        self.W_o = nn.Linear(d_model, d_model) # Output transformation\n",
    "        \n",
    "    def scaled_dot_product_attention(self, Q, K, V, mask=None):\n",
    "        # Calculate attention scores\n",
    "        attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "        \n",
    "        # Apply mask if provided (useful for preventing attention to certain parts like padding)\n",
    "        if mask is not None:\n",
    "            attn_scores = attn_scores.masked_fill(mask == 0, -1e9)\n",
    "        \n",
    "        # Softmax is applied to obtain attention probabilities\n",
    "        attn_probs = torch.softmax(attn_scores, dim=-1)\n",
    "        \n",
    "        # Multiply by values to obtain the final output\n",
    "        output = torch.matmul(attn_probs, V)\n",
    "        return output\n",
    "        \n",
    "    def split_heads(self, x):\n",
    "        # Reshape the input to have num_heads for multi-head attention\n",
    "        batch_size, seq_length, d_model = x.size()\n",
    "        return x.view(batch_size, seq_length, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        \n",
    "    def combine_heads(self, x):\n",
    "        # Combine the multiple heads back to original shape\n",
    "        batch_size, _, seq_length, d_k = x.size()\n",
    "        return x.transpose(1, 2).contiguous().view(batch_size, seq_length, self.d_model)\n",
    "        \n",
    "    def forward(self, Q, K, V, mask=None):\n",
    "        # Apply linear transformations and split heads\n",
    "        Q = self.split_heads(self.W_q(Q))\n",
    "        K = self.split_heads(self.W_k(K))\n",
    "        V = self.split_heads(self.W_v(V))\n",
    "        \n",
    "        # Perform scaled dot-product attention\n",
    "        attn_output = self.scaled_dot_product_attention(Q, K, V, mask)\n",
    "        \n",
    "        # Combine heads and apply output transformation\n",
    "        output = self.W_o(self.combine_heads(attn_output))\n",
    "        return output\n",
    "    \n",
    "\n",
    "\n",
    "class PositionWiseFeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super(PositionWiseFeedForward, self).__init__()\n",
    "        self.fc1 = nn.Linear(d_model, d_ff)\n",
    "        self.fc2 = nn.Linear(d_ff, d_model)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc2(self.relu(self.fc1(x)))\n",
    "    \n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_seq_length):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        \n",
    "        pe = torch.zeros(max_seq_length, d_model)\n",
    "        position = torch.arange(0, max_seq_length, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model))\n",
    "        \n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        \n",
    "        self.register_buffer('pe', pe.unsqueeze(0))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.size(1)]\n",
    "\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.feed_forward = PositionWiseFeedForward(d_model, d_ff)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, mask):\n",
    "        attn_output = self.self_attn(x, x, x, mask)\n",
    "        x = self.norm1(x + self.dropout(attn_output))\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.norm2(x + self.dropout(ff_output))\n",
    "        return x\n",
    "    \n",
    "\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.cross_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.feed_forward = PositionWiseFeedForward(d_model, d_ff)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, enc_output, src_mask, tgt_mask):\n",
    "        attn_output = self.self_attn(x, x, x, tgt_mask)\n",
    "        x = self.norm1(x + self.dropout(attn_output))\n",
    "        attn_output = self.cross_attn(x, enc_output, enc_output, src_mask)\n",
    "        x = self.norm2(x + self.dropout(attn_output))\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.norm3(x + self.dropout(ff_output))\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, src_vocab_size, tgt_vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_length, dropout):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.encoder_embedding = nn.Embedding(src_vocab_size, d_model)\n",
    "        self.decoder_embedding = nn.Embedding(tgt_vocab_size, d_model)\n",
    "        self.positional_encoding = PositionalEncoding(d_model, max_seq_length)\n",
    "\n",
    "        self.encoder_layers = nn.ModuleList([EncoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
    "        self.decoder_layers = nn.ModuleList([DecoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
    "\n",
    "        self.fc = nn.Linear(d_model, tgt_vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def generate_mask(self, src, tgt):\n",
    "        src_mask = (src != 0).unsqueeze(1).unsqueeze(2)\n",
    "        tgt_mask = (tgt != 0).unsqueeze(1).unsqueeze(3)\n",
    "        seq_length = tgt.size(1)\n",
    "        nopeak_mask = (1 - torch.triu(torch.ones(1, seq_length, seq_length), diagonal=1)).bool()\n",
    "        tgt_mask = tgt_mask & nopeak_mask\n",
    "        return src_mask, tgt_mask\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        src_mask, tgt_mask = self.generate_mask(src, tgt)\n",
    "        src_embedded = self.dropout(self.positional_encoding(self.encoder_embedding(src)))\n",
    "        tgt_embedded = self.dropout(self.positional_encoding(self.decoder_embedding(tgt)))\n",
    "\n",
    "\n",
    "        # print(src_embedded.shape, tgt_embedded.shape)\n",
    "        # print()\n",
    "\n",
    "        enc_output = src_embedded\n",
    "        for enc_layer in self.encoder_layers:\n",
    "            enc_output = enc_layer(enc_output, src_mask)\n",
    "\n",
    "        dec_output = tgt_embedded\n",
    "        for dec_layer in self.decoder_layers:\n",
    "            dec_output = dec_layer(dec_output, enc_output, src_mask, tgt_mask)\n",
    "\n",
    "        output = self.fc(dec_output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# src_vocab_size = 5000\n",
    "# tgt_vocab_size = 5000\n",
    "# d_model = 512\n",
    "# num_heads = 8\n",
    "# num_layers = 6\n",
    "# d_ff = 2048\n",
    "# max_seq_length = 100\n",
    "# dropout = 0.1\n",
    "\n",
    "src_vocab_size = 200\n",
    "tgt_vocab_size = 200\n",
    "d_model = 512\n",
    "num_heads = 8\n",
    "num_encoder_layers = 6\n",
    "num_decoder_layers = 6\n",
    "num_layers = num_encoder_layers\n",
    "d_ff = 1024\n",
    "max_seq_length = 50\n",
    "dropout = 0\n",
    "\n",
    "transformer = Transformer(src_vocab_size, tgt_vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_length, dropout)\n",
    "\n",
    "# Generate random sample data\n",
    "src_data = torch.randint(1, src_vocab_size, (32, max_seq_length))  # (batch_size, seq_length)\n",
    "tgt_data = torch.randint(1, tgt_vocab_size, (32, max_seq_length))  # (batch_size, seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 50]) torch.Size([32, 49])\n",
      "torch.Size([32, 49, 200])\n",
      "####\n",
      "torch.Size([1568, 200]) torch.Size([1568])\n",
      "Epoch: 1, Loss: 5.452166557312012\n",
      "\n",
      "torch.Size([32, 50]) torch.Size([32, 49])\n",
      "torch.Size([32, 49, 200])\n",
      "####\n",
      "torch.Size([1568, 200]) torch.Size([1568])\n",
      "Epoch: 2, Loss: 5.264164924621582\n",
      "\n",
      "torch.Size([32, 50]) torch.Size([32, 49])\n",
      "torch.Size([32, 49, 200])\n",
      "####\n",
      "torch.Size([1568, 200]) torch.Size([1568])\n",
      "Epoch: 3, Loss: 5.169758319854736\n",
      "\n",
      "torch.Size([32, 50]) torch.Size([32, 49])\n",
      "torch.Size([32, 49, 200])\n",
      "####\n",
      "torch.Size([1568, 200]) torch.Size([1568])\n",
      "Epoch: 4, Loss: 5.104924201965332\n",
      "\n",
      "torch.Size([32, 50]) torch.Size([32, 49])\n",
      "torch.Size([32, 49, 200])\n",
      "####\n",
      "torch.Size([1568, 200]) torch.Size([1568])\n",
      "Epoch: 5, Loss: 5.028228759765625\n",
      "\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "optimizer = optim.Adam(transformer.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)\n",
    "\n",
    "transformer.train()\n",
    "\n",
    "for epoch in range(5):\n",
    "    optimizer.zero_grad()\n",
    "    output = transformer(src_data, tgt_data[:, :-1])\n",
    "    print(src_data.shape, tgt_data[:, :-1].shape)\n",
    "    print(output.shape)\n",
    "    print(\"####\")\n",
    "    loss = criterion(output.contiguous().view(-1, tgt_vocab_size), tgt_data[:, 1:].contiguous().view(-1))\n",
    "\n",
    "    print(output.contiguous().view(-1, tgt_vocab_size).shape, tgt_data[:, 1:].contiguous().view(-1).shape)\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f\"Epoch: {epoch+1}, Loss: {loss.item()}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mInit signature:\u001b[0m\n",
      "\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCrossEntropyLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mweight\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0msize_average\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mignore_index\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mreduce\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mreduction\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'mean'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mlabel_smoothing\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDocstring:\u001b[0m     \n",
      "This criterion computes the cross entropy loss between input logits\n",
      "and target.\n",
      "\n",
      "It is useful when training a classification problem with `C` classes.\n",
      "If provided, the optional argument :attr:`weight` should be a 1D `Tensor`\n",
      "assigning weight to each of the classes.\n",
      "This is particularly useful when you have an unbalanced training set.\n",
      "\n",
      "The `input` is expected to contain the unnormalized logits for each class (which do `not` need\n",
      "to be positive or sum to 1, in general).\n",
      "`input` has to be a Tensor of size :math:`(C)` for unbatched input,\n",
      ":math:`(minibatch, C)` or :math:`(minibatch, C, d_1, d_2, ..., d_K)` with :math:`K \\geq 1` for the\n",
      "`K`-dimensional case. The last being useful for higher dimension inputs, such\n",
      "as computing cross entropy loss per-pixel for 2D images.\n",
      "\n",
      "The `target` that this criterion expects should contain either:\n",
      "\n",
      "- Class indices in the range :math:`[0, C)` where :math:`C` is the number of classes; if\n",
      "  `ignore_index` is specified, this loss also accepts this class index (this index\n",
      "  may not necessarily be in the class range). The unreduced (i.e. with :attr:`reduction`\n",
      "  set to ``'none'``) loss for this case can be described as:\n",
      "\n",
      "  .. math::\n",
      "      \\ell(x, y) = L = \\{l_1,\\dots,l_N\\}^\\top, \\quad\n",
      "      l_n = - w_{y_n} \\log \\frac{\\exp(x_{n,y_n})}{\\sum_{c=1}^C \\exp(x_{n,c})}\n",
      "      \\cdot \\mathbb{1}\\{y_n \\not= \\text{ignore\\_index}\\}\n",
      "\n",
      "  where :math:`x` is the input, :math:`y` is the target, :math:`w` is the weight,\n",
      "  :math:`C` is the number of classes, and :math:`N` spans the minibatch dimension as well as\n",
      "  :math:`d_1, ..., d_k` for the `K`-dimensional case. If\n",
      "  :attr:`reduction` is not ``'none'`` (default ``'mean'``), then\n",
      "\n",
      "  .. math::\n",
      "      \\ell(x, y) = \\begin{cases}\n",
      "          \\sum_{n=1}^N \\frac{1}{\\sum_{n=1}^N w_{y_n} \\cdot \\mathbb{1}\\{y_n \\not= \\text{ignore\\_index}\\}} l_n, &\n",
      "           \\text{if reduction} = \\text{`mean';}\\\\\n",
      "            \\sum_{n=1}^N l_n,  &\n",
      "            \\text{if reduction} = \\text{`sum'.}\n",
      "        \\end{cases}\n",
      "\n",
      "  Note that this case is equivalent to applying :class:`~torch.nn.LogSoftmax`\n",
      "  on an input, followed by :class:`~torch.nn.NLLLoss`.\n",
      "\n",
      "- Probabilities for each class; useful when labels beyond a single class per minibatch item\n",
      "  are required, such as for blended labels, label smoothing, etc. The unreduced (i.e. with\n",
      "  :attr:`reduction` set to ``'none'``) loss for this case can be described as:\n",
      "\n",
      "  .. math::\n",
      "      \\ell(x, y) = L = \\{l_1,\\dots,l_N\\}^\\top, \\quad\n",
      "      l_n = - \\sum_{c=1}^C w_c \\log \\frac{\\exp(x_{n,c})}{\\sum_{i=1}^C \\exp(x_{n,i})} y_{n,c}\n",
      "\n",
      "  where :math:`x` is the input, :math:`y` is the target, :math:`w` is the weight,\n",
      "  :math:`C` is the number of classes, and :math:`N` spans the minibatch dimension as well as\n",
      "  :math:`d_1, ..., d_k` for the `K`-dimensional case. If\n",
      "  :attr:`reduction` is not ``'none'`` (default ``'mean'``), then\n",
      "\n",
      "  .. math::\n",
      "      \\ell(x, y) = \\begin{cases}\n",
      "          \\frac{\\sum_{n=1}^N l_n}{N}, &\n",
      "           \\text{if reduction} = \\text{`mean';}\\\\\n",
      "            \\sum_{n=1}^N l_n,  &\n",
      "            \\text{if reduction} = \\text{`sum'.}\n",
      "        \\end{cases}\n",
      "\n",
      ".. note::\n",
      "    The performance of this criterion is generally better when `target` contains class\n",
      "    indices, as this allows for optimized computation. Consider providing `target` as\n",
      "    class probabilities only when a single class label per minibatch item is too restrictive.\n",
      "\n",
      "Args:\n",
      "    weight (Tensor, optional): a manual rescaling weight given to each class.\n",
      "        If given, has to be a Tensor of size `C`\n",
      "    size_average (bool, optional): Deprecated (see :attr:`reduction`). By default,\n",
      "        the losses are averaged over each loss element in the batch. Note that for\n",
      "        some losses, there are multiple elements per sample. If the field :attr:`size_average`\n",
      "        is set to ``False``, the losses are instead summed for each minibatch. Ignored\n",
      "        when :attr:`reduce` is ``False``. Default: ``True``\n",
      "    ignore_index (int, optional): Specifies a target value that is ignored\n",
      "        and does not contribute to the input gradient. When :attr:`size_average` is\n",
      "        ``True``, the loss is averaged over non-ignored targets. Note that\n",
      "        :attr:`ignore_index` is only applicable when the target contains class indices.\n",
      "    reduce (bool, optional): Deprecated (see :attr:`reduction`). By default, the\n",
      "        losses are averaged or summed over observations for each minibatch depending\n",
      "        on :attr:`size_average`. When :attr:`reduce` is ``False``, returns a loss per\n",
      "        batch element instead and ignores :attr:`size_average`. Default: ``True``\n",
      "    reduction (str, optional): Specifies the reduction to apply to the output:\n",
      "        ``'none'`` | ``'mean'`` | ``'sum'``. ``'none'``: no reduction will\n",
      "        be applied, ``'mean'``: the weighted mean of the output is taken,\n",
      "        ``'sum'``: the output will be summed. Note: :attr:`size_average`\n",
      "        and :attr:`reduce` are in the process of being deprecated, and in\n",
      "        the meantime, specifying either of those two args will override\n",
      "        :attr:`reduction`. Default: ``'mean'``\n",
      "    label_smoothing (float, optional): A float in [0.0, 1.0]. Specifies the amount\n",
      "        of smoothing when computing the loss, where 0.0 means no smoothing. The targets\n",
      "        become a mixture of the original ground truth and a uniform distribution as described in\n",
      "        `Rethinking the Inception Architecture for Computer Vision <https://arxiv.org/abs/1512.00567>`__. Default: :math:`0.0`.\n",
      "\n",
      "Shape:\n",
      "    - Input: Shape :math:`(C)`, :math:`(N, C)` or :math:`(N, C, d_1, d_2, ..., d_K)` with :math:`K \\geq 1`\n",
      "      in the case of `K`-dimensional loss.\n",
      "    - Target: If containing class indices, shape :math:`()`, :math:`(N)` or :math:`(N, d_1, d_2, ..., d_K)` with\n",
      "      :math:`K \\geq 1` in the case of K-dimensional loss where each value should be between :math:`[0, C)`.\n",
      "      If containing class probabilities, same shape as the input and each value should be between :math:`[0, 1]`.\n",
      "    - Output: If reduction is 'none', shape :math:`()`, :math:`(N)` or :math:`(N, d_1, d_2, ..., d_K)` with :math:`K \\geq 1`\n",
      "      in the case of K-dimensional loss, depending on the shape of the input. Otherwise, scalar.\n",
      "\n",
      "\n",
      "    where:\n",
      "\n",
      "    .. math::\n",
      "        \\begin{aligned}\n",
      "            C ={} & \\text{number of classes} \\\\\n",
      "            N ={} & \\text{batch size} \\\\\n",
      "        \\end{aligned}\n",
      "\n",
      "Examples::\n",
      "\n",
      "    >>> # Example of target with class indices\n",
      "    >>> loss = nn.CrossEntropyLoss()\n",
      "    >>> input = torch.randn(3, 5, requires_grad=True)\n",
      "    >>> target = torch.empty(3, dtype=torch.long).random_(5)\n",
      "    >>> output = loss(input, target)\n",
      "    >>> output.backward()\n",
      "    >>>\n",
      "    >>> # Example of target with class probabilities\n",
      "    >>> input = torch.randn(3, 5, requires_grad=True)\n",
      "    >>> target = torch.randn(3, 5).softmax(dim=1)\n",
      "    >>> output = loss(input, target)\n",
      "    >>> output.backward()\n",
      "\u001b[0;31mInit docstring:\u001b[0m Initializes internal Module state, shared by both nn.Module and ScriptModule.\n",
      "\u001b[0;31mFile:\u001b[0m           ~/.pyenv/versions/project_env/lib/python3.10/site-packages/torch/nn/modules/loss.py\n",
      "\u001b[0;31mType:\u001b[0m           type\n",
      "\u001b[0;31mSubclasses:\u001b[0m     "
     ]
    }
   ],
   "source": [
    "nn.CrossEntropyLoss?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 5.4163031578063965\n"
     ]
    }
   ],
   "source": [
    "transformer.eval()\n",
    "\n",
    "# Generate random sample validation data\n",
    "val_src_data = torch.randint(1, src_vocab_size, (64, max_seq_length))  # (batch_size, seq_length)\n",
    "val_tgt_data = torch.randint(1, tgt_vocab_size, (64, max_seq_length))  # (batch_size, seq_length)\n",
    "\n",
    "with torch.no_grad():\n",
    "\n",
    "    val_output = transformer(val_src_data, val_tgt_data[:, :-1])\n",
    "    val_loss = criterion(val_output.contiguous().view(-1, tgt_vocab_size), val_tgt_data[:, 1:].contiguous().view(-1))\n",
    "    print(f\"Validation Loss: {val_loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "\n",
    "\n",
    "Q_reshaped =  torch.tensor([[[[-4.7772e-01,  6.9994e-01, -2.4317e-01,  1.3753e-01],\n",
    "          [-2.0450e-01,  6.6614e-01,  3.5755e-01, -3.1147e-01],\n",
    "          [-9.6026e-01,  2.4673e+00, -2.9600e-01,  2.9330e+00],\n",
    "          [-8.9694e-01,  2.3048e+00,  9.7365e-01,  2.5556e+00]],\n",
    "\n",
    "         [[-9.8551e-01, -7.5628e-01,  2.1984e+00, -1.0317e+00],\n",
    "          [-1.5500e+00,  3.5095e-01, -6.8803e-02, -5.2188e-01],\n",
    "          [-1.8930e+00,  5.8479e-01,  1.4320e+00,  1.2516e-01],\n",
    "          [-3.0953e+00,  2.3745e+00,  9.2446e-01,  4.7702e-02]],\n",
    "\n",
    "         [[-8.3401e-01,  1.4440e+00,  4.0272e-01, -4.4414e-01],\n",
    "          [-5.0454e-01,  3.9886e-01, -2.7185e-02, -2.6923e-01],\n",
    "          [-3.8632e-01, -1.6106e+00, -6.3674e-01,  1.9070e-01],\n",
    "          [-6.7301e-01, -1.8924e+00, -1.4568e+00,  1.3190e-01]],\n",
    "\n",
    "         [[ 5.0697e-01,  1.3615e+00, -8.3323e-01, -1.5126e-01],\n",
    "          [ 2.2424e-01,  1.6721e+00, -1.2046e+00,  3.8664e-02],\n",
    "          [ 2.7266e+00,  1.4596e+00,  1.5861e+00,  1.1367e+00],\n",
    "          [ 3.3616e+00,  1.9529e+00,  7.4700e-01,  1.5359e+00]]],\n",
    "\n",
    "\n",
    "        [[[-8.6815e-01,  1.2308e+00, -3.3001e-02, -3.7308e-01],\n",
    "          [ 4.9939e-01,  1.5917e+00,  2.1246e-01,  7.2611e-02],\n",
    "          [ 1.1958e-01,  1.7782e+00, -8.0772e-01, -3.1812e-01],\n",
    "          [-8.7687e-01,  3.1814e-01,  2.2518e-01, -2.2660e-01]],\n",
    "\n",
    "         [[-1.9160e+00,  3.4028e-01,  1.2422e+00,  5.2760e-01],\n",
    "          [-6.4376e-01,  5.2752e-01, -7.8839e-01, -1.0246e+00],\n",
    "          [-2.1223e+00,  1.9347e-01,  2.1142e-01, -1.4601e+00],\n",
    "          [-1.2929e+00, -5.0758e-01,  2.0995e-01, -1.9992e-02]],\n",
    "\n",
    "         [[-1.9333e+00,  2.1132e-01, -1.0623e-01, -2.4567e-01],\n",
    "          [-1.4550e+00, -5.4272e-02, -6.6537e-01,  1.1971e+00],\n",
    "          [-1.6575e+00, -3.0775e-01,  1.6449e-02,  7.6033e-01],\n",
    "          [-1.0600e+00, -1.3049e+00,  6.8640e-01, -1.7993e-03]],\n",
    "\n",
    "         [[ 1.8422e-02,  1.3085e+00, -1.0644e+00, -2.7683e-01],\n",
    "          [ 2.3151e-01,  7.6883e-01, -1.3854e+00, -5.2912e-01],\n",
    "          [ 8.1380e-01,  2.1532e+00, -1.1853e+00,  3.6409e-01],\n",
    "          [-3.3871e-01,  1.6210e+00,  2.8922e-01,  1.8015e+00]]],\n",
    "\n",
    "\n",
    "        [[[-6.7476e-01, -1.2707e+00,  1.1343e+00, -1.9244e+00],\n",
    "          [-6.3113e-01,  6.2221e-01,  2.5213e-02, -1.3570e-01],\n",
    "          [-1.0503e+00,  2.2270e+00,  1.2420e+00,  2.2824e+00],\n",
    "          [-3.5791e-01,  5.8840e-01,  6.2594e-01, -5.8470e-01]],\n",
    "\n",
    "         [[ 4.6403e-01, -2.9052e-01,  3.5396e-02,  1.2491e+00],\n",
    "          [-9.0462e-01, -8.1335e-01,  2.3499e+00, -5.4459e-01],\n",
    "          [-3.0144e+00,  2.3174e+00,  1.0760e+00,  5.3479e-01],\n",
    "          [-1.4691e+00,  2.9387e-01,  8.2767e-02, -3.4799e-02]],\n",
    "\n",
    "         [[-8.6633e-01,  8.0823e-02,  2.3088e-01, -1.6974e+00],\n",
    "          [-9.7112e-01,  1.4730e+00,  1.0912e-01, -7.1013e-01],\n",
    "          [-8.1012e-01, -1.8634e+00, -1.7504e+00, -1.3410e-01],\n",
    "          [-6.4165e-01,  4.2784e-01, -3.2078e-01, -5.3522e-01]],\n",
    "\n",
    "         [[-1.9658e+00,  7.5742e-01, -1.6647e-01,  2.2219e-01],\n",
    "          [ 1.6952e-01,  1.2077e+00, -1.1462e+00, -3.7947e-01],\n",
    "          [ 3.0241e+00,  1.7991e+00,  4.3402e-01,  1.3077e+00],\n",
    "          [-1.1321e-01,  1.5183e+00, -1.5175e+00, -1.8955e-01]]]],\n",
    "       )\n",
    "\n",
    "K_reshaped =  torch.tensor([[[[ 7.8398e-01, -2.8978e+00, -1.9969e+00, -8.1522e-02],\n",
    "          [-2.4054e-01, -1.1789e+00, -3.2179e-01,  3.0028e-01],\n",
    "          [ 2.2459e+00,  1.1807e-01, -8.4272e-01, -5.2088e-01],\n",
    "          [ 3.5401e-01,  8.6656e-01,  4.8169e-01,  1.7585e-01]],\n",
    "\n",
    "         [[-2.2458e-01, -1.4127e-01, -1.5425e-02, -4.3561e-01],\n",
    "          [-4.5286e-01, -1.7143e-01,  7.3920e-01, -2.2124e-01],\n",
    "          [-8.1991e-01,  9.2554e-01,  3.2352e-02,  7.1836e-01],\n",
    "          [-1.3430e+00,  1.0354e+00,  7.8981e-01, -3.3099e-01]],\n",
    "\n",
    "         [[ 4.3926e-01,  1.0542e+00,  2.5665e-03, -2.5535e+00],\n",
    "          [ 8.3438e-01,  4.8703e-01,  3.7392e-01, -3.1019e-01],\n",
    "          [ 5.9615e-01, -2.5148e-01,  2.7867e-01, -1.7343e+00],\n",
    "          [ 1.0606e+00, -8.2105e-02, -9.2336e-01, -1.8193e-01]],\n",
    "\n",
    "         [[ 2.1036e-01,  1.3534e+00, -4.9534e-01, -2.2155e+00],\n",
    "          [-2.2480e-01,  1.1705e+00, -5.1477e-01, -4.6465e-01],\n",
    "          [ 8.3970e-01,  8.2966e-01,  1.5627e-01, -1.7664e-01],\n",
    "          [ 1.2857e+00,  1.8845e+00, -7.1693e-02, -4.5686e-01]]],\n",
    "\n",
    "\n",
    "        [[[ 1.5785e+00, -8.0012e-01, -1.2165e+00,  1.1637e+00],\n",
    "          [ 4.7456e-01, -1.2277e+00, -1.7932e+00,  6.6939e-01],\n",
    "          [ 2.9108e-01, -1.0741e+00, -4.3732e-01, -3.2007e-02],\n",
    "          [ 1.0022e+00,  3.9241e-02, -3.9030e-01,  8.0718e-01]],\n",
    "\n",
    "         [[-1.1497e+00,  1.8438e-01,  4.2984e-01,  2.9018e-01],\n",
    "          [-1.5817e+00, -4.8684e-01, -5.8341e-01, -5.8716e-01],\n",
    "          [-1.0270e+00, -4.7455e-01,  1.2170e-02,  3.4087e-01],\n",
    "          [-8.9543e-01,  4.3494e-02,  1.0376e+00,  7.5883e-01]],\n",
    "\n",
    "         [[-5.1195e-03, -4.5586e-02,  3.8176e-01, -7.6378e-01],\n",
    "          [ 1.7891e-01, -8.7227e-01,  2.0302e-01, -1.4165e+00],\n",
    "          [ 6.7352e-01,  5.4276e-02,  1.2021e+00, -4.5426e-01],\n",
    "          [ 4.1585e-01, -1.5690e+00,  5.2294e-01,  6.9571e-01]],\n",
    "\n",
    "         [[ 1.0573e+00,  9.9475e-01, -2.7674e-01, -1.8682e+00],\n",
    "          [ 2.2913e+00, -3.4779e-02,  1.0709e+00, -2.4700e+00],\n",
    "          [ 6.2046e-01,  1.0307e+00, -2.8338e-01, -1.4275e+00],\n",
    "          [ 6.6790e-01, -3.8380e-01, -1.8104e-01, -8.4886e-01]]],\n",
    "\n",
    "\n",
    "        [[[-4.2460e-01, -4.5856e-01,  7.9080e-01,  8.4442e-01],\n",
    "          [ 4.9698e-01, -2.2869e+00, -1.9085e+00,  1.1746e-02],\n",
    "          [ 6.7006e-02,  1.4774e+00,  5.7012e-01,  2.6911e-01],\n",
    "          [-5.2754e-01, -5.6802e-01, -2.3336e-01,  3.9355e-01]],\n",
    "\n",
    "         [[-2.2514e-01, -8.0630e-01,  1.2966e+00,  8.4586e-01],\n",
    "          [-5.8476e-01,  7.6748e-02, -1.4493e-01,  7.0403e-02],\n",
    "          [-1.7032e+00,  1.2534e+00,  6.6030e-01,  1.7503e-01],\n",
    "          [-8.1305e-01,  4.6589e-02,  6.0970e-01,  2.8478e-01]],\n",
    "\n",
    "         [[-6.5667e-01, -7.2372e-01,  3.2590e-01,  7.6517e-01],\n",
    "          [ 1.1380e-01,  6.9370e-01,  2.3814e-01, -2.5993e+00],\n",
    "          [ 7.3511e-01, -4.4257e-01, -6.8778e-01, -2.2767e-01],\n",
    "          [ 5.0892e-01,  1.2657e-01,  6.0949e-01, -3.5594e-01]],\n",
    "\n",
    "         [[-7.2759e-02, -1.0743e+00, -6.6493e-02, -6.7504e-01],\n",
    "          [ 7.4326e-01,  1.2273e+00, -6.6120e-01, -2.2911e+00],\n",
    "          [ 1.8186e+00,  1.7584e+00, -2.3756e-01, -5.3247e-01],\n",
    "          [ 3.0809e-01,  1.0444e+00, -6.8064e-01, -5.4025e-01]]]])\n",
    "\n",
    "\n",
    "V_reshaped =  torch.tensor([[[[-1.0260,  0.9662, -2.2913,  1.8057],\n",
    "          [ 0.5957, -0.4363, -0.6107,  0.1757],\n",
    "          [ 1.1529,  0.6390,  0.4691,  0.1807],\n",
    "          [ 2.0451, -0.5014,  1.6166, -0.3022]],\n",
    "\n",
    "         [[-0.0240, -1.3353, -1.6751, -0.9049],\n",
    "          [-0.4670, -1.8406, -0.9363, -0.2806],\n",
    "          [ 0.2908,  0.4321,  1.0739, -0.3611],\n",
    "          [-0.6475, -2.4038,  1.9287,  0.6706]],\n",
    "\n",
    "         [[ 1.5304, -1.8514, -1.5147,  0.9213],\n",
    "          [ 0.9723, -0.8812, -0.8274,  1.0724],\n",
    "          [ 1.3649, -0.4078, -1.0577, -1.9731],\n",
    "          [ 1.7294, -1.8886, -2.2192, -0.2645]],\n",
    "\n",
    "         [[ 0.5280,  0.7117, -0.2574,  1.5133],\n",
    "          [ 0.4975,  0.9409, -0.3728, -0.8591],\n",
    "          [ 0.0418, -0.3742,  0.1779,  0.3042],\n",
    "          [-0.5309, -0.6696, -0.5339, -0.4551]]],\n",
    "\n",
    "\n",
    "        [[[-0.2025, -0.9508, -1.4185, -0.6180],\n",
    "          [-0.5360,  0.3189,  0.1011,  0.4221],\n",
    "          [ 0.9998, -0.8083, -0.9948,  0.1297],\n",
    "          [ 0.0029,  0.2251, -1.0812, -1.1702]],\n",
    "\n",
    "         [[-0.2939, -0.2418, -0.6054,  1.2000],\n",
    "          [-0.2660, -1.9972, -1.2478,  1.2041],\n",
    "          [-1.2195, -0.9846, -0.4558,  0.8347],\n",
    "          [-0.4921, -0.1789,  1.0497,  1.6185]],\n",
    "\n",
    "         [[ 0.0906, -1.0008, -0.3126,  0.5912],\n",
    "          [ 1.2918,  0.2884, -0.8203,  0.6532],\n",
    "          [ 1.3875, -1.1471, -1.8779,  0.5379],\n",
    "          [ 0.0668,  0.4564,  0.1974,  0.0822]],\n",
    "\n",
    "         [[ 0.0505, -0.4071,  0.3731,  0.2199],\n",
    "          [ 2.5341,  0.5179,  0.5687, -1.4736],\n",
    "          [ 0.9913,  0.6232, -1.3847, -1.2899],\n",
    "          [-0.1291,  0.0954, -0.4044,  0.5938]]],\n",
    "\n",
    "\n",
    "        [[[-0.8220,  0.0050, -0.6552, -0.7446],\n",
    "          [-1.1864,  0.6515, -2.0828,  1.4967],\n",
    "          [ 1.8847, -0.8161,  1.8250, -0.6112],\n",
    "          [ 0.4353, -0.7510, -0.4023, -0.1333]],\n",
    "\n",
    "         [[ 0.2215, -0.0855,  0.7154,  1.1723],\n",
    "          [-0.1140, -1.0901, -1.3602, -0.3474],\n",
    "          [-0.7375, -2.1587,  2.2436,  1.2281],\n",
    "          [-0.5570, -1.5954, -0.6214,  0.2769]],\n",
    "\n",
    "         [[-1.2167,  0.5764,  1.2362,  0.0251],\n",
    "          [ 1.1200, -1.7065, -1.3507,  1.0216],\n",
    "          [ 1.3190, -1.7436, -2.0552, -0.1642],\n",
    "          [ 0.5620, -0.7362, -0.6634,  1.1727]],\n",
    "\n",
    "         [[-0.0334,  0.3178,  0.5097,  0.4712],\n",
    "          [ 0.5073,  0.2432,  0.0210,  1.4853],\n",
    "          [-0.5516, -1.1381, -0.2555, -0.4832],\n",
    "          [ 0.4768,  0.4724, -0.0945, -0.8872]]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "scale_factor = 1 / math.sqrt(Q_reshaped.size(-1))\n",
    "scale_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q_reshaped.size(-1), Q_reshaped.shape\n",
    "\n",
    "\n",
    "attn_mask = torch.tensor([[[[0., float('-inf'), float('-inf'), float('-inf')],\n",
    "          [0., 0., float('-inf'), float('-inf')],\n",
    "          [0., 0., 0., float('-inf')],\n",
    "          [0., 0., 0., 0.]]]])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 4, 4])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_mask = torch.tensor([[[[0., float('-inf'), float('-inf'), float('-inf')],\n",
    "          [0., 0., float('-inf'), float('-inf')],\n",
    "          [0., 0., 0., float('-inf')],\n",
    "          [0., 0., 0., 0.]]]])\n",
    "\n",
    "attn_mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 4, 4])\n"
     ]
    }
   ],
   "source": [
    "L, S = Q_reshaped.size(-2), K_reshaped.size(-2)\n",
    "\n",
    "attn_bias = torch.zeros(L, S, dtype=Q_reshaped.dtype)\n",
    "attn_bias = attn_bias.unsqueeze(0).unsqueeze(0)\n",
    "print(attn_bias.shape)\n",
    "\n",
    "attn_bias += attn_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0., -inf, -inf, -inf],\n",
       "          [0., 0., -inf, -inf],\n",
       "          [0., 0., 0., -inf],\n",
       "          [0., 0., 0., 0.]]]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-0.9642, -0.2954, -0.4285,  0.1722],\n",
       "          [-1.3896, -0.4724, -0.2599,  0.3112],\n",
       "          [-3.7753, -0.8509, -1.5718,  1.0857],\n",
       "          [-4.7673, -1.0236, -1.9470,  1.2991]],\n",
       "\n",
       "         [[ 0.3718,  1.2146, -0.2810,  1.3091],\n",
       "          [ 0.2635,  0.3532,  0.6093,  1.2817],\n",
       "          [ 0.1330,  0.8939,  1.1148,  2.1187],\n",
       "          [ 0.1623,  0.8337,  2.3999,  3.6650]],\n",
       "\n",
       "         [[ 1.1455,  0.1479,  0.0111, -0.6471],\n",
       "          [ 0.4431, -0.0767,  0.0291, -0.2469],\n",
       "          [-1.1781, -0.7020, -0.1667,  0.1379],\n",
       "          [-1.3156, -1.0344, -0.2800,  0.3814]],\n",
       "\n",
       "         [[ 1.3486,  0.9894,  0.7259,  1.6732],\n",
       "          [ 1.4106,  1.2545,  0.6902,  1.7540],\n",
       "          [-0.3775, -0.1246,  1.7738,  2.8116],\n",
       "          [-0.2113,  0.2160,  2.1442,  3.6235]]],\n",
       "\n",
       "\n",
       "        [[[-1.3746, -1.0568, -0.7742, -0.5550],\n",
       "          [-0.3296, -1.0248, -0.8298,  0.2693],\n",
       "          [-0.3108, -0.4454, -0.7559,  0.1240],\n",
       "          [-1.0882, -0.6811, -0.3441, -0.5686]],\n",
       "\n",
       "         [[ 1.4763,  0.9152,  1.0006,  1.7099],\n",
       "          [ 0.1006,  0.9115,  0.0260, -0.4981],\n",
       "          [ 1.0714,  1.9983,  0.7963,  0.5101],\n",
       "          [ 0.7387,  1.0907,  0.7822,  0.6691]],\n",
       "\n",
       "         [[ 0.0737, -0.1019, -0.6534, -0.6810],\n",
       "          [-0.5792, -1.0219, -1.1633, -0.0175],\n",
       "          [-0.2760, -0.5509, -0.7293,  0.1656],\n",
       "          [ 0.1642,  0.5452,  0.0206,  0.9821]],\n",
       "\n",
       "         [[ 1.0664, -0.2297,  1.0285, -0.0311],\n",
       "          [ 1.1907,  0.1635,  1.0420,  0.2798],\n",
       "          [ 1.3251, -0.1894,  1.2702, -0.1887],\n",
       "          [-1.0956, -2.4862, -0.5965, -1.2150]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0706,  0.1916, -0.8969,  0.0278],\n",
       "          [-0.0560, -0.8932,  0.4274, -0.0399],\n",
       "          [ 1.1671, -3.9792,  2.2710, -0.0512],\n",
       "          [-0.0583, -1.3625,  0.5224, -0.2608]],\n",
       "\n",
       "         [[ 0.6161, -0.1054, -0.4562, -0.0068],\n",
       "          [ 1.7229,  0.0438,  0.9888,  0.9876],\n",
       "          [ 0.3288,  0.9111,  4.4214,  1.6836],\n",
       "          [ 0.0858,  0.4336,  1.4595,  0.6243]],\n",
       "\n",
       "         [[-0.3566,  2.2123, -0.2225,  0.1571],\n",
       "          [-0.4681,  1.3916, -0.6396,  0.0057],\n",
       "          [ 0.6037, -0.7266,  0.7318, -0.8336],\n",
       "          [-0.2012,  0.7693, -0.1593, -0.1387]],\n",
       "\n",
       "         [[-0.4048, -0.4653, -1.1610,  0.0893],\n",
       "          [-0.4887,  1.6177,  1.4531,  1.1494],\n",
       "          [-1.5322,  0.5863,  3.9319,  0.9044],\n",
       "          [-0.6970,  1.6085,  1.4627,  1.3431]]]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_weight = Q_reshaped @ K_reshaped.transpose(-2, -1) * scale_factor\n",
    "\n",
    "attn_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-0.9642,    -inf,    -inf,    -inf],\n",
       "          [-1.3896, -0.4724,    -inf,    -inf],\n",
       "          [-3.7753, -0.8509, -1.5718,    -inf],\n",
       "          [-4.7673, -1.0236, -1.9470,  1.2991]],\n",
       "\n",
       "         [[ 0.3718,    -inf,    -inf,    -inf],\n",
       "          [ 0.2635,  0.3532,    -inf,    -inf],\n",
       "          [ 0.1330,  0.8939,  1.1148,    -inf],\n",
       "          [ 0.1623,  0.8337,  2.3999,  3.6650]],\n",
       "\n",
       "         [[ 1.1455,    -inf,    -inf,    -inf],\n",
       "          [ 0.4431, -0.0767,    -inf,    -inf],\n",
       "          [-1.1781, -0.7020, -0.1667,    -inf],\n",
       "          [-1.3156, -1.0344, -0.2800,  0.3814]],\n",
       "\n",
       "         [[ 1.3486,    -inf,    -inf,    -inf],\n",
       "          [ 1.4106,  1.2545,    -inf,    -inf],\n",
       "          [-0.3775, -0.1246,  1.7738,    -inf],\n",
       "          [-0.2113,  0.2160,  2.1442,  3.6235]]],\n",
       "\n",
       "\n",
       "        [[[-1.3746,    -inf,    -inf,    -inf],\n",
       "          [-0.3296, -1.0248,    -inf,    -inf],\n",
       "          [-0.3108, -0.4454, -0.7559,    -inf],\n",
       "          [-1.0882, -0.6811, -0.3441, -0.5686]],\n",
       "\n",
       "         [[ 1.4763,    -inf,    -inf,    -inf],\n",
       "          [ 0.1006,  0.9115,    -inf,    -inf],\n",
       "          [ 1.0714,  1.9983,  0.7963,    -inf],\n",
       "          [ 0.7387,  1.0907,  0.7822,  0.6691]],\n",
       "\n",
       "         [[ 0.0737,    -inf,    -inf,    -inf],\n",
       "          [-0.5792, -1.0219,    -inf,    -inf],\n",
       "          [-0.2760, -0.5509, -0.7293,    -inf],\n",
       "          [ 0.1642,  0.5452,  0.0206,  0.9821]],\n",
       "\n",
       "         [[ 1.0664,    -inf,    -inf,    -inf],\n",
       "          [ 1.1907,  0.1635,    -inf,    -inf],\n",
       "          [ 1.3251, -0.1894,  1.2702,    -inf],\n",
       "          [-1.0956, -2.4862, -0.5965, -1.2150]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0706,    -inf,    -inf,    -inf],\n",
       "          [-0.0560, -0.8932,    -inf,    -inf],\n",
       "          [ 1.1671, -3.9792,  2.2710,    -inf],\n",
       "          [-0.0583, -1.3625,  0.5224, -0.2608]],\n",
       "\n",
       "         [[ 0.6161,    -inf,    -inf,    -inf],\n",
       "          [ 1.7229,  0.0438,    -inf,    -inf],\n",
       "          [ 0.3288,  0.9111,  4.4214,    -inf],\n",
       "          [ 0.0858,  0.4336,  1.4595,  0.6243]],\n",
       "\n",
       "         [[-0.3566,    -inf,    -inf,    -inf],\n",
       "          [-0.4681,  1.3916,    -inf,    -inf],\n",
       "          [ 0.6037, -0.7266,  0.7318,    -inf],\n",
       "          [-0.2012,  0.7693, -0.1593, -0.1387]],\n",
       "\n",
       "         [[-0.4048,    -inf,    -inf,    -inf],\n",
       "          [-0.4887,  1.6177,    -inf,    -inf],\n",
       "          [-1.5322,  0.5863,  3.9319,    -inf],\n",
       "          [-0.6970,  1.6085,  1.4627,  1.3431]]]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_weight += attn_bias\n",
    "\n",
    "attn_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[1.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.2855, 0.7145, 0.0000, 0.0000],\n",
       "          [0.0349, 0.6494, 0.3158, 0.0000],\n",
       "          [0.0020, 0.0860, 0.0342, 0.8778]],\n",
       "\n",
       "         [[1.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.4776, 0.5224, 0.0000, 0.0000],\n",
       "          [0.1721, 0.3684, 0.4595, 0.0000],\n",
       "          [0.0220, 0.0430, 0.2058, 0.7292]],\n",
       "\n",
       "         [[1.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.6271, 0.3729, 0.0000, 0.0000],\n",
       "          [0.1866, 0.3004, 0.5130, 0.0000],\n",
       "          [0.0944, 0.1250, 0.2658, 0.5149]],\n",
       "\n",
       "         [[1.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.5390, 0.4610, 0.0000, 0.0000],\n",
       "          [0.0919, 0.1183, 0.7898, 0.0000],\n",
       "          [0.0168, 0.0258, 0.1776, 0.7797]]],\n",
       "\n",
       "\n",
       "        [[[1.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.6671, 0.3329, 0.0000, 0.0000],\n",
       "          [0.3976, 0.3476, 0.2548, 0.0000],\n",
       "          [0.1590, 0.2389, 0.3347, 0.2674]],\n",
       "\n",
       "         [[1.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.3077, 0.6923, 0.0000, 0.0000],\n",
       "          [0.2333, 0.5895, 0.1772, 0.0000],\n",
       "          [0.2273, 0.3232, 0.2374, 0.2120]],\n",
       "\n",
       "         [[1.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.6089, 0.3911, 0.0000, 0.0000],\n",
       "          [0.4175, 0.3172, 0.2653, 0.0000],\n",
       "          [0.1787, 0.2616, 0.1548, 0.4049]],\n",
       "\n",
       "         [[1.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.7364, 0.2636, 0.0000, 0.0000],\n",
       "          [0.4616, 0.1015, 0.4369, 0.0000],\n",
       "          [0.2643, 0.0658, 0.4354, 0.2346]]],\n",
       "\n",
       "\n",
       "        [[[1.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.6979, 0.3021, 0.0000, 0.0000],\n",
       "          [0.2486, 0.0014, 0.7499, 0.0000],\n",
       "          [0.2580, 0.0700, 0.4612, 0.2107]],\n",
       "\n",
       "         [[1.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.8428, 0.1572, 0.0000, 0.0000],\n",
       "          [0.0160, 0.0286, 0.9555, 0.0000],\n",
       "          [0.1238, 0.1752, 0.4889, 0.2121]],\n",
       "\n",
       "         [[1.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.1347, 0.8653, 0.0000, 0.0000],\n",
       "          [0.4165, 0.1101, 0.4734, 0.0000],\n",
       "          [0.1740, 0.4593, 0.1815, 0.1852]],\n",
       "\n",
       "         [[1.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.1085, 0.8915, 0.0000, 0.0000],\n",
       "          [0.0041, 0.0339, 0.9620, 0.0000],\n",
       "          [0.0365, 0.3662, 0.3165, 0.2808]]]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_weight = torch.softmax(attn_weight, dim=-1)\n",
    "\n",
    "attn_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-1.0260,  0.9662, -2.2913,  1.8057],\n",
       "          [ 0.1327, -0.0359, -1.0905,  0.6411],\n",
       "          [ 0.7151, -0.0478, -0.3283,  0.2341],\n",
       "          [ 1.8837, -0.4538,  1.3778, -0.2403]],\n",
       "\n",
       "         [[-0.0240, -1.3353, -1.6751, -0.9049],\n",
       "          [-0.2554, -1.5993, -1.2891, -0.5788],\n",
       "          [-0.0426, -0.7094, -0.1399, -0.4250],\n",
       "          [-0.4329, -1.7725,  1.5505,  0.3828]],\n",
       "\n",
       "         [[ 1.5304, -1.8514, -1.5147,  0.9213],\n",
       "          [ 1.3223, -1.4896, -1.2584,  0.9776],\n",
       "          [ 1.2779, -0.8194, -1.0738, -0.5182],\n",
       "          [ 1.5191, -1.3656, -1.6701, -0.4396]],\n",
       "\n",
       "         [[ 0.5280,  0.7117, -0.2574,  1.5133],\n",
       "          [ 0.5139,  0.8174, -0.3106,  0.4195],\n",
       "          [ 0.1404, -0.1188,  0.0727,  0.2776],\n",
       "          [-0.3848, -0.5523, -0.3987, -0.2975]]],\n",
       "\n",
       "\n",
       "        [[[-0.2025, -0.9508, -1.4185, -0.6180],\n",
       "          [-0.3135, -0.5281, -0.9126, -0.2718],\n",
       "          [-0.0121, -0.4732, -0.7824, -0.0660],\n",
       "          [ 0.1751, -0.2853, -0.8234, -0.2669]],\n",
       "\n",
       "         [[-0.2939, -0.2418, -0.6054,  1.2000],\n",
       "          [-0.2746, -1.4571, -1.0501,  1.2028],\n",
       "          [-0.4415, -1.4082, -0.9576,  1.1377],\n",
       "          [-0.5467, -0.9722, -0.4266,  1.2033]],\n",
       "\n",
       "         [[ 0.0906, -1.0008, -0.3126,  0.5912],\n",
       "          [ 0.5604, -0.4966, -0.5112,  0.6154],\n",
       "          [ 0.8157, -0.6307, -0.8889,  0.5967],\n",
       "          [ 0.5959, -0.0962, -0.4812,  0.3931]],\n",
       "\n",
       "         [[ 0.0505, -0.4071,  0.3731,  0.2199],\n",
       "          [ 0.7052, -0.1632,  0.4247, -0.2265],\n",
       "          [ 0.7137,  0.1370, -0.3751, -0.6117],\n",
       "          [ 0.5814,  0.2202, -0.5617, -0.4611]]],\n",
       "\n",
       "\n",
       "        [[[-0.8220,  0.0050, -0.6552, -0.7446],\n",
       "          [-0.9321,  0.2003, -1.0865, -0.0674],\n",
       "          [ 1.2073, -0.6098,  1.2027, -0.6413],\n",
       "          [ 0.6658, -0.4877,  0.4420, -0.3973]],\n",
       "\n",
       "         [[ 0.2215, -0.0855,  0.7154,  1.1723],\n",
       "          [ 0.1688, -0.2434,  0.3891,  0.9334],\n",
       "          [-0.7044, -2.0951,  2.1163,  1.1822],\n",
       "          [-0.4713, -1.5954,  0.8153,  0.7434]],\n",
       "\n",
       "         [[-1.2167,  0.5764,  1.2362,  0.0251],\n",
       "          [ 0.8051, -1.3989, -1.0021,  0.8873],\n",
       "          [ 0.2410, -0.7732, -0.6068,  0.0452],\n",
       "          [ 0.6461, -1.1362, -0.9011,  0.6610]],\n",
       "\n",
       "         [[-0.0334,  0.3178,  0.5097,  0.4712],\n",
       "          [ 0.4486,  0.2513,  0.0740,  1.3753],\n",
       "          [-0.5136, -1.0853, -0.2430, -0.4126],\n",
       "          [ 0.1439, -0.1269, -0.0811,  0.1590]]]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_weight @ V_reshaped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "my_project_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
