{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-head attention transformer\n",
    "### Encoder and Decoder\n",
    "### (With masking)\n",
    "\n",
    "Pytorch's implementation (in built)\n",
    "\n",
    "NOTE :- A new exmple must be used for testing the masked attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source embeddings :- \n",
      "\n",
      "tensor([[[-2.2991, -1.0892, -0.6579,  1.2630, -0.6850, -2.1738]],\n",
      "\n",
      "        [[-0.7063, -0.1283, -0.1086, -0.9614, -0.9128,  1.4580]],\n",
      "\n",
      "        [[ 0.5709,  0.3660,  0.1946,  0.2407, -0.2572,  0.0726]],\n",
      "\n",
      "        [[-0.1525, -0.2407,  0.3625, -0.2211, -0.9160,  0.5704]]],\n",
      "       grad_fn=<EmbeddingBackward0>)\n",
      "\n",
      "Target embeddings :- \n",
      "\n",
      "tensor([[[-0.0037, -0.4092,  0.6838,  0.4102, -0.8705,  0.3255]],\n",
      "\n",
      "        [[ 0.4654, -1.9693, -0.2439,  0.3972, -0.4163, -0.5562]],\n",
      "\n",
      "        [[-1.9848, -0.5422,  0.6171, -1.1887, -0.1505,  0.1084]],\n",
      "\n",
      "        [[-1.9848, -0.5422,  0.6171, -1.1887, -0.1505,  0.1084]]],\n",
      "       grad_fn=<EmbeddingBackward0>)\n",
      "\n",
      "Positional encoded source embeddings:\n",
      "tensor([[[-2.2991, -0.0892, -0.6579,  2.2630, -0.6850, -1.1738]],\n",
      "\n",
      "        [[-0.7063,  0.8717, -0.1086,  0.0386, -0.9128,  2.4580]],\n",
      "\n",
      "        [[ 0.5709,  1.3660,  0.1946,  1.2407, -0.2572,  1.0726]],\n",
      "\n",
      "        [[-0.1525,  0.7593,  0.3625,  0.7789, -0.9160,  1.5704]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "\n",
      "Positional encoded target embeddings:\n",
      "tensor([[[-0.0037,  0.5908,  0.6838,  1.4102, -0.8705,  1.3255]],\n",
      "\n",
      "        [[ 0.4654, -0.9693, -0.2439,  1.3972, -0.4163,  0.4438]],\n",
      "\n",
      "        [[-1.9848,  0.4578,  0.6171, -0.1887, -0.1505,  1.1084]],\n",
      "\n",
      "        [[-1.9848,  0.4578,  0.6171, -0.1887, -0.1505,  1.1084]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "\n",
      "query =  tensor([[[-2.2991, -0.0892, -0.6579,  2.2630, -0.6850, -1.1738]],\n",
      "\n",
      "        [[-0.7063,  0.8717, -0.1086,  0.0386, -0.9128,  2.4580]],\n",
      "\n",
      "        [[ 0.5709,  1.3660,  0.1946,  1.2407, -0.2572,  1.0726]],\n",
      "\n",
      "        [[-0.1525,  0.7593,  0.3625,  0.7789, -0.9160,  1.5704]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "query shape =  torch.Size([4, 1, 6])\n",
      "\n",
      " _in_projection_packed \n",
      "\n",
      "Q :-  tensor([[[ 0.1998,  0.7083,  0.2082,  0.1071,  0.2617, -0.8886]],\n",
      "\n",
      "        [[-0.0606, -0.1456,  0.5500,  1.3566, -0.1725,  0.0735]],\n",
      "\n",
      "        [[ 1.0679, -0.5471,  0.5807,  0.9165, -0.0318,  0.7110]],\n",
      "\n",
      "        [[ 0.1715, -0.0964,  0.6861,  1.1417, -0.2430,  0.4955]]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "\n",
      "K :-  tensor([[[-0.5979, -1.1891, -1.7691,  0.6594, -0.9362,  0.5301]],\n",
      "\n",
      "        [[-1.6906, -0.9134,  0.3297, -1.1148, -0.2258,  0.5421]],\n",
      "\n",
      "        [[-1.0899, -0.8780, -0.0898, -0.1797, -0.4961,  0.9682]],\n",
      "\n",
      "        [[-1.3269, -0.7487, -0.0347, -0.8286, -0.3978,  0.4431]]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "\n",
      "V :-  tensor([[[-1.6108,  1.0090,  1.4326, -0.1975, -0.3194, -1.7217]],\n",
      "\n",
      "        [[-1.8543, -0.0398,  0.9824, -0.9378, -0.2746, -1.1484]],\n",
      "\n",
      "        [[-0.5641,  0.2149,  0.6389,  0.2284, -0.2286, -0.8718]],\n",
      "\n",
      "        [[-1.1238, -0.0103,  1.0536, -0.4016, -0.0044, -0.8831]]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "\n",
      "tgt_len =  4\n",
      "bsz =  1\n",
      "num_heads =  2\n",
      "head_dim =  3\n",
      "\n",
      "tensor([[[ 0.1998,  0.7083,  0.2082,  0.1071,  0.2617, -0.8886]],\n",
      "\n",
      "        [[-0.0606, -0.1456,  0.5500,  1.3566, -0.1725,  0.0735]],\n",
      "\n",
      "        [[ 1.0679, -0.5471,  0.5807,  0.9165, -0.0318,  0.7110]],\n",
      "\n",
      "        [[ 0.1715, -0.0964,  0.6861,  1.1417, -0.2430,  0.4955]]],\n",
      "       grad_fn=<SelectBackward0>) torch.Size([4, 1, 6])\n",
      "Q reshaped =  tensor([[[ 0.1998,  0.7083,  0.2082],\n",
      "         [-0.0606, -0.1456,  0.5500],\n",
      "         [ 1.0679, -0.5471,  0.5807],\n",
      "         [ 0.1715, -0.0964,  0.6861]],\n",
      "\n",
      "        [[ 0.1071,  0.2617, -0.8886],\n",
      "         [ 1.3566, -0.1725,  0.0735],\n",
      "         [ 0.9165, -0.0318,  0.7110],\n",
      "         [ 1.1417, -0.2430,  0.4955]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "K reshaped =  tensor([[[-0.5979, -1.1891, -1.7691],\n",
      "         [-1.6906, -0.9134,  0.3297],\n",
      "         [-1.0899, -0.8780, -0.0898],\n",
      "         [-1.3269, -0.7487, -0.0347]],\n",
      "\n",
      "        [[ 0.6594, -0.9362,  0.5301],\n",
      "         [-1.1148, -0.2258,  0.5421],\n",
      "         [-0.1797, -0.4961,  0.9682],\n",
      "         [-0.8286, -0.3978,  0.4431]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "V reshaped =  tensor([[[-1.6108,  1.0090,  1.4326],\n",
      "         [-1.8543, -0.0398,  0.9824],\n",
      "         [-0.5641,  0.2149,  0.6389],\n",
      "         [-1.1238, -0.0103,  1.0536]],\n",
      "\n",
      "        [[-0.1975, -0.3194, -1.7217],\n",
      "         [-0.9378, -0.2746, -1.1484],\n",
      "         [ 0.2284, -0.2286, -0.8718],\n",
      "         [-0.4016, -0.0044, -0.8831]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "torch.Size([1, 2, 4, 3]) torch.Size([1, 2, 4, 3]) torch.Size([1, 2, 4, 3])\n",
      "\n",
      "scaled_dot_product_attention\n",
      "attn_output = \n",
      "tensor([[-1.2615,  0.2484,  1.0018, -0.3539, -0.2037, -1.1679],\n",
      "        [-1.2783,  0.1999,  0.9811, -0.2058, -0.2461, -1.3233],\n",
      "        [-1.2501,  0.2951,  1.0109, -0.2174, -0.2329, -1.2441],\n",
      "        [-1.2667,  0.1955,  0.9754, -0.2051, -0.2410, -1.2889]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "\n",
      "x + self_atten(x) = \n",
      "tensor([[[-3.1413, -1.0760, -1.3346,  3.5613, -1.8071, -2.3114]],\n",
      "\n",
      "        [[-1.5760, -0.2521, -0.8776,  1.3621, -1.9372,  1.1276]],\n",
      "\n",
      "        [[-0.2211,  0.3317, -0.5597,  2.5990, -1.3552, -0.1746]],\n",
      "\n",
      "        [[-1.0073, -0.3416, -0.4045,  2.0803, -1.9355,  0.2651]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "\n",
      "norm1(x + self_atten(x)) = \n",
      "tensor([[[-0.9850, -0.0268, -0.1468,  2.1245, -0.3660, -0.6000]],\n",
      "\n",
      "        [[-0.9712,  0.0852, -0.4139,  1.3732, -1.2593,  1.1860]],\n",
      "\n",
      "        [[-0.2645,  0.1861, -0.5405,  2.0343, -1.1889, -0.2266]],\n",
      "\n",
      "        [[-0.6348, -0.0954, -0.1463,  1.8671, -1.3869,  0.3962]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "\n",
      "feed_fwd_op =  tensor([[[-0.0047,  0.0146, -0.7817, -0.7899, -0.0478,  0.7238]],\n",
      "\n",
      "        [[-0.0035, -0.1482, -0.6931, -0.8764, -0.0289,  0.7043]],\n",
      "\n",
      "        [[ 0.0501, -0.0350, -0.7830, -0.8063, -0.0431,  0.6999]],\n",
      "\n",
      "        [[ 0.0331, -0.1147, -0.7854, -0.8169, -0.0406,  0.6936]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "\n",
      "norm2(norm1(x + self_atten(x)) + feed_fwd_op) = \n",
      "tensor([[[-1.0754,  0.1729, -0.9973,  1.8930, -0.3399,  0.3467]],\n",
      "\n",
      "        [[-0.7170,  0.0998, -0.8355,  0.6011, -0.9979,  1.8495]],\n",
      "\n",
      "        [[-0.0678,  0.3354, -1.2914,  1.5234, -1.1904,  0.6909]],\n",
      "\n",
      "        [[-0.4521, -0.0402, -0.7992,  1.2851, -1.3204,  1.3268]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "\n",
      "##################################\n",
      "\n",
      "################### Transformer Decoder start #####################\n",
      "\n",
      "query =  tensor([[[-0.0037,  0.5908,  0.6838,  1.4102, -0.8705,  1.3255]],\n",
      "\n",
      "        [[ 0.4654, -0.9693, -0.2439,  1.3972, -0.4163,  0.4438]],\n",
      "\n",
      "        [[-1.9848,  0.4578,  0.6171, -0.1887, -0.1505,  1.1084]],\n",
      "\n",
      "        [[-1.9848,  0.4578,  0.6171, -0.1887, -0.1505,  1.1084]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "query shape =  torch.Size([4, 1, 6])\n",
      "\n",
      " _in_projection_packed \n",
      "\n",
      "Q :-  tensor([[[-0.3784, -0.7437, -0.6235, -0.4801,  0.2827, -0.1775]],\n",
      "\n",
      "        [[-0.7424, -0.8504, -0.4722,  0.1221,  0.4419,  0.8983]],\n",
      "\n",
      "        [[ 0.1257,  0.3638,  0.5637, -0.1287, -0.7501, -1.4718]],\n",
      "\n",
      "        [[ 0.1257,  0.3638,  0.5637, -0.1287, -0.7501, -1.4718]]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "\n",
      "K :-  tensor([[[ 0.0865, -1.2798,  0.5098,  0.5999,  0.1313,  0.3402]],\n",
      "\n",
      "        [[-0.1128, -1.0767,  0.9138,  0.4384,  0.6527,  0.9106]],\n",
      "\n",
      "        [[ 0.9444, -0.9628, -0.7018, -1.2218,  0.0632, -1.2223]],\n",
      "\n",
      "        [[ 0.9444, -0.9628, -0.7018, -1.2218,  0.0632, -1.2223]]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "\n",
      "V :-  tensor([[[-1.1569, -1.4169, -0.2169,  0.5000, -1.5058,  0.7683]],\n",
      "\n",
      "        [[-0.1712, -0.1728, -0.3195,  0.0646, -0.6681,  0.5009]],\n",
      "\n",
      "        [[-1.0927, -1.3739,  0.3832,  0.4547,  0.2847, -0.4699]],\n",
      "\n",
      "        [[-1.0927, -1.3739,  0.3832,  0.4547,  0.2847, -0.4699]]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "\n",
      "tgt_len =  4\n",
      "bsz =  1\n",
      "num_heads =  2\n",
      "head_dim =  3\n",
      "\n",
      "tensor([[[-0.3784, -0.7437, -0.6235, -0.4801,  0.2827, -0.1775]],\n",
      "\n",
      "        [[-0.7424, -0.8504, -0.4722,  0.1221,  0.4419,  0.8983]],\n",
      "\n",
      "        [[ 0.1257,  0.3638,  0.5637, -0.1287, -0.7501, -1.4718]],\n",
      "\n",
      "        [[ 0.1257,  0.3638,  0.5637, -0.1287, -0.7501, -1.4718]]],\n",
      "       grad_fn=<SelectBackward0>) torch.Size([4, 1, 6])\n",
      "Q reshaped =  tensor([[[-0.3784, -0.7437, -0.6235],\n",
      "         [-0.7424, -0.8504, -0.4722],\n",
      "         [ 0.1257,  0.3638,  0.5637],\n",
      "         [ 0.1257,  0.3638,  0.5637]],\n",
      "\n",
      "        [[-0.4801,  0.2827, -0.1775],\n",
      "         [ 0.1221,  0.4419,  0.8983],\n",
      "         [-0.1287, -0.7501, -1.4718],\n",
      "         [-0.1287, -0.7501, -1.4718]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "K reshaped =  tensor([[[ 0.0865, -1.2798,  0.5098],\n",
      "         [-0.1128, -1.0767,  0.9138],\n",
      "         [ 0.9444, -0.9628, -0.7018],\n",
      "         [ 0.9444, -0.9628, -0.7018]],\n",
      "\n",
      "        [[ 0.5999,  0.1313,  0.3402],\n",
      "         [ 0.4384,  0.6527,  0.9106],\n",
      "         [-1.2218,  0.0632, -1.2223],\n",
      "         [-1.2218,  0.0632, -1.2223]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "V reshaped =  tensor([[[-1.1569, -1.4169, -0.2169],\n",
      "         [-0.1712, -0.1728, -0.3195],\n",
      "         [-1.0927, -1.3739,  0.3832],\n",
      "         [-1.0927, -1.3739,  0.3832]],\n",
      "\n",
      "        [[ 0.5000, -1.5058,  0.7683],\n",
      "         [ 0.0646, -0.6681,  0.5009],\n",
      "         [ 0.4547,  0.2847, -0.4699],\n",
      "         [ 0.4547,  0.2847, -0.4699]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "torch.Size([1, 2, 4, 3]) torch.Size([1, 2, 4, 3]) torch.Size([1, 2, 4, 3])\n",
      "\n",
      "scaled_dot_product_attention\n",
      "attn_output = \n",
      "tensor([[-9.2092e-01, -1.1400e+00,  9.2552e-02,  3.9158e-01, -1.9056e-01,\n",
      "         -8.4665e-02],\n",
      "        [-8.8051e-01, -1.0858e+00,  3.7598e-02,  2.8831e-01, -7.0011e-01,\n",
      "          3.5517e-01],\n",
      "        [-8.1839e-01, -1.0054e+00, -1.0936e-03,  4.4039e-01,  6.6905e-02,\n",
      "         -3.0436e-01],\n",
      "        [-8.1839e-01, -1.0054e+00, -1.0936e-03,  4.4039e-01,  6.6905e-02,\n",
      "         -3.0436e-01]], grad_fn=<ViewBackward0>)\n",
      "\n",
      "x =  tensor([[[-0.0037,  0.5908,  0.6838,  1.4102, -0.8705,  1.3255]],\n",
      "\n",
      "        [[ 0.4654, -0.9693, -0.2439,  1.3972, -0.4163,  0.4438]],\n",
      "\n",
      "        [[-1.9848,  0.4578,  0.6171, -0.1887, -0.1505,  1.1084]],\n",
      "\n",
      "        [[-1.9848,  0.4578,  0.6171, -0.1887, -0.1505,  1.1084]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "\n",
      "sa_op =  tensor([[[-0.8188, -0.4678,  0.6516, -0.6265, -0.2877, -0.0952]],\n",
      "\n",
      "        [[-0.6577, -0.3695,  0.6269, -0.2723, -0.1989, -0.1276]],\n",
      "\n",
      "        [[-0.7676, -0.4025,  0.4873, -0.7457, -0.2433,  0.0417]],\n",
      "\n",
      "        [[-0.7676, -0.4025,  0.4873, -0.7457, -0.2433,  0.0417]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "\n",
      "temop =  tensor([[[-0.8225,  0.1229,  1.3354,  0.7837, -1.1582,  1.2303]],\n",
      "\n",
      "        [[-0.1922, -1.3388,  0.3830,  1.1249, -0.6152,  0.3162]],\n",
      "\n",
      "        [[-2.7524,  0.0553,  1.1043, -0.9344, -0.3938,  1.1501]],\n",
      "\n",
      "        [[-2.7524,  0.0553,  1.1043, -0.9344, -0.3938,  1.1501]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "\n",
      "Decoder norm1(x + sa(x)) = \n",
      "tensor([[[-1.1113, -0.1304,  1.1276,  0.5552, -1.4596,  1.0186]],\n",
      "\n",
      "        [[-0.1763, -1.6359,  0.5559,  1.5003, -0.7148,  0.4709]],\n",
      "\n",
      "        [[-1.8464,  0.2633,  1.0516, -0.4803, -0.0742,  1.0860]],\n",
      "\n",
      "        [[-1.8464,  0.2633,  1.0516, -0.4803, -0.0742,  1.0860]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "\n",
      "Memory = \n",
      "tensor([[[-1.0754,  0.1729, -0.9973,  1.8930, -0.3399,  0.3467]],\n",
      "\n",
      "        [[-0.7170,  0.0998, -0.8355,  0.6011, -0.9979,  1.8495]],\n",
      "\n",
      "        [[-0.0678,  0.3354, -1.2914,  1.5234, -1.1904,  0.6909]],\n",
      "\n",
      "        [[-0.4521, -0.0402, -0.7992,  1.2851, -1.3204,  1.3268]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "\n",
      "query =  tensor([[[-1.1113, -0.1304,  1.1276,  0.5552, -1.4596,  1.0186]],\n",
      "\n",
      "        [[-0.1763, -1.6359,  0.5559,  1.5003, -0.7148,  0.4709]],\n",
      "\n",
      "        [[-1.8464,  0.2633,  1.0516, -0.4803, -0.0742,  1.0860]],\n",
      "\n",
      "        [[-1.8464,  0.2633,  1.0516, -0.4803, -0.0742,  1.0860]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "query shape =  torch.Size([4, 1, 6])\n",
      "\n",
      " _in_projection_packed \n",
      "\n",
      "Q :-  tensor([[[-1.5225, -1.3120, -1.9461, -0.8573, -0.2711,  0.2245]],\n",
      "\n",
      "        [[-0.1991, -0.7104, -1.7022, -0.6236, -1.2013,  0.9954]],\n",
      "\n",
      "        [[-1.0709, -0.5297, -1.1219, -1.0236, -0.4365, -0.0575]],\n",
      "\n",
      "        [[-1.0709, -0.5297, -1.1219, -1.0236, -0.4365, -0.0575]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "\n",
      "K :-  tensor([[[ 0.7639,  0.3596,  0.6521, -0.3901, -0.7216,  0.2131]],\n",
      "\n",
      "        [[ 1.4840,  0.8749, -0.3867,  0.4722, -0.8736, -0.6255]],\n",
      "\n",
      "        [[ 0.7257,  0.4635,  0.0324,  0.5111, -0.3893, -0.3831]],\n",
      "\n",
      "        [[ 1.0081,  0.8540, -0.2985,  0.3892, -0.5258, -0.4995]]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "\n",
      "V :-  tensor([[[ 0.5164,  0.2813,  0.6188,  0.3277,  0.5997,  0.3185]],\n",
      "\n",
      "        [[ 0.5019, -0.7920,  0.7854,  0.9193, -0.4360,  0.3634]],\n",
      "\n",
      "        [[ 0.7359,  0.4198,  0.5550,  0.6701,  0.4711,  0.9148]],\n",
      "\n",
      "        [[ 0.6834, -0.3629,  0.4580,  1.2092,  0.0564,  0.6098]]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "\n",
      "tgt_len =  4\n",
      "bsz =  1\n",
      "num_heads =  2\n",
      "head_dim =  3\n",
      "\n",
      "tensor([[[-1.5225, -1.3120, -1.9461, -0.8573, -0.2711,  0.2245]],\n",
      "\n",
      "        [[-0.1991, -0.7104, -1.7022, -0.6236, -1.2013,  0.9954]],\n",
      "\n",
      "        [[-1.0709, -0.5297, -1.1219, -1.0236, -0.4365, -0.0575]],\n",
      "\n",
      "        [[-1.0709, -0.5297, -1.1219, -1.0236, -0.4365, -0.0575]]],\n",
      "       grad_fn=<ViewBackward0>) torch.Size([4, 1, 6])\n",
      "Q reshaped =  tensor([[[-1.5225, -1.3120, -1.9461],\n",
      "         [-0.1991, -0.7104, -1.7022],\n",
      "         [-1.0709, -0.5297, -1.1219],\n",
      "         [-1.0709, -0.5297, -1.1219]],\n",
      "\n",
      "        [[-0.8573, -0.2711,  0.2245],\n",
      "         [-0.6236, -1.2013,  0.9954],\n",
      "         [-1.0236, -0.4365, -0.0575],\n",
      "         [-1.0236, -0.4365, -0.0575]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "K reshaped =  tensor([[[ 0.7639,  0.3596,  0.6521],\n",
      "         [ 1.4840,  0.8749, -0.3867],\n",
      "         [ 0.7257,  0.4635,  0.0324],\n",
      "         [ 1.0081,  0.8540, -0.2985]],\n",
      "\n",
      "        [[-0.3901, -0.7216,  0.2131],\n",
      "         [ 0.4722, -0.8736, -0.6255],\n",
      "         [ 0.5111, -0.3893, -0.3831],\n",
      "         [ 0.3892, -0.5258, -0.4995]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "V reshaped =  tensor([[[ 0.5164,  0.2813,  0.6188],\n",
      "         [ 0.5019, -0.7920,  0.7854],\n",
      "         [ 0.7359,  0.4198,  0.5550],\n",
      "         [ 0.6834, -0.3629,  0.4580]],\n",
      "\n",
      "        [[ 0.3277,  0.5997,  0.3185],\n",
      "         [ 0.9193, -0.4360,  0.3634],\n",
      "         [ 0.6701,  0.4711,  0.9148],\n",
      "         [ 1.2092,  0.0564,  0.6098]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "torch.Size([1, 2, 4, 3]) torch.Size([1, 2, 4, 3]) torch.Size([1, 2, 4, 3])\n",
      "\n",
      "scaled_dot_product_attention\n",
      "attn_output = \n",
      "tensor([[ 0.6349, -0.0728,  0.5854,  0.7175,  0.2306,  0.5130],\n",
      "        [ 0.6179, -0.1964,  0.6052,  0.6782,  0.2542,  0.4859],\n",
      "        [ 0.6257, -0.0899,  0.5912,  0.7232,  0.2187,  0.5095],\n",
      "        [ 0.6257, -0.0899,  0.5912,  0.7232,  0.2187,  0.5095]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "\n",
      "Decoder norm2(x + mha(x, mem)) = \n",
      "tensor([[[-1.6626, -0.2054,  1.1872,  0.8788, -0.8033,  0.6054]],\n",
      "\n",
      "        [[-0.7633, -1.5386,  0.7136,  1.5919, -0.0597,  0.0562]],\n",
      "\n",
      "        [[-2.1009,  0.1303,  1.0147, -0.0620,  0.4149,  0.6030]],\n",
      "\n",
      "        [[-2.1009,  0.1303,  1.0147, -0.0620,  0.4149,  0.6030]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "\n",
      "Decoder norm3(x + ff(x)) = \n",
      "tensor([[[-1.3175, -0.6332,  1.6384,  0.3114, -0.7350,  0.7358]],\n",
      "\n",
      "        [[-0.2502, -1.9815,  0.8999,  1.0663,  0.2534,  0.0120]],\n",
      "\n",
      "        [[-1.6847, -0.1621,  1.3101, -0.7599,  0.6732,  0.6234]],\n",
      "\n",
      "        [[-1.6847, -0.1621,  1.3101, -0.7599,  0.6732,  0.6234]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "\n",
      "tensor([[[-0.3931,  0.7839, -0.0701, -0.8229,  0.0118, -0.7191, -0.1468,\n",
      "          -0.8998,  0.1445,  0.0056]],\n",
      "\n",
      "        [[-0.4864,  1.2767,  0.2504, -1.1761, -0.0962, -0.3620, -0.3499,\n",
      "          -0.1509, -0.5999, -0.1273]],\n",
      "\n",
      "        [[-0.6542,  0.5709,  0.3987, -0.5503, -0.5313, -1.0139, -0.1259,\n",
      "          -0.5488,  1.0464, -0.3315]],\n",
      "\n",
      "        [[-0.6542,  0.5709,  0.3987, -0.5503, -0.5313, -1.0139, -0.1259,\n",
      "          -0.5488,  1.0464, -0.3315]]], grad_fn=<ViewBackward0>) torch.Size([4, 1, 10])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=512, dropout=0):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.encoding = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1).float()\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(torch.log(torch.tensor(10000.0)) / d_model))\n",
    "        self.encoding[:, 0::2] = torch.sin(position * div_term)\n",
    "        self.encoding[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.encoding = self.encoding.unsqueeze(0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.encoding[:, :x.size(1)].detach()\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, src_vocab_size, tgt_vocab_size,max_seq_len, d_model=4, nhead=2, num_encoder_layers=1, num_decoder_layers=1):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.src_embedding = nn.Embedding(src_vocab_size, d_model)\n",
    "        self.tgt_embedding = nn.Embedding(tgt_vocab_size, d_model)\n",
    "        self.positional_encoding = PositionalEncoding(d_model, dropout=0, max_len=max_seq_len)\n",
    "        self.transformer = nn.Transformer(\n",
    "            d_model=d_model,\n",
    "            nhead=nhead,\n",
    "            num_encoder_layers=num_encoder_layers,\n",
    "            num_decoder_layers=num_decoder_layers,\n",
    "            dropout=0\n",
    "        )\n",
    "        self.fc = nn.Linear(d_model, tgt_vocab_size)\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "\n",
    "        print(\"Source embeddings :- \\n\")\n",
    "        print(self.src_embedding(src))\n",
    "        print()\n",
    "\n",
    "        print(\"Target embeddings :- \\n\")\n",
    "        print(self.tgt_embedding(tgt))\n",
    "        print()\n",
    "\n",
    "\n",
    "        src = self.src_embedding(src) + self.positional_encoding(src)\n",
    "        tgt = self.tgt_embedding(tgt) + self.positional_encoding(tgt)\n",
    "\n",
    "        print(\"Positional encoded source embeddings:\")\n",
    "        print(src)\n",
    "        print()\n",
    "\n",
    "        print(\"Positional encoded target embeddings:\")\n",
    "        print(tgt)\n",
    "        print()\n",
    "\n",
    "        output = self.transformer(src, tgt)\n",
    "        output = self.fc(output)\n",
    "        \n",
    "        return output\n",
    "\n",
    "    \n",
    "\n",
    "src_vocab_size = 10  # Source language vocabulary size\n",
    "tgt_vocab_size = 10  # Target language vocabulary size\n",
    "d_model = 6  # Dimension of the model\n",
    "num_heads = 2\n",
    "max_seq_len = 4\n",
    "model = TransformerModel(src_vocab_size, tgt_vocab_size, d_model=d_model, max_seq_len = max_seq_len, nhead=num_heads)\n",
    "\n",
    "# Source sentence in the source language\n",
    "# Source token indexes from src vocabulary\n",
    "src_sentence = torch.tensor([[0], [1], [2], [3]]) \n",
    "\n",
    "\n",
    "\n",
    "# Target sentence in the target language (translation of the source sentence)\n",
    "# Target token indexes from tgt vocabulary\n",
    "tgt_sentence = torch.tensor([[1], [0], [3], [3]])\n",
    "\n",
    "# Forward pass\n",
    "output = model(src_sentence, tgt_sentence)\n",
    "print(output, output.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions to get the manual calulations for each component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import functional as F\n",
    "\n",
    "# Using the state dictionary to get the intermediate outputs\n",
    "state_dict = model.state_dict()\n",
    "\n",
    "need_weights = False\n",
    "\n",
    "src_mask = None\n",
    "\n",
    "tgt_mask = None\n",
    "\n",
    "memory_mask = None\n",
    "\n",
    "embed_dim = 6\n",
    "\n",
    "num_heads = 2\n",
    "\n",
    "max_seq_len = 4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "def look_up_table(sentence, vocab_embeds, embedding):\n",
    "\n",
    "    for i in range(sentence.size(0)):\n",
    "        for j in range(sentence.size(1)):\n",
    "            # Get the index for the current word token index in the sequence\n",
    "            word_index = sentence[i, j].item()\n",
    "\n",
    "            if word_index < 0 or word_index >= vocab_embeds.size(0):\n",
    "                raise ValueError(f\"Invalid word index: {word_index}\")\n",
    "\n",
    "            # Lookup the corresponding embedding vector for the word\n",
    "            embedding[i, j, :] = vocab_embeds[word_index, :]\n",
    "\n",
    "            print(f\"Word index: {word_index}, Embedding: {vocab_embeds[word_index, :]}\")\n",
    "    print()\n",
    "    \n",
    "\n",
    "    return embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embeddings and Positional encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding_outputs(src_sentence,  tgt_sentence, max_seq_len, state_dict, d_model):\n",
    "\n",
    "    src_vocab_embeds = state_dict[\"src_embedding.weight\"]\n",
    "\n",
    "    src_embedding = torch.zeros(src_sentence.size(0), src_sentence.size(1), d_model)\n",
    "    print(\"Source sentence embedding\")\n",
    "    src_embedding =  look_up_table(src_sentence, src_vocab_embeds, src_embedding)\n",
    "    print(src_embedding.shape)\n",
    "\n",
    "    tgt_vocab_embeds = state_dict[\"tgt_embedding.weight\"]\n",
    "\n",
    "    tgt_embedding = torch.zeros(tgt_sentence.size(0), tgt_sentence.size(1), d_model)\n",
    "\n",
    "\n",
    "    print(\"Target sentence embedding\")\n",
    "    tgt_embedding =  look_up_table(tgt_sentence, tgt_vocab_embeds, tgt_embedding)\n",
    "\n",
    "    pe = PositionalEncoding(d_model = d_model, dropout=0, max_len=max_seq_len)\n",
    "\n",
    "    print(\"PE of src :\")\n",
    "    print(pe(src_embedding))\n",
    "    print()\n",
    "    print(\"PE of tgt :\")\n",
    "    print(pe(tgt_embedding))\n",
    "    print()\n",
    "\n",
    "    pe_src_embeds = src_embedding + pe(src_embedding)\n",
    "\n",
    "    pe_tgt_embeds = tgt_embedding + pe(tgt_embedding)\n",
    "\n",
    "    print(\"PE source embeddings : \\n\")\n",
    "    print(pe_src_embeds)\n",
    "    print()\n",
    "\n",
    "    print(\"PE target embeddings : \\n\")\n",
    "    print(pe_tgt_embeds)\n",
    "    print()\n",
    "\n",
    "    return pe_src_embeds, pe_tgt_embeds\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source sentence embedding\n",
      "Word index: 0, Embedding: tensor([-2.2991, -1.0892, -0.6579,  1.2630, -0.6850, -2.1738])\n",
      "Word index: 1, Embedding: tensor([-0.7063, -0.1283, -0.1086, -0.9614, -0.9128,  1.4580])\n",
      "Word index: 2, Embedding: tensor([ 0.5709,  0.3660,  0.1946,  0.2407, -0.2572,  0.0726])\n",
      "Word index: 3, Embedding: tensor([-0.1525, -0.2407,  0.3625, -0.2211, -0.9160,  0.5704])\n",
      "\n",
      "torch.Size([4, 1, 6])\n",
      "Target sentence embedding\n",
      "Word index: 1, Embedding: tensor([-0.0037, -0.4092,  0.6838,  0.4102, -0.8705,  0.3255])\n",
      "Word index: 0, Embedding: tensor([ 0.4654, -1.9693, -0.2439,  0.3972, -0.4163, -0.5562])\n",
      "Word index: 3, Embedding: tensor([-1.9848, -0.5422,  0.6171, -1.1887, -0.1505,  0.1084])\n",
      "Word index: 3, Embedding: tensor([-1.9848, -0.5422,  0.6171, -1.1887, -0.1505,  0.1084])\n",
      "\n",
      "PE of src :\n",
      "tensor([[[0., 1., 0., 1., 0., 1.]]])\n",
      "\n",
      "PE of tgt :\n",
      "tensor([[[0., 1., 0., 1., 0., 1.]]])\n",
      "\n",
      "PE source embeddings : \n",
      "\n",
      "tensor([[[-2.2991, -0.0892, -0.6579,  2.2630, -0.6850, -1.1738]],\n",
      "\n",
      "        [[-0.7063,  0.8717, -0.1086,  0.0386, -0.9128,  2.4580]],\n",
      "\n",
      "        [[ 0.5709,  1.3660,  0.1946,  1.2407, -0.2572,  1.0726]],\n",
      "\n",
      "        [[-0.1525,  0.7593,  0.3625,  0.7789, -0.9160,  1.5704]]])\n",
      "\n",
      "PE target embeddings : \n",
      "\n",
      "tensor([[[-0.0037,  0.5908,  0.6838,  1.4102, -0.8705,  1.3255]],\n",
      "\n",
      "        [[ 0.4654, -0.9693, -0.2439,  1.3972, -0.4163,  0.4438]],\n",
      "\n",
      "        [[-1.9848,  0.4578,  0.6171, -0.1887, -0.1505,  1.1084]],\n",
      "\n",
      "        [[-1.9848,  0.4578,  0.6171, -0.1887, -0.1505,  1.1084]]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pe_src_embeds, pe_tgt_embeds = get_embedding_outputs(src_sentence=src_sentence,  tgt_sentence=tgt_sentence, state_dict=state_dict, max_seq_len=max_seq_len, d_model = d_model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder function to display the intermediate outputs and get the final outputs from the encoder\n",
    "\n",
    "### Self attention "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "def atten_product_needs_wts_false(Q, V, K, bsz, head_dim, src_len, tgt_len, attn_mask):\n",
    "\n",
    "\n",
    "    # *** For multi-head attention ***\n",
    "    #  (bsz*num_heads, src_len , head_dim) -> (bsz, num_heads, tgt_len, head_dim)\n",
    "    Q1 = Q.view(bsz, num_heads, tgt_len, head_dim)\n",
    "    K1 = K.view(bsz, num_heads, src_len, head_dim)\n",
    "    V1 = V.view(bsz, num_heads, src_len, head_dim)\n",
    "\n",
    "    L, S = Q1.size(-2), K1.size(-2)\n",
    "\n",
    "    scale_factor = 1 / math.sqrt(Q1.size(-1)) \n",
    "    # scale_factor = 1\n",
    "    attn_bias = torch.zeros(L, S, dtype=Q1.dtype)\n",
    "\n",
    "    if attn_mask is not None:\n",
    "        if attn_mask.dtype == torch.bool:\n",
    "            attn_mask.masked_fill_(attn_mask.logical_not(), float(\"-inf\"))\n",
    "        else:\n",
    "            attn_bias += attn_mask\n",
    "\n",
    "    # (bsz, num_heads, tgt_len, head_dim) @ (bsz, num_heads, head_dim, tgt_len) -> (bsz, num_heads, tgt_len, tgt_len) \n",
    "    attn_weight = Q1 @ K1.transpose(-2, -1) * scale_factor\n",
    "    attn_weight += attn_bias\n",
    "\n",
    "    # (bsz, num_heads, tgt_len, tgt_len) \n",
    "    attn_weight = torch.softmax(attn_weight, dim=-1)\n",
    "\n",
    "    print(\"Attention weights = \")\n",
    "    print(attn_weight.shape, attn_weight)\n",
    "\n",
    "    # (bsz, num_heads, tgt_len, tgt_len) @ (bsz, num_heads, tgt_len, head_dim) -> (bsz, num_heads, tgt_len, head_dim) \n",
    "    attn_output = attn_weight @ V1\n",
    "\n",
    "    # (bsz*tgt_len, embed_dim)\n",
    "    attn_output = attn_output.permute(2, 0, 1, 3).contiguous().view(bsz * tgt_len, embed_dim)\n",
    "\n",
    "    return attn_output\n",
    "\n",
    "\n",
    "\n",
    "def atten_product_needs_wts_true(Q, K, V, bsz, tgt_len, attn_mask):\n",
    "\n",
    "    # *** For multi-head attention ***\n",
    "    #  (bsz*num_heads, src_len , head_dim)\n",
    "    B, Nt, E = Q.shape\n",
    "\n",
    "    Q_scaled = Q / math.sqrt(E)\n",
    "\n",
    "    if attn_mask is not None:\n",
    "        temp_pdt_matrix = torch.baddbmm(attn_mask, Q_scaled, K.transpose(-2, -1))\n",
    "    else:\n",
    "        temp_pdt_matrix = torch.bmm(Q_scaled, K.transpose(-2, -1))\n",
    "\n",
    "    attn_wt_matrix = torch.nn.functional.softmax(temp_pdt_matrix, dim=-1)\n",
    "\n",
    "    attn_output = torch.bmm(attn_wt_matrix, V)\n",
    "\n",
    "    attn_output = attn_output.transpose(0, 1).contiguous().view(tgt_len * bsz, embed_dim)\n",
    "\n",
    "    print(\"Encoder Attention output = \")\n",
    "    print(attn_output)\n",
    "    print()\n",
    "\n",
    "    return attn_output, attn_wt_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_qkv(query, key, value ,W, b):\n",
    "\n",
    "    # embed_dim\n",
    "    E = query.size(-1)\n",
    "\n",
    "    if key is value:\n",
    "        if query is key:\n",
    "            \n",
    "            # (src_len, bsz, embed_dim) @ (embed_dim*num_heads, embed_dim).T -> (src_len, bsz, embed_dim*num_heads)\n",
    "            tempop1 = query@W.T\n",
    "\n",
    "            # reshape to 3, E and not E, 3 is deliberate for better memory coalescing and keeping same order as chunk()\n",
    "            tempop1 = tempop1.unflatten(-1, (3, E)).unsqueeze(0).transpose(0, -2).squeeze(-2).contiguous()\n",
    "\n",
    "            # (src_len, bsz, embed_dim)\n",
    "            return tempop1[0], tempop1[1], tempop1[2]\n",
    "        \n",
    "\n",
    "        else:\n",
    "\n",
    "            # (embed_dim*1, embed_dim)\n",
    "            # (embed_dim*2, embed_dim)\n",
    "            W_q, W_kv = W.split([E, E * 2])\n",
    "\n",
    "            if b is None:\n",
    "                b_q = b_kv = None\n",
    "            else:\n",
    "                b_q, b_kv = b.split([E, E * 2])\n",
    "\n",
    "            # (src_len, bsz, embed_dim) @ (embed_dim*1, embed_dim).T -> (src_len, bsz, embed_dim)\n",
    "            q_matmul = query@W_q.T\n",
    "\n",
    "            # (src_len, bsz, embed_dim) @ (embed_dim*2, embed_dim).T -> (src_len, bsz, embed_dim*2)\n",
    "            kv_matmul = key@W_kv.T\n",
    "\n",
    "            kv_matmul = kv_matmul.unflatten(-1, (2, E)).unsqueeze(0).transpose(0, -2).squeeze(-2).contiguous()\n",
    "\n",
    "            # (src_len, bsz, embed_dim)\n",
    "            return q_matmul, kv_matmul[0], kv_matmul[1]\n",
    "\n",
    "    else:\n",
    "\n",
    "        W_q, W_k, W_v = W.chunk(3)\n",
    "        if b is None:\n",
    "            b_q = b_k = b_v = None\n",
    "        else:\n",
    "            b_q, b_k, b_v = b.chunk(3)\n",
    "\n",
    "\n",
    "        q_matmul = query@W_q.T\n",
    "        k_matmul = key@W_k.T\n",
    "        v_matmul = value@W_v.T\n",
    "\n",
    "        return q_matmul, k_matmul, v_matmul\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder_block_attn_output(x, state_dict, layer_num, embed_dim, num_heads, need_weights = False, src_mask = None):\n",
    "\n",
    "    # (src_len, bsz, embed_dim)\n",
    "    query_enc = key_enc = value_enc = x\n",
    "\n",
    "    tgt_len, bsz, embed_dim = x.shape\n",
    "\n",
    "    # print(x.shape)\n",
    "    \n",
    "    # (embed_dim*num_heads, embed_dim)\n",
    "    W_enc = state_dict[\"transformer.encoder.layers.{}.self_attn.in_proj_weight\".format(layer_num)]\n",
    "    b_enc = state_dict[\"transformer.encoder.layers.{}.self_attn.in_proj_bias\".format(layer_num)]\n",
    "\n",
    "\n",
    "    head_dim = embed_dim//num_heads\n",
    "    \n",
    "    # (src_len, bsz, embed_dim)\n",
    "    Q_enc,K_enc,V_enc = get_qkv(query_enc, key_enc, value_enc ,W_enc, b_enc)\n",
    "\n",
    "    # (1, src_len, bsz, embed_dim)\n",
    "    Q_enc = Q_enc.unsqueeze(0)\n",
    "    K_enc = K_enc.unsqueeze(0)\n",
    "    V_enc = V_enc.unsqueeze(0)\n",
    "\n",
    "    # (1, src_len, bsz, embed_dim) -> ( bsz*num_heads, src_len , head_dim)\n",
    "    Q_enc = Q_enc.reshape(tgt_len, bsz * num_heads, head_dim).transpose(0, 1)\n",
    "    K_enc = K_enc.reshape(tgt_len, bsz * num_heads, head_dim).transpose(0, 1)\n",
    "    V_enc = V_enc.reshape(tgt_len, bsz * num_heads, head_dim).transpose(0, 1)\n",
    "\n",
    "    print(\"Q_enc_{} = \".format(layer_num))\n",
    "    print(Q_enc)\n",
    "    print()\n",
    "\n",
    "    print(\"K_enc_{} = \".format(layer_num))\n",
    "    print(K_enc)\n",
    "    print()\n",
    "\n",
    "    print(\"V_enc_{} = \".format(layer_num))\n",
    "    print(V_enc)\n",
    "    print()\n",
    "\n",
    "\n",
    "    src_len = K_enc.size(1)\n",
    "\n",
    "    attn_mask = src_mask\n",
    "    if attn_mask is not None:\n",
    "\n",
    "        # Ensuring attn_mask's dim is 3\n",
    "        if attn_mask.dim() == 2:\n",
    "            correct_2d_size = (tgt_len, src_len)\n",
    "            if attn_mask.shape != correct_2d_size:\n",
    "                raise RuntimeError(f\"The shape of the 2D attn_mask is {attn_mask.shape}, but should be {correct_2d_size}.\")\n",
    "            attn_mask = attn_mask.unsqueeze(0)\n",
    "        elif attn_mask.dim() == 3:\n",
    "            correct_3d_size = (bsz * num_heads, tgt_len, src_len)\n",
    "            if attn_mask.shape != correct_3d_size:\n",
    "                raise RuntimeError(f\"The shape of the 3D attn_mask is {attn_mask.shape}, but should be {correct_3d_size}.\")\n",
    "        else:\n",
    "            raise RuntimeError(f\"attn_mask's dimension {attn_mask.dim()} is not supported\")\n",
    "\n",
    "    # attn_mask can be either (L,S) or (N*num_heads, L, S)\n",
    "    # if attn_mask's shape is (1, L, S) we need to unsqueeze to (1, 1, L, S)\n",
    "    # in order to match the input for SDPA of (N, num_heads, L, S)\n",
    "    if attn_mask is not None:\n",
    "        if attn_mask.size(0) == 1 and attn_mask.dim() == 3:\n",
    "            attn_mask = attn_mask.unsqueeze(0)\n",
    "        else:\n",
    "            attn_mask = attn_mask.view(bsz, num_heads, -1, src_len)\n",
    "\n",
    "\n",
    "    if need_weights is False:\n",
    "\n",
    "        attn_output = atten_product_needs_wts_false(Q_enc, V_enc, K_enc, bsz, head_dim, src_len, tgt_len, attn_mask)\n",
    "\n",
    "        return attn_output, src_len, head_dim, None\n",
    "    \n",
    "    else:\n",
    "\n",
    "        attn_enc_output,attn_wt_matrix_enc = atten_product_needs_wts_true(Q_enc, K_enc, V_enc, bsz, tgt_len, attn_mask)\n",
    "\n",
    "        return attn_enc_output, src_len, head_dim, attn_wt_matrix_enc\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q_enc_0 = \n",
      "tensor([[[ 0.1998,  0.7083,  0.2082],\n",
      "         [-0.0606, -0.1456,  0.5500],\n",
      "         [ 1.0679, -0.5471,  0.5807],\n",
      "         [ 0.1715, -0.0964,  0.6861]],\n",
      "\n",
      "        [[ 0.1071,  0.2617, -0.8886],\n",
      "         [ 1.3566, -0.1725,  0.0735],\n",
      "         [ 0.9165, -0.0318,  0.7110],\n",
      "         [ 1.1417, -0.2430,  0.4955]]])\n",
      "\n",
      "K_enc_0 = \n",
      "tensor([[[-0.5979, -1.1891, -1.7691],\n",
      "         [-1.6906, -0.9134,  0.3297],\n",
      "         [-1.0899, -0.8780, -0.0898],\n",
      "         [-1.3269, -0.7487, -0.0347]],\n",
      "\n",
      "        [[ 0.6594, -0.9362,  0.5301],\n",
      "         [-1.1148, -0.2258,  0.5421],\n",
      "         [-0.1797, -0.4961,  0.9682],\n",
      "         [-0.8286, -0.3978,  0.4431]]])\n",
      "\n",
      "V_enc_0 = \n",
      "tensor([[[-1.6108,  1.0090,  1.4326],\n",
      "         [-1.8543, -0.0398,  0.9824],\n",
      "         [-0.5641,  0.2149,  0.6389],\n",
      "         [-1.1238, -0.0103,  1.0536]],\n",
      "\n",
      "        [[-0.1975, -0.3194, -1.7217],\n",
      "         [-0.9378, -0.2746, -1.1484],\n",
      "         [ 0.2284, -0.2286, -0.8718],\n",
      "         [-0.4016, -0.0044, -0.8831]]])\n",
      "\n",
      "Attention weights = \n",
      "torch.Size([1, 2, 4, 4]) tensor([[[[0.2025, 0.2571, 0.2658, 0.2745],\n",
      "          [0.1567, 0.3098, 0.2647, 0.2687],\n",
      "          [0.2431, 0.2297, 0.2858, 0.2414],\n",
      "          [0.1503, 0.3050, 0.2736, 0.2712]],\n",
      "\n",
      "         [[0.2607, 0.2584, 0.2112, 0.2697],\n",
      "          [0.4921, 0.1143, 0.2487, 0.1449],\n",
      "          [0.3869, 0.1501, 0.2947, 0.1682],\n",
      "          [0.4476, 0.1262, 0.2743, 0.1518]]]])\n"
     ]
    }
   ],
   "source": [
    "# need_weights = True\n",
    "\n",
    "attn_enc_output, src_len, head_dim, attn_weights = encoder_block_attn_output(pe_src_embeds, state_dict, layer_num = 0, need_weights = need_weights, embed_dim=embed_dim, num_heads=num_heads)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Post self attention in encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder_block_post_attn_output(x, attn_enc_output, state_dict, layer_num, bsz, tgt_len):\n",
    "\n",
    "    # (bsz*src_len , embed_dim) @ (embed_dim , embed_dim).T -> (bsz*src_len , embed_dim)\n",
    "    op_enc_1 = torch.matmul(attn_enc_output, state_dict[\"transformer.encoder.layers.{}.self_attn.out_proj.weight\".format(layer_num)].t()) + state_dict[\"transformer.encoder.layers.{}.self_attn.out_proj.bias\".format(layer_num)]\n",
    "\n",
    "\n",
    "\n",
    "    # (bsz*src_len , embed_dim) -> (src_len, bsz, embed_dim)\n",
    "    attn_enc_output = op_enc_1.view(tgt_len, bsz, attn_enc_output.size(1))\n",
    "\n",
    "\n",
    "    # Here src is the original passed inputs to the 1st transformer encoder layer which \n",
    "    # are pe_src_embeds \n",
    "\n",
    "    # (src_len, bsz, embed_dim)\n",
    "    output_enc_1 = attn_enc_output + x\n",
    "\n",
    "    #  (src_len, bsz, embed_dim) @ (embed_dim) -> (src_len, bsz, embed_dim) \n",
    "    linear_result_enc_1 = output_enc_1*state_dict[\"transformer.encoder.layers.{}.norm1.weight\".format(layer_num)] + state_dict[\"transformer.encoder.layers.{}.norm1.bias\".format(layer_num)]\n",
    "\n",
    "    # Layer normalization from Torch's implementation\n",
    "    layernorm_enc_1 = torch.nn.LayerNorm(normalized_shape=linear_result_enc_1.shape[2:])\n",
    "    linear_op_enc_1 = layernorm_enc_1(linear_result_enc_1)\n",
    "\n",
    "\n",
    "    # Manual Layer Normalization\n",
    "    x = linear_result_enc_1\n",
    "\n",
    "    # Obtained layer norm weights and biases (learnable)\n",
    "    w = layernorm_enc_1.weight\n",
    "    b = layernorm_enc_1.bias\n",
    "\n",
    "    linear_result_enc_1f = w*x + b\n",
    "\n",
    "    epsilon = 1e-5  \n",
    "    mean = linear_result_enc_1f.mean(dim=-1, keepdim=True)\n",
    "    std = linear_result_enc_1f.std(dim=-1, unbiased=False, keepdim=True)\n",
    "    normalized_result_enc_1 = (linear_result_enc_1f - mean) / (std + epsilon) * w + b\n",
    "\n",
    "\n",
    "    op_enc_1 = torch.matmul(normalized_result_enc_1, state_dict[\"transformer.encoder.layers.{}.linear1.weight\".format(layer_num)].t()) + state_dict[\"transformer.encoder.layers.{}.linear1.bias\".format(layer_num)]\n",
    "    op_enc_1_relu = torch.nn.functional.relu(op_enc_1)\n",
    "    op_enc_2 = torch.matmul(op_enc_1_relu, state_dict[\"transformer.encoder.layers.{}.linear2.weight\".format(layer_num)].t()) + state_dict[\"transformer.encoder.layers.{}.linear2.bias\".format(layer_num)]\n",
    "\n",
    "\n",
    "    output_enc_2 = op_enc_2 + linear_op_enc_1\n",
    "    output_enc_2_norm = output_enc_2*state_dict[\"transformer.encoder.layers.{}.norm2.weight\".format(layer_num)] + state_dict[\"transformer.encoder.layers.{}.norm2.bias\".format(layer_num)]\n",
    "\n",
    "    # Layer normalization from Torch's implementation\n",
    "    layernorm_enc_final = torch.nn.LayerNorm(normalized_shape=output_enc_2_norm.shape[2:])\n",
    "    output_enc_final = layernorm_enc_final(output_enc_2_norm)\n",
    "\n",
    "\n",
    "    # Manual Layer Normalization \n",
    "    x = output_enc_2_norm\n",
    "\n",
    "    # Obtained layer norm weights and biases (learnable)\n",
    "    w = layernorm_enc_final.weight\n",
    "    b = layernorm_enc_final.bias\n",
    "\n",
    "    linear_result_enc_2 = w*x + b\n",
    "\n",
    "    epsilon = 1e-5  \n",
    "    mean = linear_result_enc_2.mean(dim=-1, keepdim=True)\n",
    "    std = linear_result_enc_2.std(dim=-1, unbiased=False, keepdim=True)\n",
    "    output_enc_final = (linear_result_enc_2 - mean) / (std + epsilon) * w + b\n",
    "\n",
    "    print(\"Final Encoder {} Output :\".format(layer_num))\n",
    "    print(\"norm2(norm1(x + self_atten(x)) + feed_fwd_op)\\n\")\n",
    "    print(output_enc_final)\n",
    "    print()\n",
    "\n",
    "    # (src_len, bsz, embed_dim) \n",
    "    return output_enc_final\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Encoder 0 Output :\n",
      "norm2(norm1(x + self_atten(x)) + feed_fwd_op)\n",
      "\n",
      "tensor([[[-1.0754,  0.1729, -0.9973,  1.8929, -0.3399,  0.3467]],\n",
      "\n",
      "        [[-0.7170,  0.0998, -0.8355,  0.6011, -0.9979,  1.8495]],\n",
      "\n",
      "        [[-0.0678,  0.3354, -1.2914,  1.5234, -1.1904,  0.6909]],\n",
      "\n",
      "        [[-0.4521, -0.0402, -0.7991,  1.2851, -1.3204,  1.3268]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tgt_len, bsz, embed_dim = pe_src_embeds.shape\n",
    "output_enc_final = encoder_block_post_attn_output(pe_src_embeds, attn_enc_output, state_dict, layer_num = 0 , bsz = bsz, tgt_len = tgt_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder functions to display the intermediate outputs and get the final outputs from the decoder\n",
    "\n",
    "\n",
    "### Self attention outputs from a decoder block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoder_block_self_attn_output(x, state_dict, layer_num, tgt_mask = None,need_weights = False):\n",
    "\n",
    "    # (tgt_len, bsz, embed_dim)\n",
    "    query_dec = key_dec = value_dec = x\n",
    "\n",
    "    tgt_len, bsz, embed_dim = x.shape\n",
    " \n",
    "    # (embed_dim*num_heads, embed_dim)\n",
    "    W_dec = state_dict[\"transformer.decoder.layers.{}.self_attn.in_proj_weight\".format(layer_num)]\n",
    "    b_dec = state_dict[\"transformer.decoder.layers.{}.self_attn.in_proj_bias\".format(layer_num)]\n",
    "\n",
    "\n",
    "    head_dim = embed_dim//num_heads\n",
    "\n",
    "    # (tgt_len, bsz, embed_dim)\n",
    "    Q_dec,K_dec,V_dec = get_qkv(query_dec, key_dec, value_dec ,W_dec, b_dec)\n",
    "    \n",
    "    Q_dec = Q_dec.unsqueeze(0)\n",
    "    K_dec = K_dec.unsqueeze(0)\n",
    "    V_dec = V_dec.unsqueeze(0)\n",
    "\n",
    "    # (1, tgt_len, bsz, embed_dim)\n",
    "    print(Q_dec.shape, K_dec.shape , V_dec.shape)\n",
    "    print(tgt_len, bsz * num_heads, head_dim)\n",
    "\n",
    "    # (1, tgt_len, bsz, embed_dim) -> ( bsz*num_heads, tgt_len , head_dim )\n",
    "    Q_dec = Q_dec.reshape(tgt_len, bsz * num_heads, head_dim).transpose(0, 1)\n",
    "    K_dec = K_dec.reshape(tgt_len, bsz * num_heads, head_dim).transpose(0, 1)\n",
    "    V_dec = V_dec.reshape(tgt_len, bsz * num_heads, head_dim).transpose(0, 1)\n",
    "\n",
    "    print(\"Q_dec_{} = \".format(layer_num))\n",
    "    print(Q_dec)\n",
    "    print()\n",
    "\n",
    "    print(\"K_dec_{} = \".format(layer_num))\n",
    "    print(K_dec)\n",
    "    print()\n",
    "\n",
    "    print(\"V_dec_{} = \".format(layer_num))\n",
    "    print(V_dec)\n",
    "    print()\n",
    "\n",
    "    src_len = K_dec.size(1)\n",
    "\n",
    "\n",
    "    attn_mask = tgt_mask\n",
    "    if attn_mask is not None:\n",
    "\n",
    "        # Ensuring attn_mask's dim is 3\n",
    "        if attn_mask.dim() == 2:\n",
    "            correct_2d_size = (tgt_len, src_len)\n",
    "            if attn_mask.shape != correct_2d_size:\n",
    "                raise RuntimeError(f\"The shape of the 2D attn_mask is {attn_mask.shape}, but should be {correct_2d_size}.\")\n",
    "            attn_mask = attn_mask.unsqueeze(0)\n",
    "        elif attn_mask.dim() == 3:\n",
    "            correct_3d_size = (bsz * num_heads, tgt_len, src_len)\n",
    "            if attn_mask.shape != correct_3d_size:\n",
    "                raise RuntimeError(f\"The shape of the 3D attn_mask is {attn_mask.shape}, but should be {correct_3d_size}.\")\n",
    "        else:\n",
    "            raise RuntimeError(f\"attn_mask's dimension {attn_mask.dim()} is not supported\")\n",
    "\n",
    "    # attn_mask can be either (L,S) or (N*num_heads, L, S)\n",
    "    # if attn_mask's shape is (1, L, S) we need to unsqueeze to (1, 1, L, S)\n",
    "    # in order to match the input for SDPA of (N, num_heads, L, S)\n",
    "    if attn_mask is not None:\n",
    "        if attn_mask.size(0) == 1 and attn_mask.dim() == 3:\n",
    "            attn_mask = attn_mask.unsqueeze(0)\n",
    "        else:\n",
    "            attn_mask = attn_mask.view(bsz, num_heads, -1, src_len)\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    if need_weights is False:\n",
    "        attn_output = atten_product_needs_wts_false(Q_dec, V_dec, K_dec, bsz, head_dim, src_len, tgt_len, attn_mask)\n",
    "\n",
    "        print(\"Decoder Self Attention = \")\n",
    "        print(attn_output)\n",
    "        print()\n",
    "\n",
    "        op_dec_1 = torch.matmul(attn_output, state_dict[\"transformer.decoder.layers.{}.self_attn.out_proj.weight\".format(layer_num)].t()) + state_dict[\"transformer.decoder.layers.{}.self_attn.out_proj.bias\".format(layer_num)]\n",
    "        attn_dec_output = op_dec_1.view(tgt_len, bsz, attn_output.size(1))\n",
    "\n",
    "        return attn_dec_output, None\n",
    "    \n",
    "    else:\n",
    "\n",
    "        attn_dec_output,attn_wt_matrix_dec = atten_product_needs_wts_true(Q_dec, K_dec, V_dec, bsz, tgt_len, attn_mask)\n",
    "\n",
    "        print(\"Decoder Attention output = \")\n",
    "        print(attn_wt_matrix_dec)\n",
    "        print()\n",
    "\n",
    "        op_dec_1 = torch.matmul(attn_dec_output, state_dict[\"transformer.decoder.layers.{}.self_attn.out_proj.weight\".format(layer_num)].t()) + state_dict[\"transformer.decoder.layers.{}.self_attn.out_proj.bias\".format(layer_num)]\n",
    "        attn_dec_output = op_dec_1.view(tgt_len, bsz, attn_dec_output.size(1))\n",
    "    \n",
    "\n",
    "        return attn_dec_output, attn_wt_matrix_dec\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "need_weights "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 4, 1, 6]) torch.Size([1, 4, 1, 6]) torch.Size([1, 4, 1, 6])\n",
      "4 2 3\n",
      "Q_dec_0 = \n",
      "tensor([[[-0.3784, -0.7437, -0.6235],\n",
      "         [-0.7424, -0.8504, -0.4722],\n",
      "         [ 0.1257,  0.3638,  0.5637],\n",
      "         [ 0.1257,  0.3638,  0.5637]],\n",
      "\n",
      "        [[-0.4801,  0.2827, -0.1775],\n",
      "         [ 0.1221,  0.4419,  0.8983],\n",
      "         [-0.1287, -0.7501, -1.4718],\n",
      "         [-0.1287, -0.7501, -1.4718]]])\n",
      "\n",
      "K_dec_0 = \n",
      "tensor([[[ 0.0865, -1.2798,  0.5098],\n",
      "         [-0.1128, -1.0767,  0.9138],\n",
      "         [ 0.9444, -0.9628, -0.7018],\n",
      "         [ 0.9444, -0.9628, -0.7018]],\n",
      "\n",
      "        [[ 0.5999,  0.1313,  0.3402],\n",
      "         [ 0.4384,  0.6527,  0.9106],\n",
      "         [-1.2218,  0.0632, -1.2223],\n",
      "         [-1.2218,  0.0632, -1.2223]]])\n",
      "\n",
      "V_dec_0 = \n",
      "tensor([[[-1.1569, -1.4169, -0.2169],\n",
      "         [-0.1712, -0.1728, -0.3195],\n",
      "         [-1.0927, -1.3739,  0.3832],\n",
      "         [-1.0927, -1.3739,  0.3832]],\n",
      "\n",
      "        [[ 0.5000, -1.5058,  0.7683],\n",
      "         [ 0.0646, -0.6681,  0.5009],\n",
      "         [ 0.4547,  0.2847, -0.4699],\n",
      "         [ 0.4547,  0.2847, -0.4699]]])\n",
      "\n",
      "Attention weights = \n",
      "torch.Size([1, 2, 4, 4]) tensor([[[[0.2459, 0.2036, 0.2753, 0.2753],\n",
      "          [0.2831, 0.2500, 0.2334, 0.2334],\n",
      "          [0.2698, 0.3165, 0.2069, 0.2069],\n",
      "          [0.2698, 0.3165, 0.2069, 0.2069]],\n",
      "\n",
      "         [[0.1689, 0.1814, 0.3248, 0.3248],\n",
      "          [0.3042, 0.4619, 0.1169, 0.1169],\n",
      "          [0.0962, 0.0478, 0.4280, 0.4280],\n",
      "          [0.0962, 0.0478, 0.4280, 0.4280]]]])\n",
      "Decoder Self Attention = \n",
      "tensor([[-9.2092e-01, -1.1400e+00,  9.2552e-02,  3.9158e-01, -1.9056e-01,\n",
      "         -8.4665e-02],\n",
      "        [-8.8051e-01, -1.0858e+00,  3.7598e-02,  2.8831e-01, -7.0011e-01,\n",
      "          3.5517e-01],\n",
      "        [-8.1839e-01, -1.0054e+00, -1.0936e-03,  4.4039e-01,  6.6905e-02,\n",
      "         -3.0436e-01],\n",
      "        [-8.1839e-01, -1.0054e+00, -1.0936e-03,  4.4039e-01,  6.6905e-02,\n",
      "         -3.0436e-01]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# need_weights = True\n",
    "\n",
    "self_attn_dec, dec_sa_wts = decoder_block_self_attn_output(pe_tgt_embeds, state_dict, layer_num = 0, need_weights = need_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.8188, -0.4678,  0.6516, -0.6265, -0.2877, -0.0952]],\n",
       "\n",
       "        [[-0.6577, -0.3695,  0.6269, -0.2723, -0.1989, -0.1276]],\n",
       "\n",
       "        [[-0.7676, -0.4025,  0.4873, -0.7457, -0.2433,  0.0417]],\n",
       "\n",
       "        [[-0.7676, -0.4025,  0.4873, -0.7457, -0.2433,  0.0417]]])"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "self_attn_dec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dec_post_self_attn(self_attn_dec, x, state_dict, layer_num):\n",
    "\n",
    "    # (bsz*tgt_len , embed_dim) @ (embed_dim , embed_dim).T -> (bsz*tgt_len , embed_dim)\n",
    "    output_dec_1 = self_attn_dec + x\n",
    "\n",
    "    # (bsz*tgt_len , embed_dim) @ (embed_dim , embed_dim).T -> (bsz*tgt_len , embed_dim)\n",
    "    linear_result_dec_1 = output_dec_1*state_dict[\"transformer.decoder.layers.{}.norm1.weight\".format(layer_num)] + state_dict[\"transformer.decoder.layers.{}.norm1.bias\".format(layer_num)]\n",
    "\n",
    "    layernorm_dec_1 = torch.nn.LayerNorm(normalized_shape=linear_result_dec_1.shape[2:])\n",
    "    linear_op_dec_1 = layernorm_dec_1(linear_result_dec_1)\n",
    "\n",
    "    # From torch's implementation\n",
    "    # print(linear_op_dec_1)\n",
    "\n",
    "    x = linear_result_dec_1\n",
    "\n",
    "    # Obtained layer norm weights and biases (learnable)\n",
    "    w = layernorm_dec_1.weight\n",
    "    b = layernorm_dec_1.bias\n",
    "\n",
    "    linear_result_dec_1f = w*x + b\n",
    "\n",
    "    epsilon = 1e-5  \n",
    "    mean = linear_result_dec_1f.mean(dim=-1, keepdim=True)\n",
    "    std = linear_result_dec_1f.std(dim=-1, unbiased=False, keepdim=True)\n",
    "    normalized_result_dec_1 = (linear_result_dec_1f - mean) / (std + epsilon) * w + b\n",
    "\n",
    "\n",
    "    print(\"Decoder_{} norm1(x + sa(x))\".format(layer_num))\n",
    "    print(normalized_result_dec_1)\n",
    "    print()\n",
    "\n",
    "    # (tgt_len, bsz, embed_dim)\n",
    "    return normalized_result_dec_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoder_0 norm1(x + sa(x))\n",
      "tensor([[[-1.1113, -0.1304,  1.1276,  0.5552, -1.4596,  1.0186]],\n",
      "\n",
      "        [[-0.1763, -1.6359,  0.5559,  1.5003, -0.7148,  0.4709]],\n",
      "\n",
      "        [[-1.8464,  0.2633,  1.0516, -0.4803, -0.0742,  1.0860]],\n",
      "\n",
      "        [[-1.8464,  0.2633,  1.0516, -0.4803, -0.0742,  1.0860]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x_dec = dec_post_self_attn(self_attn_dec, pe_tgt_embeds, state_dict, layer_num = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross attention in decoder \n",
    "### query, key, value =  x, mem, mem \n",
    "\n",
    "### Cross attention between the encoder's final output and the decoder layer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.1113, -0.1304,  1.1276,  0.5552, -1.4596,  1.0186]],\n",
       "\n",
       "        [[-0.1763, -1.6359,  0.5559,  1.5003, -0.7148,  0.4709]],\n",
       "\n",
       "        [[-1.8464,  0.2633,  1.0516, -0.4803, -0.0742,  1.0860]],\n",
       "\n",
       "        [[-1.8464,  0.2633,  1.0516, -0.4803, -0.0742,  1.0860]]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "memory = output_enc_final\n",
    "x_dec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 1, 6])"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoder_block_cross_attn_output(x_dec, memory, state_dict, layer_num, tgt_len, src_len, head_dim, memory_mask = None,need_weights = False):\n",
    "\n",
    "    # (tgt_len, bsz, embed_dim)\n",
    "    query_dec_mha = x_dec\n",
    "\n",
    "    # (src_len, bsz, embed_dim)\n",
    "    key_dec_mha, value_dec_mha = memory, memory\n",
    "\n",
    "\n",
    "    tgt_len, bsz, embed_dim = query_dec_mha.shape\n",
    "\n",
    "\n",
    "    W_dec_mha = state_dict[\"transformer.decoder.layers.{}.multihead_attn.in_proj_weight\".format(layer_num)]\n",
    "    b_dec_mha = state_dict[\"transformer.decoder.layers.{}.multihead_attn.in_proj_bias\".format(layer_num)]\n",
    "\n",
    "\n",
    "    ########## CHANGE FROM SELF ATTENTION ############\n",
    "\n",
    "    Q_dec_mha,K_dec_mha,V_dec_mha = get_qkv(query_dec_mha, key_dec_mha, value_dec_mha ,W_dec_mha, b_dec_mha)\n",
    "\n",
    "\n",
    "    #################################################\n",
    "\n",
    "    K_dec_mha = K_dec_mha.unsqueeze(0)\n",
    "    V_dec_mha = V_dec_mha.unsqueeze(0)\n",
    "    Q_dec_mha = Q_dec_mha.unsqueeze(0)\n",
    "\n",
    "\n",
    "    Q_dec_mha = Q_dec_mha.reshape(tgt_len, bsz * num_heads, head_dim).transpose(0, 1)\n",
    "    K_dec_mha = K_dec_mha.reshape(tgt_len, bsz * num_heads, head_dim).transpose(0, 1)\n",
    "    V_dec_mha = V_dec_mha.reshape(tgt_len, bsz * num_heads, head_dim).transpose(0, 1)\n",
    "\n",
    "    print(\"Q_dec_{} = \".format(layer_num))\n",
    "    print(Q_dec_mha)\n",
    "    print()\n",
    "\n",
    "    print(\"K_dec_{} = \".format(layer_num))\n",
    "    print(K_dec_mha)\n",
    "    print()\n",
    "\n",
    "    print(\"V_dec_{} = \".format(layer_num))\n",
    "    print(V_dec_mha)\n",
    "    print()\n",
    "\n",
    "\n",
    "    attn_mask = memory_mask\n",
    "    if attn_mask is not None:\n",
    "\n",
    "        # Ensuring attn_mask's dim is 3\n",
    "        if attn_mask.dim() == 2:\n",
    "            correct_2d_size = (tgt_len, src_len)\n",
    "            if attn_mask.shape != correct_2d_size:\n",
    "                raise RuntimeError(f\"The shape of the 2D attn_mask is {attn_mask.shape}, but should be {correct_2d_size}.\")\n",
    "            attn_mask = attn_mask.unsqueeze(0)\n",
    "        elif attn_mask.dim() == 3:\n",
    "            correct_3d_size = (bsz * num_heads, tgt_len, src_len)\n",
    "            if attn_mask.shape != correct_3d_size:\n",
    "                raise RuntimeError(f\"The shape of the 3D attn_mask is {attn_mask.shape}, but should be {correct_3d_size}.\")\n",
    "        else:\n",
    "            raise RuntimeError(f\"attn_mask's dimension {attn_mask.dim()} is not supported\")\n",
    "\n",
    "    # attn_mask can be either (L,S) or (N*num_heads, L, S)\n",
    "    # if attn_mask's shape is (1, L, S) we need to unsqueeze to (1, 1, L, S)\n",
    "    # in order to match the input for SDPA of (N, num_heads, L, S)\n",
    "    if attn_mask is not None:\n",
    "        if attn_mask.size(0) == 1 and attn_mask.dim() == 3:\n",
    "            attn_mask = attn_mask.unsqueeze(0)\n",
    "        else:\n",
    "            attn_mask = attn_mask.view(bsz, num_heads, -1, src_len)\n",
    "\n",
    "\n",
    "\n",
    "    if need_weights is False:\n",
    "\n",
    "        attn_output_dec_mha = atten_product_needs_wts_false(Q_dec_mha, V_dec_mha, K_dec_mha, bsz, head_dim, src_len, tgt_len, attn_mask)\n",
    "        \n",
    "        print(\"Cross attention in decoder_{}\".format(layer_num))\n",
    "        print(attn_output_dec_mha)\n",
    "        print()\n",
    "\n",
    "        op_dec_mha_1 = torch.matmul(attn_output_dec_mha, state_dict[\"transformer.decoder.layers.{}.multihead_attn.out_proj.weight\".format(layer_num)].t()) + state_dict[\"transformer.decoder.layers.{}.multihead_attn.out_proj.bias\".format(layer_num)]\n",
    "\n",
    "        attn_dec_mha_output = op_dec_mha_1.view(tgt_len, bsz, attn_output_dec_mha.size(1))\n",
    "\n",
    "        return attn_dec_mha_output, None\n",
    "    \n",
    "    else: \n",
    "\n",
    "    \n",
    "        attn_dec_mha_output ,attn_wt_matrix_dec_mha = atten_product_needs_wts_true(Q_dec_mha, K_dec_mha, V_dec_mha, bsz, tgt_len, attn_mask)\n",
    "\n",
    "        print(\"Decoder mha Attention output = \")\n",
    "        print(attn_dec_mha_output)\n",
    "        print()\n",
    "\n",
    "\n",
    "        op_dec_mha = torch.matmul(attn_dec_mha_output, state_dict[\"transformer.decoder.layers.{}.multihead_attn.out_proj.weight\".format(layer_num)].t()) + state_dict[\"transformer.decoder.layers.{}.multihead_attn.out_proj.bias\".format(layer_num)]\n",
    "        attn_dec_output_mha = op_dec_mha.view(tgt_len, bsz, attn_dec_mha_output.size(1))\n",
    "\n",
    "\n",
    "        return attn_dec_output_mha , attn_wt_matrix_dec_mha\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q_dec_0 = \n",
      "tensor([[[-1.5225, -1.3120, -1.9461],\n",
      "         [-0.1991, -0.7104, -1.7021],\n",
      "         [-1.0709, -0.5297, -1.1219],\n",
      "         [-1.0709, -0.5297, -1.1219]],\n",
      "\n",
      "        [[-0.8573, -0.2711,  0.2245],\n",
      "         [-0.6236, -1.2013,  0.9954],\n",
      "         [-1.0236, -0.4365, -0.0575],\n",
      "         [-1.0236, -0.4365, -0.0575]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "K_dec_0 = \n",
      "tensor([[[ 0.7639,  0.3596,  0.6521],\n",
      "         [ 1.4840,  0.8749, -0.3867],\n",
      "         [ 0.7257,  0.4635,  0.0324],\n",
      "         [ 1.0081,  0.8540, -0.2985]],\n",
      "\n",
      "        [[-0.3901, -0.7216,  0.2131],\n",
      "         [ 0.4722, -0.8736, -0.6255],\n",
      "         [ 0.5111, -0.3893, -0.3831],\n",
      "         [ 0.3892, -0.5258, -0.4995]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "V_dec_0 = \n",
      "tensor([[[ 0.5164,  0.2813,  0.6188],\n",
      "         [ 0.5019, -0.7920,  0.7854],\n",
      "         [ 0.7359,  0.4198,  0.5549],\n",
      "         [ 0.6834, -0.3629,  0.4580]],\n",
      "\n",
      "        [[ 0.3277,  0.5997,  0.3185],\n",
      "         [ 0.9193, -0.4360,  0.3634],\n",
      "         [ 0.6701,  0.4711,  0.9148],\n",
      "         [ 1.2092,  0.0564,  0.6098]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "Attention weights = \n",
      "torch.Size([1, 2, 4, 4]) tensor([[[[0.1759, 0.2030, 0.3372, 0.2839],\n",
      "          [0.1458, 0.3016, 0.2580, 0.2946],\n",
      "          [0.2032, 0.2179, 0.3010, 0.2780],\n",
      "          [0.2032, 0.2179, 0.3010, 0.2780]],\n",
      "\n",
      "         [[0.3619, 0.2169, 0.2036, 0.2176],\n",
      "          [0.4258, 0.2142, 0.1735, 0.1864],\n",
      "          [0.3562, 0.2286, 0.1962, 0.2190],\n",
      "          [0.3562, 0.2286, 0.1962, 0.2190]]]], grad_fn=<SoftmaxBackward0>)\n",
      "Cross attention in decoder_0\n",
      "tensor([[ 0.6349, -0.0728,  0.5854,  0.7175,  0.2306,  0.5130],\n",
      "        [ 0.6179, -0.1964,  0.6052,  0.6782,  0.2542,  0.4859],\n",
      "        [ 0.6257, -0.0899,  0.5912,  0.7232,  0.2187,  0.5095],\n",
      "        [ 0.6257, -0.0899,  0.5912,  0.7232,  0.2187,  0.5095]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# need_weights = True\n",
    "\n",
    "attn_dec_mha_output, attn_dec_mha_wts = decoder_block_cross_attn_output(x_dec, memory, state_dict, layer_num = 0, tgt_len = tgt_len, src_len = src_len, head_dim = head_dim, need_weights = need_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoder_block_post_attn_output(x_dec, attn_dec_mha_output, state_dict, layer_num):\n",
    "\n",
    "    # (tgt_len, bsz, embed_dim)\n",
    "    output_dec_2 = attn_dec_mha_output + x_dec\n",
    "\n",
    "    linear_result_dec_2 = output_dec_2*state_dict[\"transformer.decoder.layers.{}.norm2.weight\".format(layer_num)] + state_dict[\"transformer.decoder.layers.{}.norm2.bias\".format(layer_num)]\n",
    "\n",
    "    # Layer normalization from Torch's implementation \n",
    "    layernorm_dec_2 = torch.nn.LayerNorm(normalized_shape=linear_result_dec_2.shape[2:])\n",
    "    linear_op_dec_2 = layernorm_dec_2(linear_result_dec_2)\n",
    "\n",
    "    x = linear_result_dec_2\n",
    "    w = layernorm_dec_2.weight\n",
    "    b = layernorm_dec_2.bias\n",
    "\n",
    "    linear_result_dec_2f = w*x + b\n",
    "\n",
    "    epsilon = 1e-5  \n",
    "    mean = linear_result_dec_2f.mean(dim=-1, keepdim=True)\n",
    "    std = linear_result_dec_2f.std(dim=-1, unbiased=False, keepdim=True)\n",
    "    normalized_result_dec_2 = (linear_result_dec_2f - mean) / (std + epsilon) * w + b\n",
    "\n",
    "    print(\"norm2(x' + mha(x', mem)), \\n where x' = Decoder_curr_layer norm1(x + sa(x))\")\n",
    "    print(normalized_result_dec_2)\n",
    "    print(\"\\n\\n\")\n",
    "\n",
    "    x_dec2_norm = normalized_result_dec_2\n",
    "\n",
    "    op_dec_1 = torch.matmul(x_dec2_norm, state_dict[\"transformer.decoder.layers.{}.linear1.weight\".format(layer_num)].t()) + state_dict[\"transformer.decoder.layers.{}.linear1.bias\".format(layer_num)]\n",
    "    op_dec_1_relu = torch.nn.functional.relu(op_dec_1)\n",
    "    op_dec_2 = torch.matmul(op_dec_1_relu, state_dict[\"transformer.decoder.layers.{}.linear2.weight\".format(layer_num)].t()) + state_dict[\"transformer.decoder.layers.{}.linear2.bias\".format(layer_num)]\n",
    "\n",
    "    ff_dec = op_dec_2\n",
    "\n",
    "    x_dec3_unorm = x_dec2_norm + ff_dec\n",
    "\n",
    "    linear_result_dec_3 = x_dec3_unorm*state_dict[\"transformer.decoder.layers.0.norm3.weight\"] + state_dict[\"transformer.decoder.layers.0.norm3.bias\"]\n",
    "\n",
    "    # Layer normalization from Torch's implementation \n",
    "    layernorm_dec_3 = torch.nn.LayerNorm(normalized_shape=linear_result_dec_3.shape[2:])\n",
    "    linear_op_dec_3 = layernorm_dec_3(linear_result_dec_3)\n",
    "\n",
    "    # From torch's implementation\n",
    "    x = linear_result_dec_3\n",
    "\n",
    "    # Obtained layer norm weights and biases (learnable)\n",
    "    w = layernorm_dec_3.weight\n",
    "    b = layernorm_dec_3.bias\n",
    "\n",
    "    linear_result_dec_3f = w*x + b\n",
    "\n",
    "    epsilon = 1e-5  \n",
    "    mean = linear_result_dec_3f.mean(dim=-1, keepdim=True)\n",
    "    std = linear_result_dec_3f.std(dim=-1, unbiased=False, keepdim=True)\n",
    "    normalized_result_dec_3 = (linear_result_dec_3f - mean) / (std + epsilon) * w + b\n",
    "\n",
    "    print(\"norm3(x'' + ff(x'')) \\n where, x'' = Decoder_curr_layer norm2(x' + mha(x'))\")\n",
    "    print(normalized_result_dec_3)\n",
    "    print()\n",
    "\n",
    "    return normalized_result_dec_3\n",
    "\n",
    "def feef_fwd_transformer(dec_output_final, state_dict):\n",
    "\n",
    "    # (tgt_len, bsz, embed_dim) @ (vocab_size, embed_dim).T -> (tgt_len, bsz, vocab_size)\n",
    "    final_op = dec_output_final@state_dict[\"fc.weight\"].T + state_dict[\"fc.bias\"]\n",
    "\n",
    "    return final_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 6])"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_dict[\"fc.weight\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "norm2(x' + mha(x', mem)), \n",
      " where x' = Decoder_curr_layer norm1(x + sa(x))\n",
      "tensor([[[-1.6626, -0.2054,  1.1871,  0.8788, -0.8033,  0.6054]],\n",
      "\n",
      "        [[-0.7633, -1.5386,  0.7136,  1.5919, -0.0597,  0.0562]],\n",
      "\n",
      "        [[-2.1009,  0.1303,  1.0147, -0.0620,  0.4149,  0.6030]],\n",
      "\n",
      "        [[-2.1009,  0.1303,  1.0147, -0.0620,  0.4149,  0.6030]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "\n",
      "norm3(x'' + ff(x'')) \n",
      " where, x'' = Decoder_curr_layer norm2(x' + mha(x'))\n",
      "tensor([[[-1.3175, -0.6332,  1.6384,  0.3114, -0.7350,  0.7358]],\n",
      "\n",
      "        [[-0.2502, -1.9815,  0.8999,  1.0663,  0.2534,  0.0120]],\n",
      "\n",
      "        [[-1.6847, -0.1621,  1.3100, -0.7599,  0.6732,  0.6234]],\n",
      "\n",
      "        [[-1.6847, -0.1621,  1.3100, -0.7599,  0.6732,  0.6234]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final_attn_op_decoder = decoder_block_post_attn_output(x_dec, attn_dec_mha_output, state_dict, layer_num = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 1, 6])"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_attn_op_decoder.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (tgt_len, bsz, vocab_dim)\n",
    "final_op = feef_fwd_transformer(final_attn_op_decoder, state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.3931,  0.7839, -0.0701, -0.8229,  0.0118, -0.7191, -0.1468,\n",
       "          -0.8998,  0.1445,  0.0056]],\n",
       "\n",
       "        [[-0.4864,  1.2767,  0.2504, -1.1761, -0.0962, -0.3620, -0.3499,\n",
       "          -0.1509, -0.5999, -0.1273]],\n",
       "\n",
       "        [[-0.6542,  0.5709,  0.3987, -0.5503, -0.5313, -1.0139, -0.1259,\n",
       "          -0.5488,  1.0464, -0.3315]],\n",
       "\n",
       "        [[-0.6542,  0.5709,  0.3987, -0.5503, -0.5313, -1.0139, -0.1259,\n",
       "          -0.5488,  1.0464, -0.3315]]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_op"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "########################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examaple :- \n",
    "\n",
    "Transformer with 3 encoders and 3 decoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source embeddings :- \n",
      "\n",
      "tensor([[[-0.5329,  0.4479, -1.5247, -0.6124,  0.9608, -0.5078]],\n",
      "\n",
      "        [[ 0.7777,  0.7221, -0.2767, -1.7240, -0.0987,  0.7716]],\n",
      "\n",
      "        [[-0.2460,  1.2069,  0.8297,  0.9564, -0.2230, -2.0615]],\n",
      "\n",
      "        [[-0.6652, -0.4723,  0.9936, -0.3680,  1.2617,  0.3828]]],\n",
      "       grad_fn=<EmbeddingBackward0>)\n",
      "\n",
      "Target embeddings :- \n",
      "\n",
      "tensor([[[-0.3454,  0.9345, -0.6539,  1.9464,  2.3963, -1.9435]],\n",
      "\n",
      "        [[ 0.6227,  1.4822,  3.2615, -0.7944, -0.7245, -1.0534]],\n",
      "\n",
      "        [[ 0.8797, -0.3861,  0.2234, -0.2576, -1.4788, -1.3086]],\n",
      "\n",
      "        [[ 0.8797, -0.3861,  0.2234, -0.2576, -1.4788, -1.3086]]],\n",
      "       grad_fn=<EmbeddingBackward0>)\n",
      "\n",
      "Positional encoded source embeddings:\n",
      "tensor([[[-0.5329,  1.4479, -1.5247,  0.3876,  0.9608,  0.4922]],\n",
      "\n",
      "        [[ 0.7777,  1.7221, -0.2767, -0.7240, -0.0987,  1.7716]],\n",
      "\n",
      "        [[-0.2460,  2.2069,  0.8297,  1.9564, -0.2230, -1.0615]],\n",
      "\n",
      "        [[-0.6652,  0.5277,  0.9936,  0.6320,  1.2617,  1.3828]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "\n",
      "Positional encoded target embeddings:\n",
      "tensor([[[-0.3454,  1.9345, -0.6539,  2.9464,  2.3963, -0.9435]],\n",
      "\n",
      "        [[ 0.6227,  2.4822,  3.2615,  0.2056, -0.7245, -0.0534]],\n",
      "\n",
      "        [[ 0.8797,  0.6139,  0.2234,  0.7424, -1.4788, -0.3086]],\n",
      "\n",
      "        [[ 0.8797,  0.6139,  0.2234,  0.7424, -1.4788, -0.3086]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "\n",
      "query =  tensor([[[-0.5329,  1.4479, -1.5247,  0.3876,  0.9608,  0.4922]],\n",
      "\n",
      "        [[ 0.7777,  1.7221, -0.2767, -0.7240, -0.0987,  1.7716]],\n",
      "\n",
      "        [[-0.2460,  2.2069,  0.8297,  1.9564, -0.2230, -1.0615]],\n",
      "\n",
      "        [[-0.6652,  0.5277,  0.9936,  0.6320,  1.2617,  1.3828]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "query shape =  torch.Size([4, 1, 6])\n",
      "\n",
      " _in_projection_packed \n",
      "\n",
      "Q :-  tensor([[[ 0.9694, -0.3737,  0.1014, -0.4298, -0.5519,  0.5748]],\n",
      "\n",
      "        [[ 0.3989, -0.4059,  0.7465, -0.0833, -1.1578,  0.5435]],\n",
      "\n",
      "        [[ 1.0433,  2.2405, -2.0432,  0.4456,  0.4455,  1.4093]],\n",
      "\n",
      "        [[-0.5131, -0.0364, -0.0935,  0.9812,  0.7730,  0.3288]]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "\n",
      "K :-  tensor([[[-0.2759,  0.9574, -0.0307, -0.7206,  0.6998,  0.6667]],\n",
      "\n",
      "        [[ 0.1012,  0.7665, -0.7728, -0.8337, -0.3324,  0.4448]],\n",
      "\n",
      "        [[-0.6387,  0.5390, -0.2032,  1.3872,  0.1583,  1.8742]],\n",
      "\n",
      "        [[ 0.3532,  0.1304, -0.9684, -0.5231,  0.8520,  0.6656]]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "\n",
      "V :-  tensor([[[ 0.3701,  1.5507, -0.6786,  0.0474,  0.1029, -0.3341]],\n",
      "\n",
      "        [[ 1.3674,  0.9738, -1.2993,  0.1031,  0.0802,  1.2249]],\n",
      "\n",
      "        [[ 0.0642, -0.4617, -0.5055,  0.9836,  1.4497, -0.3509]],\n",
      "\n",
      "        [[-0.0065,  0.2798, -0.2086,  0.4828,  0.0439, -0.1283]]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "\n",
      "tgt_len =  4\n",
      "bsz =  1\n",
      "num_heads =  2\n",
      "head_dim =  3\n",
      "\n",
      "tensor([[[ 0.9694, -0.3737,  0.1014, -0.4298, -0.5519,  0.5748]],\n",
      "\n",
      "        [[ 0.3989, -0.4059,  0.7465, -0.0833, -1.1578,  0.5435]],\n",
      "\n",
      "        [[ 1.0433,  2.2405, -2.0432,  0.4456,  0.4455,  1.4093]],\n",
      "\n",
      "        [[-0.5131, -0.0364, -0.0935,  0.9812,  0.7730,  0.3288]]],\n",
      "       grad_fn=<SelectBackward0>) torch.Size([4, 1, 6])\n",
      "Q reshaped =  tensor([[[ 0.9694, -0.3737,  0.1014],\n",
      "         [ 0.3989, -0.4059,  0.7465],\n",
      "         [ 1.0433,  2.2405, -2.0432],\n",
      "         [-0.5131, -0.0364, -0.0935]],\n",
      "\n",
      "        [[-0.4298, -0.5519,  0.5748],\n",
      "         [-0.0833, -1.1578,  0.5435],\n",
      "         [ 0.4456,  0.4455,  1.4093],\n",
      "         [ 0.9812,  0.7730,  0.3288]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "K reshaped =  tensor([[[-0.2759,  0.9574, -0.0307],\n",
      "         [ 0.1012,  0.7665, -0.7728],\n",
      "         [-0.6387,  0.5390, -0.2032],\n",
      "         [ 0.3532,  0.1304, -0.9684]],\n",
      "\n",
      "        [[-0.7206,  0.6998,  0.6667],\n",
      "         [-0.8337, -0.3324,  0.4448],\n",
      "         [ 1.3872,  0.1583,  1.8742],\n",
      "         [-0.5231,  0.8520,  0.6656]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "V reshaped =  tensor([[[ 0.3701,  1.5507, -0.6786],\n",
      "         [ 1.3674,  0.9738, -1.2993],\n",
      "         [ 0.0642, -0.4617, -0.5055],\n",
      "         [-0.0065,  0.2798, -0.2086]],\n",
      "\n",
      "        [[ 0.0474,  0.1029, -0.3341],\n",
      "         [ 0.1031,  0.0802,  1.2249],\n",
      "         [ 0.9836,  1.4497, -0.3509],\n",
      "         [ 0.4828,  0.0439, -0.1283]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "torch.Size([1, 2, 4, 3]) torch.Size([1, 2, 4, 3]) torch.Size([1, 2, 4, 3])\n",
      "\n",
      "scaled_dot_product_attention\n",
      "attn_output = \n",
      "tensor([[ 0.4447,  0.5909, -0.6480,  0.3865,  0.4138,  0.1882],\n",
      "        [ 0.4201,  0.5893, -0.6545,  0.4481,  0.5371,  0.2079],\n",
      "        [ 0.6643,  0.7354, -0.7979,  0.6810,  0.8932, -0.1661],\n",
      "        [ 0.4335,  0.5524, -0.6701,  0.6458,  0.8179, -0.1555]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "\n",
      "x + self_atten(x) = \n",
      "tensor([[[ 0.1547,  1.9904, -1.8421,  0.0182,  1.0476,  0.8958]],\n",
      "\n",
      "        [[ 1.5815,  2.2817, -0.6580, -1.2100, -0.0550,  2.1026]],\n",
      "\n",
      "        [[ 0.8451,  2.8639,  0.2458,  1.5830,  0.0707, -0.8321]],\n",
      "\n",
      "        [[ 0.3508,  1.0715,  0.4679,  0.2024,  1.4720,  1.4656]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "\n",
      "norm1(x + self_atten(x)) = \n",
      "tensor([[[-0.1880,  1.3612, -1.8731, -0.3031,  0.5655,  0.4374]],\n",
      "\n",
      "        [[ 0.6613,  1.1714, -0.9703, -1.3724, -0.5310,  1.0409]],\n",
      "\n",
      "        [[ 0.0415,  1.7500, -0.4657,  0.6660, -0.6138, -1.3779]],\n",
      "\n",
      "        [[-0.9358,  0.4475, -0.7111, -1.2207,  1.2161,  1.2039]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "\n",
      "feed_fwd_op =  tensor([[[ 0.5506, -0.5433,  0.0739,  0.1866,  0.3096, -0.0889]],\n",
      "\n",
      "        [[ 0.4346, -0.4968,  0.1180,  0.2986,  0.2994, -0.0738]],\n",
      "\n",
      "        [[ 0.3090, -0.3444,  0.1405,  0.1015,  0.3366, -0.0835]],\n",
      "\n",
      "        [[ 0.5502, -0.5324,  0.0046,  0.2096,  0.3937, -0.1355]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "\n",
      "norm2(norm1(x + self_atten(x)) + feed_fwd_op) = \n",
      "tensor([[[ 0.3112,  0.8151, -2.0813, -0.2191,  0.8784,  0.2957]],\n",
      "\n",
      "        [[ 1.1579,  0.6697, -1.0996, -1.3563, -0.3804,  1.0086]],\n",
      "\n",
      "        [[ 0.3009,  1.4601, -0.4415,  0.7590, -0.3888, -1.6898]],\n",
      "\n",
      "        [[-0.4939, -0.1761, -0.8331, -1.1551,  1.6152,  1.0429]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "\n",
      "##################################\n",
      "query =  tensor([[[ 0.3112,  0.8151, -2.0813, -0.2191,  0.8784,  0.2957]],\n",
      "\n",
      "        [[ 1.1579,  0.6697, -1.0996, -1.3563, -0.3804,  1.0086]],\n",
      "\n",
      "        [[ 0.3009,  1.4601, -0.4415,  0.7590, -0.3888, -1.6898]],\n",
      "\n",
      "        [[-0.4939, -0.1761, -0.8331, -1.1551,  1.6152,  1.0429]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "query shape =  torch.Size([4, 1, 6])\n",
      "\n",
      " _in_projection_packed \n",
      "\n",
      "Q :-  tensor([[[ 0.1582, -0.5874,  0.1214,  0.0474,  0.3990, -0.2255]],\n",
      "\n",
      "        [[ 0.2182, -0.5579, -0.3282,  0.2948,  0.2801, -0.1668]],\n",
      "\n",
      "        [[ 0.3140, -0.3042,  0.3891, -0.1400, -1.4475,  1.1345]],\n",
      "\n",
      "        [[-0.3295, -0.0915,  0.6706, -0.3668,  1.5381, -1.4790]]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "\n",
      "K :-  tensor([[[ 0.2946, -0.1200,  1.3455, -0.4681, -0.5777, -0.2227]],\n",
      "\n",
      "        [[ 1.0143,  0.9952,  0.7335, -1.2081,  0.0680, -0.0584]],\n",
      "\n",
      "        [[-0.3556, -1.4456, -0.1802, -0.3622, -1.4537,  0.0772]],\n",
      "\n",
      "        [[ 0.4098,  0.6703,  0.7619,  0.2869,  0.3001, -0.3839]]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "\n",
      "V :-  tensor([[[-1.0751,  0.4705, -0.0284,  0.3559, -0.7036,  0.5059]],\n",
      "\n",
      "        [[ 0.5148,  0.9883,  0.2391,  1.0776, -1.7891, -0.3165]],\n",
      "\n",
      "        [[-1.1569,  0.0931,  0.6943, -0.5253,  0.3234, -0.2551]],\n",
      "\n",
      "        [[-0.3562,  0.3436, -0.6694,  0.3510, -0.2131,  0.1841]]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "\n",
      "tgt_len =  4\n",
      "bsz =  1\n",
      "num_heads =  2\n",
      "head_dim =  3\n",
      "\n",
      "tensor([[[ 0.1582, -0.5874,  0.1214,  0.0474,  0.3990, -0.2255]],\n",
      "\n",
      "        [[ 0.2182, -0.5579, -0.3282,  0.2948,  0.2801, -0.1668]],\n",
      "\n",
      "        [[ 0.3140, -0.3042,  0.3891, -0.1400, -1.4475,  1.1345]],\n",
      "\n",
      "        [[-0.3295, -0.0915,  0.6706, -0.3668,  1.5381, -1.4790]]],\n",
      "       grad_fn=<SelectBackward0>) torch.Size([4, 1, 6])\n",
      "Q reshaped =  tensor([[[ 0.1582, -0.5874,  0.1214],\n",
      "         [ 0.2182, -0.5579, -0.3282],\n",
      "         [ 0.3140, -0.3042,  0.3891],\n",
      "         [-0.3295, -0.0915,  0.6706]],\n",
      "\n",
      "        [[ 0.0474,  0.3990, -0.2255],\n",
      "         [ 0.2948,  0.2801, -0.1668],\n",
      "         [-0.1400, -1.4475,  1.1345],\n",
      "         [-0.3668,  1.5381, -1.4790]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "K reshaped =  tensor([[[ 0.2946, -0.1200,  1.3455],\n",
      "         [ 1.0143,  0.9952,  0.7335],\n",
      "         [-0.3556, -1.4456, -0.1802],\n",
      "         [ 0.4098,  0.6703,  0.7619]],\n",
      "\n",
      "        [[-0.4681, -0.5777, -0.2227],\n",
      "         [-1.2081,  0.0680, -0.0584],\n",
      "         [-0.3622, -1.4537,  0.0772],\n",
      "         [ 0.2869,  0.3001, -0.3839]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "V reshaped =  tensor([[[-1.0751,  0.4705, -0.0284],\n",
      "         [ 0.5148,  0.9883,  0.2391],\n",
      "         [-1.1569,  0.0931,  0.6943],\n",
      "         [-0.3562,  0.3436, -0.6694]],\n",
      "\n",
      "        [[ 0.3559, -0.7036,  0.5059],\n",
      "         [ 1.0776, -1.7891, -0.3165],\n",
      "         [-0.5253,  0.3234, -0.2551],\n",
      "         [ 0.3510, -0.2131,  0.1841]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "torch.Size([1, 2, 4, 3]) torch.Size([1, 2, 4, 3]) torch.Size([1, 2, 4, 3])\n",
      "\n",
      "scaled_dot_product_attention\n",
      "attn_output = \n",
      "tensor([[-0.6669,  0.4089,  0.1496,  0.3804, -0.6492,  0.0449],\n",
      "        [-0.6766,  0.3886,  0.1942,  0.3396, -0.5859,  0.0547],\n",
      "        [-0.5465,  0.4774,  0.0583, -0.0166, -0.2632, -0.0603],\n",
      "        [-0.5817,  0.4663,  0.0301,  0.5423, -0.8086,  0.0452]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "\n",
      "x + self_atten(x) = \n",
      "tensor([[[ 0.1963,  0.4920, -2.1264, -0.6533,  0.4510, -0.0387]],\n",
      "\n",
      "        [[ 1.0294,  0.4094, -1.1902, -1.7622, -0.7732,  0.6821]],\n",
      "\n",
      "        [[ 0.2284,  1.2878, -0.5135,  0.3751, -0.6045, -1.9736]],\n",
      "\n",
      "        [[-0.5251, -0.6645, -0.7112, -1.6382,  1.1017,  0.7344]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "\n",
      "norm1(x + self_atten(x)) = \n",
      "tensor([[[ 0.5235,  0.8486, -2.0301, -0.4106,  0.8035,  0.2651]],\n",
      "\n",
      "        [[ 1.2573,  0.6562, -0.8946, -1.4492, -0.4903,  0.9206]],\n",
      "\n",
      "        [[ 0.4237,  1.4713, -0.3100,  0.5688, -0.4000, -1.7539]],\n",
      "\n",
      "        [[-0.2598, -0.4099, -0.4601, -1.4581,  1.4916,  1.0962]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "\n",
      "feed_fwd_op =  tensor([[[-0.1334,  0.1037,  0.0870, -0.0220,  0.3027,  0.2429]],\n",
      "\n",
      "        [[-0.1761,  0.0735,  0.0066,  0.0746,  0.2471,  0.2863]],\n",
      "\n",
      "        [[-0.1050,  0.0496, -0.0369,  0.0666,  0.2167,  0.1894]],\n",
      "\n",
      "        [[-0.1874,  0.0071,  0.0219, -0.0368,  0.2311,  0.2585]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "\n",
      "norm2(norm1(x + self_atten(x)) + feed_fwd_op) = \n",
      "tensor([[[ 0.2829,  0.8252, -1.9679, -0.5107,  0.9738,  0.3967]],\n",
      "\n",
      "        [[ 1.0085,  0.6525, -0.9856, -1.4783, -0.3327,  1.1356]],\n",
      "\n",
      "        [[ 0.2693,  1.5369, -0.4327,  0.6032, -0.2601, -1.7166]],\n",
      "\n",
      "        [[-0.4416, -0.4020, -0.4336, -1.3739,  1.4892,  1.1618]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "\n",
      "##################################\n",
      "query =  tensor([[[ 0.2829,  0.8252, -1.9679, -0.5107,  0.9738,  0.3967]],\n",
      "\n",
      "        [[ 1.0085,  0.6525, -0.9856, -1.4783, -0.3327,  1.1356]],\n",
      "\n",
      "        [[ 0.2693,  1.5369, -0.4327,  0.6032, -0.2601, -1.7166]],\n",
      "\n",
      "        [[-0.4416, -0.4020, -0.4336, -1.3739,  1.4892,  1.1618]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "query shape =  torch.Size([4, 1, 6])\n",
      "\n",
      " _in_projection_packed \n",
      "\n",
      "Q :-  tensor([[[-0.8163, -0.9035, -0.3460, -0.1043, -0.3659,  0.0583]],\n",
      "\n",
      "        [[ 0.4852, -1.6041, -0.7942, -0.4536, -0.2208,  0.6211]],\n",
      "\n",
      "        [[-0.3084,  0.1832,  1.1653, -1.1148,  0.9869, -1.4382]],\n",
      "\n",
      "        [[-0.0497,  0.1680, -1.3590,  1.1141, -1.5910,  1.4742]]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "\n",
      "K :-  tensor([[[-0.5884, -0.4992,  0.9580, -0.7957, -0.4586,  1.0056]],\n",
      "\n",
      "        [[-0.5826, -0.3851, -0.0341, -0.9059, -0.7629,  0.5773]],\n",
      "\n",
      "        [[ 0.1464,  0.3144, -0.0108,  0.2492,  1.3993, -0.1864]],\n",
      "\n",
      "        [[-0.1748,  0.0303,  0.2794, -0.3146, -1.4700,  1.0550]]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "\n",
      "V :-  tensor([[[ 0.5679, -0.0306, -0.0299,  1.2492,  0.0498, -1.3543]],\n",
      "\n",
      "        [[-0.4406, -0.0313,  0.9458,  0.0986,  0.2459, -1.3428]],\n",
      "\n",
      "        [[ 1.2800,  0.4034, -0.9645,  1.5861, -0.1953,  0.3250]],\n",
      "\n",
      "        [[-0.5464, -0.7746,  0.5304, -0.4118,  0.4807, -1.2010]]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "\n",
      "tgt_len =  4\n",
      "bsz =  1\n",
      "num_heads =  2\n",
      "head_dim =  3\n",
      "\n",
      "tensor([[[-0.8163, -0.9035, -0.3460, -0.1043, -0.3659,  0.0583]],\n",
      "\n",
      "        [[ 0.4852, -1.6041, -0.7942, -0.4536, -0.2208,  0.6211]],\n",
      "\n",
      "        [[-0.3084,  0.1832,  1.1653, -1.1148,  0.9869, -1.4382]],\n",
      "\n",
      "        [[-0.0497,  0.1680, -1.3590,  1.1141, -1.5910,  1.4742]]],\n",
      "       grad_fn=<SelectBackward0>) torch.Size([4, 1, 6])\n",
      "Q reshaped =  tensor([[[-0.8163, -0.9035, -0.3460],\n",
      "         [ 0.4852, -1.6041, -0.7942],\n",
      "         [-0.3084,  0.1832,  1.1653],\n",
      "         [-0.0497,  0.1680, -1.3590]],\n",
      "\n",
      "        [[-0.1043, -0.3659,  0.0583],\n",
      "         [-0.4536, -0.2208,  0.6211],\n",
      "         [-1.1148,  0.9869, -1.4382],\n",
      "         [ 1.1141, -1.5910,  1.4742]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "K reshaped =  tensor([[[-0.5884, -0.4992,  0.9580],\n",
      "         [-0.5826, -0.3851, -0.0341],\n",
      "         [ 0.1464,  0.3144, -0.0108],\n",
      "         [-0.1748,  0.0303,  0.2794]],\n",
      "\n",
      "        [[-0.7957, -0.4586,  1.0056],\n",
      "         [-0.9059, -0.7629,  0.5773],\n",
      "         [ 0.2492,  1.3993, -0.1864],\n",
      "         [-0.3146, -1.4700,  1.0550]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "V reshaped =  tensor([[[ 0.5679, -0.0306, -0.0299],\n",
      "         [-0.4406, -0.0313,  0.9458],\n",
      "         [ 1.2800,  0.4034, -0.9645],\n",
      "         [-0.5464, -0.7746,  0.5304]],\n",
      "\n",
      "        [[ 1.2492,  0.0498, -1.3543],\n",
      "         [ 0.0986,  0.2459, -1.3428],\n",
      "         [ 1.5861, -0.1953,  0.3250],\n",
      "         [-0.4118,  0.4807, -1.2010]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "torch.Size([1, 2, 4, 3]) torch.Size([1, 2, 4, 3]) torch.Size([1, 2, 4, 3])\n",
      "\n",
      "scaled_dot_product_attention\n",
      "attn_output = \n",
      "tensor([[ 0.1143, -0.1150,  0.2605,  0.4710,  0.1989, -1.0392],\n",
      "        [ 0.1369, -0.1029,  0.2210,  0.4625,  0.2071, -1.1067],\n",
      "        [ 0.2414, -0.1242,  0.1176,  1.1279, -0.0326, -0.3422],\n",
      "        [ 0.2113, -0.0771,  0.1115,  0.0100,  0.3519, -1.2150]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "\n",
      "x + self_atten(x) = \n",
      "tensor([[[ 7.0104e-01,  5.8325e-02, -2.3350e+00, -6.6827e-01,  9.0167e-01,\n",
      "           1.1560e+00]],\n",
      "\n",
      "        [[ 1.4597e+00, -1.8067e-01, -1.4110e+00, -1.6085e+00, -4.2357e-01,\n",
      "           1.9584e+00]],\n",
      "\n",
      "        [[ 4.8736e-01,  1.0405e+00, -3.7533e-04,  8.3305e-01, -4.8406e-02,\n",
      "          -1.0347e+00]],\n",
      "\n",
      "        [[ 1.6591e-02, -1.3315e+00, -1.3295e+00, -1.4961e+00,  1.4064e+00,\n",
      "           1.9336e+00]]], grad_fn=<AddBackward0>)\n",
      "\n",
      "norm1(x + self_atten(x)) = \n",
      "tensor([[[ 0.6137,  0.0749, -1.9315, -0.5342,  0.7819,  0.9951]],\n",
      "\n",
      "        [[ 1.1162, -0.1094, -1.0285, -1.1761, -0.2909,  1.4888]],\n",
      "\n",
      "        [[ 0.4007,  1.2082, -0.3114,  0.9054, -0.3815, -1.8214]],\n",
      "\n",
      "        [[ 0.1089, -0.8696, -0.8682, -0.9891,  1.1176,  1.5003]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "\n",
      "feed_fwd_op =  tensor([[[-0.1537, -0.0024, -0.0627, -0.0883,  0.1560, -0.5378]],\n",
      "\n",
      "        [[-0.1934,  0.0068,  0.0242, -0.0520,  0.2305, -0.4177]],\n",
      "\n",
      "        [[-0.0546,  0.1271, -0.0139, -0.0109, -0.2949, -0.5222]],\n",
      "\n",
      "        [[-0.2308,  0.0339,  0.0801, -0.1285,  0.3080, -0.4909]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "\n",
      "norm2(norm1(x + self_atten(x)) + feed_fwd_op) = \n",
      "tensor([[[ 0.5955,  0.1941, -1.9469, -0.5260,  1.0905,  0.5928]],\n",
      "\n",
      "        [[ 1.1424, -0.0411, -1.0821, -1.3404,  0.0076,  1.3136]],\n",
      "\n",
      "        [[ 0.3948,  1.2182, -0.1640,  0.8513, -0.4562, -1.8440]],\n",
      "\n",
      "        [[-0.0523, -0.7908, -0.7416, -1.0825,  1.5490,  1.1183]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "\n",
      "##################################\n",
      "\n",
      "################### Transformer Decoder start #####################\n",
      "\n",
      "query =  tensor([[[-0.3454,  1.9345, -0.6539,  2.9464,  2.3963, -0.9435]],\n",
      "\n",
      "        [[ 0.6227,  2.4822,  3.2615,  0.2056, -0.7245, -0.0534]],\n",
      "\n",
      "        [[ 0.8797,  0.6139,  0.2234,  0.7424, -1.4788, -0.3086]],\n",
      "\n",
      "        [[ 0.8797,  0.6139,  0.2234,  0.7424, -1.4788, -0.3086]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "query shape =  torch.Size([4, 1, 6])\n",
      "\n",
      " _in_projection_packed \n",
      "\n",
      "Q :-  tensor([[[ 1.5263, -0.0920,  0.8640,  1.7978, -0.5208, -3.1598]],\n",
      "\n",
      "        [[-0.1765, -0.4730,  1.3362, -0.7475, -1.3612, -0.3957]],\n",
      "\n",
      "        [[ 0.0228, -1.0756,  0.7398, -0.2298,  0.2056, -0.0381]],\n",
      "\n",
      "        [[ 0.0228, -1.0756,  0.7398, -0.2298,  0.2056, -0.0381]]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "\n",
      "K :-  tensor([[[-0.3191, -0.5648, -0.5542, -0.9690,  1.2324,  1.5072]],\n",
      "\n",
      "        [[ 1.7961,  0.1075, -0.2672, -2.3272,  0.2400,  1.9377]],\n",
      "\n",
      "        [[ 0.2835, -0.1488, -0.0277, -0.8103, -1.0167,  0.6870]],\n",
      "\n",
      "        [[ 0.2835, -0.1488, -0.0277, -0.8103, -1.0167,  0.6870]]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "\n",
      "V :-  tensor([[[ 0.2705, -0.9981, -2.4461,  2.0536, -0.5573,  0.1047]],\n",
      "\n",
      "        [[-0.6342, -2.0790,  0.6601, -0.8751, -1.8104,  0.7144]],\n",
      "\n",
      "        [[ 0.2722, -0.5439,  0.2677, -0.1405, -0.7309,  0.1020]],\n",
      "\n",
      "        [[ 0.2722, -0.5439,  0.2677, -0.1405, -0.7309,  0.1020]]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "\n",
      "tgt_len =  4\n",
      "bsz =  1\n",
      "num_heads =  2\n",
      "head_dim =  3\n",
      "\n",
      "tensor([[[ 1.5263, -0.0920,  0.8640,  1.7978, -0.5208, -3.1598]],\n",
      "\n",
      "        [[-0.1765, -0.4730,  1.3362, -0.7475, -1.3612, -0.3957]],\n",
      "\n",
      "        [[ 0.0228, -1.0756,  0.7398, -0.2298,  0.2056, -0.0381]],\n",
      "\n",
      "        [[ 0.0228, -1.0756,  0.7398, -0.2298,  0.2056, -0.0381]]],\n",
      "       grad_fn=<SelectBackward0>) torch.Size([4, 1, 6])\n",
      "Q reshaped =  tensor([[[ 1.5263, -0.0920,  0.8640],\n",
      "         [-0.1765, -0.4730,  1.3362],\n",
      "         [ 0.0228, -1.0756,  0.7398],\n",
      "         [ 0.0228, -1.0756,  0.7398]],\n",
      "\n",
      "        [[ 1.7978, -0.5208, -3.1598],\n",
      "         [-0.7475, -1.3612, -0.3957],\n",
      "         [-0.2298,  0.2056, -0.0381],\n",
      "         [-0.2298,  0.2056, -0.0381]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "K reshaped =  tensor([[[-0.3191, -0.5648, -0.5542],\n",
      "         [ 1.7961,  0.1075, -0.2672],\n",
      "         [ 0.2835, -0.1488, -0.0277],\n",
      "         [ 0.2835, -0.1488, -0.0277]],\n",
      "\n",
      "        [[-0.9690,  1.2324,  1.5072],\n",
      "         [-2.3272,  0.2400,  1.9377],\n",
      "         [-0.8103, -1.0167,  0.6870],\n",
      "         [-0.8103, -1.0167,  0.6870]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "V reshaped =  tensor([[[ 0.2705, -0.9981, -2.4461],\n",
      "         [-0.6342, -2.0790,  0.6601],\n",
      "         [ 0.2722, -0.5439,  0.2677],\n",
      "         [ 0.2722, -0.5439,  0.2677]],\n",
      "\n",
      "        [[ 2.0536, -0.5573,  0.1047],\n",
      "         [-0.8751, -1.8104,  0.7144],\n",
      "         [-0.1405, -0.7309,  0.1020],\n",
      "         [-0.1405, -0.7309,  0.1020]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "torch.Size([1, 2, 4, 3]) torch.Size([1, 2, 4, 3]) torch.Size([1, 2, 4, 3])\n",
      "\n",
      "scaled_dot_product_attention\n",
      "attn_output = \n",
      "tensor([[-0.2483, -1.4615,  0.2760, -0.0451, -0.7304,  0.1063],\n",
      "        [ 0.0977, -0.9430, -0.2797, -0.1639, -0.9373,  0.2248],\n",
      "        [ 0.0850, -0.9825, -0.3819,  0.2559, -1.0003,  0.2830],\n",
      "        [ 0.0850, -0.9825, -0.3819,  0.2559, -1.0003,  0.2830]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "\n",
      "x =  tensor([[[-0.3454,  1.9345, -0.6539,  2.9464,  2.3963, -0.9435]],\n",
      "\n",
      "        [[ 0.6227,  2.4822,  3.2615,  0.2056, -0.7245, -0.0534]],\n",
      "\n",
      "        [[ 0.8797,  0.6139,  0.2234,  0.7424, -1.4788, -0.3086]],\n",
      "\n",
      "        [[ 0.8797,  0.6139,  0.2234,  0.7424, -1.4788, -0.3086]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "\n",
      "sa_op =  tensor([[[ 0.7618, -0.0128, -0.2079,  0.2480,  0.6411, -0.1380]],\n",
      "\n",
      "        [[ 0.5343,  0.2968,  0.2491, -0.1165,  0.1058, -0.0367]],\n",
      "\n",
      "        [[ 0.7362,  0.1612,  0.3551, -0.0878, -0.2080,  0.1495]],\n",
      "\n",
      "        [[ 0.7362,  0.1612,  0.3551, -0.0878, -0.2080,  0.1495]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "\n",
      "temop =  tensor([[[ 0.4164,  1.9217, -0.8618,  3.1944,  3.0374, -1.0815]],\n",
      "\n",
      "        [[ 1.1570,  2.7790,  3.5106,  0.0891, -0.6187, -0.0900]],\n",
      "\n",
      "        [[ 1.6159,  0.7751,  0.5784,  0.6546, -1.6867, -0.1591]],\n",
      "\n",
      "        [[ 1.6159,  0.7751,  0.5784,  0.6546, -1.6867, -0.1591]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "\n",
      "Decoder norm1(x + sa(x)) = \n",
      "tensor([[[-0.3985,  0.4733, -1.1387,  1.2103,  1.1194, -1.2659]],\n",
      "\n",
      "        [[ 0.0125,  1.0738,  1.5524, -0.6861, -1.1492, -0.8033]],\n",
      "\n",
      "        [[ 1.2858,  0.4666,  0.2748,  0.3491, -1.9325, -0.4438]],\n",
      "\n",
      "        [[ 1.2858,  0.4666,  0.2748,  0.3491, -1.9325, -0.4438]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "\n",
      "Memory = \n",
      "tensor([[[ 0.5955,  0.1941, -1.9469, -0.5260,  1.0905,  0.5928]],\n",
      "\n",
      "        [[ 1.1424, -0.0411, -1.0821, -1.3404,  0.0076,  1.3136]],\n",
      "\n",
      "        [[ 0.3948,  1.2182, -0.1640,  0.8513, -0.4562, -1.8440]],\n",
      "\n",
      "        [[-0.0523, -0.7908, -0.7416, -1.0825,  1.5490,  1.1183]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "\n",
      "query =  tensor([[[-0.3985,  0.4733, -1.1387,  1.2103,  1.1194, -1.2659]],\n",
      "\n",
      "        [[ 0.0125,  1.0738,  1.5524, -0.6861, -1.1492, -0.8033]],\n",
      "\n",
      "        [[ 1.2858,  0.4666,  0.2748,  0.3491, -1.9325, -0.4438]],\n",
      "\n",
      "        [[ 1.2858,  0.4666,  0.2748,  0.3491, -1.9325, -0.4438]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "query shape =  torch.Size([4, 1, 6])\n",
      "\n",
      " _in_projection_packed \n",
      "\n",
      "Q :-  tensor([[[-0.3132, -0.5748,  1.0150,  1.3564,  0.1577,  0.2171]],\n",
      "\n",
      "        [[ 0.1260,  0.9751,  0.2347,  0.2129,  0.8575, -1.2803]],\n",
      "\n",
      "        [[-0.3754, -0.1783,  0.1096,  1.0184,  1.0515, -1.3438]],\n",
      "\n",
      "        [[-0.3754, -0.1783,  0.1096,  1.0184,  1.0515, -1.3438]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "\n",
      "K :-  tensor([[[ 0.8679, -0.3120,  0.0513,  1.2845, -0.3969, -0.8609]],\n",
      "\n",
      "        [[ 0.3726, -0.1253, -0.3014,  0.7557, -0.4005, -0.0751]],\n",
      "\n",
      "        [[ 0.0990,  0.8646,  1.4213,  0.5656,  0.8158,  0.0260]],\n",
      "\n",
      "        [[ 0.5624, -0.5779, -0.9155,  0.3598, -0.6380, -0.7300]]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "\n",
      "V :-  tensor([[[ 1.2654, -0.4361,  1.3083,  1.0698, -0.0392,  0.6787]],\n",
      "\n",
      "        [[ 0.7738, -1.2616,  0.9193,  0.4712,  0.6643,  0.0268]],\n",
      "\n",
      "        [[ 0.2346,  0.3587, -0.1074,  0.2587,  0.4253,  0.4300]],\n",
      "\n",
      "        [[ 0.4998, -0.1963,  0.7233,  0.5793, -0.6686,  0.1322]]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "\n",
      "tgt_len =  4\n",
      "bsz =  1\n",
      "num_heads =  2\n",
      "head_dim =  3\n",
      "\n",
      "tensor([[[-0.3132, -0.5748,  1.0150,  1.3564,  0.1577,  0.2171]],\n",
      "\n",
      "        [[ 0.1260,  0.9751,  0.2347,  0.2129,  0.8575, -1.2803]],\n",
      "\n",
      "        [[-0.3754, -0.1783,  0.1096,  1.0184,  1.0515, -1.3438]],\n",
      "\n",
      "        [[-0.3754, -0.1783,  0.1096,  1.0184,  1.0515, -1.3438]]],\n",
      "       grad_fn=<ViewBackward0>) torch.Size([4, 1, 6])\n",
      "Q reshaped =  tensor([[[-0.3132, -0.5748,  1.0150],\n",
      "         [ 0.1260,  0.9751,  0.2347],\n",
      "         [-0.3754, -0.1783,  0.1096],\n",
      "         [-0.3754, -0.1783,  0.1096]],\n",
      "\n",
      "        [[ 1.3564,  0.1577,  0.2171],\n",
      "         [ 0.2129,  0.8575, -1.2803],\n",
      "         [ 1.0184,  1.0515, -1.3438],\n",
      "         [ 1.0184,  1.0515, -1.3438]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "K reshaped =  tensor([[[ 0.8679, -0.3120,  0.0513],\n",
      "         [ 0.3726, -0.1253, -0.3014],\n",
      "         [ 0.0990,  0.8646,  1.4213],\n",
      "         [ 0.5624, -0.5779, -0.9155]],\n",
      "\n",
      "        [[ 1.2845, -0.3969, -0.8609],\n",
      "         [ 0.7557, -0.4005, -0.0751],\n",
      "         [ 0.5656,  0.8158,  0.0260],\n",
      "         [ 0.3598, -0.6380, -0.7300]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "V reshaped =  tensor([[[ 1.2654, -0.4361,  1.3083],\n",
      "         [ 0.7738, -1.2616,  0.9193],\n",
      "         [ 0.2346,  0.3587, -0.1074],\n",
      "         [ 0.4998, -0.1963,  0.7233]],\n",
      "\n",
      "        [[ 1.0698, -0.0392,  0.6787],\n",
      "         [ 0.4712,  0.6643,  0.0268],\n",
      "         [ 0.2587,  0.4253,  0.4300],\n",
      "         [ 0.5793, -0.6686,  0.1322]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "torch.Size([1, 2, 4, 3]) torch.Size([1, 2, 4, 3]) torch.Size([1, 2, 4, 3])\n",
      "\n",
      "scaled_dot_product_attention\n",
      "attn_output = \n",
      "tensor([[ 0.6262, -0.2358,  0.5593,  0.6422,  0.1455,  0.3654],\n",
      "        [ 0.5924, -0.2169,  0.5122,  0.6296,  0.0631,  0.3732],\n",
      "        [ 0.6779, -0.3717,  0.6870,  0.6688,  0.0844,  0.4117],\n",
      "        [ 0.6779, -0.3717,  0.6870,  0.6688,  0.0844,  0.4117]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "\n",
      "Decoder norm2(x + mha(x, mem)) = \n",
      "tensor([[[-1.2077,  0.3426, -0.6898,  0.9182,  1.5230, -0.8864]],\n",
      "\n",
      "        [[-0.8041,  0.9494,  1.7817, -0.8000, -0.5982, -0.5288]],\n",
      "\n",
      "        [[ 0.3956,  0.4804,  1.1372,  0.1761, -2.0673, -0.1219]],\n",
      "\n",
      "        [[ 0.3956,  0.4804,  1.1372,  0.1761, -2.0673, -0.1219]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "\n",
      "Decoder norm3(x + ff(x)) = \n",
      "tensor([[[-1.4797,  0.2166, -0.1863,  0.9378,  1.4217, -0.9100]],\n",
      "\n",
      "        [[-0.9290,  0.8128,  1.8555, -0.6980, -0.5662, -0.4750]],\n",
      "\n",
      "        [[ 0.1088,  0.3915,  1.3837,  0.2170, -1.9632, -0.1379]],\n",
      "\n",
      "        [[ 0.1088,  0.3915,  1.3837,  0.2170, -1.9632, -0.1379]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "\n",
      "\n",
      "################### Transformer Decoder start #####################\n",
      "\n",
      "query =  tensor([[[-1.4797,  0.2166, -0.1863,  0.9378,  1.4217, -0.9100]],\n",
      "\n",
      "        [[-0.9290,  0.8128,  1.8555, -0.6980, -0.5662, -0.4750]],\n",
      "\n",
      "        [[ 0.1088,  0.3915,  1.3837,  0.2170, -1.9632, -0.1379]],\n",
      "\n",
      "        [[ 0.1088,  0.3915,  1.3837,  0.2170, -1.9632, -0.1379]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "query shape =  torch.Size([4, 1, 6])\n",
      "\n",
      " _in_projection_packed \n",
      "\n",
      "Q :-  tensor([[[-1.3524,  0.7480, -0.9949, -0.3929,  0.2824,  0.1878]],\n",
      "\n",
      "        [[-0.5110, -1.0154,  0.9501,  0.5617, -1.2253,  1.2739]],\n",
      "\n",
      "        [[ 0.0535, -1.2236,  0.8311,  1.3682, -0.3467,  0.8462]],\n",
      "\n",
      "        [[ 0.0535, -1.2236,  0.8311,  1.3682, -0.3467,  0.8462]]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "\n",
      "K :-  tensor([[[-0.0523,  0.1468,  1.1768,  0.3591, -1.2796, -0.8246]],\n",
      "\n",
      "        [[ 0.0839, -0.1600,  0.4761, -0.0641,  0.2123, -0.4256]],\n",
      "\n",
      "        [[-0.3198,  0.5040, -0.8077, -0.9525,  0.4314, -0.4933]],\n",
      "\n",
      "        [[-0.3198,  0.5040, -0.8077, -0.9525,  0.4314, -0.4933]]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "\n",
      "V :-  tensor([[[ 0.4509,  0.4816, -0.1915, -0.7499, -0.0103,  0.9292]],\n",
      "\n",
      "        [[-1.2860, -0.2816,  0.8666,  0.5826,  0.0565, -0.7175]],\n",
      "\n",
      "        [[-1.2906, -1.2389,  1.0060,  1.1209,  0.0460, -1.4021]],\n",
      "\n",
      "        [[-1.2906, -1.2389,  1.0060,  1.1209,  0.0460, -1.4021]]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "\n",
      "tgt_len =  4\n",
      "bsz =  1\n",
      "num_heads =  2\n",
      "head_dim =  3\n",
      "\n",
      "tensor([[[-1.3524,  0.7480, -0.9949, -0.3929,  0.2824,  0.1878]],\n",
      "\n",
      "        [[-0.5110, -1.0154,  0.9501,  0.5617, -1.2253,  1.2739]],\n",
      "\n",
      "        [[ 0.0535, -1.2236,  0.8311,  1.3682, -0.3467,  0.8462]],\n",
      "\n",
      "        [[ 0.0535, -1.2236,  0.8311,  1.3682, -0.3467,  0.8462]]],\n",
      "       grad_fn=<SelectBackward0>) torch.Size([4, 1, 6])\n",
      "Q reshaped =  tensor([[[-1.3524,  0.7480, -0.9949],\n",
      "         [-0.5110, -1.0154,  0.9501],\n",
      "         [ 0.0535, -1.2236,  0.8311],\n",
      "         [ 0.0535, -1.2236,  0.8311]],\n",
      "\n",
      "        [[-0.3929,  0.2824,  0.1878],\n",
      "         [ 0.5617, -1.2253,  1.2739],\n",
      "         [ 1.3682, -0.3467,  0.8462],\n",
      "         [ 1.3682, -0.3467,  0.8462]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "K reshaped =  tensor([[[-0.0523,  0.1468,  1.1768],\n",
      "         [ 0.0839, -0.1600,  0.4761],\n",
      "         [-0.3198,  0.5040, -0.8077],\n",
      "         [-0.3198,  0.5040, -0.8077]],\n",
      "\n",
      "        [[ 0.3591, -1.2796, -0.8246],\n",
      "         [-0.0641,  0.2123, -0.4256],\n",
      "         [-0.9525,  0.4314, -0.4933],\n",
      "         [-0.9525,  0.4314, -0.4933]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "V reshaped =  tensor([[[ 0.4509,  0.4816, -0.1915],\n",
      "         [-1.2860, -0.2816,  0.8666],\n",
      "         [-1.2906, -1.2389,  1.0060],\n",
      "         [-1.2906, -1.2389,  1.0060]],\n",
      "\n",
      "        [[-0.7499, -0.0103,  0.9292],\n",
      "         [ 0.5826,  0.0565, -0.7175],\n",
      "         [ 1.1209,  0.0460, -1.4021],\n",
      "         [ 1.1209,  0.0460, -1.4021]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "torch.Size([1, 2, 4, 3]) torch.Size([1, 2, 4, 3]) torch.Size([1, 2, 4, 3])\n",
      "\n",
      "scaled_dot_product_attention\n",
      "attn_output = \n",
      "tensor([[-1.1342, -0.9839,  0.8841,  0.6888,  0.0394, -0.8604],\n",
      "        [-0.5555, -0.1985,  0.4556,  0.0233,  0.0187, -0.0314],\n",
      "        [-0.5883, -0.2036,  0.4743,  0.1297,  0.0239, -0.1629],\n",
      "        [-0.5883, -0.2036,  0.4743,  0.1297,  0.0239, -0.1629]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "\n",
      "x =  tensor([[[-1.4797,  0.2166, -0.1863,  0.9378,  1.4217, -0.9100]],\n",
      "\n",
      "        [[-0.9290,  0.8128,  1.8555, -0.6980, -0.5662, -0.4750]],\n",
      "\n",
      "        [[ 0.1088,  0.3915,  1.3837,  0.2170, -1.9632, -0.1379]],\n",
      "\n",
      "        [[ 0.1088,  0.3915,  1.3837,  0.2170, -1.9632, -0.1379]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "\n",
      "sa_op =  tensor([[[-0.2911,  0.3842,  0.5646,  0.3084, -1.1274,  0.9865]],\n",
      "\n",
      "        [[-0.5662,  0.4271, -0.0624,  0.0225, -0.3012,  0.3578]],\n",
      "\n",
      "        [[-0.4997,  0.4068,  0.0211,  0.0097, -0.3623,  0.4077]],\n",
      "\n",
      "        [[-0.4997,  0.4068,  0.0211,  0.0097, -0.3623,  0.4077]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "\n",
      "temop =  tensor([[[-1.7708,  0.6008,  0.3783,  1.2462,  0.2943,  0.0765]],\n",
      "\n",
      "        [[-1.4952,  1.2398,  1.7931, -0.6755, -0.8674, -0.1173]],\n",
      "\n",
      "        [[-0.3908,  0.7983,  1.4048,  0.2267, -2.3255,  0.2699]],\n",
      "\n",
      "        [[-0.3908,  0.7983,  1.4048,  0.2267, -2.3255,  0.2699]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "\n",
      "Decoder norm1(x + sa(x)) = \n",
      "tensor([[[-2.0555,  0.4990,  0.2593,  1.1941,  0.1688, -0.0658]],\n",
      "\n",
      "        [[-1.2609,  1.0775,  1.5505, -0.5601, -0.7242, -0.0828]],\n",
      "\n",
      "        [[-0.3300,  0.6813,  1.1971,  0.1952, -1.9754,  0.2319]],\n",
      "\n",
      "        [[-0.3300,  0.6813,  1.1971,  0.1952, -1.9754,  0.2319]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "\n",
      "Memory = \n",
      "tensor([[[ 0.5955,  0.1941, -1.9469, -0.5260,  1.0905,  0.5928]],\n",
      "\n",
      "        [[ 1.1424, -0.0411, -1.0821, -1.3404,  0.0076,  1.3136]],\n",
      "\n",
      "        [[ 0.3948,  1.2182, -0.1640,  0.8513, -0.4562, -1.8440]],\n",
      "\n",
      "        [[-0.0523, -0.7908, -0.7416, -1.0825,  1.5490,  1.1183]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "\n",
      "query =  tensor([[[-2.0555,  0.4990,  0.2593,  1.1941,  0.1688, -0.0658]],\n",
      "\n",
      "        [[-1.2609,  1.0775,  1.5505, -0.5601, -0.7242, -0.0828]],\n",
      "\n",
      "        [[-0.3300,  0.6813,  1.1971,  0.1952, -1.9754,  0.2319]],\n",
      "\n",
      "        [[-0.3300,  0.6813,  1.1971,  0.1952, -1.9754,  0.2319]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "query shape =  torch.Size([4, 1, 6])\n",
      "\n",
      " _in_projection_packed \n",
      "\n",
      "Q :-  tensor([[[-0.1854,  0.5648,  0.5430,  0.9398,  0.3039, -1.3338]],\n",
      "\n",
      "        [[-0.0059,  0.7083, -0.8105,  1.0112, -0.2463, -0.5395]],\n",
      "\n",
      "        [[ 0.1540,  0.0608, -1.2232,  0.4331, -0.3300, -0.7668]],\n",
      "\n",
      "        [[ 0.1540,  0.0608, -1.2232,  0.4331, -0.3300, -0.7668]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "\n",
      "K :-  tensor([[[ 1.2322,  0.4004, -0.8374,  0.6489, -0.6018,  0.9821]],\n",
      "\n",
      "        [[ 0.8701,  0.1032, -0.3402,  0.3076, -0.2460,  1.2735]],\n",
      "\n",
      "        [[ 0.1902,  1.4932, -0.3451,  0.3678,  1.1194, -0.8406]],\n",
      "\n",
      "        [[ 0.2764, -0.8330, -0.5372,  0.1253, -1.1538,  1.1789]]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "\n",
      "V :-  tensor([[[ 6.1422e-02,  4.7279e-01,  9.1312e-01,  1.2651e+00,  3.2665e-01,\n",
      "           5.8276e-01]],\n",
      "\n",
      "        [[ 7.3326e-01,  3.3956e-01,  5.7623e-01,  8.9173e-01, -1.1800e-01,\n",
      "           8.8540e-01]],\n",
      "\n",
      "        [[-2.5899e-01, -1.3498e-01, -1.0249e+00, -7.7608e-01,  2.7372e-01,\n",
      "          -2.7050e-01]],\n",
      "\n",
      "        [[ 1.5410e-01,  2.8329e-01,  1.4504e+00,  1.4252e+00,  1.2698e-03,\n",
      "           3.4959e-01]]], grad_fn=<SelectBackward0>)\n",
      "\n",
      "tgt_len =  4\n",
      "bsz =  1\n",
      "num_heads =  2\n",
      "head_dim =  3\n",
      "\n",
      "tensor([[[-0.1854,  0.5648,  0.5430,  0.9398,  0.3039, -1.3338]],\n",
      "\n",
      "        [[-0.0059,  0.7083, -0.8105,  1.0112, -0.2463, -0.5395]],\n",
      "\n",
      "        [[ 0.1540,  0.0608, -1.2232,  0.4331, -0.3300, -0.7668]],\n",
      "\n",
      "        [[ 0.1540,  0.0608, -1.2232,  0.4331, -0.3300, -0.7668]]],\n",
      "       grad_fn=<ViewBackward0>) torch.Size([4, 1, 6])\n",
      "Q reshaped =  tensor([[[-0.1854,  0.5648,  0.5430],\n",
      "         [-0.0059,  0.7083, -0.8105],\n",
      "         [ 0.1540,  0.0608, -1.2232],\n",
      "         [ 0.1540,  0.0608, -1.2232]],\n",
      "\n",
      "        [[ 0.9398,  0.3039, -1.3338],\n",
      "         [ 1.0112, -0.2463, -0.5395],\n",
      "         [ 0.4331, -0.3300, -0.7668],\n",
      "         [ 0.4331, -0.3300, -0.7668]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "K reshaped =  tensor([[[ 1.2322,  0.4004, -0.8374],\n",
      "         [ 0.8701,  0.1032, -0.3402],\n",
      "         [ 0.1902,  1.4932, -0.3451],\n",
      "         [ 0.2764, -0.8330, -0.5372]],\n",
      "\n",
      "        [[ 0.6489, -0.6018,  0.9821],\n",
      "         [ 0.3076, -0.2460,  1.2735],\n",
      "         [ 0.3678,  1.1194, -0.8406],\n",
      "         [ 0.1253, -1.1538,  1.1789]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "V reshaped =  tensor([[[ 6.1422e-02,  4.7279e-01,  9.1312e-01],\n",
      "         [ 7.3326e-01,  3.3956e-01,  5.7623e-01],\n",
      "         [-2.5899e-01, -1.3498e-01, -1.0249e+00],\n",
      "         [ 1.5410e-01,  2.8329e-01,  1.4504e+00]],\n",
      "\n",
      "        [[ 1.2651e+00,  3.2665e-01,  5.8276e-01],\n",
      "         [ 8.9173e-01, -1.1800e-01,  8.8540e-01],\n",
      "         [-7.7608e-01,  2.7372e-01, -2.7050e-01],\n",
      "         [ 1.4252e+00,  1.2698e-03,  3.4959e-01]]],\n",
      "       grad_fn=<TransposeBackward0>)\n",
      "\n",
      "torch.Size([1, 2, 4, 3]) torch.Size([1, 2, 4, 3]) torch.Size([1, 2, 4, 3])\n",
      "\n",
      "scaled_dot_product_attention\n",
      "attn_output = \n",
      "tensor([[ 0.1073,  0.1729,  0.1715, -0.1333,  0.2190,  0.0193],\n",
      "        [ 0.0964,  0.1992,  0.2315,  0.5665,  0.1554,  0.3186],\n",
      "        [ 0.1612,  0.2661,  0.5395,  0.4920,  0.1567,  0.2783],\n",
      "        [ 0.1612,  0.2661,  0.5395,  0.4920,  0.1567,  0.2783]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "\n",
      "Decoder norm2(x + mha(x, mem)) = \n",
      "tensor([[[-2.0072,  0.4303,  0.3046,  1.2800,  0.1618, -0.1695]],\n",
      "\n",
      "        [[-1.2220,  1.2535,  1.4748, -0.5105, -0.4549, -0.5409]],\n",
      "\n",
      "        [[-0.1629,  0.7569,  1.2682,  0.2965, -1.9083, -0.2504]],\n",
      "\n",
      "        [[-0.1629,  0.7569,  1.2682,  0.2965, -1.9083, -0.2504]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "\n",
      "Decoder norm3(x + ff(x)) = \n",
      "tensor([[[-1.9210,  0.5572,  0.1643,  1.3773,  0.0834, -0.2612]],\n",
      "\n",
      "        [[-1.1794,  1.3299,  1.3943, -0.2781, -0.5425, -0.7242]],\n",
      "\n",
      "        [[-0.1347,  0.8070,  1.1641,  0.4986, -1.8748, -0.4604]],\n",
      "\n",
      "        [[-0.1347,  0.8070,  1.1641,  0.4986, -1.8748, -0.4604]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "\n",
      "\n",
      "################### Transformer Decoder start #####################\n",
      "\n",
      "query =  tensor([[[-1.9210,  0.5572,  0.1643,  1.3773,  0.0834, -0.2612]],\n",
      "\n",
      "        [[-1.1794,  1.3299,  1.3943, -0.2781, -0.5425, -0.7242]],\n",
      "\n",
      "        [[-0.1347,  0.8070,  1.1641,  0.4986, -1.8748, -0.4604]],\n",
      "\n",
      "        [[-0.1347,  0.8070,  1.1641,  0.4986, -1.8748, -0.4604]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "query shape =  torch.Size([4, 1, 6])\n",
      "\n",
      " _in_projection_packed \n",
      "\n",
      "Q :-  tensor([[[ 0.5941, -0.1705,  0.5284,  0.3753,  0.6452, -0.1056]],\n",
      "\n",
      "        [[-0.6461,  0.4200,  1.0642,  1.3662,  0.0807,  0.3409]],\n",
      "\n",
      "        [[-0.7124, -0.2093,  0.1047,  0.8027, -0.7164, -0.6526]],\n",
      "\n",
      "        [[-0.7124, -0.2093,  0.1047,  0.8027, -0.7164, -0.6526]]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "\n",
      "K :-  tensor([[[ 0.4684, -0.1347, -0.9458, -0.8799, -0.2084, -0.3137]],\n",
      "\n",
      "        [[-0.0412, -0.1545, -0.1991, -0.8236, -0.9536, -0.6278]],\n",
      "\n",
      "        [[-0.6865,  0.1744, -0.5111, -0.1793, -0.6342,  0.4552]],\n",
      "\n",
      "        [[-0.6865,  0.1744, -0.5111, -0.1793, -0.6342,  0.4552]]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "\n",
      "V :-  tensor([[[-1.3309, -0.6459, -0.1294, -0.1401, -0.9291, -0.0915]],\n",
      "\n",
      "        [[-0.5010,  0.4829,  0.1706, -0.4107,  0.3162, -1.1544]],\n",
      "\n",
      "        [[-0.2801, -0.0192, -0.1772, -0.5083,  0.2837, -1.3440]],\n",
      "\n",
      "        [[-0.2801, -0.0192, -0.1772, -0.5083,  0.2837, -1.3440]]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "\n",
      "tgt_len =  4\n",
      "bsz =  1\n",
      "num_heads =  2\n",
      "head_dim =  3\n",
      "\n",
      "tensor([[[ 0.5941, -0.1705,  0.5284,  0.3753,  0.6452, -0.1056]],\n",
      "\n",
      "        [[-0.6461,  0.4200,  1.0642,  1.3662,  0.0807,  0.3409]],\n",
      "\n",
      "        [[-0.7124, -0.2093,  0.1047,  0.8027, -0.7164, -0.6526]],\n",
      "\n",
      "        [[-0.7124, -0.2093,  0.1047,  0.8027, -0.7164, -0.6526]]],\n",
      "       grad_fn=<SelectBackward0>) torch.Size([4, 1, 6])\n",
      "Q reshaped =  tensor([[[ 0.5941, -0.1705,  0.5284],\n",
      "         [-0.6461,  0.4200,  1.0642],\n",
      "         [-0.7124, -0.2093,  0.1047],\n",
      "         [-0.7124, -0.2093,  0.1047]],\n",
      "\n",
      "        [[ 0.3753,  0.6452, -0.1056],\n",
      "         [ 1.3662,  0.0807,  0.3409],\n",
      "         [ 0.8027, -0.7164, -0.6526],\n",
      "         [ 0.8027, -0.7164, -0.6526]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "K reshaped =  tensor([[[ 0.4684, -0.1347, -0.9458],\n",
      "         [-0.0412, -0.1545, -0.1991],\n",
      "         [-0.6865,  0.1744, -0.5111],\n",
      "         [-0.6865,  0.1744, -0.5111]],\n",
      "\n",
      "        [[-0.8799, -0.2084, -0.3137],\n",
      "         [-0.8236, -0.9536, -0.6278],\n",
      "         [-0.1793, -0.6342,  0.4552],\n",
      "         [-0.1793, -0.6342,  0.4552]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "V reshaped =  tensor([[[-1.3309, -0.6459, -0.1294],\n",
      "         [-0.5010,  0.4829,  0.1706],\n",
      "         [-0.2801, -0.0192, -0.1772],\n",
      "         [-0.2801, -0.0192, -0.1772]],\n",
      "\n",
      "        [[-0.1401, -0.9291, -0.0915],\n",
      "         [-0.4107,  0.3162, -1.1544],\n",
      "         [-0.5083,  0.2837, -1.3440],\n",
      "         [-0.5083,  0.2837, -1.3440]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "torch.Size([1, 2, 4, 3]) torch.Size([1, 2, 4, 3]) torch.Size([1, 2, 4, 3])\n",
      "\n",
      "scaled_dot_product_attention\n",
      "attn_output = \n",
      "tensor([[-0.6421, -0.0463, -0.0601, -0.3874, -0.0392, -0.9631],\n",
      "        [-0.4835,  0.0263, -0.0791, -0.4303,  0.0839, -1.1018],\n",
      "        [-0.5243, -0.0151, -0.0863, -0.4048,  0.0533, -1.0365],\n",
      "        [-0.5243, -0.0151, -0.0863, -0.4048,  0.0533, -1.0365]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "\n",
      "x =  tensor([[[-1.9210,  0.5572,  0.1643,  1.3773,  0.0834, -0.2612]],\n",
      "\n",
      "        [[-1.1794,  1.3299,  1.3943, -0.2781, -0.5425, -0.7242]],\n",
      "\n",
      "        [[-0.1347,  0.8070,  1.1641,  0.4986, -1.8748, -0.4604]],\n",
      "\n",
      "        [[-0.1347,  0.8070,  1.1641,  0.4986, -1.8748, -0.4604]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "\n",
      "sa_op =  tensor([[[ 0.8869,  0.3679,  0.3620,  0.4243, -0.1237, -0.1943]],\n",
      "\n",
      "        [[ 0.8312,  0.2634,  0.2768,  0.4128, -0.3017, -0.2948]],\n",
      "\n",
      "        [[ 0.8353,  0.2778,  0.2989,  0.4192, -0.2490, -0.2535]],\n",
      "\n",
      "        [[ 0.8353,  0.2778,  0.2989,  0.4192, -0.2490, -0.2535]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "\n",
      "temop =  tensor([[[-1.0342,  0.9251,  0.5263,  1.8015, -0.0403, -0.4555]],\n",
      "\n",
      "        [[-0.3483,  1.5933,  1.6711,  0.1347, -0.8441, -1.0190]],\n",
      "\n",
      "        [[ 0.7007,  1.0849,  1.4631,  0.9178, -2.1237, -0.7139]],\n",
      "\n",
      "        [[ 0.7007,  1.0849,  1.4631,  0.9178, -2.1237, -0.7139]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "\n",
      "Decoder norm1(x + sa(x)) = \n",
      "tensor([[[-1.4245,  0.6878,  0.2578,  1.6326, -0.3530, -0.8007]],\n",
      "\n",
      "        [[-0.5062,  1.2931,  1.3652, -0.0586, -0.9657, -1.1278]],\n",
      "\n",
      "        [[ 0.3833,  0.6907,  0.9932,  0.5571, -1.8760, -0.7482]],\n",
      "\n",
      "        [[ 0.3833,  0.6907,  0.9932,  0.5571, -1.8760, -0.7482]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "\n",
      "Memory = \n",
      "tensor([[[ 0.5955,  0.1941, -1.9469, -0.5260,  1.0905,  0.5928]],\n",
      "\n",
      "        [[ 1.1424, -0.0411, -1.0821, -1.3404,  0.0076,  1.3136]],\n",
      "\n",
      "        [[ 0.3948,  1.2182, -0.1640,  0.8513, -0.4562, -1.8440]],\n",
      "\n",
      "        [[-0.0523, -0.7908, -0.7416, -1.0825,  1.5490,  1.1183]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "\n",
      "query =  tensor([[[-1.4245,  0.6878,  0.2578,  1.6326, -0.3530, -0.8007]],\n",
      "\n",
      "        [[-0.5062,  1.2931,  1.3652, -0.0586, -0.9657, -1.1278]],\n",
      "\n",
      "        [[ 0.3833,  0.6907,  0.9932,  0.5571, -1.8760, -0.7482]],\n",
      "\n",
      "        [[ 0.3833,  0.6907,  0.9932,  0.5571, -1.8760, -0.7482]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "query shape =  torch.Size([4, 1, 6])\n",
      "\n",
      " _in_projection_packed \n",
      "\n",
      "Q :-  tensor([[[-1.6276e+00, -1.7026e-01,  7.3439e-01, -7.6756e-01, -3.2277e-01,\n",
      "          -1.4383e-01]],\n",
      "\n",
      "        [[-7.5510e-01,  1.3660e-03,  1.0863e+00, -1.5444e+00, -1.8857e-01,\n",
      "           6.4778e-01]],\n",
      "\n",
      "        [[-3.1950e-01,  3.0669e-02, -9.8023e-03, -1.2919e+00, -2.1074e-01,\n",
      "           1.0605e+00]],\n",
      "\n",
      "        [[-3.1950e-01,  3.0669e-02, -9.8023e-03, -1.2919e+00, -2.1074e-01,\n",
      "           1.0605e+00]]], grad_fn=<ViewBackward0>)\n",
      "\n",
      "K :-  tensor([[[-0.6834, -0.5823,  0.4507, -0.0166, -1.3817, -0.1756]],\n",
      "\n",
      "        [[-0.6421, -0.0546, -0.2358, -0.7292, -1.1708, -0.6771]],\n",
      "\n",
      "        [[ 1.3053, -0.1177,  0.2241,  1.4036, -0.4212, -0.6506]],\n",
      "\n",
      "        [[-1.4741, -0.4167,  0.3985, -1.0448, -0.5296,  0.6559]]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "\n",
      "V :-  tensor([[[ 1.1678, -0.4077, -0.9678, -0.3686, -0.6365,  0.0449]],\n",
      "\n",
      "        [[ 0.0193, -0.1389, -0.2205, -0.7646, -0.7369, -0.2845]],\n",
      "\n",
      "        [[ 0.0616,  1.1591,  0.5875,  0.1118,  0.7294,  0.7683]],\n",
      "\n",
      "        [[ 0.8653, -0.7580, -0.8965, -0.1984, -0.7455, -0.6358]]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "\n",
      "tgt_len =  4\n",
      "bsz =  1\n",
      "num_heads =  2\n",
      "head_dim =  3\n",
      "\n",
      "tensor([[[-1.6276e+00, -1.7026e-01,  7.3439e-01, -7.6756e-01, -3.2277e-01,\n",
      "          -1.4383e-01]],\n",
      "\n",
      "        [[-7.5510e-01,  1.3660e-03,  1.0863e+00, -1.5444e+00, -1.8857e-01,\n",
      "           6.4778e-01]],\n",
      "\n",
      "        [[-3.1950e-01,  3.0669e-02, -9.8023e-03, -1.2919e+00, -2.1074e-01,\n",
      "           1.0605e+00]],\n",
      "\n",
      "        [[-3.1950e-01,  3.0669e-02, -9.8023e-03, -1.2919e+00, -2.1074e-01,\n",
      "           1.0605e+00]]], grad_fn=<ViewBackward0>) torch.Size([4, 1, 6])\n",
      "Q reshaped =  tensor([[[-1.6276e+00, -1.7026e-01,  7.3439e-01],\n",
      "         [-7.5510e-01,  1.3660e-03,  1.0863e+00],\n",
      "         [-3.1950e-01,  3.0669e-02, -9.8023e-03],\n",
      "         [-3.1950e-01,  3.0669e-02, -9.8023e-03]],\n",
      "\n",
      "        [[-7.6756e-01, -3.2277e-01, -1.4383e-01],\n",
      "         [-1.5444e+00, -1.8857e-01,  6.4778e-01],\n",
      "         [-1.2919e+00, -2.1074e-01,  1.0605e+00],\n",
      "         [-1.2919e+00, -2.1074e-01,  1.0605e+00]]],\n",
      "       grad_fn=<TransposeBackward0>)\n",
      "\n",
      "K reshaped =  tensor([[[-0.6834, -0.5823,  0.4507],\n",
      "         [-0.6421, -0.0546, -0.2358],\n",
      "         [ 1.3053, -0.1177,  0.2241],\n",
      "         [-1.4741, -0.4167,  0.3985]],\n",
      "\n",
      "        [[-0.0166, -1.3817, -0.1756],\n",
      "         [-0.7292, -1.1708, -0.6771],\n",
      "         [ 1.4036, -0.4212, -0.6506],\n",
      "         [-1.0448, -0.5296,  0.6559]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "V reshaped =  tensor([[[ 1.1678, -0.4077, -0.9678],\n",
      "         [ 0.0193, -0.1389, -0.2205],\n",
      "         [ 0.0616,  1.1591,  0.5875],\n",
      "         [ 0.8653, -0.7580, -0.8965]],\n",
      "\n",
      "        [[-0.3686, -0.6365,  0.0449],\n",
      "         [-0.7646, -0.7369, -0.2845],\n",
      "         [ 0.1118,  0.7294,  0.7683],\n",
      "         [-0.1984, -0.7455, -0.6358]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "torch.Size([1, 2, 4, 3]) torch.Size([1, 2, 4, 3]) torch.Size([1, 2, 4, 3])\n",
      "\n",
      "scaled_dot_product_attention\n",
      "attn_output = \n",
      "tensor([[ 0.7656, -0.4898, -0.7431, -0.3950, -0.5490, -0.1926],\n",
      "        [ 0.7077, -0.3292, -0.6289, -0.3642, -0.6711, -0.3767],\n",
      "        [ 0.5780, -0.1594, -0.4706, -0.3374, -0.6646, -0.3835],\n",
      "        [ 0.5780, -0.1594, -0.4706, -0.3374, -0.6646, -0.3835]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "\n",
      "Decoder norm2(x + mha(x, mem)) = \n",
      "tensor([[[-1.1305,  0.1514, -0.0687,  1.9161,  0.1340, -1.0024]],\n",
      "\n",
      "        [[-0.3292,  0.6692,  1.2940,  0.6506, -0.5356, -1.7490]],\n",
      "\n",
      "        [[ 0.5165,  0.0494,  0.9650,  1.1221, -1.4294, -1.2236]],\n",
      "\n",
      "        [[ 0.5165,  0.0494,  0.9650,  1.1221, -1.4294, -1.2236]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "\n",
      "Decoder norm3(x + ff(x)) = \n",
      "tensor([[[-1.3369,  0.4242,  0.2732,  1.5165,  0.3595, -1.2365]],\n",
      "\n",
      "        [[-0.4143,  0.8070,  1.3914,  0.2149, -0.2259, -1.7731]],\n",
      "\n",
      "        [[ 0.3225,  0.2839,  1.2815,  0.7723, -1.1908, -1.4693]],\n",
      "\n",
      "        [[ 0.3225,  0.2839,  1.2815,  0.7723, -1.1908, -1.4693]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "\n",
      "tensor([[[ 0.2080, -0.2101, -0.3444, -0.4398,  0.0395,  0.8219, -0.1725,\n",
      "           0.7168, -0.5166, -0.1150]],\n",
      "\n",
      "        [[-0.5421,  0.0616, -0.9904,  0.5963,  0.2021,  1.1265,  0.1808,\n",
      "           0.8478, -0.8467,  0.4492]],\n",
      "\n",
      "        [[-0.8845, -0.1459, -1.2355,  0.2183,  0.2404,  0.4411, -0.5962,\n",
      "           0.2322, -0.0240,  0.6122]],\n",
      "\n",
      "        [[-0.8845, -0.1459, -1.2355,  0.2183,  0.2404,  0.4411, -0.5962,\n",
      "           0.2322, -0.0240,  0.6122]]], grad_fn=<ViewBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sreevaatsav/.pyenv/versions/project_env/lib/python3.10/site-packages/torch/nn/modules/transformer.py:282: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=512, dropout=0):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.encoding = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1).float()\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(torch.log(torch.tensor(10000.0)) / d_model))\n",
    "        self.encoding[:, 0::2] = torch.sin(position * div_term)\n",
    "        self.encoding[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.encoding = self.encoding.unsqueeze(0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.encoding[:, :x.size(1)].detach()\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, src_vocab_size, tgt_vocab_size,max_seq_len, num_encoder_layers=1, num_decoder_layers=1, d_model=4, nhead=2):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.src_embedding = nn.Embedding(src_vocab_size, d_model)\n",
    "        self.tgt_embedding = nn.Embedding(tgt_vocab_size, d_model)\n",
    "        self.positional_encoding = PositionalEncoding(d_model, dropout=0, max_len=max_seq_len)\n",
    "        self.transformer = nn.Transformer(\n",
    "            d_model=d_model,\n",
    "            nhead=nhead,\n",
    "            num_encoder_layers=num_encoder_layers,\n",
    "            num_decoder_layers=num_decoder_layers,\n",
    "            dropout=0\n",
    "        )\n",
    "        self.fc = nn.Linear(d_model, tgt_vocab_size)\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "\n",
    "        print(\"Source embeddings :- \\n\")\n",
    "        print(self.src_embedding(src))\n",
    "        print()\n",
    "\n",
    "        print(\"Target embeddings :- \\n\")\n",
    "        print(self.tgt_embedding(tgt))\n",
    "        print()\n",
    "\n",
    "\n",
    "        src = self.src_embedding(src) + self.positional_encoding(src)\n",
    "        tgt = self.tgt_embedding(tgt) + self.positional_encoding(tgt)\n",
    "\n",
    "        print(\"Positional encoded source embeddings:\")\n",
    "        print(src)\n",
    "        print()\n",
    "\n",
    "        print(\"Positional encoded target embeddings:\")\n",
    "        print(tgt)\n",
    "        print()\n",
    "\n",
    "        output = self.transformer(src, tgt)\n",
    "        output = self.fc(output)\n",
    "        \n",
    "        return output\n",
    "    \n",
    "\n",
    "src_vocab_size = 10  # Source language vocabulary size\n",
    "tgt_vocab_size = 10  # Target language vocabulary size\n",
    "d_model = 4  # Dimension of the model\n",
    "num_heads = 2\n",
    "num_encoder_layers = 3\n",
    "num_decoder_layers = 3\n",
    "\n",
    "need_weights = False\n",
    "\n",
    "src_mask = None\n",
    "tgt_mask = None\n",
    "memory_mask = None\n",
    "\n",
    "max_seq_len = 4\n",
    "\n",
    "d_model = 6\n",
    "\n",
    "\n",
    "model = TransformerModel(src_vocab_size, tgt_vocab_size, num_encoder_layers=num_encoder_layers, num_decoder_layers=num_decoder_layers, d_model=d_model, max_seq_len = max_seq_len, nhead=num_heads)\n",
    "# Source sentence in the source language\n",
    "src_sentence = torch.tensor([[0], [1], [2], [3]])  # Source sequence\n",
    "\n",
    "# Target sentence in the target language (translation of the source sentence)\n",
    "tgt_sentence = torch.tensor([[1], [0], [3], [3]])  # Target sequence\n",
    "\n",
    "# Forward pass\n",
    "output = model(src_sentence, tgt_sentence)\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_intermediate_outputs(src_sentence, tgt_sentence,d_model, model, num_encoder_layers , num_decoder_layers):\n",
    "\n",
    "    state_dict = model.state_dict()\n",
    "\n",
    "    pe_src_embeds, pe_tgt_embeds = get_embedding_outputs(src_sentence=src_sentence,  tgt_sentence=tgt_sentence, state_dict=state_dict, max_seq_len=max_seq_len, d_model = d_model)\n",
    "    print(\"###\"*25)\n",
    "    print(\"### Encoder Start ###\")\n",
    "    print()\n",
    "\n",
    "    x_enc = pe_src_embeds\n",
    "\n",
    "    for lno in range(num_encoder_layers):\n",
    "        attn_enc_output, src_len, head_dim, attn_weights = encoder_block_attn_output(x_enc, state_dict, layer_num = lno, need_weights = False, embed_dim = d_model, num_heads= num_heads, src_mask=None)\n",
    "\n",
    "        if lno == 0:\n",
    "            tgt_len, bsz, embed_dim = x_enc.shape\n",
    "\n",
    "        output_enc_final = encoder_block_post_attn_output(x_enc, attn_enc_output, state_dict, layer_num = lno , bsz = bsz, tgt_len = tgt_len)\n",
    "\n",
    "        x_enc = output_enc_final\n",
    "\n",
    "    \n",
    "    print(\"### Encoder Done ###\")\n",
    "    print(\"\\n\\n\\n\")\n",
    "    print(\"### Decoder Start ###\")\n",
    "\n",
    "    x_dec = pe_tgt_embeds\n",
    "    memory = x_enc\n",
    "\n",
    "    for lno in range(num_decoder_layers):\n",
    "\n",
    "        self_attn_dec, dec_sa_wts = decoder_block_self_attn_output(x_dec, state_dict, layer_num = lno, need_weights = False, tgt_mask=None)\n",
    "        x_dec = dec_post_self_attn(self_attn_dec, x_dec, state_dict, layer_num = lno)\n",
    "\n",
    "        attn_dec_mha_output, attn_dec_mha_wts = decoder_block_cross_attn_output(x_dec, memory, state_dict, layer_num = lno, tgt_len = tgt_len, src_len = src_len, head_dim = head_dim, need_weights = False, memory_mask=None)\n",
    "        final_op = decoder_block_post_attn_output(x_dec, attn_dec_mha_output, state_dict, layer_num = lno)\n",
    "\n",
    "        print(pe_tgt_embeds.shape, final_op.shape)\n",
    "        x_dec = final_op\n",
    "\n",
    "    print(\"### Decoder Done ###\")\n",
    "\n",
    "\n",
    "    final_op = feef_fwd_transformer(final_op, state_dict)\n",
    "\n",
    "\n",
    "    \n",
    "    return final_op\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source sentence embedding\n",
      "Word index: 0, Embedding: tensor([-0.5329,  0.4479, -1.5247, -0.6124,  0.9608, -0.5078])\n",
      "Word index: 1, Embedding: tensor([ 0.7777,  0.7221, -0.2767, -1.7240, -0.0987,  0.7716])\n",
      "Word index: 2, Embedding: tensor([-0.2460,  1.2069,  0.8297,  0.9564, -0.2230, -2.0615])\n",
      "Word index: 3, Embedding: tensor([-0.6652, -0.4723,  0.9936, -0.3680,  1.2617,  0.3828])\n",
      "\n",
      "torch.Size([4, 1, 6])\n",
      "Target sentence embedding\n",
      "Word index: 1, Embedding: tensor([-0.3454,  0.9345, -0.6539,  1.9464,  2.3963, -1.9435])\n",
      "Word index: 0, Embedding: tensor([ 0.6227,  1.4822,  3.2615, -0.7944, -0.7245, -1.0534])\n",
      "Word index: 3, Embedding: tensor([ 0.8797, -0.3861,  0.2234, -0.2576, -1.4788, -1.3086])\n",
      "Word index: 3, Embedding: tensor([ 0.8797, -0.3861,  0.2234, -0.2576, -1.4788, -1.3086])\n",
      "\n",
      "PE of src :\n",
      "tensor([[[0., 1., 0., 1., 0., 1.]]])\n",
      "\n",
      "PE of tgt :\n",
      "tensor([[[0., 1., 0., 1., 0., 1.]]])\n",
      "\n",
      "PE source embeddings : \n",
      "\n",
      "tensor([[[-0.5329,  1.4479, -1.5247,  0.3876,  0.9608,  0.4922]],\n",
      "\n",
      "        [[ 0.7777,  1.7221, -0.2767, -0.7240, -0.0987,  1.7716]],\n",
      "\n",
      "        [[-0.2460,  2.2069,  0.8297,  1.9564, -0.2230, -1.0615]],\n",
      "\n",
      "        [[-0.6652,  0.5277,  0.9936,  0.6320,  1.2617,  1.3828]]])\n",
      "\n",
      "PE target embeddings : \n",
      "\n",
      "tensor([[[-0.3454,  1.9345, -0.6539,  2.9464,  2.3963, -0.9435]],\n",
      "\n",
      "        [[ 0.6227,  2.4822,  3.2615,  0.2056, -0.7245, -0.0534]],\n",
      "\n",
      "        [[ 0.8797,  0.6139,  0.2234,  0.7424, -1.4788, -0.3086]],\n",
      "\n",
      "        [[ 0.8797,  0.6139,  0.2234,  0.7424, -1.4788, -0.3086]]])\n",
      "\n",
      "###########################################################################\n",
      "### Encoder Start ###\n",
      "\n",
      "Q_enc_0 = \n",
      "tensor([[[ 0.9694, -0.3737,  0.1014],\n",
      "         [ 0.3989, -0.4059,  0.7465],\n",
      "         [ 1.0433,  2.2405, -2.0432],\n",
      "         [-0.5131, -0.0364, -0.0935]],\n",
      "\n",
      "        [[-0.4298, -0.5519,  0.5748],\n",
      "         [-0.0833, -1.1578,  0.5435],\n",
      "         [ 0.4456,  0.4455,  1.4093],\n",
      "         [ 0.9812,  0.7730,  0.3288]]])\n",
      "\n",
      "K_enc_0 = \n",
      "tensor([[[-0.2759,  0.9574, -0.0307],\n",
      "         [ 0.1012,  0.7665, -0.7728],\n",
      "         [-0.6387,  0.5390, -0.2032],\n",
      "         [ 0.3532,  0.1304, -0.9684]],\n",
      "\n",
      "        [[-0.7206,  0.6998,  0.6667],\n",
      "         [-0.8337, -0.3324,  0.4448],\n",
      "         [ 1.3872,  0.1583,  1.8742],\n",
      "         [-0.5231,  0.8520,  0.6656]]])\n",
      "\n",
      "V_enc_0 = \n",
      "tensor([[[ 0.3701,  1.5507, -0.6786],\n",
      "         [ 1.3674,  0.9738, -1.2993],\n",
      "         [ 0.0642, -0.4617, -0.5055],\n",
      "         [-0.0065,  0.2798, -0.2086]],\n",
      "\n",
      "        [[ 0.0474,  0.1029, -0.3341],\n",
      "         [ 0.1031,  0.0802,  1.2249],\n",
      "         [ 0.9836,  1.4497, -0.3509],\n",
      "         [ 0.4828,  0.0439, -0.1283]]])\n",
      "\n",
      "Attention weights = \n",
      "torch.Size([1, 2, 4, 4]) tensor([[[[0.2116, 0.2607, 0.1871, 0.3405],\n",
      "          [0.2698, 0.2235, 0.2541, 0.2527],\n",
      "          [0.1838, 0.4324, 0.1054, 0.2784],\n",
      "          [0.2527, 0.2362, 0.2865, 0.2245]],\n",
      "\n",
      "         [[0.2333, 0.3098, 0.2454, 0.2116],\n",
      "          [0.1767, 0.3304, 0.3349, 0.1580],\n",
      "          [0.1490, 0.0926, 0.5955, 0.1629],\n",
      "          [0.1660, 0.0942, 0.5411, 0.1987]]]])\n",
      "Final Encoder 0 Output :\n",
      "norm2(norm1(x + self_atten(x)) + feed_fwd_op)\n",
      "\n",
      "tensor([[[ 0.3112,  0.8151, -2.0813, -0.2191,  0.8784,  0.2956]],\n",
      "\n",
      "        [[ 1.1579,  0.6697, -1.0996, -1.3563, -0.3803,  1.0086]],\n",
      "\n",
      "        [[ 0.3009,  1.4600, -0.4414,  0.7590, -0.3888, -1.6898]],\n",
      "\n",
      "        [[-0.4939, -0.1761, -0.8331, -1.1551,  1.6152,  1.0429]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "\n",
      "Q_enc_1 = \n",
      "tensor([[[ 0.1582, -0.5874,  0.1214],\n",
      "         [ 0.2182, -0.5579, -0.3282],\n",
      "         [ 0.3140, -0.3042,  0.3891],\n",
      "         [-0.3295, -0.0915,  0.6706]],\n",
      "\n",
      "        [[ 0.0474,  0.3990, -0.2255],\n",
      "         [ 0.2948,  0.2801, -0.1668],\n",
      "         [-0.1400, -1.4475,  1.1345],\n",
      "         [-0.3668,  1.5381, -1.4790]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "K_enc_1 = \n",
      "tensor([[[ 0.2946, -0.1200,  1.3455],\n",
      "         [ 1.0143,  0.9952,  0.7335],\n",
      "         [-0.3556, -1.4456, -0.1802],\n",
      "         [ 0.4098,  0.6703,  0.7619]],\n",
      "\n",
      "        [[-0.4681, -0.5777, -0.2227],\n",
      "         [-1.2081,  0.0680, -0.0584],\n",
      "         [-0.3622, -1.4537,  0.0772],\n",
      "         [ 0.2869,  0.3001, -0.3839]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "V_enc_1 = \n",
      "tensor([[[-1.0751,  0.4705, -0.0284],\n",
      "         [ 0.5148,  0.9883,  0.2391],\n",
      "         [-1.1569,  0.0931,  0.6943],\n",
      "         [-0.3562,  0.3436, -0.6694]],\n",
      "\n",
      "        [[ 0.3559, -0.7036,  0.5059],\n",
      "         [ 1.0776, -1.7891, -0.3165],\n",
      "         [-0.5253,  0.3234, -0.2551],\n",
      "         [ 0.3510, -0.2131,  0.1841]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "Attention weights = \n",
      "torch.Size([1, 2, 4, 4]) tensor([[[[0.2652, 0.1859, 0.3521, 0.1968],\n",
      "          [0.2163, 0.1857, 0.4079, 0.1901],\n",
      "          [0.2948, 0.2407, 0.2347, 0.2298],\n",
      "          [0.3258, 0.2113, 0.2190, 0.2439]],\n",
      "\n",
      "         [[0.2394, 0.2664, 0.1887, 0.3055],\n",
      "          [0.2408, 0.2320, 0.2067, 0.3205],\n",
      "          [0.2172, 0.1497, 0.5449, 0.0883],\n",
      "          [0.1893, 0.3413, 0.0658, 0.4036]]]], grad_fn=<SoftmaxBackward0>)\n",
      "Final Encoder 1 Output :\n",
      "norm2(norm1(x + self_atten(x)) + feed_fwd_op)\n",
      "\n",
      "tensor([[[ 0.2829,  0.8252, -1.9678, -0.5107,  0.9738,  0.3967]],\n",
      "\n",
      "        [[ 1.0085,  0.6525, -0.9856, -1.4783, -0.3327,  1.1356]],\n",
      "\n",
      "        [[ 0.2693,  1.5369, -0.4327,  0.6032, -0.2601, -1.7166]],\n",
      "\n",
      "        [[-0.4416, -0.4020, -0.4336, -1.3739,  1.4892,  1.1618]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "\n",
      "Q_enc_2 = \n",
      "tensor([[[-0.8163, -0.9035, -0.3460],\n",
      "         [ 0.4852, -1.6041, -0.7942],\n",
      "         [-0.3084,  0.1832,  1.1653],\n",
      "         [-0.0497,  0.1680, -1.3590]],\n",
      "\n",
      "        [[-0.1043, -0.3659,  0.0583],\n",
      "         [-0.4536, -0.2208,  0.6211],\n",
      "         [-1.1148,  0.9869, -1.4382],\n",
      "         [ 1.1141, -1.5910,  1.4742]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "K_enc_2 = \n",
      "tensor([[[-0.5884, -0.4992,  0.9580],\n",
      "         [-0.5826, -0.3851, -0.0341],\n",
      "         [ 0.1464,  0.3144, -0.0108],\n",
      "         [-0.1748,  0.0303,  0.2794]],\n",
      "\n",
      "        [[-0.7957, -0.4586,  1.0056],\n",
      "         [-0.9059, -0.7629,  0.5773],\n",
      "         [ 0.2492,  1.3992, -0.1864],\n",
      "         [-0.3146, -1.4700,  1.0550]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "V_enc_2 = \n",
      "tensor([[[ 0.5679, -0.0306, -0.0299],\n",
      "         [-0.4406, -0.0313,  0.9458],\n",
      "         [ 1.2799,  0.4034, -0.9645],\n",
      "         [-0.5464, -0.7746,  0.5304]],\n",
      "\n",
      "        [[ 1.2492,  0.0498, -1.3543],\n",
      "         [ 0.0986,  0.2459, -1.3427],\n",
      "         [ 1.5861, -0.1953,  0.3250],\n",
      "         [-0.4118,  0.4807, -1.2010]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "Attention weights = \n",
      "torch.Size([1, 2, 4, 4]) tensor([[[[0.2922, 0.3348, 0.1641, 0.2089],\n",
      "          [0.2347, 0.3334, 0.2116, 0.2203],\n",
      "          [0.3789, 0.1965, 0.1888, 0.2358],\n",
      "          [0.1381, 0.3041, 0.3130, 0.2447]],\n",
      "\n",
      "         [[0.2582, 0.2733, 0.1573, 0.3111],\n",
      "          [0.3003, 0.2756, 0.1176, 0.3066],\n",
      "          [0.1506, 0.1939, 0.5959, 0.0596],\n",
      "          [0.1791, 0.1533, 0.0231, 0.6446]]]], grad_fn=<SoftmaxBackward0>)\n",
      "Final Encoder 2 Output :\n",
      "norm2(norm1(x + self_atten(x)) + feed_fwd_op)\n",
      "\n",
      "tensor([[[ 0.5955,  0.1941, -1.9469, -0.5260,  1.0905,  0.5928]],\n",
      "\n",
      "        [[ 1.1424, -0.0411, -1.0821, -1.3404,  0.0076,  1.3136]],\n",
      "\n",
      "        [[ 0.3948,  1.2182, -0.1640,  0.8512, -0.4562, -1.8440]],\n",
      "\n",
      "        [[-0.0523, -0.7908, -0.7416, -1.0825,  1.5490,  1.1183]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "\n",
      "### Encoder Done ###\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "### Decoder Start ###\n",
      "torch.Size([1, 4, 1, 6]) torch.Size([1, 4, 1, 6]) torch.Size([1, 4, 1, 6])\n",
      "4 2 3\n",
      "Q_dec_0 = \n",
      "tensor([[[ 1.5263, -0.0920,  0.8640],\n",
      "         [-0.1765, -0.4730,  1.3362],\n",
      "         [ 0.0228, -1.0756,  0.7398],\n",
      "         [ 0.0228, -1.0756,  0.7398]],\n",
      "\n",
      "        [[ 1.7978, -0.5208, -3.1598],\n",
      "         [-0.7475, -1.3612, -0.3957],\n",
      "         [-0.2298,  0.2056, -0.0381],\n",
      "         [-0.2298,  0.2056, -0.0381]]])\n",
      "\n",
      "K_dec_0 = \n",
      "tensor([[[-0.3191, -0.5648, -0.5542],\n",
      "         [ 1.7961,  0.1075, -0.2672],\n",
      "         [ 0.2835, -0.1488, -0.0277],\n",
      "         [ 0.2835, -0.1488, -0.0277]],\n",
      "\n",
      "        [[-0.9690,  1.2324,  1.5072],\n",
      "         [-2.3272,  0.2400,  1.9377],\n",
      "         [-0.8103, -1.0167,  0.6870],\n",
      "         [-0.8103, -1.0167,  0.6870]]])\n",
      "\n",
      "V_dec_0 = \n",
      "tensor([[[ 0.2705, -0.9981, -2.4461],\n",
      "         [-0.6342, -2.0790,  0.6601],\n",
      "         [ 0.2722, -0.5439,  0.2677],\n",
      "         [ 0.2722, -0.5439,  0.2677]],\n",
      "\n",
      "        [[ 2.0536, -0.5573,  0.1047],\n",
      "         [-0.8751, -1.8104,  0.7144],\n",
      "         [-0.1405, -0.7309,  0.1020],\n",
      "         [-0.1405, -0.7309,  0.1020]]])\n",
      "\n",
      "Attention weights = \n",
      "torch.Size([1, 2, 4, 4]) tensor([[[[0.0800, 0.5741, 0.1730, 0.1730],\n",
      "          [0.2295, 0.1921, 0.2892, 0.2892],\n",
      "          [0.2692, 0.2061, 0.2624, 0.2624],\n",
      "          [0.2692, 0.2061, 0.2624, 0.2624]],\n",
      "\n",
      "         [[0.0458, 0.0069, 0.4737, 0.4737],\n",
      "          [0.0564, 0.2002, 0.3717, 0.3717],\n",
      "          [0.2792, 0.2944, 0.2132, 0.2132],\n",
      "          [0.2792, 0.2944, 0.2132, 0.2132]]]])\n",
      "Decoder Self Attention = \n",
      "tensor([[-0.2483, -1.4615,  0.2760, -0.0451, -0.7304,  0.1063],\n",
      "        [ 0.0977, -0.9430, -0.2797, -0.1639, -0.9373,  0.2248],\n",
      "        [ 0.0850, -0.9825, -0.3819,  0.2559, -1.0003,  0.2830],\n",
      "        [ 0.0850, -0.9825, -0.3819,  0.2559, -1.0003,  0.2830]])\n",
      "\n",
      "Decoder_0 norm1(x + sa(x))\n",
      "tensor([[[-0.3985,  0.4733, -1.1387,  1.2103,  1.1194, -1.2659]],\n",
      "\n",
      "        [[ 0.0125,  1.0738,  1.5524, -0.6861, -1.1492, -0.8033]],\n",
      "\n",
      "        [[ 1.2858,  0.4665,  0.2748,  0.3491, -1.9325, -0.4438]],\n",
      "\n",
      "        [[ 1.2858,  0.4665,  0.2748,  0.3491, -1.9325, -0.4438]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "\n",
      "Q_dec_0 = \n",
      "tensor([[[-0.3132, -0.5748,  1.0150],\n",
      "         [ 0.1260,  0.9751,  0.2347],\n",
      "         [-0.3754, -0.1783,  0.1096],\n",
      "         [-0.3754, -0.1783,  0.1096]],\n",
      "\n",
      "        [[ 1.3564,  0.1577,  0.2171],\n",
      "         [ 0.2129,  0.8575, -1.2803],\n",
      "         [ 1.0184,  1.0515, -1.3438],\n",
      "         [ 1.0184,  1.0515, -1.3438]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "K_dec_0 = \n",
      "tensor([[[ 0.8679, -0.3120,  0.0513],\n",
      "         [ 0.3726, -0.1253, -0.3014],\n",
      "         [ 0.0990,  0.8646,  1.4213],\n",
      "         [ 0.5624, -0.5779, -0.9155]],\n",
      "\n",
      "        [[ 1.2845, -0.3969, -0.8609],\n",
      "         [ 0.7556, -0.4005, -0.0751],\n",
      "         [ 0.5656,  0.8158,  0.0260],\n",
      "         [ 0.3598, -0.6380, -0.7300]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "V_dec_0 = \n",
      "tensor([[[ 1.2654, -0.4361,  1.3083],\n",
      "         [ 0.7738, -1.2616,  0.9193],\n",
      "         [ 0.2346,  0.3587, -0.1074],\n",
      "         [ 0.4998, -0.1963,  0.7233]],\n",
      "\n",
      "        [[ 1.0698, -0.0392,  0.6787],\n",
      "         [ 0.4712,  0.6643,  0.0268],\n",
      "         [ 0.2587,  0.4253,  0.4300],\n",
      "         [ 0.5793, -0.6686,  0.1322]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "Attention weights = \n",
      "torch.Size([1, 2, 4, 4]) tensor([[[[0.2366, 0.1978, 0.4107, 0.1550],\n",
      "          [0.2013, 0.2056, 0.4444, 0.1487],\n",
      "          [0.2357, 0.2518, 0.2690, 0.2435],\n",
      "          [0.2357, 0.2518, 0.2690, 0.2435]],\n",
      "\n",
      "         [[0.3422, 0.2495, 0.2432, 0.1650],\n",
      "          [0.3217, 0.1683, 0.2787, 0.2313],\n",
      "          [0.3940, 0.1566, 0.2709, 0.1785],\n",
      "          [0.3940, 0.1566, 0.2709, 0.1785]]]], grad_fn=<SoftmaxBackward0>)\n",
      "Cross attention in decoder_0\n",
      "tensor([[ 0.6262, -0.2358,  0.5593,  0.6422,  0.1455,  0.3654],\n",
      "        [ 0.5924, -0.2169,  0.5122,  0.6296,  0.0631,  0.3732],\n",
      "        [ 0.6779, -0.3717,  0.6870,  0.6688,  0.0844,  0.4117],\n",
      "        [ 0.6779, -0.3717,  0.6870,  0.6688,  0.0844,  0.4117]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "\n",
      "norm2(x' + mha(x', mem)), \n",
      " where x' = Decoder_curr_layer norm1(x + sa(x))\n",
      "tensor([[[-1.2077,  0.3426, -0.6898,  0.9182,  1.5230, -0.8864]],\n",
      "\n",
      "        [[-0.8041,  0.9494,  1.7817, -0.8000, -0.5982, -0.5288]],\n",
      "\n",
      "        [[ 0.3956,  0.4804,  1.1372,  0.1761, -2.0673, -0.1219]],\n",
      "\n",
      "        [[ 0.3956,  0.4804,  1.1372,  0.1761, -2.0673, -0.1219]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "\n",
      "norm3(x'' + ff(x'')) \n",
      " where, x'' = Decoder_curr_layer norm2(x' + mha(x'))\n",
      "tensor([[[-1.4797,  0.2166, -0.1863,  0.9378,  1.4217, -0.9100]],\n",
      "\n",
      "        [[-0.9290,  0.8128,  1.8555, -0.6980, -0.5662, -0.4750]],\n",
      "\n",
      "        [[ 0.1088,  0.3915,  1.3837,  0.2170, -1.9632, -0.1379]],\n",
      "\n",
      "        [[ 0.1088,  0.3915,  1.3837,  0.2170, -1.9632, -0.1379]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "\n",
      "torch.Size([4, 1, 6]) torch.Size([4, 1, 6])\n",
      "torch.Size([1, 4, 1, 6]) torch.Size([1, 4, 1, 6]) torch.Size([1, 4, 1, 6])\n",
      "4 2 3\n",
      "Q_dec_1 = \n",
      "tensor([[[-1.3524,  0.7480, -0.9949],\n",
      "         [-0.5110, -1.0154,  0.9501],\n",
      "         [ 0.0535, -1.2236,  0.8311],\n",
      "         [ 0.0535, -1.2236,  0.8311]],\n",
      "\n",
      "        [[-0.3929,  0.2824,  0.1878],\n",
      "         [ 0.5617, -1.2253,  1.2739],\n",
      "         [ 1.3682, -0.3467,  0.8462],\n",
      "         [ 1.3682, -0.3467,  0.8462]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "K_dec_1 = \n",
      "tensor([[[-0.0523,  0.1468,  1.1768],\n",
      "         [ 0.0839, -0.1600,  0.4761],\n",
      "         [-0.3198,  0.5040, -0.8077],\n",
      "         [-0.3198,  0.5040, -0.8077]],\n",
      "\n",
      "        [[ 0.3591, -1.2796, -0.8246],\n",
      "         [-0.0641,  0.2123, -0.4256],\n",
      "         [-0.9525,  0.4314, -0.4933],\n",
      "         [-0.9525,  0.4314, -0.4933]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "V_dec_1 = \n",
      "tensor([[[ 0.4509,  0.4816, -0.1915],\n",
      "         [-1.2860, -0.2816,  0.8666],\n",
      "         [-1.2906, -1.2389,  1.0060],\n",
      "         [-1.2906, -1.2389,  1.0060]],\n",
      "\n",
      "        [[-0.7499, -0.0103,  0.9292],\n",
      "         [ 0.5826,  0.0565, -0.7175],\n",
      "         [ 1.1209,  0.0460, -1.4021],\n",
      "         [ 1.1209,  0.0460, -1.4021]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "Attention weights = \n",
      "torch.Size([1, 2, 4, 4]) tensor([[[[0.0895, 0.1055, 0.4025, 0.4025],\n",
      "          [0.4212, 0.3298, 0.1245, 0.1245],\n",
      "          [0.4023, 0.3585, 0.1196, 0.1196],\n",
      "          [0.4023, 0.3585, 0.1196, 0.1196]],\n",
      "\n",
      "         [[0.1624, 0.2381, 0.2997, 0.2997],\n",
      "          [0.5252, 0.2137, 0.1305, 0.1305],\n",
      "          [0.4469, 0.2884, 0.1324, 0.1324],\n",
      "          [0.4469, 0.2884, 0.1324, 0.1324]]]], grad_fn=<SoftmaxBackward0>)\n",
      "Decoder Self Attention = \n",
      "tensor([[-1.1342, -0.9839,  0.8841,  0.6888,  0.0394, -0.8603],\n",
      "        [-0.5555, -0.1985,  0.4556,  0.0233,  0.0187, -0.0314],\n",
      "        [-0.5883, -0.2036,  0.4743,  0.1297,  0.0239, -0.1629],\n",
      "        [-0.5883, -0.2036,  0.4743,  0.1297,  0.0239, -0.1629]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "\n",
      "Decoder_1 norm1(x + sa(x))\n",
      "tensor([[[-2.0555,  0.4990,  0.2593,  1.1941,  0.1688, -0.0658]],\n",
      "\n",
      "        [[-1.2609,  1.0775,  1.5505, -0.5601, -0.7242, -0.0828]],\n",
      "\n",
      "        [[-0.3300,  0.6813,  1.1971,  0.1952, -1.9754,  0.2319]],\n",
      "\n",
      "        [[-0.3300,  0.6813,  1.1971,  0.1952, -1.9754,  0.2319]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "\n",
      "Q_dec_1 = \n",
      "tensor([[[-0.1854,  0.5648,  0.5430],\n",
      "         [-0.0059,  0.7083, -0.8105],\n",
      "         [ 0.1540,  0.0608, -1.2232],\n",
      "         [ 0.1540,  0.0608, -1.2232]],\n",
      "\n",
      "        [[ 0.9398,  0.3039, -1.3338],\n",
      "         [ 1.0112, -0.2463, -0.5395],\n",
      "         [ 0.4331, -0.3300, -0.7668],\n",
      "         [ 0.4331, -0.3300, -0.7668]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "K_dec_1 = \n",
      "tensor([[[ 1.2322,  0.4004, -0.8374],\n",
      "         [ 0.8701,  0.1032, -0.3402],\n",
      "         [ 0.1902,  1.4931, -0.3451],\n",
      "         [ 0.2764, -0.8330, -0.5372]],\n",
      "\n",
      "        [[ 0.6489, -0.6018,  0.9821],\n",
      "         [ 0.3076, -0.2460,  1.2735],\n",
      "         [ 0.3678,  1.1194, -0.8406],\n",
      "         [ 0.1252, -1.1538,  1.1789]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "V_dec_1 = \n",
      "tensor([[[ 6.1422e-02,  4.7279e-01,  9.1312e-01],\n",
      "         [ 7.3325e-01,  3.3956e-01,  5.7623e-01],\n",
      "         [-2.5899e-01, -1.3498e-01, -1.0249e+00],\n",
      "         [ 1.5410e-01,  2.8329e-01,  1.4503e+00]],\n",
      "\n",
      "        [[ 1.2651e+00,  3.2665e-01,  5.8276e-01],\n",
      "         [ 8.9173e-01, -1.1800e-01,  8.8539e-01],\n",
      "         [-7.7607e-01,  2.7372e-01, -2.7049e-01],\n",
      "         [ 1.4252e+00,  1.2690e-03,  3.4959e-01]]],\n",
      "       grad_fn=<TransposeBackward0>)\n",
      "\n",
      "Attention weights = \n",
      "torch.Size([1, 2, 4, 4]) tensor([[[[0.2092, 0.2307, 0.3898, 0.1703],\n",
      "          [0.2877, 0.2022, 0.3586, 0.1515],\n",
      "          [0.3273, 0.2208, 0.2190, 0.2329],\n",
      "          [0.3273, 0.2208, 0.2190, 0.2329]],\n",
      "\n",
      "         [[0.1425, 0.1007, 0.6732, 0.0836],\n",
      "          [0.2753, 0.1958, 0.3226, 0.2063],\n",
      "          [0.2408, 0.1816, 0.3624, 0.2151],\n",
      "          [0.2408, 0.1816, 0.3624, 0.2151]]]], grad_fn=<SoftmaxBackward0>)\n",
      "Cross attention in decoder_1\n",
      "tensor([[ 0.1073,  0.1729,  0.1715, -0.1333,  0.2190,  0.0193],\n",
      "        [ 0.0964,  0.1992,  0.2315,  0.5665,  0.1554,  0.3186],\n",
      "        [ 0.1612,  0.2661,  0.5395,  0.4920,  0.1567,  0.2783],\n",
      "        [ 0.1612,  0.2661,  0.5395,  0.4920,  0.1567,  0.2783]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "\n",
      "norm2(x' + mha(x', mem)), \n",
      " where x' = Decoder_curr_layer norm1(x + sa(x))\n",
      "tensor([[[-2.0072,  0.4303,  0.3046,  1.2800,  0.1618, -0.1695]],\n",
      "\n",
      "        [[-1.2220,  1.2535,  1.4748, -0.5105, -0.4549, -0.5409]],\n",
      "\n",
      "        [[-0.1629,  0.7569,  1.2682,  0.2965, -1.9083, -0.2504]],\n",
      "\n",
      "        [[-0.1629,  0.7569,  1.2682,  0.2965, -1.9083, -0.2504]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "\n",
      "norm3(x'' + ff(x'')) \n",
      " where, x'' = Decoder_curr_layer norm2(x' + mha(x'))\n",
      "tensor([[[-1.9210,  0.5572,  0.1643,  1.3773,  0.0834, -0.2612]],\n",
      "\n",
      "        [[-1.1794,  1.3299,  1.3943, -0.2781, -0.5425, -0.7242]],\n",
      "\n",
      "        [[-0.1347,  0.8070,  1.1641,  0.4986, -1.8748, -0.4603]],\n",
      "\n",
      "        [[-0.1347,  0.8070,  1.1641,  0.4986, -1.8748, -0.4603]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "\n",
      "torch.Size([4, 1, 6]) torch.Size([4, 1, 6])\n",
      "torch.Size([1, 4, 1, 6]) torch.Size([1, 4, 1, 6]) torch.Size([1, 4, 1, 6])\n",
      "4 2 3\n",
      "Q_dec_2 = \n",
      "tensor([[[ 0.5941, -0.1705,  0.5284],\n",
      "         [-0.6461,  0.4200,  1.0642],\n",
      "         [-0.7124, -0.2093,  0.1047],\n",
      "         [-0.7124, -0.2093,  0.1047]],\n",
      "\n",
      "        [[ 0.3753,  0.6452, -0.1056],\n",
      "         [ 1.3662,  0.0807,  0.3409],\n",
      "         [ 0.8027, -0.7164, -0.6526],\n",
      "         [ 0.8027, -0.7164, -0.6526]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "K_dec_2 = \n",
      "tensor([[[ 0.4683, -0.1347, -0.9457],\n",
      "         [-0.0412, -0.1545, -0.1991],\n",
      "         [-0.6865,  0.1744, -0.5111],\n",
      "         [-0.6865,  0.1744, -0.5111]],\n",
      "\n",
      "        [[-0.8799, -0.2084, -0.3137],\n",
      "         [-0.8236, -0.9536, -0.6278],\n",
      "         [-0.1793, -0.6342,  0.4552],\n",
      "         [-0.1793, -0.6342,  0.4552]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "V_dec_2 = \n",
      "tensor([[[-1.3309, -0.6459, -0.1294],\n",
      "         [-0.5010,  0.4829,  0.1706],\n",
      "         [-0.2801, -0.0192, -0.1772],\n",
      "         [-0.2801, -0.0192, -0.1772]],\n",
      "\n",
      "        [[-0.1401, -0.9291, -0.0915],\n",
      "         [-0.4107,  0.3162, -1.1544],\n",
      "         [-0.5083,  0.2837, -1.3440],\n",
      "         [-0.5083,  0.2837, -1.3440]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "Attention weights = \n",
      "torch.Size([1, 2, 4, 4]) tensor([[[[0.2819, 0.2978, 0.2101, 0.2101],\n",
      "          [0.1382, 0.2632, 0.2993, 0.2993],\n",
      "          [0.1827, 0.2362, 0.2905, 0.2905],\n",
      "          [0.1827, 0.2362, 0.2905, 0.2905]],\n",
      "\n",
      "         [[0.2719, 0.2126, 0.2577, 0.2577],\n",
      "          [0.1691, 0.1605, 0.3352, 0.3352],\n",
      "          [0.1983, 0.3118, 0.2449, 0.2449],\n",
      "          [0.1983, 0.3118, 0.2449, 0.2449]]]], grad_fn=<SoftmaxBackward0>)\n",
      "Decoder Self Attention = \n",
      "tensor([[-0.6421, -0.0463, -0.0601, -0.3874, -0.0392, -0.9631],\n",
      "        [-0.4835,  0.0263, -0.0791, -0.4303,  0.0839, -1.1018],\n",
      "        [-0.5243, -0.0151, -0.0863, -0.4048,  0.0533, -1.0365],\n",
      "        [-0.5243, -0.0151, -0.0863, -0.4048,  0.0533, -1.0365]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "\n",
      "Decoder_2 norm1(x + sa(x))\n",
      "tensor([[[-1.4245,  0.6878,  0.2578,  1.6326, -0.3530, -0.8007]],\n",
      "\n",
      "        [[-0.5062,  1.2931,  1.3652, -0.0586, -0.9657, -1.1277]],\n",
      "\n",
      "        [[ 0.3833,  0.6907,  0.9932,  0.5571, -1.8760, -0.7482]],\n",
      "\n",
      "        [[ 0.3833,  0.6907,  0.9932,  0.5571, -1.8760, -0.7482]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "\n",
      "Q_dec_2 = \n",
      "tensor([[[-1.6276e+00, -1.7025e-01,  7.3439e-01],\n",
      "         [-7.5509e-01,  1.3663e-03,  1.0863e+00],\n",
      "         [-3.1950e-01,  3.0669e-02, -9.8045e-03],\n",
      "         [-3.1950e-01,  3.0669e-02, -9.8045e-03]],\n",
      "\n",
      "        [[-7.6756e-01, -3.2277e-01, -1.4383e-01],\n",
      "         [-1.5444e+00, -1.8857e-01,  6.4777e-01],\n",
      "         [-1.2919e+00, -2.1074e-01,  1.0605e+00],\n",
      "         [-1.2919e+00, -2.1074e-01,  1.0605e+00]]],\n",
      "       grad_fn=<TransposeBackward0>)\n",
      "\n",
      "K_dec_2 = \n",
      "tensor([[[-0.6834, -0.5823,  0.4507],\n",
      "         [-0.6421, -0.0546, -0.2358],\n",
      "         [ 1.3053, -0.1177,  0.2241],\n",
      "         [-1.4741, -0.4167,  0.3985]],\n",
      "\n",
      "        [[-0.0166, -1.3817, -0.1756],\n",
      "         [-0.7292, -1.1708, -0.6771],\n",
      "         [ 1.4036, -0.4212, -0.6506],\n",
      "         [-1.0448, -0.5296,  0.6559]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "V_dec_2 = \n",
      "tensor([[[ 1.1678, -0.4077, -0.9678],\n",
      "         [ 0.0193, -0.1389, -0.2204],\n",
      "         [ 0.0616,  1.1591,  0.5875],\n",
      "         [ 0.8653, -0.7580, -0.8965]],\n",
      "\n",
      "        [[-0.3686, -0.6365,  0.0449],\n",
      "         [-0.7646, -0.7369, -0.2845],\n",
      "         [ 0.1118,  0.7294,  0.7683],\n",
      "         [-0.1984, -0.7455, -0.6358]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "Attention weights = \n",
      "torch.Size([1, 2, 4, 4]) tensor([[[[0.2604, 0.1778, 0.0349, 0.5269],\n",
      "          [0.2968, 0.1896, 0.1082, 0.4055],\n",
      "          [0.2587, 0.2601, 0.1810, 0.3003],\n",
      "          [0.2587, 0.2601, 0.1810, 0.3003]],\n",
      "\n",
      "         [[0.2443, 0.3358, 0.1132, 0.3068],\n",
      "          [0.1708, 0.2613, 0.0363, 0.5315],\n",
      "          [0.1761, 0.2148, 0.0406, 0.5686],\n",
      "          [0.1761, 0.2148, 0.0406, 0.5686]]]], grad_fn=<SoftmaxBackward0>)\n",
      "Cross attention in decoder_2\n",
      "tensor([[ 0.7656, -0.4898, -0.7431, -0.3950, -0.5490, -0.1926],\n",
      "        [ 0.7077, -0.3292, -0.6289, -0.3642, -0.6711, -0.3767],\n",
      "        [ 0.5780, -0.1594, -0.4706, -0.3374, -0.6646, -0.3835],\n",
      "        [ 0.5780, -0.1594, -0.4706, -0.3374, -0.6646, -0.3835]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "\n",
      "norm2(x' + mha(x', mem)), \n",
      " where x' = Decoder_curr_layer norm1(x + sa(x))\n",
      "tensor([[[-1.1305,  0.1514, -0.0687,  1.9161,  0.1340, -1.0024]],\n",
      "\n",
      "        [[-0.3292,  0.6692,  1.2940,  0.6506, -0.5356, -1.7490]],\n",
      "\n",
      "        [[ 0.5165,  0.0494,  0.9650,  1.1221, -1.4294, -1.2236]],\n",
      "\n",
      "        [[ 0.5165,  0.0494,  0.9650,  1.1221, -1.4294, -1.2236]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "\n",
      "norm3(x'' + ff(x'')) \n",
      " where, x'' = Decoder_curr_layer norm2(x' + mha(x'))\n",
      "tensor([[[-1.3369,  0.4242,  0.2732,  1.5165,  0.3595, -1.2365]],\n",
      "\n",
      "        [[-0.4143,  0.8070,  1.3914,  0.2149, -0.2259, -1.7731]],\n",
      "\n",
      "        [[ 0.3225,  0.2839,  1.2815,  0.7723, -1.1908, -1.4692]],\n",
      "\n",
      "        [[ 0.3225,  0.2839,  1.2815,  0.7723, -1.1908, -1.4692]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "\n",
      "torch.Size([4, 1, 6]) torch.Size([4, 1, 6])\n",
      "### Decoder Done ###\n"
     ]
    }
   ],
   "source": [
    "d_model = 6\n",
    "num_encoder_layers = 3\n",
    "num_decoder_layers = 3\n",
    "\n",
    "final_op = get_all_intermediate_outputs(src_sentence, tgt_sentence, model = model, num_encoder_layers = num_encoder_layers , num_decoder_layers = num_encoder_layers, d_model=d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 1, 10]), torch.Size([4, 1, 10]))"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape, final_op.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.2080, -0.2101, -0.3444, -0.4398,  0.0395,  0.8219, -0.1725,\n",
       "           0.7168, -0.5166, -0.1150]],\n",
       "\n",
       "        [[-0.5421,  0.0616, -0.9904,  0.5963,  0.2021,  1.1265,  0.1808,\n",
       "           0.8478, -0.8467,  0.4492]],\n",
       "\n",
       "        [[-0.8845, -0.1459, -1.2355,  0.2183,  0.2404,  0.4411, -0.5962,\n",
       "           0.2322, -0.0240,  0.6122]],\n",
       "\n",
       "        [[-0.8845, -0.1459, -1.2355,  0.2183,  0.2404,  0.4411, -0.5962,\n",
       "           0.2322, -0.0240,  0.6122]]], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.2080, -0.2101, -0.3444, -0.4398,  0.0395,  0.8219, -0.1725,\n",
       "           0.7168, -0.5166, -0.1150]],\n",
       "\n",
       "        [[-0.5421,  0.0616, -0.9904,  0.5963,  0.2021,  1.1265,  0.1808,\n",
       "           0.8478, -0.8467,  0.4492]],\n",
       "\n",
       "        [[-0.8845, -0.1459, -1.2354,  0.2183,  0.2404,  0.4411, -0.5962,\n",
       "           0.2322, -0.0240,  0.6122]],\n",
       "\n",
       "        [[-0.8845, -0.1459, -1.2354,  0.2183,  0.2404,  0.4411, -0.5962,\n",
       "           0.2322, -0.0240,  0.6122]]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "my_project_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
