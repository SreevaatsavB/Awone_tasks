{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-head attention transformer\n",
    "### Encoder and Decoder\n",
    "### (With masking)\n",
    "\n",
    "Pytorch's implementation (in built)\n",
    "\n",
    "NOTE :- A new exmple must be used for testing the masked attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source embeddings :- \n",
      "\n",
      "tensor([[[-0.1344,  0.5592, -1.1815,  0.4653, -0.7154, -1.0503]],\n",
      "\n",
      "        [[ 0.0315,  0.4340, -0.7013,  2.0887, -0.2541,  1.3491]],\n",
      "\n",
      "        [[-0.8416, -0.0752, -0.0664, -0.7628,  0.1541,  1.2462]],\n",
      "\n",
      "        [[-1.3622, -0.6211, -0.8772,  0.4195, -0.0854,  0.3847]]],\n",
      "       grad_fn=<EmbeddingBackward0>)\n",
      "\n",
      "Target embeddings :- \n",
      "\n",
      "tensor([[[ 0.5611,  0.5838,  1.1292,  0.8900,  1.5531, -1.4184]],\n",
      "\n",
      "        [[-0.5187, -0.1657, -0.2895, -0.3737, -0.3584,  1.3122]],\n",
      "\n",
      "        [[ 0.0646,  1.1269, -0.5113,  0.5007,  1.5989,  0.3129]],\n",
      "\n",
      "        [[ 0.0646,  1.1269, -0.5113,  0.5007,  1.5989,  0.3129]]],\n",
      "       grad_fn=<EmbeddingBackward0>)\n",
      "\n",
      "Positional encoded source embeddings:\n",
      "tensor([[[-0.1344,  1.5592, -1.1815,  1.4653, -0.7154, -0.0503]],\n",
      "\n",
      "        [[ 0.0315,  1.4340, -0.7013,  3.0887, -0.2541,  2.3491]],\n",
      "\n",
      "        [[-0.8416,  0.9248, -0.0664,  0.2372,  0.1541,  2.2462]],\n",
      "\n",
      "        [[-1.3622,  0.3789, -0.8772,  1.4195, -0.0854,  1.3847]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "\n",
      "Positional encoded target embeddings:\n",
      "tensor([[[ 0.5611,  1.5838,  1.1292,  1.8900,  1.5531, -0.4184]],\n",
      "\n",
      "        [[-0.5187,  0.8343, -0.2895,  0.6263, -0.3584,  2.3122]],\n",
      "\n",
      "        [[ 0.0646,  2.1269, -0.5113,  1.5007,  1.5989,  1.3129]],\n",
      "\n",
      "        [[ 0.0646,  2.1269, -0.5113,  1.5007,  1.5989,  1.3129]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "\n",
      "query =  tensor([[[-0.1344,  1.5592, -1.1815,  1.4653, -0.7154, -0.0503]],\n",
      "\n",
      "        [[ 0.0315,  1.4340, -0.7013,  3.0887, -0.2541,  2.3491]],\n",
      "\n",
      "        [[-0.8416,  0.9248, -0.0664,  0.2372,  0.1541,  2.2462]],\n",
      "\n",
      "        [[-1.3622,  0.3789, -0.8772,  1.4195, -0.0854,  1.3847]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "query shape =  torch.Size([4, 1, 6])\n",
      "\n",
      " _in_projection_packed \n",
      "\n",
      "Q :-  tensor([[[-0.7205,  0.8394, -1.2265, -0.1023,  0.5319,  0.1854]],\n",
      "\n",
      "        [[ 0.0944,  1.7884, -0.8154, -0.0114,  0.0831,  0.6531]],\n",
      "\n",
      "        [[ 0.3803,  1.0893,  0.3105, -0.4294,  0.6794,  1.1543]],\n",
      "\n",
      "        [[ 0.2034,  1.1417, -0.0092,  0.2220,  0.7552,  0.1356]]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "\n",
      "K :-  tensor([[[-0.6167, -0.2793,  0.0149,  0.3259,  1.1248,  0.0105]],\n",
      "\n",
      "        [[ 1.3568, -0.3901, -0.2702,  1.6123,  1.7022,  0.0453]],\n",
      "\n",
      "        [[ 0.6660, -0.4651,  0.7754,  1.1180,  1.7583, -0.8162]],\n",
      "\n",
      "        [[ 0.9486, -0.1334,  0.4902,  0.9340,  1.5642,  0.3434]]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "\n",
      "V :-  tensor([[[ 0.1706, -0.3731, -0.7106, -1.5414, -1.2804,  0.9048]],\n",
      "\n",
      "        [[ 0.9077, -1.3990, -0.6533, -1.3778, -2.2945,  2.0395]],\n",
      "\n",
      "        [[ 0.5089, -0.4129,  0.0070,  0.0209, -1.3055,  0.5223]],\n",
      "\n",
      "        [[ 0.3045, -0.4677, -0.2549, -0.8220, -1.4117,  1.0460]]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "\n",
      "tgt_len =  4\n",
      "bsz =  1\n",
      "num_heads =  2\n",
      "head_dim =  3\n",
      "\n",
      "tensor([[[-0.7205,  0.8394, -1.2265, -0.1023,  0.5319,  0.1854]],\n",
      "\n",
      "        [[ 0.0944,  1.7884, -0.8154, -0.0114,  0.0831,  0.6531]],\n",
      "\n",
      "        [[ 0.3803,  1.0893,  0.3105, -0.4294,  0.6794,  1.1543]],\n",
      "\n",
      "        [[ 0.2034,  1.1417, -0.0092,  0.2220,  0.7552,  0.1356]]],\n",
      "       grad_fn=<SelectBackward0>) torch.Size([4, 1, 6])\n",
      "Q reshaped =  tensor([[[-0.7205,  0.8394, -1.2265],\n",
      "         [ 0.0944,  1.7884, -0.8154],\n",
      "         [ 0.3803,  1.0893,  0.3105],\n",
      "         [ 0.2034,  1.1417, -0.0092]],\n",
      "\n",
      "        [[-0.1023,  0.5319,  0.1854],\n",
      "         [-0.0114,  0.0831,  0.6531],\n",
      "         [-0.4294,  0.6794,  1.1543],\n",
      "         [ 0.2220,  0.7552,  0.1356]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "K reshaped =  tensor([[[-0.6167, -0.2793,  0.0149],\n",
      "         [ 1.3568, -0.3901, -0.2702],\n",
      "         [ 0.6660, -0.4651,  0.7754],\n",
      "         [ 0.9486, -0.1334,  0.4902]],\n",
      "\n",
      "        [[ 0.3259,  1.1248,  0.0105],\n",
      "         [ 1.6123,  1.7022,  0.0453],\n",
      "         [ 1.1180,  1.7583, -0.8162],\n",
      "         [ 0.9340,  1.5642,  0.3434]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "V reshaped =  tensor([[[ 0.1706, -0.3731, -0.7106],\n",
      "         [ 0.9077, -1.3990, -0.6533],\n",
      "         [ 0.5089, -0.4129,  0.0070],\n",
      "         [ 0.3045, -0.4677, -0.2549]],\n",
      "\n",
      "        [[-1.5414, -1.2804,  0.9048],\n",
      "         [-1.3778, -2.2945,  2.0395],\n",
      "         [ 0.0209, -1.3055,  0.5223],\n",
      "         [-0.8220, -1.4117,  1.0460]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "torch.Size([1, 2, 4, 3]) torch.Size([1, 2, 4, 3]) torch.Size([1, 2, 4, 3])\n",
      "\n",
      "scaled_dot_product_attention\n",
      "attn_output = \n",
      "tensor([[ 0.4115, -0.6312, -0.5145, -0.9245, -1.5824,  1.1402],\n",
      "        [ 0.4844, -0.7144, -0.4530, -0.9898, -1.5897,  1.1709],\n",
      "        [ 0.4835, -0.6721, -0.3745, -1.0123, -1.5762,  1.1686],\n",
      "        [ 0.4779, -0.6763, -0.3999, -0.9007, -1.6155,  1.1709]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "\n",
      "x + self_atten(x) = \n",
      "tensor([[[ 0.9441,  1.7432,  0.0380,  0.1424, -2.1655,  0.8998]],\n",
      "\n",
      "        [[ 1.1120,  1.6072,  0.5939,  1.7424, -1.6928,  3.1871]],\n",
      "\n",
      "        [[ 0.2907,  1.1377,  1.2204, -1.0880, -1.2836,  3.0293]],\n",
      "\n",
      "        [[-0.2793,  0.6459,  0.4309,  0.1466, -1.4906,  2.2831]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "\n",
      "norm1(x + self_atten(x)) = \n",
      "tensor([[[ 0.5521,  1.2038, -0.1867, -0.1016, -1.9836,  0.5160]],\n",
      "\n",
      "        [[ 0.0138,  0.3492, -0.3371,  0.4407, -1.8857,  1.4191]],\n",
      "\n",
      "        [[-0.1766,  0.3977,  0.4538, -1.1112, -1.2438,  1.6801]],\n",
      "\n",
      "        [[-0.5042,  0.3160,  0.1254, -0.1266, -1.5780,  1.7674]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "\n",
      "feed_fwd_op =  tensor([[[-0.0937,  0.4022,  0.1802,  0.0744,  0.6893,  0.1399]],\n",
      "\n",
      "        [[-0.1401,  0.3539,  0.1887,  0.0665,  0.6582,  0.2014]],\n",
      "\n",
      "        [[-0.1244,  0.4259,  0.1348,  0.0648,  0.5799,  0.2387]],\n",
      "\n",
      "        [[-0.1338,  0.3700,  0.1528,  0.0562,  0.6039,  0.2368]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "\n",
      "norm2(norm1(x + self_atten(x)) + feed_fwd_op) = \n",
      "tensor([[[ 0.2594,  1.5738, -0.2733, -0.2970, -1.7484,  0.4855]],\n",
      "\n",
      "        [[-0.3960,  0.5484, -0.4211,  0.3254, -1.6497,  1.5930]],\n",
      "\n",
      "        [[-0.5191,  0.6015,  0.3674, -1.2619, -0.8807,  1.6928]],\n",
      "\n",
      "        [[-0.8783,  0.4861,  0.0658, -0.2935, -1.2247,  1.8445]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "\n",
      "##################################\n",
      "\n",
      "################### Transformer Decoder start #####################\n",
      "\n",
      "query =  tensor([[[ 0.5611,  1.5838,  1.1292,  1.8900,  1.5531, -0.4184]],\n",
      "\n",
      "        [[-0.5187,  0.8343, -0.2895,  0.6263, -0.3584,  2.3122]],\n",
      "\n",
      "        [[ 0.0646,  2.1269, -0.5113,  1.5007,  1.5989,  1.3129]],\n",
      "\n",
      "        [[ 0.0646,  2.1269, -0.5113,  1.5007,  1.5989,  1.3129]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "query shape =  torch.Size([4, 1, 6])\n",
      "\n",
      " _in_projection_packed \n",
      "\n",
      "Q :-  tensor([[[-1.5232,  0.6053,  0.0973,  0.9852, -1.2133,  0.6450]],\n",
      "\n",
      "        [[-1.3771,  0.0798,  0.8412, -0.3394,  0.5405,  0.8201]],\n",
      "\n",
      "        [[-2.3907,  0.1429,  0.2497,  0.1710, -0.7554,  0.9578]],\n",
      "\n",
      "        [[-2.3907,  0.1429,  0.2497,  0.1710, -0.7554,  0.9578]]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "\n",
      "K :-  tensor([[[ 0.2481,  0.0367, -1.0680, -0.9174, -0.9733, -0.6989]],\n",
      "\n",
      "        [[ 1.0136, -0.5203, -0.0541,  1.2305,  0.1586, -0.1118]],\n",
      "\n",
      "        [[ 1.0379, -0.2288, -1.2677,  0.6141, -0.3169, -0.5412]],\n",
      "\n",
      "        [[ 1.0379, -0.2288, -1.2677,  0.6141, -0.3169, -0.5412]]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "\n",
      "V :-  tensor([[[-0.2468,  0.7665,  1.3806,  0.6172,  0.1710, -0.3901]],\n",
      "\n",
      "        [[ 0.6520,  1.2991,  0.1176,  0.5168,  1.0014,  0.0482]],\n",
      "\n",
      "        [[-0.1219,  2.1069,  1.2360,  0.9632,  0.8988,  0.1064]],\n",
      "\n",
      "        [[-0.1219,  2.1069,  1.2360,  0.9632,  0.8988,  0.1064]]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "\n",
      "tgt_len =  4\n",
      "bsz =  1\n",
      "num_heads =  2\n",
      "head_dim =  3\n",
      "\n",
      "tensor([[[-1.5232,  0.6053,  0.0973,  0.9852, -1.2133,  0.6450]],\n",
      "\n",
      "        [[-1.3771,  0.0798,  0.8412, -0.3394,  0.5405,  0.8201]],\n",
      "\n",
      "        [[-2.3907,  0.1429,  0.2497,  0.1710, -0.7554,  0.9578]],\n",
      "\n",
      "        [[-2.3907,  0.1429,  0.2497,  0.1710, -0.7554,  0.9578]]],\n",
      "       grad_fn=<SelectBackward0>) torch.Size([4, 1, 6])\n",
      "Q reshaped =  tensor([[[-1.5232,  0.6053,  0.0973],\n",
      "         [-1.3771,  0.0798,  0.8412],\n",
      "         [-2.3907,  0.1429,  0.2497],\n",
      "         [-2.3907,  0.1429,  0.2497]],\n",
      "\n",
      "        [[ 0.9852, -1.2133,  0.6450],\n",
      "         [-0.3394,  0.5405,  0.8201],\n",
      "         [ 0.1710, -0.7554,  0.9578],\n",
      "         [ 0.1710, -0.7554,  0.9578]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "K reshaped =  tensor([[[ 0.2481,  0.0367, -1.0680],\n",
      "         [ 1.0136, -0.5203, -0.0541],\n",
      "         [ 1.0379, -0.2288, -1.2677],\n",
      "         [ 1.0379, -0.2288, -1.2677]],\n",
      "\n",
      "        [[-0.9174, -0.9733, -0.6989],\n",
      "         [ 1.2305,  0.1586, -0.1118],\n",
      "         [ 0.6141, -0.3169, -0.5412],\n",
      "         [ 0.6141, -0.3169, -0.5412]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "V reshaped =  tensor([[[-0.2468,  0.7665,  1.3806],\n",
      "         [ 0.6520,  1.2991,  0.1176],\n",
      "         [-0.1219,  2.1069,  1.2360],\n",
      "         [-0.1219,  2.1069,  1.2360]],\n",
      "\n",
      "        [[ 0.6172,  0.1710, -0.3901],\n",
      "         [ 0.5168,  1.0014,  0.0482],\n",
      "         [ 0.9632,  0.8988,  0.1064],\n",
      "         [ 0.9632,  0.8988,  0.1064]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "torch.Size([1, 2, 4, 3]) torch.Size([1, 2, 4, 3]) torch.Size([1, 2, 4, 3])\n",
      "\n",
      "scaled_dot_product_attention\n",
      "attn_output = \n",
      "tensor([[-0.0285,  1.3820,  1.0856,  0.7670,  0.8118,  0.0069],\n",
      "        [ 0.0716,  1.3842,  0.9436,  0.7493,  0.7554, -0.0291],\n",
      "        [-0.0366,  1.2911,  1.0950,  0.7576,  0.7417, -0.0347],\n",
      "        [-0.0366,  1.2911,  1.0950,  0.7576,  0.7417, -0.0347]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "\n",
      "x =  tensor([[[ 0.5611,  1.5838,  1.1292,  1.8900,  1.5531, -0.4184]],\n",
      "\n",
      "        [[-0.5187,  0.8343, -0.2895,  0.6263, -0.3584,  2.3122]],\n",
      "\n",
      "        [[ 0.0646,  2.1269, -0.5113,  1.5007,  1.5989,  1.3129]],\n",
      "\n",
      "        [[ 0.0646,  2.1269, -0.5113,  1.5007,  1.5989,  1.3129]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "\n",
      "sa_op =  tensor([[[ 0.4614,  0.5021,  1.5303, -0.4597, -1.4669,  0.7289]],\n",
      "\n",
      "        [[ 0.4314,  0.4641,  1.3604, -0.4626, -1.4211,  0.7002]],\n",
      "\n",
      "        [[ 0.4716,  0.4735,  1.4909, -0.3833, -1.4329,  0.6967]],\n",
      "\n",
      "        [[ 0.4716,  0.4735,  1.4909, -0.3833, -1.4329,  0.6967]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "\n",
      "temop =  tensor([[[ 1.0224,  2.0859,  2.6596,  1.4303,  0.0862,  0.3104]],\n",
      "\n",
      "        [[-0.0873,  1.2984,  1.0708,  0.1636, -1.7795,  3.0124]],\n",
      "\n",
      "        [[ 0.5362,  2.6004,  0.9796,  1.1174,  0.1660,  2.0097]],\n",
      "\n",
      "        [[ 0.5362,  2.6004,  0.9796,  1.1174,  0.1660,  2.0097]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "\n",
      "Decoder norm1(x + sa(x)) = \n",
      "tensor([[[-0.2664,  0.8979,  1.5259,  0.1801, -1.2915, -1.0459]],\n",
      "\n",
      "        [[-0.4785,  0.4683,  0.3127, -0.3071, -1.6347,  1.6393]],\n",
      "\n",
      "        [[-0.8376,  1.6371, -0.3061, -0.1408, -1.2814,  0.9289]],\n",
      "\n",
      "        [[-0.8376,  1.6371, -0.3061, -0.1408, -1.2814,  0.9289]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "\n",
      "Memory = \n",
      "tensor([[[ 0.2594,  1.5738, -0.2733, -0.2970, -1.7484,  0.4855]],\n",
      "\n",
      "        [[-0.3960,  0.5484, -0.4211,  0.3254, -1.6497,  1.5930]],\n",
      "\n",
      "        [[-0.5191,  0.6015,  0.3674, -1.2619, -0.8807,  1.6928]],\n",
      "\n",
      "        [[-0.8783,  0.4861,  0.0658, -0.2935, -1.2247,  1.8445]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "\n",
      "query =  tensor([[[-0.2664,  0.8979,  1.5259,  0.1801, -1.2915, -1.0459]],\n",
      "\n",
      "        [[-0.4785,  0.4683,  0.3127, -0.3071, -1.6347,  1.6393]],\n",
      "\n",
      "        [[-0.8376,  1.6371, -0.3061, -0.1408, -1.2814,  0.9289]],\n",
      "\n",
      "        [[-0.8376,  1.6371, -0.3061, -0.1408, -1.2814,  0.9289]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "query shape =  torch.Size([4, 1, 6])\n",
      "\n",
      " _in_projection_packed \n",
      "\n",
      "Q :-  tensor([[[-0.8250, -0.5931, -0.4025, -0.7654, -0.8175,  0.6158]],\n",
      "\n",
      "        [[-0.0903,  0.5603,  0.7427,  0.5967,  0.2556, -0.2416]],\n",
      "\n",
      "        [[ 0.0511, -0.2605, -0.1479,  0.7425, -0.3892,  0.5577]],\n",
      "\n",
      "        [[ 0.0511, -0.2605, -0.1479,  0.7425, -0.3892,  0.5577]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "\n",
      "K :-  tensor([[[-0.0112,  1.4857, -1.1612,  1.0297, -1.4764, -0.4470]],\n",
      "\n",
      "        [[-0.4279,  0.9543, -0.8157,  1.2456, -1.7020, -0.3399]],\n",
      "\n",
      "        [[-0.4221,  0.6399, -0.6253,  0.5936, -1.1203, -0.1688]],\n",
      "\n",
      "        [[-0.4290,  0.5838, -0.5780,  1.1306, -1.4338, -0.1289]]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "\n",
      "V :-  tensor([[[ 0.4800,  0.5421, -0.1278,  0.8969,  0.6116,  0.3663]],\n",
      "\n",
      "        [[ 0.1234,  0.9803, -0.7109,  0.9777, -0.2162,  1.2567]],\n",
      "\n",
      "        [[ 0.0316,  0.9253,  0.1574,  0.1571, -0.6704,  0.5209]],\n",
      "\n",
      "        [[ 0.0551,  0.8494, -0.4480,  0.5234, -0.6350,  0.9909]]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "\n",
      "tgt_len =  4\n",
      "bsz =  1\n",
      "num_heads =  2\n",
      "head_dim =  3\n",
      "\n",
      "tensor([[[-0.8250, -0.5931, -0.4025, -0.7654, -0.8175,  0.6158]],\n",
      "\n",
      "        [[-0.0903,  0.5603,  0.7427,  0.5967,  0.2556, -0.2416]],\n",
      "\n",
      "        [[ 0.0511, -0.2605, -0.1479,  0.7425, -0.3892,  0.5577]],\n",
      "\n",
      "        [[ 0.0511, -0.2605, -0.1479,  0.7425, -0.3892,  0.5577]]],\n",
      "       grad_fn=<ViewBackward0>) torch.Size([4, 1, 6])\n",
      "Q reshaped =  tensor([[[-0.8250, -0.5931, -0.4025],\n",
      "         [-0.0903,  0.5603,  0.7427],\n",
      "         [ 0.0511, -0.2605, -0.1479],\n",
      "         [ 0.0511, -0.2605, -0.1479]],\n",
      "\n",
      "        [[-0.7654, -0.8175,  0.6158],\n",
      "         [ 0.5967,  0.2556, -0.2416],\n",
      "         [ 0.7425, -0.3892,  0.5577],\n",
      "         [ 0.7425, -0.3892,  0.5577]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "K reshaped =  tensor([[[-0.0112,  1.4857, -1.1612],\n",
      "         [-0.4279,  0.9543, -0.8157],\n",
      "         [-0.4221,  0.6399, -0.6253],\n",
      "         [-0.4290,  0.5838, -0.5780]],\n",
      "\n",
      "        [[ 1.0297, -1.4764, -0.4470],\n",
      "         [ 1.2456, -1.7020, -0.3399],\n",
      "         [ 0.5936, -1.1203, -0.1688],\n",
      "         [ 1.1306, -1.4338, -0.1289]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "V reshaped =  tensor([[[ 0.4800,  0.5421, -0.1278],\n",
      "         [ 0.1234,  0.9803, -0.7109],\n",
      "         [ 0.0316,  0.9253,  0.1574],\n",
      "         [ 0.0551,  0.8494, -0.4480]],\n",
      "\n",
      "        [[ 0.8969,  0.6116,  0.3663],\n",
      "         [ 0.9777, -0.2162,  1.2567],\n",
      "         [ 0.1571, -0.6704,  0.5209],\n",
      "         [ 0.5234, -0.6350,  0.9909]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "torch.Size([1, 2, 4, 3]) torch.Size([1, 2, 4, 3]) torch.Size([1, 2, 4, 3])\n",
      "\n",
      "scaled_dot_product_attention\n",
      "attn_output = \n",
      "tensor([[ 0.1474,  0.8452, -0.2888,  0.6265, -0.2459,  0.7836],\n",
      "        [ 0.1739,  0.8235, -0.2838,  0.6570, -0.2126,  0.7953],\n",
      "        [ 0.1675,  0.8279, -0.2818,  0.6693, -0.2239,  0.8232],\n",
      "        [ 0.1675,  0.8279, -0.2818,  0.6693, -0.2239,  0.8232]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "\n",
      "Decoder norm2(x + mha(x, mem)) = \n",
      "tensor([[[-0.3695,  0.9990,  1.6560, -0.2832, -1.1398, -0.8624]],\n",
      "\n",
      "        [[-0.6123,  0.8346,  1.0848, -0.6791, -1.5577,  0.9296]],\n",
      "\n",
      "        [[-0.8573,  1.6522,  0.6363, -0.5546, -1.2864,  0.4097]],\n",
      "\n",
      "        [[-0.8573,  1.6522,  0.6363, -0.5546, -1.2864,  0.4097]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "\n",
      "Decoder norm3(x + ff(x)) = \n",
      "tensor([[[-0.0660,  1.2602,  1.3936, -0.4797, -1.1353, -0.9728]],\n",
      "\n",
      "        [[-0.2781,  1.1315,  0.7439, -0.9239, -1.5632,  0.8898]],\n",
      "\n",
      "        [[-0.4928,  1.8664,  0.3272, -0.7470, -1.2364,  0.2826]],\n",
      "\n",
      "        [[-0.4928,  1.8664,  0.3272, -0.7470, -1.2364,  0.2826]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "\n",
      "tensor([[[ 0.1671,  0.6496,  0.3877, -0.1723, -1.2654,  0.9748, -0.3107,\n",
      "           0.1239,  0.4715, -1.3378]],\n",
      "\n",
      "        [[-0.2422,  0.6775,  1.1204, -0.6104, -1.0052,  0.3941,  0.0118,\n",
      "           0.5823,  0.6551, -1.2290]],\n",
      "\n",
      "        [[-0.3014,  0.9889,  0.4547, -0.9121, -0.9980,  0.1475, -0.2022,\n",
      "           0.6603,  1.0222, -1.2369]],\n",
      "\n",
      "        [[-0.3014,  0.9889,  0.4547, -0.9121, -0.9980,  0.1475, -0.2022,\n",
      "           0.6603,  1.0222, -1.2369]]], grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=512, dropout=0):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.encoding = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1).float()\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(torch.log(torch.tensor(10000.0)) / d_model))\n",
    "        self.encoding[:, 0::2] = torch.sin(position * div_term)\n",
    "        self.encoding[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.encoding = self.encoding.unsqueeze(0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.encoding[:, :x.size(1)].detach()\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, src_vocab_size, tgt_vocab_size,max_seq_len, d_model=4, nhead=2, num_encoder_layers=1, num_decoder_layers=1):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.src_embedding = nn.Embedding(src_vocab_size, d_model)\n",
    "        self.tgt_embedding = nn.Embedding(tgt_vocab_size, d_model)\n",
    "        self.positional_encoding = PositionalEncoding(d_model, dropout=0, max_len=max_seq_len)\n",
    "        self.transformer = nn.Transformer(\n",
    "            d_model=d_model,\n",
    "            nhead=nhead,\n",
    "            num_encoder_layers=num_encoder_layers,\n",
    "            num_decoder_layers=num_decoder_layers,\n",
    "            dropout=0\n",
    "        )\n",
    "        self.fc = nn.Linear(d_model, tgt_vocab_size)\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "\n",
    "        print(\"Source embeddings :- \\n\")\n",
    "        print(self.src_embedding(src))\n",
    "        print()\n",
    "\n",
    "        print(\"Target embeddings :- \\n\")\n",
    "        print(self.tgt_embedding(tgt))\n",
    "        print()\n",
    "\n",
    "\n",
    "        src = self.src_embedding(src) + self.positional_encoding(src)\n",
    "        tgt = self.tgt_embedding(tgt) + self.positional_encoding(tgt)\n",
    "\n",
    "        print(\"Positional encoded source embeddings:\")\n",
    "        print(src)\n",
    "        print()\n",
    "\n",
    "        print(\"Positional encoded target embeddings:\")\n",
    "        print(tgt)\n",
    "        print()\n",
    "\n",
    "        output = self.transformer(src, tgt)\n",
    "        output = self.fc(output)\n",
    "        \n",
    "        return output\n",
    "\n",
    "    \n",
    "\n",
    "src_vocab_size = 10  # Source language vocabulary size\n",
    "tgt_vocab_size = 10  # Target language vocabulary size\n",
    "d_model = 6  # Dimension of the model\n",
    "num_heads = 2\n",
    "max_seq_len = 4\n",
    "model = TransformerModel(src_vocab_size, tgt_vocab_size, d_model=d_model, max_seq_len = max_seq_len, nhead=num_heads)\n",
    "\n",
    "# Source sentence in the source language\n",
    "# Source token indexes from src vocabulary\n",
    "src_sentence = torch.tensor([[0], [1], [2], [3]]) \n",
    "\n",
    "\n",
    "\n",
    "# Target sentence in the target language (translation of the source sentence)\n",
    "# Target token indexes from tgt vocabulary\n",
    "tgt_sentence = torch.tensor([[1], [0], [3], [3]])\n",
    "\n",
    "# Forward pass\n",
    "output = model(src_sentence, tgt_sentence)\n",
    "print(output)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions to get the manual calulations for each component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import functional as F\n",
    "\n",
    "# Using the state dictionary to get the intermediate outputs\n",
    "state_dict = model.state_dict()\n",
    "\n",
    "need_weights = False\n",
    "\n",
    "src_mask = None\n",
    "\n",
    "tgt_mask = None\n",
    "\n",
    "memory_mask = None\n",
    "\n",
    "embed_dim = 6\n",
    "\n",
    "num_heads = 2\n",
    "\n",
    "max_seq_len = 4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def look_up_table(sentence, vocab_embeds, embedding):\n",
    "\n",
    "    for i in range(sentence.size(0)):\n",
    "        for j in range(sentence.size(1)):\n",
    "            # Get the index for the current word token index in the sequence\n",
    "            word_index = sentence[i, j].item()\n",
    "\n",
    "            if word_index < 0 or word_index >= vocab_embeds.size(0):\n",
    "                raise ValueError(f\"Invalid word index: {word_index}\")\n",
    "\n",
    "            # Lookup the corresponding embedding vector for the word\n",
    "            embedding[i, j, :] = vocab_embeds[word_index, :]\n",
    "\n",
    "            print(f\"Word index: {word_index}, Embedding: {vocab_embeds[word_index, :]}\")\n",
    "    print()\n",
    "    \n",
    "\n",
    "    return embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embeddings and Positional encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding_outputs(src_sentence,  tgt_sentence, max_seq_len, state_dict, d_model):\n",
    "\n",
    "    src_vocab_embeds = state_dict[\"src_embedding.weight\"]\n",
    "\n",
    "    src_embedding = torch.zeros(src_sentence.size(0), src_sentence.size(1), d_model)\n",
    "    print(\"Source sentence embedding\")\n",
    "    src_embedding =  look_up_table(src_sentence, src_vocab_embeds, src_embedding)\n",
    "    print(src_embedding.shape)\n",
    "\n",
    "    tgt_vocab_embeds = state_dict[\"tgt_embedding.weight\"]\n",
    "\n",
    "    tgt_embedding = torch.zeros(tgt_sentence.size(0), tgt_sentence.size(1), d_model)\n",
    "\n",
    "\n",
    "    print(\"Target sentence embedding\")\n",
    "    tgt_embedding =  look_up_table(tgt_sentence, tgt_vocab_embeds, tgt_embedding)\n",
    "\n",
    "    pe = PositionalEncoding(d_model = d_model, dropout=0, max_len=max_seq_len)\n",
    "\n",
    "    print(\"PE of src :\")\n",
    "    print(pe(src_embedding))\n",
    "    print()\n",
    "    print(\"PE of tgt :\")\n",
    "    print(pe(tgt_embedding))\n",
    "    print()\n",
    "\n",
    "    pe_src_embeds = src_embedding + pe(src_embedding)\n",
    "\n",
    "    pe_tgt_embeds = tgt_embedding + pe(tgt_embedding)\n",
    "\n",
    "    print(\"PE source embeddings : \\n\")\n",
    "    print(pe_src_embeds)\n",
    "    print()\n",
    "\n",
    "    print(\"PE target embeddings : \\n\")\n",
    "    print(pe_tgt_embeds)\n",
    "    print()\n",
    "\n",
    "    return pe_src_embeds, pe_tgt_embeds\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source sentence embedding\n",
      "Word index: 0, Embedding: tensor([-0.1344,  0.5592, -1.1815,  0.4653, -0.7154, -1.0503])\n",
      "Word index: 1, Embedding: tensor([ 0.0315,  0.4340, -0.7013,  2.0887, -0.2541,  1.3491])\n",
      "Word index: 2, Embedding: tensor([-0.8416, -0.0752, -0.0664, -0.7628,  0.1541,  1.2462])\n",
      "Word index: 3, Embedding: tensor([-1.3622, -0.6211, -0.8772,  0.4195, -0.0854,  0.3847])\n",
      "\n",
      "torch.Size([4, 1, 6])\n",
      "Target sentence embedding\n",
      "Word index: 1, Embedding: tensor([ 0.5611,  0.5838,  1.1292,  0.8900,  1.5531, -1.4184])\n",
      "Word index: 0, Embedding: tensor([-0.5187, -0.1657, -0.2895, -0.3737, -0.3584,  1.3122])\n",
      "Word index: 3, Embedding: tensor([ 0.0646,  1.1269, -0.5113,  0.5007,  1.5989,  0.3129])\n",
      "Word index: 3, Embedding: tensor([ 0.0646,  1.1269, -0.5113,  0.5007,  1.5989,  0.3129])\n",
      "\n",
      "PE of src :\n",
      "tensor([[[0., 1., 0., 1., 0., 1.]]])\n",
      "\n",
      "PE of tgt :\n",
      "tensor([[[0., 1., 0., 1., 0., 1.]]])\n",
      "\n",
      "PE source embeddings : \n",
      "\n",
      "tensor([[[-0.1344,  1.5592, -1.1815,  1.4653, -0.7154, -0.0503]],\n",
      "\n",
      "        [[ 0.0315,  1.4340, -0.7013,  3.0887, -0.2541,  2.3491]],\n",
      "\n",
      "        [[-0.8416,  0.9248, -0.0664,  0.2372,  0.1541,  2.2462]],\n",
      "\n",
      "        [[-1.3622,  0.3789, -0.8772,  1.4195, -0.0854,  1.3847]]])\n",
      "\n",
      "PE target embeddings : \n",
      "\n",
      "tensor([[[ 0.5611,  1.5838,  1.1292,  1.8900,  1.5531, -0.4184]],\n",
      "\n",
      "        [[-0.5187,  0.8343, -0.2895,  0.6263, -0.3584,  2.3122]],\n",
      "\n",
      "        [[ 0.0646,  2.1269, -0.5113,  1.5007,  1.5989,  1.3129]],\n",
      "\n",
      "        [[ 0.0646,  2.1269, -0.5113,  1.5007,  1.5989,  1.3129]]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pe_src_embeds, pe_tgt_embeds = get_embedding_outputs(src_sentence=src_sentence,  tgt_sentence=tgt_sentence, state_dict=state_dict, max_seq_len=max_seq_len, d_model = d_model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder function to display the intermediate outputs and get the final outputs from the encoder\n",
    "\n",
    "### Self attention "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def atten_product_needs_wts_false(Q, V, K, bsz, head_dim, src_len, tgt_len, attn_mask):\n",
    "\n",
    "\n",
    "    # *** For multi-head attention ***\n",
    "    #  (bsz*num_heads, src_len , head_dim) -> (bsz, num_heads, tgt_len, head_dim)\n",
    "    Q1 = Q.view(bsz, num_heads, tgt_len, head_dim)\n",
    "    K1 = K.view(bsz, num_heads, src_len, head_dim)\n",
    "    V1 = V.view(bsz, num_heads, src_len, head_dim)\n",
    "\n",
    "    L, S = Q1.size(-2), K1.size(-2)\n",
    "\n",
    "    scale_factor = 1 / math.sqrt(Q1.size(-1)) \n",
    "    # scale_factor = 1\n",
    "    attn_bias = torch.zeros(L, S, dtype=Q1.dtype)\n",
    "\n",
    "    if attn_mask is not None:\n",
    "        if attn_mask.dtype == torch.bool:\n",
    "            attn_mask.masked_fill_(attn_mask.logical_not(), float(\"-inf\"))\n",
    "        else:\n",
    "            attn_bias += attn_mask\n",
    "\n",
    "    # (bsz, num_heads, tgt_len, head_dim) @ (bsz, num_heads, head_dim, tgt_len) -> (bsz, num_heads, tgt_len, tgt_len) \n",
    "    attn_weight = Q1 @ K1.transpose(-2, -1) * scale_factor\n",
    "    attn_weight += attn_bias\n",
    "\n",
    "    # (bsz, num_heads, tgt_len, tgt_len) \n",
    "    attn_weight = torch.softmax(attn_weight, dim=-1)\n",
    "\n",
    "    print(\"Attention weights = \")\n",
    "    print(attn_weight.shape, attn_weight)\n",
    "\n",
    "    # (bsz, num_heads, tgt_len, tgt_len) @ (bsz, num_heads, tgt_len, head_dim) -> (bsz, num_heads, tgt_len, head_dim) \n",
    "    attn_output = attn_weight @ V1\n",
    "\n",
    "    # (bsz*tgt_len, embed_dim)\n",
    "    attn_output = attn_output.permute(2, 0, 1, 3).contiguous().view(bsz * tgt_len, embed_dim)\n",
    "\n",
    "    return attn_output\n",
    "\n",
    "\n",
    "\n",
    "def atten_product_needs_wts_true(Q, K, V, bsz, tgt_len, attn_mask):\n",
    "\n",
    "    # *** For multi-head attention ***\n",
    "    #  (bsz*num_heads, src_len , head_dim)\n",
    "    B, Nt, E = Q.shape\n",
    "\n",
    "    Q_scaled = Q / math.sqrt(E)\n",
    "\n",
    "    if attn_mask is not None:\n",
    "        temp_pdt_matrix = torch.baddbmm(attn_mask, Q_scaled, K.transpose(-2, -1))\n",
    "    else:\n",
    "        temp_pdt_matrix = torch.bmm(Q_scaled, K.transpose(-2, -1))\n",
    "\n",
    "    attn_wt_matrix = torch.nn.functional.softmax(temp_pdt_matrix, dim=-1)\n",
    "\n",
    "    attn_output = torch.bmm(attn_wt_matrix, V)\n",
    "\n",
    "    attn_output = attn_output.transpose(0, 1).contiguous().view(tgt_len * bsz, embed_dim)\n",
    "\n",
    "    print(\"Encoder Attention output = \")\n",
    "    print(attn_output)\n",
    "    print()\n",
    "\n",
    "    return attn_output, attn_wt_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_qkv(query, key, value ,W, b):\n",
    "\n",
    "    # embed_dim\n",
    "    E = query.size(-1)\n",
    "\n",
    "    if key is value:\n",
    "        if query is key:\n",
    "            \n",
    "            # (src_len, bsz, embed_dim) @ (embed_dim*num_heads, embed_dim).T -> (src_len, bsz, embed_dim*num_heads)\n",
    "            tempop1 = query@W.T\n",
    "\n",
    "            # reshape to 3, E and not E, 3 is deliberate for better memory coalescing and keeping same order as chunk()\n",
    "            tempop1 = tempop1.unflatten(-1, (3, E)).unsqueeze(0).transpose(0, -2).squeeze(-2).contiguous()\n",
    "\n",
    "            # (src_len, bsz, embed_dim)\n",
    "            return tempop1[0], tempop1[1], tempop1[2]\n",
    "        \n",
    "\n",
    "        else:\n",
    "\n",
    "            # (embed_dim*1, embed_dim)\n",
    "            # (embed_dim*2, embed_dim)\n",
    "            W_q, W_kv = W.split([E, E * 2])\n",
    "\n",
    "            if b is None:\n",
    "                b_q = b_kv = None\n",
    "            else:\n",
    "                b_q, b_kv = b.split([E, E * 2])\n",
    "\n",
    "            # (src_len, bsz, embed_dim) @ (embed_dim*1, embed_dim).T -> (src_len, bsz, embed_dim)\n",
    "            q_matmul = query@W_q.T\n",
    "\n",
    "            # (src_len, bsz, embed_dim) @ (embed_dim*2, embed_dim).T -> (src_len, bsz, embed_dim*2)\n",
    "            kv_matmul = key@W_kv.T\n",
    "\n",
    "            kv_matmul = kv_matmul.unflatten(-1, (2, E)).unsqueeze(0).transpose(0, -2).squeeze(-2).contiguous()\n",
    "\n",
    "            # (src_len, bsz, embed_dim)\n",
    "            return q_matmul, kv_matmul[0], kv_matmul[1]\n",
    "\n",
    "    else:\n",
    "\n",
    "        W_q, W_k, W_v = W.chunk(3)\n",
    "        if b is None:\n",
    "            b_q = b_k = b_v = None\n",
    "        else:\n",
    "            b_q, b_k, b_v = b.chunk(3)\n",
    "\n",
    "\n",
    "        q_matmul = query@W_q.T\n",
    "        k_matmul = key@W_k.T\n",
    "        v_matmul = value@W_v.T\n",
    "\n",
    "        return q_matmul, k_matmul, v_matmul\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder_block_attn_output(x, state_dict, layer_num, embed_dim, num_heads, need_weights = False, src_mask = None):\n",
    "\n",
    "    # (src_len, bsz, embed_dim)\n",
    "    query_enc = key_enc = value_enc = x\n",
    "\n",
    "    tgt_len, bsz, embed_dim = x.shape\n",
    "\n",
    "    # print(x.shape)\n",
    "    \n",
    "    # (embed_dim*num_heads, embed_dim)\n",
    "    W_enc = state_dict[\"transformer.encoder.layers.{}.self_attn.in_proj_weight\".format(layer_num)]\n",
    "    b_enc = state_dict[\"transformer.encoder.layers.{}.self_attn.in_proj_bias\".format(layer_num)]\n",
    "\n",
    "\n",
    "    head_dim = embed_dim//num_heads\n",
    "    \n",
    "    # (src_len, bsz, embed_dim)\n",
    "    Q_enc,K_enc,V_enc = get_qkv(query_enc, key_enc, value_enc ,W_enc, b_enc)\n",
    "\n",
    "    # (1, src_len, bsz, embed_dim)\n",
    "    Q_enc = Q_enc.unsqueeze(0)\n",
    "    K_enc = K_enc.unsqueeze(0)\n",
    "    V_enc = V_enc.unsqueeze(0)\n",
    "\n",
    "    # (1, src_len, bsz, embed_dim) -> ( bsz*num_heads, src_len , head_dim)\n",
    "    Q_enc = Q_enc.reshape(tgt_len, bsz * num_heads, head_dim).transpose(0, 1)\n",
    "    K_enc = K_enc.reshape(tgt_len, bsz * num_heads, head_dim).transpose(0, 1)\n",
    "    V_enc = V_enc.reshape(tgt_len, bsz * num_heads, head_dim).transpose(0, 1)\n",
    "\n",
    "    print(\"Q_enc_{} = \".format(layer_num))\n",
    "    print(Q_enc)\n",
    "    print()\n",
    "\n",
    "    print(\"K_enc_{} = \".format(layer_num))\n",
    "    print(K_enc)\n",
    "    print()\n",
    "\n",
    "    print(\"V_enc_{} = \".format(layer_num))\n",
    "    print(V_enc)\n",
    "    print()\n",
    "\n",
    "\n",
    "    src_len = K_enc.size(1)\n",
    "\n",
    "    attn_mask = src_mask\n",
    "    if attn_mask is not None:\n",
    "\n",
    "        # Ensuring attn_mask's dim is 3\n",
    "        if attn_mask.dim() == 2:\n",
    "            correct_2d_size = (tgt_len, src_len)\n",
    "            if attn_mask.shape != correct_2d_size:\n",
    "                raise RuntimeError(f\"The shape of the 2D attn_mask is {attn_mask.shape}, but should be {correct_2d_size}.\")\n",
    "            attn_mask = attn_mask.unsqueeze(0)\n",
    "        elif attn_mask.dim() == 3:\n",
    "            correct_3d_size = (bsz * num_heads, tgt_len, src_len)\n",
    "            if attn_mask.shape != correct_3d_size:\n",
    "                raise RuntimeError(f\"The shape of the 3D attn_mask is {attn_mask.shape}, but should be {correct_3d_size}.\")\n",
    "        else:\n",
    "            raise RuntimeError(f\"attn_mask's dimension {attn_mask.dim()} is not supported\")\n",
    "\n",
    "    # attn_mask can be either (L,S) or (N*num_heads, L, S)\n",
    "    # if attn_mask's shape is (1, L, S) we need to unsqueeze to (1, 1, L, S)\n",
    "    # in order to match the input for SDPA of (N, num_heads, L, S)\n",
    "    if attn_mask is not None:\n",
    "        if attn_mask.size(0) == 1 and attn_mask.dim() == 3:\n",
    "            attn_mask = attn_mask.unsqueeze(0)\n",
    "        else:\n",
    "            attn_mask = attn_mask.view(bsz, num_heads, -1, src_len)\n",
    "\n",
    "\n",
    "    if need_weights is False:\n",
    "\n",
    "        attn_output = atten_product_needs_wts_false(Q_enc, V_enc, K_enc, bsz, head_dim, src_len, tgt_len, attn_mask)\n",
    "\n",
    "        return attn_output, src_len, head_dim, None\n",
    "    \n",
    "    else:\n",
    "\n",
    "        attn_enc_output,attn_wt_matrix_enc = atten_product_needs_wts_true(Q_enc, K_enc, V_enc, bsz, tgt_len, attn_mask)\n",
    "\n",
    "        return attn_enc_output, src_len, head_dim, attn_wt_matrix_enc\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "torch.Size([4, 1, 6])\n",
      "torch.Size([18, 6])\n",
      "Q_enc_0 = \n",
      "torch.Size([2, 4, 3])\n",
      "tensor([[[-0.7205,  0.8394, -1.2265],\n",
      "         [ 0.0944,  1.7884, -0.8154],\n",
      "         [ 0.3803,  1.0893,  0.3105],\n",
      "         [ 0.2034,  1.1417, -0.0092]],\n",
      "\n",
      "        [[-0.1023,  0.5319,  0.1854],\n",
      "         [-0.0114,  0.0831,  0.6531],\n",
      "         [-0.4294,  0.6794,  1.1543],\n",
      "         [ 0.2220,  0.7552,  0.1356]]])\n",
      "\n",
      "K_enc_0 = \n",
      "tensor([[[-0.6167, -0.2793,  0.0149],\n",
      "         [ 1.3568, -0.3901, -0.2702],\n",
      "         [ 0.6660, -0.4651,  0.7754],\n",
      "         [ 0.9486, -0.1334,  0.4902]],\n",
      "\n",
      "        [[ 0.3259,  1.1248,  0.0105],\n",
      "         [ 1.6123,  1.7022,  0.0453],\n",
      "         [ 1.1180,  1.7583, -0.8162],\n",
      "         [ 0.9340,  1.5642,  0.3434]]])\n",
      "\n",
      "V_enc_0 = \n",
      "tensor([[[ 0.1706, -0.3731, -0.7106],\n",
      "         [ 0.9077, -1.3990, -0.6533],\n",
      "         [ 0.5089, -0.4129,  0.0070],\n",
      "         [ 0.3045, -0.4677, -0.2549]],\n",
      "\n",
      "        [[-1.5414, -1.2804,  0.9048],\n",
      "         [-1.3778, -2.2945,  2.0395],\n",
      "         [ 0.0209, -1.3055,  0.5223],\n",
      "         [-0.8220, -1.4117,  1.0460]]])\n",
      "\n",
      "SA\n",
      "torch.Size([2, 4, 3])\n",
      "torch.Size([1, 2, 4, 3])\n",
      "torch.Size([1, 2, 4, 3]) torch.Size([1, 2, 4, 3]) torch.Size([1, 2, 3, 4])\n",
      "Attention weights = \n",
      "torch.Size([1, 2, 4, 4]) tensor([[[[0.4499, 0.2296, 0.1407, 0.1798],\n",
      "          [0.2655, 0.3015, 0.1643, 0.2687],\n",
      "          [0.1851, 0.2531, 0.2502, 0.3116],\n",
      "          [0.2213, 0.2598, 0.2267, 0.2921]],\n",
      "\n",
      "         [[0.2317, 0.2574, 0.2458, 0.2651],\n",
      "          [0.2540, 0.2623, 0.1907, 0.2929],\n",
      "          [0.2621, 0.2445, 0.1591, 0.3343],\n",
      "          [0.1911, 0.2907, 0.2614, 0.2568]]]])\n"
     ]
    }
   ],
   "source": [
    "# need_weights = True\n",
    "\n",
    "attn_enc_output, src_len, head_dim, attn_weights = encoder_block_attn_output(pe_src_embeds, state_dict, layer_num = 0, need_weights = need_weights, embed_dim=embed_dim, num_heads=num_heads)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_dict[\"transformer.encoder.layers.{}.norm1.weight\".format(0)].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Post self attention in encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder_block_post_attn_output(x, attn_enc_output, state_dict, layer_num, bsz, tgt_len):\n",
    "\n",
    "    # (bsz*src_len , embed_dim) @ (embed_dim , embed_dim).T -> (bsz*src_len , embed_dim)\n",
    "    op_enc_1 = torch.matmul(attn_enc_output, state_dict[\"transformer.encoder.layers.{}.self_attn.out_proj.weight\".format(layer_num)].t()) + state_dict[\"transformer.encoder.layers.{}.self_attn.out_proj.bias\".format(layer_num)]\n",
    "\n",
    "\n",
    "\n",
    "    # (bsz*src_len , embed_dim) -> (src_len, bsz, embed_dim)\n",
    "    attn_enc_output = op_enc_1.view(tgt_len, bsz, attn_enc_output.size(1))\n",
    "\n",
    "\n",
    "    # Here src is the original passed inputs to the 1st transformer encoder layer which \n",
    "    # are pe_src_embeds \n",
    "\n",
    "    # (src_len, bsz, embed_dim)\n",
    "    output_enc_1 = attn_enc_output + x\n",
    "\n",
    "    #  (src_len, bsz, embed_dim) @ (embed_dim) -> (src_len, bsz, embed_dim) \n",
    "    linear_result_enc_1 = output_enc_1*state_dict[\"transformer.encoder.layers.{}.norm1.weight\".format(layer_num)] + state_dict[\"transformer.encoder.layers.{}.norm1.bias\".format(layer_num)]\n",
    "\n",
    "    # Layer normalization from Torch's implementation\n",
    "    layernorm_enc_1 = torch.nn.LayerNorm(normalized_shape=linear_result_enc_1.shape[2:])\n",
    "    linear_op_enc_1 = layernorm_enc_1(linear_result_enc_1)\n",
    "\n",
    "\n",
    "    # Manual Layer Normalization\n",
    "    x = linear_result_enc_1\n",
    "\n",
    "    # Obtained layer norm weights and biases (learnable)\n",
    "    w = layernorm_enc_1.weight\n",
    "    b = layernorm_enc_1.bias\n",
    "\n",
    "    linear_result_enc_1f = w*x + b\n",
    "\n",
    "    epsilon = 1e-5  \n",
    "    mean = linear_result_enc_1f.mean(dim=-1, keepdim=True)\n",
    "    std = linear_result_enc_1f.std(dim=-1, unbiased=False, keepdim=True)\n",
    "    normalized_result_enc_1 = (linear_result_enc_1f - mean) / (std + epsilon) * w + b\n",
    "\n",
    "\n",
    "    op_enc_1 = torch.matmul(normalized_result_enc_1, state_dict[\"transformer.encoder.layers.{}.linear1.weight\".format(layer_num)].t()) + state_dict[\"transformer.encoder.layers.{}.linear1.bias\".format(layer_num)]\n",
    "    op_enc_1_relu = torch.nn.functional.relu(op_enc_1)\n",
    "    op_enc_2 = torch.matmul(op_enc_1_relu, state_dict[\"transformer.encoder.layers.{}.linear2.weight\".format(layer_num)].t()) + state_dict[\"transformer.encoder.layers.{}.linear2.bias\".format(layer_num)]\n",
    "\n",
    "\n",
    "    output_enc_2 = op_enc_2 + linear_op_enc_1\n",
    "    output_enc_2_norm = output_enc_2*state_dict[\"transformer.encoder.layers.{}.norm2.weight\".format(layer_num)] + state_dict[\"transformer.encoder.layers.{}.norm2.bias\".format(layer_num)]\n",
    "\n",
    "    # Layer normalization from Torch's implementation\n",
    "    layernorm_enc_final = torch.nn.LayerNorm(normalized_shape=output_enc_2_norm.shape[2:])\n",
    "    output_enc_final = layernorm_enc_final(output_enc_2_norm)\n",
    "\n",
    "\n",
    "    # Manual Layer Normalization \n",
    "    x = output_enc_2_norm\n",
    "\n",
    "    # Obtained layer norm weights and biases (learnable)\n",
    "    w = layernorm_enc_final.weight\n",
    "    b = layernorm_enc_final.bias\n",
    "\n",
    "    linear_result_enc_2 = w*x + b\n",
    "\n",
    "    epsilon = 1e-5  \n",
    "    mean = linear_result_enc_2.mean(dim=-1, keepdim=True)\n",
    "    std = linear_result_enc_2.std(dim=-1, unbiased=False, keepdim=True)\n",
    "    output_enc_final = (linear_result_enc_2 - mean) / (std + epsilon) * w + b\n",
    "\n",
    "    print(\"Final Encoder {} Output :\".format(layer_num))\n",
    "    print(\"norm2(norm1(x + self_atten(x)) + feed_fwd_op)\\n\")\n",
    "    print(output_enc_final)\n",
    "    print()\n",
    "\n",
    "    # (src_len, bsz, embed_dim) \n",
    "    return output_enc_final\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Encoder 0 Output :\n",
      "norm2(norm1(x + self_atten(x)) + feed_fwd_op)\n",
      "\n",
      "tensor([[[ 0.2594,  1.5738, -0.2733, -0.2970, -1.7484,  0.4855]],\n",
      "\n",
      "        [[-0.3960,  0.5484, -0.4211,  0.3254, -1.6497,  1.5930]],\n",
      "\n",
      "        [[-0.5191,  0.6015,  0.3674, -1.2619, -0.8807,  1.6928]],\n",
      "\n",
      "        [[-0.8783,  0.4861,  0.0658, -0.2935, -1.2247,  1.8444]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "torch.Size([4, 1, 6])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tgt_len, bsz, embed_dim = pe_src_embeds.shape\n",
    "output_enc_final = encoder_block_post_attn_output(pe_src_embeds, attn_enc_output, state_dict, layer_num = 0 , bsz = bsz, tgt_len = tgt_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder functions to display the intermediate outputs and get the final outputs from the decoder\n",
    "\n",
    "\n",
    "### Self attention outputs from a decoder block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoder_block_self_attn_output(x, state_dict, layer_num, tgt_mask = None,need_weights = False):\n",
    "\n",
    "    # (tgt_len, bsz, embed_dim)\n",
    "    query_dec = key_dec = value_dec = x\n",
    "\n",
    "    tgt_len, bsz, embed_dim = x.shape\n",
    " \n",
    "    # (embed_dim*num_heads, embed_dim)\n",
    "    W_dec = state_dict[\"transformer.decoder.layers.{}.self_attn.in_proj_weight\".format(layer_num)]\n",
    "    b_dec = state_dict[\"transformer.decoder.layers.{}.self_attn.in_proj_bias\".format(layer_num)]\n",
    "\n",
    "\n",
    "    head_dim = embed_dim//num_heads\n",
    "\n",
    "    # (tgt_len, bsz, embed_dim)\n",
    "    Q_dec,K_dec,V_dec = get_qkv(query_dec, key_dec, value_dec ,W_dec, b_dec)\n",
    "    \n",
    "    Q_dec = Q_dec.unsqueeze(0)\n",
    "    K_dec = K_dec.unsqueeze(0)\n",
    "    V_dec = V_dec.unsqueeze(0)\n",
    "\n",
    "    # (1, tgt_len, bsz, embed_dim)\n",
    "    print(Q_dec.shape, K_dec.shape , V_dec.shape)\n",
    "    print(tgt_len, bsz * num_heads, head_dim)\n",
    "\n",
    "    # (1, tgt_len, bsz, embed_dim) -> ( bsz*num_heads, tgt_len , head_dim )\n",
    "    Q_dec = Q_dec.reshape(tgt_len, bsz * num_heads, head_dim).transpose(0, 1)\n",
    "    K_dec = K_dec.reshape(tgt_len, bsz * num_heads, head_dim).transpose(0, 1)\n",
    "    V_dec = V_dec.reshape(tgt_len, bsz * num_heads, head_dim).transpose(0, 1)\n",
    "\n",
    "    print(\"Q_dec_{} = \".format(layer_num))\n",
    "    print(Q_dec)\n",
    "    print()\n",
    "\n",
    "    print(\"K_dec_{} = \".format(layer_num))\n",
    "    print(K_dec)\n",
    "    print()\n",
    "\n",
    "    print(\"V_dec_{} = \".format(layer_num))\n",
    "    print(V_dec)\n",
    "    print()\n",
    "\n",
    "    src_len = K_dec.size(1)\n",
    "\n",
    "\n",
    "    attn_mask = tgt_mask\n",
    "    if attn_mask is not None:\n",
    "\n",
    "        # Ensuring attn_mask's dim is 3\n",
    "        if attn_mask.dim() == 2:\n",
    "            correct_2d_size = (tgt_len, src_len)\n",
    "            if attn_mask.shape != correct_2d_size:\n",
    "                raise RuntimeError(f\"The shape of the 2D attn_mask is {attn_mask.shape}, but should be {correct_2d_size}.\")\n",
    "            attn_mask = attn_mask.unsqueeze(0)\n",
    "        elif attn_mask.dim() == 3:\n",
    "            correct_3d_size = (bsz * num_heads, tgt_len, src_len)\n",
    "            if attn_mask.shape != correct_3d_size:\n",
    "                raise RuntimeError(f\"The shape of the 3D attn_mask is {attn_mask.shape}, but should be {correct_3d_size}.\")\n",
    "        else:\n",
    "            raise RuntimeError(f\"attn_mask's dimension {attn_mask.dim()} is not supported\")\n",
    "\n",
    "    # attn_mask can be either (L,S) or (N*num_heads, L, S)\n",
    "    # if attn_mask's shape is (1, L, S) we need to unsqueeze to (1, 1, L, S)\n",
    "    # in order to match the input for SDPA of (N, num_heads, L, S)\n",
    "    if attn_mask is not None:\n",
    "        if attn_mask.size(0) == 1 and attn_mask.dim() == 3:\n",
    "            attn_mask = attn_mask.unsqueeze(0)\n",
    "        else:\n",
    "            attn_mask = attn_mask.view(bsz, num_heads, -1, src_len)\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    if need_weights is False:\n",
    "        attn_output = atten_product_needs_wts_false(Q_dec, V_dec, K_dec, bsz, head_dim, src_len, tgt_len, attn_mask)\n",
    "\n",
    "        print(\"Decoder Self Attention = \")\n",
    "        print(attn_output)\n",
    "        print()\n",
    "\n",
    "        op_dec_1 = torch.matmul(attn_output, state_dict[\"transformer.decoder.layers.{}.self_attn.out_proj.weight\".format(layer_num)].t()) + state_dict[\"transformer.decoder.layers.{}.self_attn.out_proj.bias\".format(layer_num)]\n",
    "        attn_dec_output = op_dec_1.view(tgt_len, bsz, attn_output.size(1))\n",
    "\n",
    "        return attn_dec_output, None\n",
    "    \n",
    "    else:\n",
    "\n",
    "        attn_dec_output,attn_wt_matrix_dec = atten_product_needs_wts_true(Q_dec, K_dec, V_dec, bsz, tgt_len, attn_mask)\n",
    "\n",
    "        print(\"Decoder Attention output = \")\n",
    "        print(attn_wt_matrix_dec)\n",
    "        print()\n",
    "\n",
    "        op_dec_1 = torch.matmul(attn_dec_output, state_dict[\"transformer.decoder.layers.{}.self_attn.out_proj.weight\".format(layer_num)].t()) + state_dict[\"transformer.decoder.layers.{}.self_attn.out_proj.bias\".format(layer_num)]\n",
    "        attn_dec_output = op_dec_1.view(tgt_len, bsz, attn_dec_output.size(1))\n",
    "    \n",
    "\n",
    "        return attn_dec_output, attn_wt_matrix_dec\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "need_weights "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "torch.Size([4, 1, 6])\n",
      "torch.Size([18, 6])\n",
      "torch.Size([1, 4, 1, 6]) torch.Size([1, 4, 1, 6]) torch.Size([1, 4, 1, 6])\n",
      "4 2 3\n",
      "Q_dec_0 = \n",
      "tensor([[[-1.5232,  0.6053,  0.0973],\n",
      "         [-1.3771,  0.0798,  0.8412],\n",
      "         [-2.3907,  0.1429,  0.2497],\n",
      "         [-2.3907,  0.1429,  0.2497]],\n",
      "\n",
      "        [[ 0.9852, -1.2133,  0.6450],\n",
      "         [-0.3394,  0.5405,  0.8201],\n",
      "         [ 0.1710, -0.7554,  0.9578],\n",
      "         [ 0.1710, -0.7554,  0.9578]]])\n",
      "\n",
      "K_dec_0 = \n",
      "tensor([[[ 0.2481,  0.0367, -1.0680],\n",
      "         [ 1.0136, -0.5203, -0.0541],\n",
      "         [ 1.0379, -0.2288, -1.2677],\n",
      "         [ 1.0379, -0.2288, -1.2677]],\n",
      "\n",
      "        [[-0.9174, -0.9733, -0.6989],\n",
      "         [ 1.2305,  0.1586, -0.1118],\n",
      "         [ 0.6141, -0.3169, -0.5412],\n",
      "         [ 0.6141, -0.3169, -0.5412]]])\n",
      "\n",
      "V_dec_0 = \n",
      "tensor([[[-0.2468,  0.7665,  1.3806],\n",
      "         [ 0.6520,  1.2991,  0.1176],\n",
      "         [-0.1219,  2.1069,  1.2360],\n",
      "         [-0.1219,  2.1069,  1.2360]],\n",
      "\n",
      "        [[ 0.6172,  0.1710, -0.3901],\n",
      "         [ 0.5168,  1.0014,  0.0482],\n",
      "         [ 0.9632,  0.8988,  0.1064],\n",
      "         [ 0.9632,  0.8988,  0.1064]]])\n",
      "\n",
      "Attention weights = \n",
      "torch.Size([1, 2, 4, 4]) tensor([[[[0.4265, 0.1896, 0.1919, 0.1919],\n",
      "          [0.3540, 0.3072, 0.1694, 0.1694],\n",
      "          [0.4942, 0.1899, 0.1579, 0.1579],\n",
      "          [0.4942, 0.1899, 0.1579, 0.1579]],\n",
      "\n",
      "         [[0.1637, 0.3127, 0.2618, 0.2618],\n",
      "          [0.2385, 0.2943, 0.2336, 0.2336],\n",
      "          [0.2531, 0.2643, 0.2413, 0.2413],\n",
      "          [0.2531, 0.2643, 0.2413, 0.2413]]]])\n",
      "Decoder Self Attention = \n",
      "tensor([[-0.0285,  1.3820,  1.0856,  0.7670,  0.8118,  0.0069],\n",
      "        [ 0.0716,  1.3842,  0.9436,  0.7493,  0.7554, -0.0291],\n",
      "        [-0.0366,  1.2911,  1.0950,  0.7576,  0.7417, -0.0347],\n",
      "        [-0.0366,  1.2911,  1.0950,  0.7576,  0.7417, -0.0347]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# need_weights = True\n",
    "\n",
    "self_attn_dec, dec_sa_wts = decoder_block_self_attn_output(pe_tgt_embeds, state_dict, layer_num = 0, need_weights = need_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.4614,  0.5021,  1.5303, -0.4597, -1.4669,  0.7289]],\n",
       "\n",
       "        [[ 0.4314,  0.4641,  1.3604, -0.4626, -1.4211,  0.7002]],\n",
       "\n",
       "        [[ 0.4716,  0.4735,  1.4909, -0.3833, -1.4329,  0.6967]],\n",
       "\n",
       "        [[ 0.4716,  0.4735,  1.4909, -0.3833, -1.4329,  0.6967]]])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "self_attn_dec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dec_post_self_attn(self_attn_dec, x, state_dict, layer_num):\n",
    "\n",
    "    # (bsz*tgt_len , embed_dim) @ (embed_dim , embed_dim).T -> (bsz*tgt_len , embed_dim)\n",
    "    output_dec_1 = self_attn_dec + x\n",
    "\n",
    "    # (bsz*tgt_len , embed_dim) @ (embed_dim , embed_dim).T -> (bsz*tgt_len , embed_dim)\n",
    "    linear_result_dec_1 = output_dec_1*state_dict[\"transformer.decoder.layers.{}.norm1.weight\".format(layer_num)] + state_dict[\"transformer.decoder.layers.{}.norm1.bias\".format(layer_num)]\n",
    "\n",
    "    layernorm_dec_1 = torch.nn.LayerNorm(normalized_shape=linear_result_dec_1.shape[2:])\n",
    "    linear_op_dec_1 = layernorm_dec_1(linear_result_dec_1)\n",
    "\n",
    "    # From torch's implementation\n",
    "    # print(linear_op_dec_1)\n",
    "\n",
    "    x = linear_result_dec_1\n",
    "\n",
    "    # Obtained layer norm weights and biases (learnable)\n",
    "    w = layernorm_dec_1.weight\n",
    "    b = layernorm_dec_1.bias\n",
    "\n",
    "    linear_result_dec_1f = w*x + b\n",
    "\n",
    "    epsilon = 1e-5  \n",
    "    mean = linear_result_dec_1f.mean(dim=-1, keepdim=True)\n",
    "    std = linear_result_dec_1f.std(dim=-1, unbiased=False, keepdim=True)\n",
    "    normalized_result_dec_1 = (linear_result_dec_1f - mean) / (std + epsilon) * w + b\n",
    "\n",
    "\n",
    "    print(\"Decoder_{} norm1(x + sa(x))\".format(layer_num))\n",
    "    print(normalized_result_dec_1)\n",
    "    print()\n",
    "\n",
    "    # (tgt_len, bsz, embed_dim)\n",
    "    return normalized_result_dec_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoder_0 norm1(x + sa(x))\n",
      "tensor([[[-0.2664,  0.8979,  1.5259,  0.1801, -1.2915, -1.0459]],\n",
      "\n",
      "        [[-0.4785,  0.4683,  0.3127, -0.3071, -1.6347,  1.6393]],\n",
      "\n",
      "        [[-0.8376,  1.6371, -0.3061, -0.1408, -1.2814,  0.9289]],\n",
      "\n",
      "        [[-0.8376,  1.6371, -0.3061, -0.1408, -1.2814,  0.9289]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x_dec = dec_post_self_attn(self_attn_dec, pe_tgt_embeds, state_dict, layer_num = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross attention in decoder \n",
    "### query, key, value =  x, mem, mem \n",
    "\n",
    "### Cross attention between the encoder's final output and the decoder layer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.2664,  0.8979,  1.5259,  0.1801, -1.2915, -1.0459]],\n",
       "\n",
       "        [[-0.4785,  0.4683,  0.3127, -0.3071, -1.6347,  1.6393]],\n",
       "\n",
       "        [[-0.8376,  1.6371, -0.3061, -0.1408, -1.2814,  0.9289]],\n",
       "\n",
       "        [[-0.8376,  1.6371, -0.3061, -0.1408, -1.2814,  0.9289]]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "memory = output_enc_final\n",
    "x_dec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 1, 6])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoder_block_cross_attn_output(x_dec, memory, state_dict, layer_num, tgt_len, src_len, head_dim, memory_mask = None,need_weights = False):\n",
    "\n",
    "    # (tgt_len, bsz, embed_dim)\n",
    "    query_dec_mha = x_dec\n",
    "\n",
    "    # (src_len, bsz, embed_dim)\n",
    "    key_dec_mha, value_dec_mha = memory, memory\n",
    "\n",
    "\n",
    "    tgt_len, bsz, embed_dim = query_dec_mha.shape\n",
    "\n",
    "\n",
    "    W_dec_mha = state_dict[\"transformer.decoder.layers.{}.multihead_attn.in_proj_weight\".format(layer_num)]\n",
    "    b_dec_mha = state_dict[\"transformer.decoder.layers.{}.multihead_attn.in_proj_bias\".format(layer_num)]\n",
    "\n",
    "\n",
    "    ########## CHANGE FROM SELF ATTENTION ############\n",
    "\n",
    "    Q_dec_mha,K_dec_mha,V_dec_mha = get_qkv(query_dec_mha, key_dec_mha, value_dec_mha ,W_dec_mha, b_dec_mha)\n",
    "\n",
    "\n",
    "    #################################################\n",
    "\n",
    "    K_dec_mha = K_dec_mha.unsqueeze(0)\n",
    "    V_dec_mha = V_dec_mha.unsqueeze(0)\n",
    "    Q_dec_mha = Q_dec_mha.unsqueeze(0)\n",
    "\n",
    "\n",
    "    Q_dec_mha = Q_dec_mha.reshape(tgt_len, bsz * num_heads, head_dim).transpose(0, 1)\n",
    "    K_dec_mha = K_dec_mha.reshape(tgt_len, bsz * num_heads, head_dim).transpose(0, 1)\n",
    "    V_dec_mha = V_dec_mha.reshape(tgt_len, bsz * num_heads, head_dim).transpose(0, 1)\n",
    "\n",
    "    print(\"Q_dec_{} = \".format(layer_num))\n",
    "    print(Q_dec_mha)\n",
    "    print()\n",
    "\n",
    "    print(\"K_dec_{} = \".format(layer_num))\n",
    "    print(K_dec_mha)\n",
    "    print()\n",
    "\n",
    "    print(\"V_dec_{} = \".format(layer_num))\n",
    "    print(V_dec_mha)\n",
    "    print()\n",
    "\n",
    "\n",
    "    attn_mask = memory_mask\n",
    "    if attn_mask is not None:\n",
    "\n",
    "        # Ensuring attn_mask's dim is 3\n",
    "        if attn_mask.dim() == 2:\n",
    "            correct_2d_size = (tgt_len, src_len)\n",
    "            if attn_mask.shape != correct_2d_size:\n",
    "                raise RuntimeError(f\"The shape of the 2D attn_mask is {attn_mask.shape}, but should be {correct_2d_size}.\")\n",
    "            attn_mask = attn_mask.unsqueeze(0)\n",
    "        elif attn_mask.dim() == 3:\n",
    "            correct_3d_size = (bsz * num_heads, tgt_len, src_len)\n",
    "            if attn_mask.shape != correct_3d_size:\n",
    "                raise RuntimeError(f\"The shape of the 3D attn_mask is {attn_mask.shape}, but should be {correct_3d_size}.\")\n",
    "        else:\n",
    "            raise RuntimeError(f\"attn_mask's dimension {attn_mask.dim()} is not supported\")\n",
    "\n",
    "    # attn_mask can be either (L,S) or (N*num_heads, L, S)\n",
    "    # if attn_mask's shape is (1, L, S) we need to unsqueeze to (1, 1, L, S)\n",
    "    # in order to match the input for SDPA of (N, num_heads, L, S)\n",
    "    if attn_mask is not None:\n",
    "        if attn_mask.size(0) == 1 and attn_mask.dim() == 3:\n",
    "            attn_mask = attn_mask.unsqueeze(0)\n",
    "        else:\n",
    "            attn_mask = attn_mask.view(bsz, num_heads, -1, src_len)\n",
    "\n",
    "\n",
    "\n",
    "    if need_weights is False:\n",
    "\n",
    "        attn_output_dec_mha = atten_product_needs_wts_false(Q_dec_mha, V_dec_mha, K_dec_mha, bsz, head_dim, src_len, tgt_len, attn_mask)\n",
    "        \n",
    "        print(\"Cross attention in decoder_{}\".format(layer_num))\n",
    "        print(attn_output_dec_mha)\n",
    "        print()\n",
    "\n",
    "        op_dec_mha_1 = torch.matmul(attn_output_dec_mha, state_dict[\"transformer.decoder.layers.{}.multihead_attn.out_proj.weight\".format(layer_num)].t()) + state_dict[\"transformer.decoder.layers.{}.multihead_attn.out_proj.bias\".format(layer_num)]\n",
    "\n",
    "        attn_dec_mha_output = op_dec_mha_1.view(tgt_len, bsz, attn_output_dec_mha.size(1))\n",
    "\n",
    "        return attn_dec_mha_output, None\n",
    "    \n",
    "    else: \n",
    "\n",
    "    \n",
    "        attn_dec_mha_output ,attn_wt_matrix_dec_mha = atten_product_needs_wts_true(Q_dec_mha, K_dec_mha, V_dec_mha, bsz, tgt_len, attn_mask)\n",
    "\n",
    "        print(\"Decoder mha Attention output = \")\n",
    "        print(attn_dec_mha_output)\n",
    "        print()\n",
    "\n",
    "\n",
    "        op_dec_mha = torch.matmul(attn_dec_mha_output, state_dict[\"transformer.decoder.layers.{}.multihead_attn.out_proj.weight\".format(layer_num)].t()) + state_dict[\"transformer.decoder.layers.{}.multihead_attn.out_proj.bias\".format(layer_num)]\n",
    "        attn_dec_output_mha = op_dec_mha.view(tgt_len, bsz, attn_dec_mha_output.size(1))\n",
    "\n",
    "\n",
    "        return attn_dec_output_mha , attn_wt_matrix_dec_mha\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "torch.Size([4, 1, 6])\n",
      "torch.Size([18, 6])\n",
      "Q_dec_0 = \n",
      "tensor([[[-0.8250, -0.5931, -0.4025],\n",
      "         [-0.0903,  0.5603,  0.7427],\n",
      "         [ 0.0511, -0.2605, -0.1479],\n",
      "         [ 0.0511, -0.2605, -0.1479]],\n",
      "\n",
      "        [[-0.7654, -0.8175,  0.6158],\n",
      "         [ 0.5966,  0.2556, -0.2416],\n",
      "         [ 0.7424, -0.3892,  0.5577],\n",
      "         [ 0.7424, -0.3892,  0.5577]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "K_dec_0 = \n",
      "tensor([[[-0.0112,  1.4857, -1.1612],\n",
      "         [-0.4279,  0.9543, -0.8157],\n",
      "         [-0.4221,  0.6399, -0.6253],\n",
      "         [-0.4290,  0.5838, -0.5780]],\n",
      "\n",
      "        [[ 1.0297, -1.4764, -0.4470],\n",
      "         [ 1.2456, -1.7020, -0.3399],\n",
      "         [ 0.5936, -1.1203, -0.1688],\n",
      "         [ 1.1306, -1.4338, -0.1289]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "V_dec_0 = \n",
      "tensor([[[ 0.4800,  0.5421, -0.1278],\n",
      "         [ 0.1234,  0.9803, -0.7109],\n",
      "         [ 0.0316,  0.9253,  0.1574],\n",
      "         [ 0.0551,  0.8494, -0.4480]],\n",
      "\n",
      "        [[ 0.8969,  0.6116,  0.3663],\n",
      "         [ 0.9777, -0.2162,  1.2566],\n",
      "         [ 0.1571, -0.6704,  0.5209],\n",
      "         [ 0.5234, -0.6350,  0.9909]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "Attention weights = \n",
      "torch.Size([1, 2, 4, 4]) tensor([[[[0.1910, 0.2579, 0.2740, 0.2772],\n",
      "          [0.2528, 0.2523, 0.2472, 0.2478],\n",
      "          [0.2388, 0.2481, 0.2560, 0.2571],\n",
      "          [0.2388, 0.2481, 0.2560, 0.2571]],\n",
      "\n",
      "         [[0.2363, 0.2482, 0.2674, 0.2480],\n",
      "          [0.2567, 0.2635, 0.2239, 0.2558],\n",
      "          [0.2394, 0.2859, 0.2005, 0.2743],\n",
      "          [0.2394, 0.2859, 0.2005, 0.2743]]]], grad_fn=<SoftmaxBackward0>)\n",
      "Cross attention in decoder_0\n",
      "tensor([[ 0.1474,  0.8452, -0.2888,  0.6265, -0.2459,  0.7836],\n",
      "        [ 0.1739,  0.8235, -0.2838,  0.6569, -0.2126,  0.7953],\n",
      "        [ 0.1675,  0.8279, -0.2818,  0.6693, -0.2239,  0.8232],\n",
      "        [ 0.1675,  0.8279, -0.2818,  0.6693, -0.2239,  0.8232]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# need_weights = True\n",
    "\n",
    "attn_dec_mha_output, attn_dec_mha_wts = decoder_block_cross_attn_output(x_dec, memory, state_dict, layer_num = 0, tgt_len = tgt_len, src_len = src_len, head_dim = head_dim, need_weights = need_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 1, 6])"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_dec_mha_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoder_block_post_attn_output(x_dec, attn_dec_mha_output, state_dict, layer_num):\n",
    "\n",
    "    # (tgt_len, bsz, embed_dim)\n",
    "    output_dec_2 = attn_dec_mha_output + x_dec\n",
    "\n",
    "    linear_result_dec_2 = output_dec_2*state_dict[\"transformer.decoder.layers.{}.norm2.weight\".format(layer_num)] + state_dict[\"transformer.decoder.layers.{}.norm2.bias\".format(layer_num)]\n",
    "\n",
    "    # Layer normalization from Torch's implementation \n",
    "    layernorm_dec_2 = torch.nn.LayerNorm(normalized_shape=linear_result_dec_2.shape[2:])\n",
    "    linear_op_dec_2 = layernorm_dec_2(linear_result_dec_2)\n",
    "\n",
    "    x = linear_result_dec_2\n",
    "    w = layernorm_dec_2.weight\n",
    "    b = layernorm_dec_2.bias\n",
    "\n",
    "    linear_result_dec_2f = w*x + b\n",
    "\n",
    "    epsilon = 1e-5  \n",
    "    mean = linear_result_dec_2f.mean(dim=-1, keepdim=True)\n",
    "    std = linear_result_dec_2f.std(dim=-1, unbiased=False, keepdim=True)\n",
    "    normalized_result_dec_2 = (linear_result_dec_2f - mean) / (std + epsilon) * w + b\n",
    "\n",
    "    print(\"norm2(x' + mha(x', mem)), \\n where x' = Decoder_curr_layer norm1(x + sa(x))\")\n",
    "    print(normalized_result_dec_2)\n",
    "    print(\"\\n\\n\")\n",
    "\n",
    "    x_dec2_norm = normalized_result_dec_2\n",
    "\n",
    "    op_dec_1 = torch.matmul(x_dec2_norm, state_dict[\"transformer.decoder.layers.{}.linear1.weight\".format(layer_num)].t()) + state_dict[\"transformer.decoder.layers.{}.linear1.bias\".format(layer_num)]\n",
    "    op_dec_1_relu = torch.nn.functional.relu(op_dec_1)\n",
    "    op_dec_2 = torch.matmul(op_dec_1_relu, state_dict[\"transformer.decoder.layers.{}.linear2.weight\".format(layer_num)].t()) + state_dict[\"transformer.decoder.layers.{}.linear2.bias\".format(layer_num)]\n",
    "\n",
    "    ff_dec = op_dec_2\n",
    "\n",
    "    x_dec3_unorm = x_dec2_norm + ff_dec\n",
    "\n",
    "    linear_result_dec_3 = x_dec3_unorm*state_dict[\"transformer.decoder.layers.0.norm3.weight\"] + state_dict[\"transformer.decoder.layers.0.norm3.bias\"]\n",
    "\n",
    "    # Layer normalization from Torch's implementation \n",
    "    layernorm_dec_3 = torch.nn.LayerNorm(normalized_shape=linear_result_dec_3.shape[2:])\n",
    "    linear_op_dec_3 = layernorm_dec_3(linear_result_dec_3)\n",
    "\n",
    "    # From torch's implementation\n",
    "    x = linear_result_dec_3\n",
    "\n",
    "    # Obtained layer norm weights and biases (learnable)\n",
    "    w = layernorm_dec_3.weight\n",
    "    b = layernorm_dec_3.bias\n",
    "\n",
    "    linear_result_dec_3f = w*x + b\n",
    "\n",
    "    epsilon = 1e-5  \n",
    "    mean = linear_result_dec_3f.mean(dim=-1, keepdim=True)\n",
    "    std = linear_result_dec_3f.std(dim=-1, unbiased=False, keepdim=True)\n",
    "    normalized_result_dec_3 = (linear_result_dec_3f - mean) / (std + epsilon) * w + b\n",
    "\n",
    "    print(\"norm3(x'' + ff(x'')) \\n where, x'' = Decoder_curr_layer norm2(x' + mha(x'))\")\n",
    "    print(normalized_result_dec_3)\n",
    "    print()\n",
    "\n",
    "    return normalized_result_dec_3\n",
    "\n",
    "def feef_fwd_transformer(dec_output_final, state_dict):\n",
    "\n",
    "    # (tgt_len, bsz, embed_dim) @ (vocab_size, embed_dim).T -> (tgt_len, bsz, vocab_size)\n",
    "    final_op = dec_output_final@state_dict[\"fc.weight\"].T + state_dict[\"fc.bias\"]\n",
    "\n",
    "    return final_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 6])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_dict[\"fc.weight\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "norm2(x' + mha(x', mem)), \n",
      " where x' = Decoder_curr_layer norm1(x + sa(x))\n",
      "tensor([[[-0.3694,  0.9990,  1.6560, -0.2832, -1.1398, -0.8624]],\n",
      "\n",
      "        [[-0.6122,  0.8346,  1.0848, -0.6791, -1.5577,  0.9296]],\n",
      "\n",
      "        [[-0.8573,  1.6522,  0.6363, -0.5546, -1.2864,  0.4097]],\n",
      "\n",
      "        [[-0.8573,  1.6522,  0.6363, -0.5546, -1.2864,  0.4097]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "\n",
      "norm3(x'' + ff(x'')) \n",
      " where, x'' = Decoder_curr_layer norm2(x' + mha(x'))\n",
      "tensor([[[-0.0660,  1.2602,  1.3936, -0.4797, -1.1353, -0.9728]],\n",
      "\n",
      "        [[-0.2781,  1.1315,  0.7439, -0.9239, -1.5632,  0.8898]],\n",
      "\n",
      "        [[-0.4928,  1.8664,  0.3272, -0.7470, -1.2364,  0.2826]],\n",
      "\n",
      "        [[-0.4928,  1.8664,  0.3272, -0.7470, -1.2364,  0.2826]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final_attn_op_decoder = decoder_block_post_attn_output(x_dec, attn_dec_mha_output, state_dict, layer_num = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 1, 6])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_attn_op_decoder.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (tgt_len, bsz, vocab_dim)\n",
    "final_op = feef_fwd_transformer(final_attn_op_decoder, state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.1671,  0.6496,  0.3877, -0.1723, -1.2654,  0.9748, -0.3106,\n",
       "           0.1239,  0.4715, -1.3378]],\n",
       "\n",
       "        [[-0.2422,  0.6775,  1.1204, -0.6104, -1.0052,  0.3941,  0.0118,\n",
       "           0.5823,  0.6551, -1.2290]],\n",
       "\n",
       "        [[-0.3014,  0.9889,  0.4547, -0.9121, -0.9980,  0.1475, -0.2022,\n",
       "           0.6603,  1.0222, -1.2369]],\n",
       "\n",
       "        [[-0.3014,  0.9889,  0.4547, -0.9121, -0.9980,  0.1475, -0.2022,\n",
       "           0.6603,  1.0222, -1.2369]]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_op"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "########################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examaple :- \n",
    "\n",
    "Transformer with 3 encoders and 3 decoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source embeddings :- \n",
      "\n",
      "tensor([[[-0.8011, -0.1107, -1.6151,  1.4762]],\n",
      "\n",
      "        [[-0.3233, -1.9852, -0.9699,  0.9073]],\n",
      "\n",
      "        [[-0.8814, -0.9857,  0.9917, -1.8681]],\n",
      "\n",
      "        [[ 0.3493, -0.4863,  0.4214, -0.6675]]], grad_fn=<EmbeddingBackward0>)\n",
      "\n",
      "Target embeddings :- \n",
      "\n",
      "tensor([[[-0.2207, -0.4344, -0.1539, -0.3831]],\n",
      "\n",
      "        [[-1.3731, -0.7337, -0.0944, -0.0221]],\n",
      "\n",
      "        [[ 0.4218,  0.4912, -1.6193, -2.3457]],\n",
      "\n",
      "        [[ 0.4218,  0.4912, -1.6193, -2.3457]]], grad_fn=<EmbeddingBackward0>)\n",
      "\n",
      "Positional encoded source embeddings:\n",
      "tensor([[[-0.8011,  0.8893, -1.6151,  2.4762],\n",
      "         [ 0.1989,  1.8893, -0.6151,  3.4762],\n",
      "         [ 1.1989,  2.8893,  0.3849,  4.4762],\n",
      "         [ 2.1989,  3.8893,  1.3849,  5.4762]],\n",
      "\n",
      "        [[-0.3233, -0.9852, -0.9699,  1.9073],\n",
      "         [ 0.6767,  0.0148,  0.0301,  2.9073],\n",
      "         [ 1.6767,  1.0148,  1.0301,  3.9073],\n",
      "         [ 2.6767,  2.0148,  2.0301,  4.9073]],\n",
      "\n",
      "        [[-0.8814,  0.0143,  0.9917, -0.8681],\n",
      "         [ 0.1186,  1.0143,  1.9917,  0.1319],\n",
      "         [ 1.1186,  2.0143,  2.9917,  1.1319],\n",
      "         [ 2.1186,  3.0143,  3.9917,  2.1319]],\n",
      "\n",
      "        [[ 0.3493,  0.5137,  0.4214,  0.3325],\n",
      "         [ 1.3493,  1.5137,  1.4214,  1.3325],\n",
      "         [ 2.3493,  2.5137,  2.4214,  2.3325],\n",
      "         [ 3.3493,  3.5137,  3.4214,  3.3325]]], grad_fn=<AddBackward0>)\n",
      "\n",
      "Positional encoded target embeddings:\n",
      "tensor([[[ 0.7793,  1.5656,  0.8461,  1.6169],\n",
      "         [-0.2207,  0.5656, -0.1539,  0.6169],\n",
      "         [ 2.7793,  3.5656,  2.8461,  3.6169],\n",
      "         [ 2.7793,  3.5656,  2.8461,  3.6169]],\n",
      "\n",
      "        [[-0.3731,  1.2663,  0.9056,  1.9779],\n",
      "         [-1.3731,  0.2663, -0.0944,  0.9779],\n",
      "         [ 1.6269,  3.2663,  2.9056,  3.9779],\n",
      "         [ 1.6269,  3.2663,  2.9056,  3.9779]],\n",
      "\n",
      "        [[ 1.4218,  2.4912, -0.6193, -0.3457],\n",
      "         [ 0.4218,  1.4912, -1.6193, -1.3457],\n",
      "         [ 3.4218,  4.4912,  1.3807,  1.6543],\n",
      "         [ 3.4218,  4.4912,  1.3807,  1.6543]],\n",
      "\n",
      "        [[ 1.4218,  2.4912, -0.6193, -0.3457],\n",
      "         [ 0.4218,  1.4912, -1.6193, -1.3457],\n",
      "         [ 3.4218,  4.4912,  1.3807,  1.6543],\n",
      "         [ 3.4218,  4.4912,  1.3807,  1.6543]]], grad_fn=<AddBackward0>)\n",
      "\n",
      "query =  tensor([[[-0.8011,  0.8893, -1.6151,  2.4762],\n",
      "         [ 0.1989,  1.8893, -0.6151,  3.4762],\n",
      "         [ 1.1989,  2.8893,  0.3849,  4.4762],\n",
      "         [ 2.1989,  3.8893,  1.3849,  5.4762]],\n",
      "\n",
      "        [[-0.3233, -0.9852, -0.9699,  1.9073],\n",
      "         [ 0.6767,  0.0148,  0.0301,  2.9073],\n",
      "         [ 1.6767,  1.0148,  1.0301,  3.9073],\n",
      "         [ 2.6767,  2.0148,  2.0301,  4.9073]],\n",
      "\n",
      "        [[-0.8814,  0.0143,  0.9917, -0.8681],\n",
      "         [ 0.1186,  1.0143,  1.9917,  0.1319],\n",
      "         [ 1.1186,  2.0143,  2.9917,  1.1319],\n",
      "         [ 2.1186,  3.0143,  3.9917,  2.1319]],\n",
      "\n",
      "        [[ 0.3493,  0.5137,  0.4214,  0.3325],\n",
      "         [ 1.3493,  1.5137,  1.4214,  1.3325],\n",
      "         [ 2.3493,  2.5137,  2.4214,  2.3325],\n",
      "         [ 3.3493,  3.5137,  3.4214,  3.3325]]], grad_fn=<AddBackward0>)\n",
      "query shape =  torch.Size([4, 4, 4])\n",
      "\n",
      " _in_projection_packed \n",
      "\n",
      "Q :-  tensor([[[ 1.1692e+00, -3.4754e-01, -5.7117e-02,  1.0919e+00],\n",
      "         [ 2.7108e+00, -5.8195e-01,  1.0914e-02,  6.3649e-01],\n",
      "         [ 4.2523e+00, -8.1637e-01,  7.8946e-02,  1.8111e-01],\n",
      "         [ 5.7938e+00, -1.0508e+00,  1.4698e-01, -2.7427e-01]],\n",
      "\n",
      "        [[ 5.6561e-02, -1.2562e+00, -3.2538e-01,  1.1722e+00],\n",
      "         [ 1.5981e+00, -1.4906e+00, -2.5735e-01,  7.1679e-01],\n",
      "         [ 3.1396e+00, -1.7250e+00, -1.8931e-01,  2.6141e-01],\n",
      "         [ 4.6812e+00, -1.9595e+00, -1.2128e-01, -1.9397e-01]],\n",
      "\n",
      "        [[-5.9954e-01,  4.1997e-01, -2.7642e-01, -1.8666e-01],\n",
      "         [ 9.4200e-01,  1.8555e-01, -2.0838e-01, -6.4204e-01],\n",
      "         [ 2.4835e+00, -4.8863e-02, -1.4035e-01, -1.0974e+00],\n",
      "         [ 4.0251e+00, -2.8328e-01, -7.2321e-02, -1.5528e+00]],\n",
      "\n",
      "        [[ 6.3646e-01,  4.8874e-03,  4.0824e-02, -2.2408e-01],\n",
      "         [ 2.1780e+00, -2.2953e-01,  1.0886e-01, -6.7947e-01],\n",
      "         [ 3.7195e+00, -4.6394e-01,  1.7689e-01, -1.1348e+00],\n",
      "         [ 5.2611e+00, -6.9836e-01,  2.4492e-01, -1.5902e+00]]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "\n",
      "K :-  tensor([[[-1.0390,  0.2198,  0.4348, -1.2000],\n",
      "         [-2.9655, -0.3558, -0.1874, -1.0216],\n",
      "         [-4.8921, -0.9315, -0.8097, -0.8431],\n",
      "         [-6.8187, -1.5072, -1.4320, -0.6646]],\n",
      "\n",
      "        [[-0.1149,  0.7552,  0.8130,  0.0533],\n",
      "         [-2.0415,  0.1795,  0.1908,  0.2318],\n",
      "         [-3.9680, -0.3962, -0.4315,  0.4103],\n",
      "         [-5.8946, -0.9719, -1.0538,  0.5888]],\n",
      "\n",
      "        [[ 0.6616, -0.0217, -0.7709,  0.4205],\n",
      "         [-1.2650, -0.5974, -1.3932,  0.5989],\n",
      "         [-3.1916, -1.1731, -2.0155,  0.7774],\n",
      "         [-5.1181, -1.7488, -2.6377,  0.9559]],\n",
      "\n",
      "        [[-0.7741, -0.2753, -0.3118,  0.0204],\n",
      "         [-2.7006, -0.8510, -0.9340,  0.1988],\n",
      "         [-4.6272, -1.4267, -1.5563,  0.3773],\n",
      "         [-6.5538, -2.0024, -2.1786,  0.5558]]], grad_fn=<SelectBackward0>)\n",
      "\n",
      "V :-  tensor([[[ 1.1734,  1.5581, -0.6262, -1.3653],\n",
      "         [ 0.5602,  1.8068, -0.0203, -1.4007],\n",
      "         [-0.0530,  2.0556,  0.5856, -1.4362],\n",
      "         [-0.6662,  2.3044,  1.1915, -1.4716]],\n",
      "\n",
      "        [[ 0.9299,  0.6206, -0.9657, -0.2515],\n",
      "         [ 0.3167,  0.8693, -0.3598, -0.2869],\n",
      "         [-0.2965,  1.1181,  0.2461, -0.3224],\n",
      "         [-0.9096,  1.3669,  0.8520, -0.3578]],\n",
      "\n",
      "        [[-0.6012, -0.2033, -0.0120, -0.3402],\n",
      "         [-1.2144,  0.0454,  0.5939, -0.3756],\n",
      "         [-1.8276,  0.2942,  1.1998, -0.4111],\n",
      "         [-2.4408,  0.5430,  1.8057, -0.4465]],\n",
      "\n",
      "        [[-0.2759,  0.1176,  0.2829, -0.0716],\n",
      "         [-0.8891,  0.3664,  0.8888, -0.1071],\n",
      "         [-1.5023,  0.6151,  1.4947, -0.1425],\n",
      "         [-2.1154,  0.8639,  2.1006, -0.1780]]], grad_fn=<SelectBackward0>)\n",
      "\n",
      "tgt_len =  4\n",
      "bsz =  4\n",
      "num_heads =  2\n",
      "head_dim =  2\n",
      "\n",
      "tensor([[[ 1.1692e+00, -3.4754e-01, -5.7117e-02,  1.0919e+00],\n",
      "         [ 2.7108e+00, -5.8195e-01,  1.0914e-02,  6.3649e-01],\n",
      "         [ 4.2523e+00, -8.1637e-01,  7.8946e-02,  1.8111e-01],\n",
      "         [ 5.7938e+00, -1.0508e+00,  1.4698e-01, -2.7427e-01]],\n",
      "\n",
      "        [[ 5.6561e-02, -1.2562e+00, -3.2538e-01,  1.1722e+00],\n",
      "         [ 1.5981e+00, -1.4906e+00, -2.5735e-01,  7.1679e-01],\n",
      "         [ 3.1396e+00, -1.7250e+00, -1.8931e-01,  2.6141e-01],\n",
      "         [ 4.6812e+00, -1.9595e+00, -1.2128e-01, -1.9397e-01]],\n",
      "\n",
      "        [[-5.9954e-01,  4.1997e-01, -2.7642e-01, -1.8666e-01],\n",
      "         [ 9.4200e-01,  1.8555e-01, -2.0838e-01, -6.4204e-01],\n",
      "         [ 2.4835e+00, -4.8863e-02, -1.4035e-01, -1.0974e+00],\n",
      "         [ 4.0251e+00, -2.8328e-01, -7.2321e-02, -1.5528e+00]],\n",
      "\n",
      "        [[ 6.3646e-01,  4.8874e-03,  4.0824e-02, -2.2408e-01],\n",
      "         [ 2.1780e+00, -2.2953e-01,  1.0886e-01, -6.7947e-01],\n",
      "         [ 3.7195e+00, -4.6394e-01,  1.7689e-01, -1.1348e+00],\n",
      "         [ 5.2611e+00, -6.9836e-01,  2.4492e-01, -1.5902e+00]]],\n",
      "       grad_fn=<SelectBackward0>) torch.Size([4, 4, 4])\n",
      "Q reshaped =  tensor([[[ 1.1692e+00, -3.4754e-01],\n",
      "         [ 5.6561e-02, -1.2562e+00],\n",
      "         [-5.9954e-01,  4.1997e-01],\n",
      "         [ 6.3646e-01,  4.8874e-03]],\n",
      "\n",
      "        [[-5.7117e-02,  1.0919e+00],\n",
      "         [-3.2538e-01,  1.1722e+00],\n",
      "         [-2.7642e-01, -1.8666e-01],\n",
      "         [ 4.0824e-02, -2.2408e-01]],\n",
      "\n",
      "        [[ 2.7108e+00, -5.8195e-01],\n",
      "         [ 1.5981e+00, -1.4906e+00],\n",
      "         [ 9.4200e-01,  1.8555e-01],\n",
      "         [ 2.1780e+00, -2.2953e-01]],\n",
      "\n",
      "        [[ 1.0914e-02,  6.3649e-01],\n",
      "         [-2.5735e-01,  7.1679e-01],\n",
      "         [-2.0838e-01, -6.4204e-01],\n",
      "         [ 1.0886e-01, -6.7947e-01]],\n",
      "\n",
      "        [[ 4.2523e+00, -8.1637e-01],\n",
      "         [ 3.1396e+00, -1.7250e+00],\n",
      "         [ 2.4835e+00, -4.8863e-02],\n",
      "         [ 3.7195e+00, -4.6394e-01]],\n",
      "\n",
      "        [[ 7.8946e-02,  1.8111e-01],\n",
      "         [-1.8931e-01,  2.6141e-01],\n",
      "         [-1.4035e-01, -1.0974e+00],\n",
      "         [ 1.7689e-01, -1.1348e+00]],\n",
      "\n",
      "        [[ 5.7938e+00, -1.0508e+00],\n",
      "         [ 4.6812e+00, -1.9595e+00],\n",
      "         [ 4.0251e+00, -2.8328e-01],\n",
      "         [ 5.2611e+00, -6.9836e-01]],\n",
      "\n",
      "        [[ 1.4698e-01, -2.7427e-01],\n",
      "         [-1.2128e-01, -1.9397e-01],\n",
      "         [-7.2321e-02, -1.5528e+00],\n",
      "         [ 2.4492e-01, -1.5902e+00]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "K reshaped =  tensor([[[-1.0390,  0.2198],\n",
      "         [-0.1149,  0.7552],\n",
      "         [ 0.6616, -0.0217],\n",
      "         [-0.7741, -0.2753]],\n",
      "\n",
      "        [[ 0.4348, -1.2000],\n",
      "         [ 0.8130,  0.0533],\n",
      "         [-0.7709,  0.4205],\n",
      "         [-0.3118,  0.0204]],\n",
      "\n",
      "        [[-2.9655, -0.3558],\n",
      "         [-2.0415,  0.1795],\n",
      "         [-1.2650, -0.5974],\n",
      "         [-2.7006, -0.8510]],\n",
      "\n",
      "        [[-0.1874, -1.0216],\n",
      "         [ 0.1908,  0.2318],\n",
      "         [-1.3932,  0.5989],\n",
      "         [-0.9340,  0.1988]],\n",
      "\n",
      "        [[-4.8921, -0.9315],\n",
      "         [-3.9680, -0.3962],\n",
      "         [-3.1916, -1.1731],\n",
      "         [-4.6272, -1.4267]],\n",
      "\n",
      "        [[-0.8097, -0.8431],\n",
      "         [-0.4315,  0.4103],\n",
      "         [-2.0155,  0.7774],\n",
      "         [-1.5563,  0.3773]],\n",
      "\n",
      "        [[-6.8187, -1.5072],\n",
      "         [-5.8946, -0.9719],\n",
      "         [-5.1181, -1.7488],\n",
      "         [-6.5538, -2.0024]],\n",
      "\n",
      "        [[-1.4320, -0.6646],\n",
      "         [-1.0538,  0.5888],\n",
      "         [-2.6377,  0.9559],\n",
      "         [-2.1786,  0.5558]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "V reshaped =  tensor([[[ 1.1734,  1.5581],\n",
      "         [ 0.9299,  0.6206],\n",
      "         [-0.6012, -0.2033],\n",
      "         [-0.2759,  0.1176]],\n",
      "\n",
      "        [[-0.6262, -1.3653],\n",
      "         [-0.9657, -0.2515],\n",
      "         [-0.0120, -0.3402],\n",
      "         [ 0.2829, -0.0716]],\n",
      "\n",
      "        [[ 0.5602,  1.8068],\n",
      "         [ 0.3167,  0.8693],\n",
      "         [-1.2144,  0.0454],\n",
      "         [-0.8891,  0.3664]],\n",
      "\n",
      "        [[-0.0203, -1.4007],\n",
      "         [-0.3598, -0.2869],\n",
      "         [ 0.5939, -0.3756],\n",
      "         [ 0.8888, -0.1071]],\n",
      "\n",
      "        [[-0.0530,  2.0556],\n",
      "         [-0.2965,  1.1181],\n",
      "         [-1.8276,  0.2942],\n",
      "         [-1.5023,  0.6151]],\n",
      "\n",
      "        [[ 0.5856, -1.4362],\n",
      "         [ 0.2461, -0.3224],\n",
      "         [ 1.1998, -0.4111],\n",
      "         [ 1.4947, -0.1425]],\n",
      "\n",
      "        [[-0.6662,  2.3044],\n",
      "         [-0.9096,  1.3669],\n",
      "         [-2.4408,  0.5430],\n",
      "         [-2.1154,  0.8639]],\n",
      "\n",
      "        [[ 1.1915, -1.4716],\n",
      "         [ 0.8520, -0.3578],\n",
      "         [ 1.8057, -0.4465],\n",
      "         [ 2.1006, -0.1780]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "torch.Size([4, 2, 4, 2]) torch.Size([4, 2, 4, 2]) torch.Size([4, 2, 4, 2])\n",
      "\n",
      "scaled_dot_product_attention\n",
      "attn_output = \n",
      "tensor([[-0.0078,  0.2334, -0.2448, -0.3488],\n",
      "        [-0.9499,  0.2180,  0.3205, -0.4378],\n",
      "        [-1.7269,  0.3544,  0.8806, -0.5492],\n",
      "        [-2.4036,  0.5641,  1.4369, -0.6837],\n",
      "        [ 0.1195,  0.4125, -0.1901, -0.3334],\n",
      "        [-0.8504,  0.3244,  0.3779, -0.4142],\n",
      "        [-1.6924,  0.3862,  0.9389, -0.5169],\n",
      "        [-2.3942,  0.5717,  1.4935, -0.6436],\n",
      "        [ 0.4724,  0.6791, -0.2952, -0.5223],\n",
      "        [-0.4862,  0.5736,  0.2536, -0.6686],\n",
      "        [-1.4679,  0.5256,  0.8011, -0.8299],\n",
      "        [-2.2952,  0.6290,  1.3525, -0.9939],\n",
      "        [ 0.1699,  0.3792, -0.3570, -0.5575],\n",
      "        [-0.8217,  0.3072,  0.1963, -0.7083],\n",
      "        [-1.6685,  0.3905,  0.7513, -0.8699],\n",
      "        [-2.3808,  0.5772,  1.3120, -1.0300]], grad_fn=<ViewBackward0>)\n",
      "\n",
      "x + self_atten(x) = \n",
      "tensor([[[-1.2197,  0.9049, -1.7477,  2.5971],\n",
      "         [-0.6377,  1.8017, -1.3776,  3.5708],\n",
      "         [-0.0451,  2.5681, -0.9289,  4.3993],\n",
      "         [ 0.5661,  3.2652, -0.4258,  5.1563]],\n",
      "\n",
      "        [[-0.7367, -1.1417, -1.0521,  1.8240],\n",
      "         [-0.1180, -0.1981, -0.6783,  2.8521],\n",
      "         [ 0.4810,  0.6285, -0.2556,  3.7488],\n",
      "         [ 1.0908,  1.3480,  0.2353,  4.5287]],\n",
      "\n",
      "        [[-1.3178, -0.3364,  1.0810, -1.1122],\n",
      "         [-0.7028,  0.6212,  1.4671, -0.0402],\n",
      "         [-0.1551,  1.5542,  1.8146,  1.0054],\n",
      "         [ 0.4047,  2.3554,  2.2337,  1.8964]],\n",
      "\n",
      "        [[-0.1521,  0.4568,  0.3607,  0.4424],\n",
      "         [ 0.4119,  1.4027,  0.7097,  1.5003],\n",
      "         [ 0.9741,  2.2184,  1.1210,  2.4081],\n",
      "         [ 1.5711,  2.9336,  1.6051,  3.1983]]], grad_fn=<AddBackward0>)\n",
      "\n",
      "norm1(x + self_atten(x)) = \n",
      "tensor([[[-0.7802,  0.4446, -1.0847,  1.4202],\n",
      "         [-0.7507,  0.4892, -1.1268,  1.3883],\n",
      "         [-0.7310,  0.5066, -1.1495,  1.3739],\n",
      "         [-0.7144,  0.5104, -1.1645,  1.3686]],\n",
      "\n",
      "        [[-0.3765, -0.7079, -0.6345,  1.7189],\n",
      "         [-0.4175, -0.4749, -0.8191,  1.7115],\n",
      "         [-0.4357, -0.3398, -0.9150,  1.6904],\n",
      "         [-0.4361, -0.2781, -0.9616,  1.6757]],\n",
      "\n",
      "        [[-0.9522,  0.0902,  1.5958, -0.7338],\n",
      "         [-1.2935,  0.3546,  1.4076, -0.4687],\n",
      "         [-1.5980,  0.6596,  1.0036, -0.0652],\n",
      "         [-1.6912,  0.8122,  0.6560,  0.2231]],\n",
      "\n",
      "        [[-1.7133,  0.7179,  0.3346,  0.6608],\n",
      "         [-1.2949,  0.8640, -0.6459,  1.0767],\n",
      "         [-1.1062,  0.8425, -0.8760,  1.1397],\n",
      "         [-1.0147,  0.8143, -0.9692,  1.1696]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "\n",
      "feed_fwd_op =  tensor([[[-0.4001,  0.3437,  0.0904, -0.2010],\n",
      "         [-0.3995,  0.3476,  0.0893, -0.1980],\n",
      "         [-0.3989,  0.3493,  0.0892, -0.1965],\n",
      "         [-0.3985,  0.3500,  0.0894, -0.1958]],\n",
      "\n",
      "        [[-0.4085,  0.2934,  0.1426, -0.2357],\n",
      "         [-0.4039,  0.3061,  0.1344, -0.2270],\n",
      "         [-0.4020,  0.3130,  0.1297, -0.2216],\n",
      "         [-0.4011,  0.3166,  0.1280, -0.2191]],\n",
      "\n",
      "        [[-0.3933,  0.2455,  0.0521, -0.2194],\n",
      "         [-0.4028,  0.2590,  0.0366, -0.2168],\n",
      "         [-0.4181,  0.2791,  0.0255, -0.2112],\n",
      "         [-0.4224,  0.2912,  0.0247, -0.2052]],\n",
      "\n",
      "        [[-0.4242,  0.2886,  0.0368, -0.2072],\n",
      "         [-0.4154,  0.3362,  0.0533, -0.1950],\n",
      "         [-0.4081,  0.3494,  0.0621, -0.1900],\n",
      "         [-0.4048,  0.3532,  0.0666, -0.1880]]], grad_fn=<ViewBackward0>)\n",
      "\n",
      "norm2(norm1(x + self_atten(x)) + feed_fwd_op) = \n",
      "tensor([[[-1.0755,  0.7841, -0.8997,  1.1912],\n",
      "         [-1.0454,  0.8258, -0.9393,  1.1588],\n",
      "         [-1.0267,  0.8427, -0.9613,  1.1453],\n",
      "         [-1.0123,  0.8473, -0.9767,  1.1417]],\n",
      "\n",
      "        [[-0.8170, -0.4040, -0.4903,  1.7114],\n",
      "         [-0.8434, -0.1321, -0.6944,  1.6700],\n",
      "         [-0.8510,  0.0199, -0.7948,  1.6259],\n",
      "         [-0.8467,  0.0880, -0.8429,  1.6015]],\n",
      "\n",
      "        [[-1.0781,  0.3527,  1.4695, -0.7442],\n",
      "         [-1.3433,  0.5777,  1.2684, -0.5028],\n",
      "         [-1.5727,  0.8289,  0.9024, -0.1587],\n",
      "         [-1.6452,  0.9547,  0.6131,  0.0774]],\n",
      "\n",
      "        [[-1.6967,  0.8916,  0.3687,  0.4364],\n",
      "         [-1.4137,  1.0724, -0.4590,  0.8003],\n",
      "         [-1.2787,  1.0791, -0.6685,  0.8680],\n",
      "         [-1.2130,  1.0672, -0.7574,  0.9033]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "\n",
      "##################################\n",
      "query =  tensor([[[-1.0755,  0.7841, -0.8997,  1.1912],\n",
      "         [-1.0454,  0.8258, -0.9393,  1.1588],\n",
      "         [-1.0267,  0.8427, -0.9613,  1.1453],\n",
      "         [-1.0123,  0.8473, -0.9767,  1.1417]],\n",
      "\n",
      "        [[-0.8170, -0.4040, -0.4903,  1.7114],\n",
      "         [-0.8434, -0.1321, -0.6944,  1.6700],\n",
      "         [-0.8510,  0.0199, -0.7948,  1.6259],\n",
      "         [-0.8467,  0.0880, -0.8429,  1.6015]],\n",
      "\n",
      "        [[-1.0781,  0.3527,  1.4695, -0.7442],\n",
      "         [-1.3433,  0.5777,  1.2684, -0.5028],\n",
      "         [-1.5727,  0.8289,  0.9024, -0.1587],\n",
      "         [-1.6452,  0.9547,  0.6131,  0.0774]],\n",
      "\n",
      "        [[-1.6967,  0.8916,  0.3687,  0.4364],\n",
      "         [-1.4137,  1.0724, -0.4590,  0.8003],\n",
      "         [-1.2787,  1.0791, -0.6685,  0.8680],\n",
      "         [-1.2130,  1.0672, -0.7574,  0.9033]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "query shape =  torch.Size([4, 4, 4])\n",
      "\n",
      " _in_projection_packed \n",
      "\n",
      "Q :-  tensor([[[-0.9445,  1.5328,  0.2663,  0.2969],\n",
      "         [-0.9523,  1.5387,  0.3013,  0.3200],\n",
      "         [-0.9571,  1.5390,  0.3201,  0.3351],\n",
      "         [-0.9613,  1.5366,  0.3323,  0.3481]],\n",
      "\n",
      "        [[-0.7726,  0.8446, -0.0759,  0.3830],\n",
      "         [-0.8779,  1.0354,  0.0637,  0.4557],\n",
      "         [-0.9239,  1.1287,  0.1372,  0.4866],\n",
      "         [-0.9435,  1.1660,  0.1739,  0.5041]],\n",
      "\n",
      "        [[ 0.6795,  0.1554, -0.9309, -1.4404],\n",
      "         [ 0.4678,  0.5289, -0.8691, -1.3933],\n",
      "         [ 0.1571,  0.9684, -0.7145, -1.2186],\n",
      "         [-0.0574,  1.2116, -0.5753, -1.0406]],\n",
      "\n",
      "        [[-0.2673,  1.3727, -0.4866, -0.8362],\n",
      "         [-0.7114,  1.6307,  0.0171, -0.1940],\n",
      "         [-0.8049,  1.6360,  0.1552, -0.0093],\n",
      "         [-0.8440,  1.6283,  0.2134,  0.0750]]], grad_fn=<SelectBackward0>)\n",
      "\n",
      "K :-  tensor([[[-0.1676,  0.2603, -0.4620,  0.1106],\n",
      "         [-0.1651,  0.2971, -0.4332,  0.0689],\n",
      "         [-0.1666,  0.3136, -0.4216,  0.0488],\n",
      "         [-0.1709,  0.3207, -0.4187,  0.0387]],\n",
      "\n",
      "        [[-0.5094, -0.4906, -1.0858,  0.7254],\n",
      "         [-0.4843, -0.3098, -0.9901,  0.5718],\n",
      "         [-0.4627, -0.2069, -0.9245,  0.4815],\n",
      "         [-0.4539, -0.1587, -0.8926,  0.4369]],\n",
      "\n",
      "        [[ 0.8221, -0.0446,  0.5694,  0.2895],\n",
      "         [ 0.8064,  0.0412,  0.4945,  0.2953],\n",
      "         [ 0.7230,  0.1551,  0.3646,  0.2718],\n",
      "         [ 0.6312,  0.2235,  0.2603,  0.2419]],\n",
      "\n",
      "        [[ 0.4731,  0.1674,  0.0338,  0.3213],\n",
      "         [ 0.1653,  0.3686, -0.1445,  0.0868],\n",
      "         [ 0.0739,  0.4093, -0.1876,  0.0186],\n",
      "         [ 0.0287,  0.4186, -0.2139, -0.0052]]], grad_fn=<SelectBackward0>)\n",
      "\n",
      "V :-  tensor([[[ 0.7822,  0.1640,  0.4078,  0.4556],\n",
      "         [ 0.7322,  0.2052,  0.4398,  0.4877],\n",
      "         [ 0.7097,  0.2285,  0.4559,  0.5048],\n",
      "         [ 0.7006,  0.2449,  0.4650,  0.5158]],\n",
      "\n",
      "        [[ 1.5860, -0.0715, -0.0843,  0.0768],\n",
      "         [ 1.4438,  0.0667,  0.0720,  0.2266],\n",
      "         [ 1.3481,  0.1382,  0.1557,  0.3040],\n",
      "         [ 1.2996,  0.1757,  0.1955,  0.3415]],\n",
      "\n",
      "        [[-0.3192, -1.3732, -0.5649, -0.8715],\n",
      "         [-0.1757, -1.3437, -0.4614, -0.7649],\n",
      "         [ 0.0135, -1.1919, -0.2858, -0.5614],\n",
      "         [ 0.1378, -1.0303, -0.1543, -0.3975]],\n",
      "\n",
      "        [[ 0.4230, -0.9020, -0.0970, -0.2859],\n",
      "         [ 0.4963, -0.2409,  0.2956,  0.2226],\n",
      "         [ 0.4962, -0.0490,  0.3919,  0.3537],\n",
      "         [ 0.5046,  0.0350,  0.4293,  0.4081]]], grad_fn=<SelectBackward0>)\n",
      "\n",
      "tgt_len =  4\n",
      "bsz =  4\n",
      "num_heads =  2\n",
      "head_dim =  2\n",
      "\n",
      "tensor([[[-0.9445,  1.5328,  0.2663,  0.2969],\n",
      "         [-0.9523,  1.5387,  0.3013,  0.3200],\n",
      "         [-0.9571,  1.5390,  0.3201,  0.3351],\n",
      "         [-0.9613,  1.5366,  0.3323,  0.3481]],\n",
      "\n",
      "        [[-0.7726,  0.8446, -0.0759,  0.3830],\n",
      "         [-0.8779,  1.0354,  0.0637,  0.4557],\n",
      "         [-0.9239,  1.1287,  0.1372,  0.4866],\n",
      "         [-0.9435,  1.1660,  0.1739,  0.5041]],\n",
      "\n",
      "        [[ 0.6795,  0.1554, -0.9309, -1.4404],\n",
      "         [ 0.4678,  0.5289, -0.8691, -1.3933],\n",
      "         [ 0.1571,  0.9684, -0.7145, -1.2186],\n",
      "         [-0.0574,  1.2116, -0.5753, -1.0406]],\n",
      "\n",
      "        [[-0.2673,  1.3727, -0.4866, -0.8362],\n",
      "         [-0.7114,  1.6307,  0.0171, -0.1940],\n",
      "         [-0.8049,  1.6360,  0.1552, -0.0093],\n",
      "         [-0.8440,  1.6283,  0.2134,  0.0750]]], grad_fn=<SelectBackward0>) torch.Size([4, 4, 4])\n",
      "Q reshaped =  tensor([[[-0.9445,  1.5328],\n",
      "         [-0.7726,  0.8446],\n",
      "         [ 0.6795,  0.1554],\n",
      "         [-0.2673,  1.3727]],\n",
      "\n",
      "        [[ 0.2663,  0.2969],\n",
      "         [-0.0759,  0.3830],\n",
      "         [-0.9309, -1.4404],\n",
      "         [-0.4866, -0.8362]],\n",
      "\n",
      "        [[-0.9523,  1.5387],\n",
      "         [-0.8779,  1.0354],\n",
      "         [ 0.4678,  0.5289],\n",
      "         [-0.7114,  1.6307]],\n",
      "\n",
      "        [[ 0.3013,  0.3200],\n",
      "         [ 0.0637,  0.4557],\n",
      "         [-0.8691, -1.3933],\n",
      "         [ 0.0171, -0.1940]],\n",
      "\n",
      "        [[-0.9571,  1.5390],\n",
      "         [-0.9239,  1.1287],\n",
      "         [ 0.1571,  0.9684],\n",
      "         [-0.8049,  1.6360]],\n",
      "\n",
      "        [[ 0.3201,  0.3351],\n",
      "         [ 0.1372,  0.4866],\n",
      "         [-0.7145, -1.2186],\n",
      "         [ 0.1552, -0.0093]],\n",
      "\n",
      "        [[-0.9613,  1.5366],\n",
      "         [-0.9435,  1.1660],\n",
      "         [-0.0574,  1.2116],\n",
      "         [-0.8440,  1.6283]],\n",
      "\n",
      "        [[ 0.3323,  0.3481],\n",
      "         [ 0.1739,  0.5041],\n",
      "         [-0.5753, -1.0406],\n",
      "         [ 0.2134,  0.0750]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "K reshaped =  tensor([[[-0.1676,  0.2603],\n",
      "         [-0.5094, -0.4906],\n",
      "         [ 0.8221, -0.0446],\n",
      "         [ 0.4731,  0.1674]],\n",
      "\n",
      "        [[-0.4620,  0.1106],\n",
      "         [-1.0858,  0.7254],\n",
      "         [ 0.5694,  0.2895],\n",
      "         [ 0.0338,  0.3213]],\n",
      "\n",
      "        [[-0.1651,  0.2971],\n",
      "         [-0.4843, -0.3098],\n",
      "         [ 0.8064,  0.0412],\n",
      "         [ 0.1653,  0.3686]],\n",
      "\n",
      "        [[-0.4332,  0.0689],\n",
      "         [-0.9901,  0.5718],\n",
      "         [ 0.4945,  0.2953],\n",
      "         [-0.1445,  0.0868]],\n",
      "\n",
      "        [[-0.1666,  0.3136],\n",
      "         [-0.4627, -0.2069],\n",
      "         [ 0.7230,  0.1551],\n",
      "         [ 0.0739,  0.4093]],\n",
      "\n",
      "        [[-0.4216,  0.0488],\n",
      "         [-0.9245,  0.4815],\n",
      "         [ 0.3646,  0.2718],\n",
      "         [-0.1876,  0.0186]],\n",
      "\n",
      "        [[-0.1709,  0.3207],\n",
      "         [-0.4539, -0.1587],\n",
      "         [ 0.6312,  0.2235],\n",
      "         [ 0.0287,  0.4186]],\n",
      "\n",
      "        [[-0.4187,  0.0387],\n",
      "         [-0.8926,  0.4369],\n",
      "         [ 0.2603,  0.2419],\n",
      "         [-0.2139, -0.0052]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "V reshaped =  tensor([[[ 0.7822,  0.1640],\n",
      "         [ 1.5860, -0.0715],\n",
      "         [-0.3192, -1.3732],\n",
      "         [ 0.4230, -0.9020]],\n",
      "\n",
      "        [[ 0.4078,  0.4556],\n",
      "         [-0.0843,  0.0768],\n",
      "         [-0.5649, -0.8715],\n",
      "         [-0.0970, -0.2859]],\n",
      "\n",
      "        [[ 0.7322,  0.2052],\n",
      "         [ 1.4438,  0.0667],\n",
      "         [-0.1757, -1.3437],\n",
      "         [ 0.4963, -0.2409]],\n",
      "\n",
      "        [[ 0.4398,  0.4877],\n",
      "         [ 0.0720,  0.2266],\n",
      "         [-0.4614, -0.7649],\n",
      "         [ 0.2956,  0.2226]],\n",
      "\n",
      "        [[ 0.7097,  0.2285],\n",
      "         [ 1.3481,  0.1382],\n",
      "         [ 0.0135, -1.1919],\n",
      "         [ 0.4962, -0.0490]],\n",
      "\n",
      "        [[ 0.4559,  0.5048],\n",
      "         [ 0.1557,  0.3040],\n",
      "         [-0.2858, -0.5614],\n",
      "         [ 0.3919,  0.3537]],\n",
      "\n",
      "        [[ 0.7006,  0.2449],\n",
      "         [ 1.2996,  0.1757],\n",
      "         [ 0.1378, -1.0303],\n",
      "         [ 0.5046,  0.0350]],\n",
      "\n",
      "        [[ 0.4650,  0.5158],\n",
      "         [ 0.1955,  0.3415],\n",
      "         [-0.1543, -0.3975],\n",
      "         [ 0.4293,  0.4081]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "torch.Size([4, 2, 4, 2]) torch.Size([4, 2, 4, 2]) torch.Size([4, 2, 4, 2])\n",
      "\n",
      "scaled_dot_product_attention\n",
      "attn_output = \n",
      "tensor([[ 0.7135, -0.3643, -0.1134, -0.2022],\n",
      "        [ 0.6958, -0.1688,  0.0542, -0.0069],\n",
      "        [ 0.6840, -0.0876,  0.1537,  0.1091],\n",
      "        [ 0.6842, -0.0411,  0.2145,  0.1847],\n",
      "        [ 0.7290, -0.3966, -0.0839, -0.1439],\n",
      "        [ 0.7135, -0.1807,  0.0714,  0.0272],\n",
      "        [ 0.7008, -0.0912,  0.1615,  0.1272],\n",
      "        [ 0.6997, -0.0412,  0.2182,  0.1957],\n",
      "        [ 0.4471, -0.6924,  0.0158, -0.0062],\n",
      "        [ 0.5141, -0.4147,  0.1711,  0.1657],\n",
      "        [ 0.5793, -0.2423,  0.2364,  0.2343],\n",
      "        [ 0.6152, -0.1421,  0.2705,  0.2716],\n",
      "        [ 0.5930, -0.4931, -0.0289, -0.0744],\n",
      "        [ 0.6590, -0.1997,  0.0896,  0.0437],\n",
      "        [ 0.6638, -0.1049,  0.1710,  0.1339],\n",
      "        [ 0.6702, -0.0531,  0.2244,  0.1989]], grad_fn=<ViewBackward0>)\n",
      "\n",
      "x + self_atten(x) = \n",
      "tensor([[[-1.3438,  0.3836, -0.4834,  1.6311],\n",
      "         [-1.3140,  0.5425, -0.3924,  1.4452],\n",
      "         [-1.2766,  0.6206, -0.3307,  1.3647],\n",
      "         [-1.2491,  0.6578, -0.2881,  1.3262]],\n",
      "\n",
      "        [[-1.0411, -0.8138, -0.0183,  2.1852],\n",
      "         [-1.0939, -0.4224, -0.1105,  1.9736],\n",
      "         [-1.0958, -0.2090, -0.1411,  1.8554],\n",
      "         [-1.0837, -0.1078, -0.1378,  1.7926]],\n",
      "\n",
      "        [[-0.9422,  0.0106,  1.9228, -0.1393],\n",
      "         [-1.3047,  0.3377,  1.8777, -0.0883],\n",
      "         [-1.6243,  0.6323,  1.5902,  0.1456],\n",
      "         [-1.7489,  0.7821,  1.3417,  0.3181]],\n",
      "\n",
      "        [[-1.7711,  0.5299,  0.8285,  0.9385],\n",
      "         [-1.6218,  0.8065,  0.1100,  1.0993],\n",
      "         [-1.4968,  0.8658, -0.0282,  1.0946],\n",
      "         [-1.4294,  0.8831, -0.0645,  1.0927]]], grad_fn=<AddBackward0>)\n",
      "\n",
      "norm1(x + self_atten(x)) = \n",
      "tensor([[[-1.2645,  0.3061, -0.4821,  1.4405],\n",
      "         [-1.3440,  0.4584, -0.4492,  1.3348],\n",
      "         [-1.3796,  0.5293, -0.4278,  1.2781],\n",
      "         [-1.3989,  0.5614, -0.4110,  1.2485]],\n",
      "\n",
      "        [[-0.8781, -0.6997, -0.0756,  1.6534],\n",
      "         [-1.0303, -0.4443, -0.1721,  1.6467],\n",
      "         [-1.1095, -0.2883, -0.2255,  1.6233],\n",
      "         [-1.1483, -0.2141, -0.2428,  1.6052]],\n",
      "\n",
      "        [[-1.0985, -0.1925,  1.6260, -0.3350],\n",
      "         [-1.3271,  0.1161,  1.4692, -0.2582],\n",
      "         [-1.5509,  0.3824,  1.2031, -0.0346],\n",
      "         [-1.6465,  0.5216,  1.0008,  0.1241]],\n",
      "\n",
      "        [[-1.7162,  0.3594,  0.6288,  0.7280],\n",
      "         [-1.6287,  0.6703,  0.0109,  0.9475],\n",
      "         [-1.5780,  0.7439, -0.1347,  0.9687],\n",
      "         [-1.5571,  0.7662, -0.1859,  0.9767]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "\n",
      "feed_fwd_op =  tensor([[[ 0.2446, -0.1098, -0.2232, -0.2181],\n",
      "         [ 0.2392, -0.1120, -0.2331, -0.2186],\n",
      "         [ 0.2363, -0.1128, -0.2379, -0.2192],\n",
      "         [ 0.2347, -0.1130, -0.2405, -0.2197]],\n",
      "\n",
      "        [[ 0.2806, -0.0671, -0.1776, -0.2258],\n",
      "         [ 0.2733, -0.0794, -0.1941, -0.2241],\n",
      "         [ 0.2684, -0.0863, -0.2034, -0.2232],\n",
      "         [ 0.2660, -0.0892, -0.2075, -0.2228]],\n",
      "\n",
      "        [[ 0.2572, -0.0020, -0.2062, -0.2688],\n",
      "         [ 0.2438, -0.0183, -0.2338, -0.2664],\n",
      "         [ 0.2333, -0.0395, -0.2590, -0.2575],\n",
      "         [ 0.2293, -0.0531, -0.2711, -0.2507]],\n",
      "\n",
      "        [[ 0.2466, -0.0735, -0.2689, -0.2400],\n",
      "         [ 0.2328, -0.1019, -0.2656, -0.2278],\n",
      "         [ 0.2278, -0.1079, -0.2627, -0.2253],\n",
      "         [ 0.2259, -0.1100, -0.2612, -0.2245]]], grad_fn=<ViewBackward0>)\n",
      "\n",
      "norm2(norm1(x + self_atten(x)) + feed_fwd_op) = \n",
      "tensor([[[-1.0807,  0.3127, -0.7204,  1.4884],\n",
      "         [-1.1770,  0.4916, -0.6913,  1.3766],\n",
      "         [-1.2203,  0.5756, -0.6705,  1.3152],\n",
      "         [-1.2444,  0.6144, -0.6534,  1.2834]],\n",
      "\n",
      "        [[-0.6312, -0.8254, -0.2360,  1.6925],\n",
      "         [-0.8104, -0.5406, -0.3586,  1.7095],\n",
      "         [-0.9041, -0.3634, -0.4263,  1.6938],\n",
      "         [-0.9506, -0.2785, -0.4492,  1.6784]],\n",
      "\n",
      "        [[-0.8912, -0.1581,  1.6714, -0.6221],\n",
      "         [-1.1783,  0.1932,  1.5145, -0.5295],\n",
      "         [-1.4773,  0.5059,  1.2238, -0.2525],\n",
      "         [-1.6060,  0.6696,  0.9849, -0.0485]],\n",
      "\n",
      "        [[-1.7250,  0.4605,  0.5525,  0.7120],\n",
      "         [-1.5541,  0.7847, -0.1953,  0.9648],\n",
      "         [-1.4765,  0.8544, -0.3584,  0.9805],\n",
      "         [-1.4463,  0.8741, -0.4140,  0.9862]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "\n",
      "##################################\n",
      "query =  tensor([[[-1.0807,  0.3127, -0.7204,  1.4884],\n",
      "         [-1.1770,  0.4916, -0.6913,  1.3766],\n",
      "         [-1.2203,  0.5756, -0.6705,  1.3152],\n",
      "         [-1.2444,  0.6144, -0.6534,  1.2834]],\n",
      "\n",
      "        [[-0.6312, -0.8254, -0.2360,  1.6925],\n",
      "         [-0.8104, -0.5406, -0.3586,  1.7095],\n",
      "         [-0.9041, -0.3634, -0.4263,  1.6938],\n",
      "         [-0.9506, -0.2785, -0.4492,  1.6784]],\n",
      "\n",
      "        [[-0.8912, -0.1581,  1.6714, -0.6221],\n",
      "         [-1.1783,  0.1932,  1.5145, -0.5295],\n",
      "         [-1.4773,  0.5059,  1.2238, -0.2525],\n",
      "         [-1.6060,  0.6696,  0.9849, -0.0485]],\n",
      "\n",
      "        [[-1.7250,  0.4605,  0.5525,  0.7120],\n",
      "         [-1.5541,  0.7847, -0.1953,  0.9648],\n",
      "         [-1.4765,  0.8544, -0.3584,  0.9805],\n",
      "         [-1.4463,  0.8741, -0.4140,  0.9862]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "query shape =  torch.Size([4, 4, 4])\n",
      "\n",
      " _in_projection_packed \n",
      "\n",
      "Q :-  tensor([[[ 0.6110,  0.3636, -0.1023, -0.5693],\n",
      "         [ 0.5606,  0.3410, -0.0893, -0.7511],\n",
      "         [ 0.5328,  0.3285, -0.0830, -0.8362],\n",
      "         [ 0.5175,  0.3222, -0.0808, -0.8772]],\n",
      "\n",
      "        [[ 0.6454,  0.4092, -0.2229,  0.4976],\n",
      "         [ 0.6599,  0.4157, -0.2029,  0.2092],\n",
      "         [ 0.6586,  0.4132, -0.1879,  0.0350],\n",
      "         [ 0.6537,  0.4105, -0.1811, -0.0495]],\n",
      "\n",
      "        [[-0.4904, -0.0794, -0.1931, -0.2476],\n",
      "         [-0.4474, -0.0528, -0.1784, -0.6244],\n",
      "         [-0.3177,  0.0142, -0.1692, -0.9654],\n",
      "         [-0.2151,  0.0601, -0.1584, -1.1301]],\n",
      "\n",
      "        [[ 0.1242,  0.2314, -0.2016, -0.9703],\n",
      "         [ 0.3240,  0.2677, -0.1100, -1.1443],\n",
      "         [ 0.3544,  0.2652, -0.0840, -1.1684],\n",
      "         [ 0.3651,  0.2644, -0.0752, -1.1720]]], grad_fn=<SelectBackward0>)\n",
      "\n",
      "K :-  tensor([[[-0.4109, -0.0646, -0.7506,  1.1329],\n",
      "         [-0.4756, -0.0031, -0.7376,  1.2011],\n",
      "         [-0.5070,  0.0265, -0.7252,  1.2273],\n",
      "         [-0.5258,  0.0402, -0.7135,  1.2363]],\n",
      "\n",
      "        [[-0.2731, -0.4326, -0.2860,  0.3274],\n",
      "         [-0.3444, -0.3494, -0.4121,  0.5654],\n",
      "         [-0.3812, -0.2948, -0.4809,  0.6982],\n",
      "         [-0.4024, -0.2681, -0.5061,  0.7562]],\n",
      "\n",
      "        [[-0.9525, -0.0697,  1.3044, -0.7023],\n",
      "         [-1.0758,  0.0239,  1.1355, -0.3697],\n",
      "         [-1.1684,  0.0912,  0.8512,  0.0530],\n",
      "         [-1.1775,  0.1244,  0.6322,  0.3188]],\n",
      "\n",
      "        [[-1.1217, -0.0048,  0.2484,  0.6350],\n",
      "         [-0.8271,  0.0987, -0.3636,  1.1228],\n",
      "         [-0.7403,  0.1262, -0.4918,  1.2075],\n",
      "         [-0.7085,  0.1345, -0.5351,  1.2335]]], grad_fn=<SelectBackward0>)\n",
      "\n",
      "V :-  tensor([[[ 0.0663, -0.7889, -0.8201,  0.1853],\n",
      "         [-0.0229, -0.8402, -0.9104,  0.1986],\n",
      "         [-0.0690, -0.8632, -0.9517,  0.2047],\n",
      "         [-0.0968, -0.8776, -0.9741,  0.2084]],\n",
      "\n",
      "        [[ 0.1208, -0.6337, -0.3947,  0.1440],\n",
      "         [ 0.0858, -0.7268, -0.5517,  0.1672],\n",
      "         [ 0.0640, -0.7708, -0.6362,  0.1784],\n",
      "         [ 0.0466, -0.7937, -0.6783,  0.1842]],\n",
      "\n",
      "        [[-1.4102, -0.8487, -0.7254,  0.2074],\n",
      "         [-1.4717, -1.0170, -0.9689,  0.2486],\n",
      "         [-1.4376, -1.1936, -1.2128,  0.2909],\n",
      "         [-1.3510, -1.2583, -1.3160,  0.3061]],\n",
      "\n",
      "        [[-1.0516, -1.3639, -1.3707,  0.3279],\n",
      "         [-0.5494, -1.1291, -1.2419,  0.2709],\n",
      "         [-0.4262, -1.0452, -1.1832,  0.2508],\n",
      "         [-0.3820, -1.0142, -1.1599,  0.2433]]], grad_fn=<SelectBackward0>)\n",
      "\n",
      "tgt_len =  4\n",
      "bsz =  4\n",
      "num_heads =  2\n",
      "head_dim =  2\n",
      "\n",
      "tensor([[[ 0.6110,  0.3636, -0.1023, -0.5693],\n",
      "         [ 0.5606,  0.3410, -0.0893, -0.7511],\n",
      "         [ 0.5328,  0.3285, -0.0830, -0.8362],\n",
      "         [ 0.5175,  0.3222, -0.0808, -0.8772]],\n",
      "\n",
      "        [[ 0.6454,  0.4092, -0.2229,  0.4976],\n",
      "         [ 0.6599,  0.4157, -0.2029,  0.2092],\n",
      "         [ 0.6586,  0.4132, -0.1879,  0.0350],\n",
      "         [ 0.6537,  0.4105, -0.1811, -0.0495]],\n",
      "\n",
      "        [[-0.4904, -0.0794, -0.1931, -0.2476],\n",
      "         [-0.4474, -0.0528, -0.1784, -0.6244],\n",
      "         [-0.3177,  0.0142, -0.1692, -0.9654],\n",
      "         [-0.2151,  0.0601, -0.1584, -1.1301]],\n",
      "\n",
      "        [[ 0.1242,  0.2314, -0.2016, -0.9703],\n",
      "         [ 0.3240,  0.2677, -0.1100, -1.1443],\n",
      "         [ 0.3544,  0.2652, -0.0840, -1.1684],\n",
      "         [ 0.3651,  0.2644, -0.0752, -1.1720]]], grad_fn=<SelectBackward0>) torch.Size([4, 4, 4])\n",
      "Q reshaped =  tensor([[[ 0.6110,  0.3636],\n",
      "         [ 0.6454,  0.4092],\n",
      "         [-0.4904, -0.0794],\n",
      "         [ 0.1242,  0.2314]],\n",
      "\n",
      "        [[-0.1023, -0.5693],\n",
      "         [-0.2229,  0.4976],\n",
      "         [-0.1931, -0.2476],\n",
      "         [-0.2016, -0.9703]],\n",
      "\n",
      "        [[ 0.5606,  0.3410],\n",
      "         [ 0.6599,  0.4157],\n",
      "         [-0.4474, -0.0528],\n",
      "         [ 0.3240,  0.2677]],\n",
      "\n",
      "        [[-0.0893, -0.7511],\n",
      "         [-0.2029,  0.2092],\n",
      "         [-0.1784, -0.6244],\n",
      "         [-0.1100, -1.1443]],\n",
      "\n",
      "        [[ 0.5328,  0.3285],\n",
      "         [ 0.6586,  0.4132],\n",
      "         [-0.3177,  0.0142],\n",
      "         [ 0.3544,  0.2652]],\n",
      "\n",
      "        [[-0.0830, -0.8362],\n",
      "         [-0.1879,  0.0350],\n",
      "         [-0.1692, -0.9654],\n",
      "         [-0.0840, -1.1684]],\n",
      "\n",
      "        [[ 0.5175,  0.3222],\n",
      "         [ 0.6537,  0.4105],\n",
      "         [-0.2151,  0.0601],\n",
      "         [ 0.3651,  0.2644]],\n",
      "\n",
      "        [[-0.0808, -0.8772],\n",
      "         [-0.1811, -0.0495],\n",
      "         [-0.1584, -1.1301],\n",
      "         [-0.0752, -1.1720]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "K reshaped =  tensor([[[-0.4109, -0.0646],\n",
      "         [-0.2731, -0.4326],\n",
      "         [-0.9525, -0.0697],\n",
      "         [-1.1217, -0.0048]],\n",
      "\n",
      "        [[-0.7506,  1.1329],\n",
      "         [-0.2860,  0.3274],\n",
      "         [ 1.3044, -0.7023],\n",
      "         [ 0.2484,  0.6350]],\n",
      "\n",
      "        [[-0.4756, -0.0031],\n",
      "         [-0.3444, -0.3494],\n",
      "         [-1.0758,  0.0239],\n",
      "         [-0.8271,  0.0987]],\n",
      "\n",
      "        [[-0.7376,  1.2011],\n",
      "         [-0.4121,  0.5654],\n",
      "         [ 1.1355, -0.3697],\n",
      "         [-0.3636,  1.1228]],\n",
      "\n",
      "        [[-0.5070,  0.0265],\n",
      "         [-0.3812, -0.2948],\n",
      "         [-1.1684,  0.0912],\n",
      "         [-0.7403,  0.1262]],\n",
      "\n",
      "        [[-0.7252,  1.2273],\n",
      "         [-0.4809,  0.6982],\n",
      "         [ 0.8512,  0.0530],\n",
      "         [-0.4918,  1.2075]],\n",
      "\n",
      "        [[-0.5258,  0.0402],\n",
      "         [-0.4024, -0.2681],\n",
      "         [-1.1775,  0.1244],\n",
      "         [-0.7085,  0.1345]],\n",
      "\n",
      "        [[-0.7135,  1.2363],\n",
      "         [-0.5061,  0.7562],\n",
      "         [ 0.6322,  0.3188],\n",
      "         [-0.5351,  1.2335]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "V reshaped =  tensor([[[ 0.0663, -0.7889],\n",
      "         [ 0.1208, -0.6337],\n",
      "         [-1.4102, -0.8487],\n",
      "         [-1.0516, -1.3639]],\n",
      "\n",
      "        [[-0.8201,  0.1853],\n",
      "         [-0.3947,  0.1440],\n",
      "         [-0.7254,  0.2074],\n",
      "         [-1.3707,  0.3279]],\n",
      "\n",
      "        [[-0.0229, -0.8402],\n",
      "         [ 0.0858, -0.7268],\n",
      "         [-1.4717, -1.0170],\n",
      "         [-0.5494, -1.1291]],\n",
      "\n",
      "        [[-0.9104,  0.1986],\n",
      "         [-0.5517,  0.1672],\n",
      "         [-0.9689,  0.2486],\n",
      "         [-1.2419,  0.2709]],\n",
      "\n",
      "        [[-0.0690, -0.8632],\n",
      "         [ 0.0640, -0.7708],\n",
      "         [-1.4376, -1.1936],\n",
      "         [-0.4262, -1.0452]],\n",
      "\n",
      "        [[-0.9517,  0.2047],\n",
      "         [-0.6362,  0.1784],\n",
      "         [-1.2128,  0.2909],\n",
      "         [-1.1832,  0.2508]],\n",
      "\n",
      "        [[-0.0968, -0.8776],\n",
      "         [ 0.0466, -0.7937],\n",
      "         [-1.3510, -1.2583],\n",
      "         [-0.3820, -1.0142]],\n",
      "\n",
      "        [[-0.9741,  0.2084],\n",
      "         [-0.6783,  0.1842],\n",
      "         [-1.3160,  0.3061],\n",
      "         [-1.1599,  0.2433]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "torch.Size([4, 2, 4, 2]) torch.Size([4, 2, 4, 2]) torch.Size([4, 2, 4, 2])\n",
      "\n",
      "scaled_dot_product_attention\n",
      "attn_output = \n",
      "tensor([[-0.4902, -0.8822, -0.7985,  0.2131],\n",
      "        [-0.4371, -0.9190, -0.9039,  0.2230],\n",
      "        [-0.4170, -0.9556, -1.0063,  0.2371],\n",
      "        [-0.4021, -0.9725, -1.0447,  0.2411],\n",
      "        [-0.4870, -0.8813, -0.8420,  0.2145],\n",
      "        [-0.4288, -0.9176, -0.9205,  0.2194],\n",
      "        [-0.4061, -0.9529, -0.9863,  0.2282],\n",
      "        [-0.3914, -0.9692, -1.0225,  0.2332],\n",
      "        [-0.6417, -0.9358, -0.8117,  0.2133],\n",
      "        [-0.5425, -0.9392, -0.9029,  0.2214],\n",
      "        [-0.5079, -0.9793, -1.0034,  0.2368],\n",
      "        [-0.4729, -0.9948, -1.0453,  0.2419],\n",
      "        [-0.5603, -0.9068, -0.7786,  0.2108],\n",
      "        [-0.4617, -0.9241, -0.9012,  0.2249],\n",
      "        [-0.4351, -0.9604, -1.0154,  0.2406],\n",
      "        [-0.4159, -0.9769, -1.0529,  0.2439]], grad_fn=<ViewBackward0>)\n",
      "\n",
      "x + self_atten(x) = \n",
      "tensor([[[-1.0489,  1.0584,  0.0671,  2.1568],\n",
      "         [-1.1042,  1.2399,  0.0765,  2.0726],\n",
      "         [-1.1235,  1.3538,  0.0982,  2.0596],\n",
      "         [-1.1336,  1.3969,  0.1133,  2.0397]],\n",
      "\n",
      "        [[-0.5928, -0.0640,  0.5476,  2.3868],\n",
      "         [-0.7293,  0.2057,  0.4043,  2.4088],\n",
      "         [-0.7999,  0.3933,  0.3381,  2.4148],\n",
      "         [-0.8339,  0.4824,  0.3123,  2.4109]],\n",
      "\n",
      "        [[-0.9017,  0.7069,  2.5991,  0.1330],\n",
      "         [-1.1403,  1.0211,  2.3699,  0.2242],\n",
      "         [-1.4097,  1.3520,  2.0717,  0.5394],\n",
      "         [-1.5172,  1.5063,  1.8150,  0.7461]],\n",
      "\n",
      "        [[-1.7149,  1.2495,  1.4062,  1.4032],\n",
      "         [-1.4915,  1.5519,  0.5919,  1.6739],\n",
      "         [-1.3868,  1.6521,  0.4242,  1.7423],\n",
      "         [-1.3406,  1.6720,  0.3636,  1.7565]]], grad_fn=<AddBackward0>)\n",
      "\n",
      "norm1(x + self_atten(x)) = \n",
      "tensor([[[-1.3548,  0.4215, -0.4141,  1.3474],\n",
      "         [-1.3970,  0.5576, -0.4125,  1.2519],\n",
      "         [-1.4142,  0.6220, -0.4100,  1.2021],\n",
      "         [-1.4248,  0.6501, -0.4024,  1.1771]],\n",
      "\n",
      "        [[-1.0338, -0.5634, -0.0194,  1.6166],\n",
      "         [-1.1384, -0.3207, -0.1470,  1.6061],\n",
      "         [-1.1973, -0.1669, -0.2146,  1.5788],\n",
      "         [-1.2244, -0.0948, -0.2408,  1.5601]],\n",
      "\n",
      "        [[-1.2071,  0.0570,  1.5441, -0.3940],\n",
      "         [-1.3822,  0.3162,  1.3760, -0.3100],\n",
      "         [-1.5745,  0.5486,  1.1019, -0.0761],\n",
      "         [-1.6531,  0.6665,  0.9033,  0.0832]],\n",
      "\n",
      "        [[-1.7301,  0.4989,  0.6167,  0.6145],\n",
      "         [-1.6347,  0.7652,  0.0081,  0.8614],\n",
      "         [-1.5783,  0.8261, -0.1454,  0.8975],\n",
      "         [-1.5556,  0.8434, -0.1985,  0.9107]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "\n",
      "feed_fwd_op =  tensor([[[-0.1301,  0.2823,  0.1002, -0.3423],\n",
      "         [-0.1253,  0.2842,  0.1002, -0.3522],\n",
      "         [-0.1223,  0.2851,  0.1006, -0.3577],\n",
      "         [-0.1209,  0.2856,  0.1010, -0.3611]],\n",
      "\n",
      "        [[-0.1374,  0.2645,  0.1250, -0.3415],\n",
      "         [-0.1391,  0.2688,  0.1146, -0.3362],\n",
      "         [-0.1395,  0.2716,  0.1088, -0.3342],\n",
      "         [-0.1391,  0.2728,  0.1069, -0.3343]],\n",
      "\n",
      "        [[-0.0716,  0.2909,  0.2440, -0.5621],\n",
      "         [-0.0647,  0.2957,  0.2213, -0.5600],\n",
      "         [-0.0658,  0.3005,  0.1901, -0.5433],\n",
      "         [-0.0687,  0.3012,  0.1729, -0.5263]],\n",
      "\n",
      "        [[-0.0986,  0.2977,  0.1378, -0.4796],\n",
      "         [-0.1071,  0.2928,  0.1089, -0.4227],\n",
      "         [-0.1081,  0.2914,  0.1067, -0.4083],\n",
      "         [-0.1083,  0.2910,  0.1063, -0.4029]]], grad_fn=<ViewBackward0>)\n",
      "\n",
      "norm2(norm1(x + self_atten(x)) + feed_fwd_op) = \n",
      "tensor([[[-1.4990,  0.7444, -0.2987,  1.0533],\n",
      "         [-1.5121,  0.8727, -0.2916,  0.9310],\n",
      "         [-1.5148,  0.9319, -0.2861,  0.8690],\n",
      "         [-1.5182,  0.9572, -0.2769,  0.8379]],\n",
      "\n",
      "        [[-1.3058, -0.3143,  0.1455,  1.4747],\n",
      "         [-1.3926, -0.0321, -0.0105,  1.4351],\n",
      "         [-1.4340,  0.1398, -0.0900,  1.3842],\n",
      "         [-1.4516,  0.2181, -0.1197,  1.3531]],\n",
      "\n",
      "        [[-1.0356,  0.3077,  1.4971, -0.7692],\n",
      "         [-1.1819,  0.5317,  1.3520, -0.7017],\n",
      "         [-1.3785,  0.7521,  1.1311, -0.5047],\n",
      "         [-1.4763,  0.8709,  0.9656, -0.3603]],\n",
      "\n",
      "        [[-1.6791,  0.7794,  0.7400,  0.1597],\n",
      "         [-1.6386,  1.0445,  0.1429,  0.4511],\n",
      "         [-1.5924,  1.1025, -0.0088,  0.4986],\n",
      "         [-1.5740,  1.1191, -0.0613,  0.5162]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "\n",
      "##################################\n",
      "\n",
      "################### Transformer Decoder start #####################\n",
      "\n",
      "query =  tensor([[[ 0.7793,  1.5656,  0.8461,  1.6169],\n",
      "         [-0.2207,  0.5656, -0.1539,  0.6169],\n",
      "         [ 2.7793,  3.5656,  2.8461,  3.6169],\n",
      "         [ 2.7793,  3.5656,  2.8461,  3.6169]],\n",
      "\n",
      "        [[-0.3731,  1.2663,  0.9056,  1.9779],\n",
      "         [-1.3731,  0.2663, -0.0944,  0.9779],\n",
      "         [ 1.6269,  3.2663,  2.9056,  3.9779],\n",
      "         [ 1.6269,  3.2663,  2.9056,  3.9779]],\n",
      "\n",
      "        [[ 1.4218,  2.4912, -0.6193, -0.3457],\n",
      "         [ 0.4218,  1.4912, -1.6193, -1.3457],\n",
      "         [ 3.4218,  4.4912,  1.3807,  1.6543],\n",
      "         [ 3.4218,  4.4912,  1.3807,  1.6543]],\n",
      "\n",
      "        [[ 1.4218,  2.4912, -0.6193, -0.3457],\n",
      "         [ 0.4218,  1.4912, -1.6193, -1.3457],\n",
      "         [ 3.4218,  4.4912,  1.3807,  1.6543],\n",
      "         [ 3.4218,  4.4912,  1.3807,  1.6543]]], grad_fn=<AddBackward0>)\n",
      "query shape =  torch.Size([4, 4, 4])\n",
      "\n",
      " _in_projection_packed \n",
      "\n",
      "Q :-  tensor([[[-0.7885,  1.1865,  0.8490,  0.0050],\n",
      "         [-0.3683,  0.3795,  0.2103,  0.5547],\n",
      "         [-1.6290,  2.8004,  2.1264, -1.0945],\n",
      "         [-1.6290,  2.8004,  2.1264, -1.0945]],\n",
      "\n",
      "        [[-0.7394,  0.5141,  0.1118,  0.7839],\n",
      "         [-0.3192, -0.2928, -0.5269,  1.3336],\n",
      "         [-1.5799,  2.1281,  1.3892, -0.3156],\n",
      "         [-1.5799,  2.1281,  1.3892, -0.3156]],\n",
      "\n",
      "        [[-0.8008,  1.8334,  1.8169, -0.6278],\n",
      "         [-0.3806,  1.0264,  1.1782, -0.0780],\n",
      "         [-1.6413,  3.4473,  3.0943, -1.7272],\n",
      "         [-1.6413,  3.4473,  3.0943, -1.7272]],\n",
      "\n",
      "        [[-0.8008,  1.8334,  1.8169, -0.6278],\n",
      "         [-0.3806,  1.0264,  1.1782, -0.0780],\n",
      "         [-1.6413,  3.4473,  3.0943, -1.7272],\n",
      "         [-1.6413,  3.4473,  3.0943, -1.7272]]], grad_fn=<SelectBackward0>)\n",
      "\n",
      "K :-  tensor([[[-4.7693e-01,  2.5232e+00,  4.8025e-01, -9.4048e-01],\n",
      "         [-1.9006e-01,  4.5797e-01, -1.2757e-03, -1.8467e-01],\n",
      "         [-1.0507e+00,  6.6537e+00,  1.4433e+00, -2.4521e+00],\n",
      "         [-1.0507e+00,  6.6537e+00,  1.4433e+00, -2.4521e+00]],\n",
      "\n",
      "        [[ 1.0854e-01,  2.1713e+00,  5.0683e-01, -8.4301e-01],\n",
      "         [ 3.9540e-01,  1.0605e-01,  2.5305e-02, -8.7211e-02],\n",
      "         [-4.6519e-01,  6.3017e+00,  1.4699e+00, -2.3546e+00],\n",
      "         [-4.6519e-01,  6.3017e+00,  1.4699e+00, -2.3546e+00]],\n",
      "\n",
      "        [[-1.8438e+00,  1.1542e+00, -1.0318e+00,  6.0348e-02],\n",
      "         [-1.5569e+00, -9.1103e-01, -1.5133e+00,  8.1615e-01],\n",
      "         [-2.4175e+00,  5.2847e+00, -6.8767e-02, -1.4513e+00],\n",
      "         [-2.4175e+00,  5.2847e+00, -6.8767e-02, -1.4513e+00]],\n",
      "\n",
      "        [[-1.8438e+00,  1.1542e+00, -1.0318e+00,  6.0348e-02],\n",
      "         [-1.5569e+00, -9.1103e-01, -1.5133e+00,  8.1615e-01],\n",
      "         [-2.4175e+00,  5.2847e+00, -6.8767e-02, -1.4513e+00],\n",
      "         [-2.4175e+00,  5.2847e+00, -6.8767e-02, -1.4513e+00]]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "\n",
      "V :-  tensor([[[-2.4422e+00,  4.8449e-01, -1.7597e+00, -1.1242e-01],\n",
      "         [-3.6649e-01,  1.5799e-01, -2.2292e-01,  3.3488e-01],\n",
      "         [-6.5935e+00,  1.1375e+00, -4.8332e+00, -1.0070e+00],\n",
      "         [-6.5935e+00,  1.1375e+00, -4.8332e+00, -1.0070e+00]],\n",
      "\n",
      "        [[-1.9011e+00,  3.9233e-01, -1.6004e+00,  6.3111e-01],\n",
      "         [ 1.7460e-01,  6.5828e-02, -6.3583e-02,  1.0784e+00],\n",
      "         [-6.0525e+00,  1.0453e+00, -4.6739e+00, -2.6350e-01],\n",
      "         [-6.0525e+00,  1.0453e+00, -4.6739e+00, -2.6350e-01]],\n",
      "\n",
      "        [[-1.5094e+00,  1.1968e+00, -6.0689e-03, -1.0117e+00],\n",
      "         [ 5.6626e-01,  8.7033e-01,  1.5307e+00, -5.6444e-01],\n",
      "         [-5.6608e+00,  1.8498e+00, -3.0796e+00, -1.9063e+00],\n",
      "         [-5.6608e+00,  1.8498e+00, -3.0796e+00, -1.9063e+00]],\n",
      "\n",
      "        [[-1.5094e+00,  1.1968e+00, -6.0689e-03, -1.0117e+00],\n",
      "         [ 5.6626e-01,  8.7033e-01,  1.5307e+00, -5.6444e-01],\n",
      "         [-5.6608e+00,  1.8498e+00, -3.0796e+00, -1.9063e+00],\n",
      "         [-5.6608e+00,  1.8498e+00, -3.0796e+00, -1.9063e+00]]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "\n",
      "tgt_len =  4\n",
      "bsz =  4\n",
      "num_heads =  2\n",
      "head_dim =  2\n",
      "\n",
      "tensor([[[-0.7885,  1.1865,  0.8490,  0.0050],\n",
      "         [-0.3683,  0.3795,  0.2103,  0.5547],\n",
      "         [-1.6290,  2.8004,  2.1264, -1.0945],\n",
      "         [-1.6290,  2.8004,  2.1264, -1.0945]],\n",
      "\n",
      "        [[-0.7394,  0.5141,  0.1118,  0.7839],\n",
      "         [-0.3192, -0.2928, -0.5269,  1.3336],\n",
      "         [-1.5799,  2.1281,  1.3892, -0.3156],\n",
      "         [-1.5799,  2.1281,  1.3892, -0.3156]],\n",
      "\n",
      "        [[-0.8008,  1.8334,  1.8169, -0.6278],\n",
      "         [-0.3806,  1.0264,  1.1782, -0.0780],\n",
      "         [-1.6413,  3.4473,  3.0943, -1.7272],\n",
      "         [-1.6413,  3.4473,  3.0943, -1.7272]],\n",
      "\n",
      "        [[-0.8008,  1.8334,  1.8169, -0.6278],\n",
      "         [-0.3806,  1.0264,  1.1782, -0.0780],\n",
      "         [-1.6413,  3.4473,  3.0943, -1.7272],\n",
      "         [-1.6413,  3.4473,  3.0943, -1.7272]]], grad_fn=<SelectBackward0>) torch.Size([4, 4, 4])\n",
      "Q reshaped =  tensor([[[-0.7885,  1.1865],\n",
      "         [-0.7394,  0.5141],\n",
      "         [-0.8008,  1.8334],\n",
      "         [-0.8008,  1.8334]],\n",
      "\n",
      "        [[ 0.8490,  0.0050],\n",
      "         [ 0.1118,  0.7839],\n",
      "         [ 1.8169, -0.6278],\n",
      "         [ 1.8169, -0.6278]],\n",
      "\n",
      "        [[-0.3683,  0.3795],\n",
      "         [-0.3192, -0.2928],\n",
      "         [-0.3806,  1.0264],\n",
      "         [-0.3806,  1.0264]],\n",
      "\n",
      "        [[ 0.2103,  0.5547],\n",
      "         [-0.5269,  1.3336],\n",
      "         [ 1.1782, -0.0780],\n",
      "         [ 1.1782, -0.0780]],\n",
      "\n",
      "        [[-1.6290,  2.8004],\n",
      "         [-1.5799,  2.1281],\n",
      "         [-1.6413,  3.4473],\n",
      "         [-1.6413,  3.4473]],\n",
      "\n",
      "        [[ 2.1264, -1.0945],\n",
      "         [ 1.3892, -0.3156],\n",
      "         [ 3.0943, -1.7272],\n",
      "         [ 3.0943, -1.7272]],\n",
      "\n",
      "        [[-1.6290,  2.8004],\n",
      "         [-1.5799,  2.1281],\n",
      "         [-1.6413,  3.4473],\n",
      "         [-1.6413,  3.4473]],\n",
      "\n",
      "        [[ 2.1264, -1.0945],\n",
      "         [ 1.3892, -0.3156],\n",
      "         [ 3.0943, -1.7272],\n",
      "         [ 3.0943, -1.7272]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "K reshaped =  tensor([[[-4.7693e-01,  2.5232e+00],\n",
      "         [ 1.0854e-01,  2.1713e+00],\n",
      "         [-1.8438e+00,  1.1542e+00],\n",
      "         [-1.8438e+00,  1.1542e+00]],\n",
      "\n",
      "        [[ 4.8025e-01, -9.4048e-01],\n",
      "         [ 5.0683e-01, -8.4301e-01],\n",
      "         [-1.0318e+00,  6.0348e-02],\n",
      "         [-1.0318e+00,  6.0348e-02]],\n",
      "\n",
      "        [[-1.9006e-01,  4.5797e-01],\n",
      "         [ 3.9540e-01,  1.0605e-01],\n",
      "         [-1.5569e+00, -9.1103e-01],\n",
      "         [-1.5569e+00, -9.1103e-01]],\n",
      "\n",
      "        [[-1.2757e-03, -1.8467e-01],\n",
      "         [ 2.5305e-02, -8.7211e-02],\n",
      "         [-1.5133e+00,  8.1615e-01],\n",
      "         [-1.5133e+00,  8.1615e-01]],\n",
      "\n",
      "        [[-1.0507e+00,  6.6537e+00],\n",
      "         [-4.6519e-01,  6.3017e+00],\n",
      "         [-2.4175e+00,  5.2847e+00],\n",
      "         [-2.4175e+00,  5.2847e+00]],\n",
      "\n",
      "        [[ 1.4433e+00, -2.4521e+00],\n",
      "         [ 1.4699e+00, -2.3546e+00],\n",
      "         [-6.8767e-02, -1.4513e+00],\n",
      "         [-6.8767e-02, -1.4513e+00]],\n",
      "\n",
      "        [[-1.0507e+00,  6.6537e+00],\n",
      "         [-4.6519e-01,  6.3017e+00],\n",
      "         [-2.4175e+00,  5.2847e+00],\n",
      "         [-2.4175e+00,  5.2847e+00]],\n",
      "\n",
      "        [[ 1.4433e+00, -2.4521e+00],\n",
      "         [ 1.4699e+00, -2.3546e+00],\n",
      "         [-6.8767e-02, -1.4513e+00],\n",
      "         [-6.8767e-02, -1.4513e+00]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "V reshaped =  tensor([[[-2.4422e+00,  4.8449e-01],\n",
      "         [-1.9011e+00,  3.9233e-01],\n",
      "         [-1.5094e+00,  1.1968e+00],\n",
      "         [-1.5094e+00,  1.1968e+00]],\n",
      "\n",
      "        [[-1.7597e+00, -1.1242e-01],\n",
      "         [-1.6004e+00,  6.3111e-01],\n",
      "         [-6.0689e-03, -1.0117e+00],\n",
      "         [-6.0689e-03, -1.0117e+00]],\n",
      "\n",
      "        [[-3.6649e-01,  1.5799e-01],\n",
      "         [ 1.7460e-01,  6.5828e-02],\n",
      "         [ 5.6626e-01,  8.7033e-01],\n",
      "         [ 5.6626e-01,  8.7033e-01]],\n",
      "\n",
      "        [[-2.2292e-01,  3.3488e-01],\n",
      "         [-6.3583e-02,  1.0784e+00],\n",
      "         [ 1.5307e+00, -5.6444e-01],\n",
      "         [ 1.5307e+00, -5.6444e-01]],\n",
      "\n",
      "        [[-6.5935e+00,  1.1375e+00],\n",
      "         [-6.0525e+00,  1.0453e+00],\n",
      "         [-5.6608e+00,  1.8498e+00],\n",
      "         [-5.6608e+00,  1.8498e+00]],\n",
      "\n",
      "        [[-4.8332e+00, -1.0070e+00],\n",
      "         [-4.6739e+00, -2.6350e-01],\n",
      "         [-3.0796e+00, -1.9063e+00],\n",
      "         [-3.0796e+00, -1.9063e+00]],\n",
      "\n",
      "        [[-6.5935e+00,  1.1375e+00],\n",
      "         [-6.0525e+00,  1.0453e+00],\n",
      "         [-5.6608e+00,  1.8498e+00],\n",
      "         [-5.6608e+00,  1.8498e+00]],\n",
      "\n",
      "        [[-4.8332e+00, -1.0070e+00],\n",
      "         [-4.6739e+00, -2.6350e-01],\n",
      "         [-3.0796e+00, -1.9063e+00],\n",
      "         [-3.0796e+00, -1.9063e+00]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "torch.Size([4, 2, 4, 2]) torch.Size([4, 2, 4, 2]) torch.Size([4, 2, 4, 2])\n",
      "\n",
      "scaled_dot_product_attention\n",
      "attn_output = \n",
      "tensor([[-1.9041,  0.8017, -1.2000, -0.1027],\n",
      "        [ 0.2367,  0.5136,  0.7557,  0.0283],\n",
      "        [-6.2053,  1.3664, -4.6778, -0.7001],\n",
      "        [-6.2053,  1.3664, -4.6778, -0.7001],\n",
      "        [-1.7965,  0.8983, -0.6742, -0.4995],\n",
      "        [ 0.3320,  0.6057,  1.2172, -0.3231],\n",
      "        [-6.0853,  1.4632, -4.4972, -0.8291],\n",
      "        [-6.0853,  1.4632, -4.4972, -0.8291],\n",
      "        [-2.0167,  0.7045, -1.5389,  0.1503],\n",
      "        [ 0.1299,  0.4146,  0.2093,  0.4418],\n",
      "        [-6.3117,  1.2832, -4.7376, -0.6604],\n",
      "        [-6.3117,  1.2832, -4.7376, -0.6604],\n",
      "        [-2.0167,  0.7045, -1.5389,  0.1503],\n",
      "        [ 0.1299,  0.4146,  0.2093,  0.4418],\n",
      "        [-6.3117,  1.2832, -4.7376, -0.6604],\n",
      "        [-6.3117,  1.2832, -4.7376, -0.6604]], grad_fn=<ViewBackward0>)\n",
      "\n",
      "x =  tensor([[[ 0.7793,  1.5656,  0.8461,  1.6169],\n",
      "         [-0.2207,  0.5656, -0.1539,  0.6169],\n",
      "         [ 2.7793,  3.5656,  2.8461,  3.6169],\n",
      "         [ 2.7793,  3.5656,  2.8461,  3.6169]],\n",
      "\n",
      "        [[-0.3731,  1.2663,  0.9056,  1.9779],\n",
      "         [-1.3731,  0.2663, -0.0944,  0.9779],\n",
      "         [ 1.6269,  3.2663,  2.9056,  3.9779],\n",
      "         [ 1.6269,  3.2663,  2.9056,  3.9779]],\n",
      "\n",
      "        [[ 1.4218,  2.4912, -0.6193, -0.3457],\n",
      "         [ 0.4218,  1.4912, -1.6193, -1.3457],\n",
      "         [ 3.4218,  4.4912,  1.3807,  1.6543],\n",
      "         [ 3.4218,  4.4912,  1.3807,  1.6543]],\n",
      "\n",
      "        [[ 1.4218,  2.4912, -0.6193, -0.3457],\n",
      "         [ 0.4218,  1.4912, -1.6193, -1.3457],\n",
      "         [ 3.4218,  4.4912,  1.3807,  1.6543],\n",
      "         [ 3.4218,  4.4912,  1.3807,  1.6543]]], grad_fn=<AddBackward0>)\n",
      "\n",
      "sa_op =  tensor([[[-1.9507e+00,  4.7462e-01, -1.0075e-01,  4.6415e-01],\n",
      "         [ 8.3321e-01,  1.5961e-01,  3.9454e-01, -6.7873e-01],\n",
      "         [-7.1701e+00,  8.5733e-01, -8.5307e-01,  2.5738e+00],\n",
      "         [-7.1701e+00,  8.5733e-01, -8.5307e-01,  2.5738e+00]],\n",
      "\n",
      "        [[-1.4465e+00,  2.0915e-01,  2.7669e-01,  1.4224e-01],\n",
      "         [ 1.2769e+00, -7.2815e-02,  7.3196e-01, -9.6654e-01],\n",
      "         [-6.9459e+00,  7.8626e-01, -6.7149e-01,  2.3988e+00],\n",
      "         [-6.9459e+00,  7.8626e-01, -6.7149e-01,  2.3988e+00]],\n",
      "\n",
      "        [[-2.3023e+00,  6.3529e-01, -3.7264e-01,  7.0634e-01],\n",
      "         [ 3.1209e-01,  4.3628e-01,  3.7288e-03, -3.4649e-01],\n",
      "         [-7.2852e+00,  8.6666e-01, -9.5669e-01,  2.6836e+00],\n",
      "         [-7.2852e+00,  8.6666e-01, -9.5669e-01,  2.6836e+00]],\n",
      "\n",
      "        [[-2.3023e+00,  6.3529e-01, -3.7264e-01,  7.0634e-01],\n",
      "         [ 3.1209e-01,  4.3628e-01,  3.7288e-03, -3.4649e-01],\n",
      "         [-7.2852e+00,  8.6666e-01, -9.5669e-01,  2.6836e+00],\n",
      "         [-7.2852e+00,  8.6666e-01, -9.5669e-01,  2.6836e+00]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "\n",
      "temop =  tensor([[[-1.1714,  2.0402,  0.7454,  2.0811],\n",
      "         [ 0.6125,  0.7252,  0.2407, -0.0618],\n",
      "         [-4.3908,  4.4229,  1.9931,  6.1907],\n",
      "         [-4.3908,  4.4229,  1.9931,  6.1907]],\n",
      "\n",
      "        [[-1.8196,  1.4754,  1.1822,  2.1202],\n",
      "         [-0.0961,  0.1934,  0.6375,  0.0114],\n",
      "         [-5.3190,  4.0525,  2.2341,  6.3768],\n",
      "         [-5.3190,  4.0525,  2.2341,  6.3768]],\n",
      "\n",
      "        [[-0.8805,  3.1265, -0.9919,  0.3606],\n",
      "         [ 0.7339,  1.9275, -1.6155, -1.6922],\n",
      "         [-3.8634,  5.3578,  0.4240,  4.3379],\n",
      "         [-3.8634,  5.3578,  0.4240,  4.3379]],\n",
      "\n",
      "        [[-0.8805,  3.1265, -0.9919,  0.3606],\n",
      "         [ 0.7339,  1.9275, -1.6155, -1.6922],\n",
      "         [-3.8634,  5.3578,  0.4240,  4.3379],\n",
      "         [-3.8634,  5.3578,  0.4240,  4.3379]]], grad_fn=<AddBackward0>)\n",
      "\n",
      "Decoder norm1(x + sa(x)) = \n",
      "tensor([[[-1.5830,  0.8435, -0.1348,  0.8743],\n",
      "         [ 0.7494,  1.1113, -0.4446, -1.4161],\n",
      "         [-1.6079,  0.5910, -0.0152,  1.0321],\n",
      "         [-1.6079,  0.5910, -0.0152,  1.0321]],\n",
      "\n",
      "        [[-1.6881,  0.4854,  0.2920,  0.9107],\n",
      "         [-1.0088,  0.0246,  1.6094, -0.6252],\n",
      "         [-1.6320,  0.5056,  0.0908,  1.0357],\n",
      "         [-1.6320,  0.5056,  0.0908,  1.0357]],\n",
      "\n",
      "        [[-0.7740,  1.6410, -0.8411, -0.0259],\n",
      "         [ 0.5773,  1.3469, -0.9374, -0.9868],\n",
      "         [-1.4932,  1.0438, -0.3137,  0.7631],\n",
      "         [-1.4932,  1.0438, -0.3137,  0.7631]],\n",
      "\n",
      "        [[-0.7740,  1.6410, -0.8411, -0.0259],\n",
      "         [ 0.5773,  1.3469, -0.9374, -0.9868],\n",
      "         [-1.4932,  1.0438, -0.3137,  0.7631],\n",
      "         [-1.4932,  1.0438, -0.3137,  0.7631]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "\n",
      "Memory = \n",
      "tensor([[[-1.4990,  0.7444, -0.2987,  1.0533],\n",
      "         [-1.5121,  0.8727, -0.2916,  0.9310],\n",
      "         [-1.5148,  0.9319, -0.2861,  0.8690],\n",
      "         [-1.5182,  0.9572, -0.2769,  0.8379]],\n",
      "\n",
      "        [[-1.3058, -0.3143,  0.1455,  1.4747],\n",
      "         [-1.3926, -0.0321, -0.0105,  1.4351],\n",
      "         [-1.4340,  0.1398, -0.0900,  1.3842],\n",
      "         [-1.4516,  0.2181, -0.1197,  1.3531]],\n",
      "\n",
      "        [[-1.0356,  0.3077,  1.4970, -0.7692],\n",
      "         [-1.1819,  0.5317,  1.3520, -0.7017],\n",
      "         [-1.3785,  0.7521,  1.1311, -0.5047],\n",
      "         [-1.4763,  0.8709,  0.9656, -0.3603]],\n",
      "\n",
      "        [[-1.6791,  0.7794,  0.7400,  0.1597],\n",
      "         [-1.6386,  1.0445,  0.1429,  0.4511],\n",
      "         [-1.5924,  1.1025, -0.0088,  0.4986],\n",
      "         [-1.5740,  1.1191, -0.0613,  0.5162]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "\n",
      "query =  tensor([[[-1.5830,  0.8435, -0.1348,  0.8743],\n",
      "         [ 0.7494,  1.1113, -0.4446, -1.4161],\n",
      "         [-1.6079,  0.5910, -0.0152,  1.0321],\n",
      "         [-1.6079,  0.5910, -0.0152,  1.0321]],\n",
      "\n",
      "        [[-1.6881,  0.4854,  0.2920,  0.9107],\n",
      "         [-1.0088,  0.0246,  1.6094, -0.6252],\n",
      "         [-1.6320,  0.5056,  0.0908,  1.0357],\n",
      "         [-1.6320,  0.5056,  0.0908,  1.0357]],\n",
      "\n",
      "        [[-0.7740,  1.6410, -0.8411, -0.0259],\n",
      "         [ 0.5773,  1.3469, -0.9374, -0.9868],\n",
      "         [-1.4932,  1.0438, -0.3137,  0.7631],\n",
      "         [-1.4932,  1.0438, -0.3137,  0.7631]],\n",
      "\n",
      "        [[-0.7740,  1.6410, -0.8411, -0.0259],\n",
      "         [ 0.5773,  1.3469, -0.9374, -0.9868],\n",
      "         [-1.4932,  1.0438, -0.3137,  0.7631],\n",
      "         [-1.4932,  1.0438, -0.3137,  0.7631]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "query shape =  torch.Size([4, 4, 4])\n",
      "\n",
      " _in_projection_packed \n",
      "\n",
      "Q :-  tensor([[[-1.5499, -0.1699, -1.1429, -0.1188],\n",
      "         [ 0.6232,  0.6521,  0.0169, -0.3579],\n",
      "         [-1.5266, -0.2734, -1.0515, -0.0299],\n",
      "         [-1.5266, -0.2734, -1.0515, -0.0299]],\n",
      "\n",
      "        [[-1.3632, -0.3399, -0.9174,  0.1513],\n",
      "         [ 0.3821, -0.3124,  0.2599,  0.9101],\n",
      "         [-1.4775, -0.3137, -0.9949,  0.0368],\n",
      "         [-1.4775, -0.3137, -0.9949,  0.0368]],\n",
      "\n",
      "        [[-1.1641,  0.3842, -1.1776, -0.5957],\n",
      "         [ 0.0683,  0.6909, -0.3859, -0.6488],\n",
      "         [-1.5703, -0.0657, -1.2190, -0.2373],\n",
      "         [-1.5703, -0.0657, -1.2190, -0.2373]],\n",
      "\n",
      "        [[-1.1641,  0.3842, -1.1776, -0.5957],\n",
      "         [ 0.0683,  0.6909, -0.3859, -0.6488],\n",
      "         [-1.5703, -0.0657, -1.2190, -0.2373],\n",
      "         [-1.5703, -0.0657, -1.2190, -0.2373]]], grad_fn=<ViewBackward0>)\n",
      "\n",
      "K :-  tensor([[[ 1.1857, -0.1569, -1.2063, -1.6157],\n",
      "         [ 1.1432, -0.1255, -1.2047, -1.6500],\n",
      "         [ 1.1186, -0.1094, -1.2010, -1.6613],\n",
      "         [ 1.1024, -0.0986, -1.2008, -1.6617]],\n",
      "\n",
      "        [[ 0.9291, -0.1052, -1.1116, -0.8156],\n",
      "         [ 1.0645, -0.1449, -1.1690, -1.0901],\n",
      "         [ 1.1217, -0.1586, -1.1933, -1.2344],\n",
      "         [ 1.1397, -0.1609, -1.2029, -1.2934]],\n",
      "\n",
      "        [[-0.9526,  0.8701, -0.7294,  0.4585],\n",
      "         [-0.7640,  0.8184, -0.8414,  0.1610],\n",
      "         [-0.4547,  0.7157, -1.0012, -0.2426],\n",
      "         [-0.2437,  0.6332, -1.0839, -0.4956]],\n",
      "\n",
      "        [[ 0.1841,  0.4583, -1.2785, -0.8344],\n",
      "         [ 0.7077,  0.1616, -1.2630, -1.4005],\n",
      "         [ 0.8171,  0.0876, -1.2299, -1.5140],\n",
      "         [ 0.8544,  0.0614, -1.2167, -1.5506]]], grad_fn=<SelectBackward0>)\n",
      "\n",
      "V :-  tensor([[[-0.8540,  0.4412,  1.5958,  0.0235],\n",
      "         [-0.8267,  0.3937,  1.6096,  0.0514],\n",
      "         [-0.8103,  0.3686,  1.6115,  0.0647],\n",
      "         [-0.7971,  0.3513,  1.6103,  0.0706]],\n",
      "\n",
      "        [[-0.5160,  0.2915,  1.1008, -0.1689],\n",
      "         [-0.6615,  0.3793,  1.2878, -0.1228],\n",
      "         [-0.7287,  0.4139,  1.3819, -0.0929],\n",
      "         [-0.7519,  0.4225,  1.4197, -0.0787]],\n",
      "\n",
      "        [[ 1.1423, -1.3873,  0.0668,  0.0747],\n",
      "         [ 0.9688, -1.2757,  0.2995,  0.1035],\n",
      "         [ 0.6925, -1.0715,  0.6238,  0.1228],\n",
      "         [ 0.4989, -0.9155,  0.8192,  0.1310]],\n",
      "\n",
      "        [[ 0.1516, -0.5990,  1.1377,  0.0801],\n",
      "         [-0.3979, -0.0817,  1.4724,  0.1058],\n",
      "         [-0.5228,  0.0437,  1.5239,  0.1132],\n",
      "         [-0.5656,  0.0877,  1.5394,  0.1152]]], grad_fn=<SelectBackward0>)\n",
      "\n",
      "tgt_len =  4\n",
      "bsz =  4\n",
      "num_heads =  2\n",
      "head_dim =  2\n",
      "\n",
      "tensor([[[-1.5499, -0.1699, -1.1429, -0.1188],\n",
      "         [ 0.6232,  0.6521,  0.0169, -0.3579],\n",
      "         [-1.5266, -0.2734, -1.0515, -0.0299],\n",
      "         [-1.5266, -0.2734, -1.0515, -0.0299]],\n",
      "\n",
      "        [[-1.3632, -0.3399, -0.9174,  0.1513],\n",
      "         [ 0.3821, -0.3124,  0.2599,  0.9101],\n",
      "         [-1.4775, -0.3137, -0.9949,  0.0368],\n",
      "         [-1.4775, -0.3137, -0.9949,  0.0368]],\n",
      "\n",
      "        [[-1.1641,  0.3842, -1.1776, -0.5957],\n",
      "         [ 0.0683,  0.6909, -0.3859, -0.6488],\n",
      "         [-1.5703, -0.0657, -1.2190, -0.2373],\n",
      "         [-1.5703, -0.0657, -1.2190, -0.2373]],\n",
      "\n",
      "        [[-1.1641,  0.3842, -1.1776, -0.5957],\n",
      "         [ 0.0683,  0.6909, -0.3859, -0.6488],\n",
      "         [-1.5703, -0.0657, -1.2190, -0.2373],\n",
      "         [-1.5703, -0.0657, -1.2190, -0.2373]]], grad_fn=<ViewBackward0>) torch.Size([4, 4, 4])\n",
      "Q reshaped =  tensor([[[-1.5499, -0.1699],\n",
      "         [-1.3632, -0.3399],\n",
      "         [-1.1641,  0.3842],\n",
      "         [-1.1641,  0.3842]],\n",
      "\n",
      "        [[-1.1429, -0.1188],\n",
      "         [-0.9174,  0.1513],\n",
      "         [-1.1776, -0.5957],\n",
      "         [-1.1776, -0.5957]],\n",
      "\n",
      "        [[ 0.6232,  0.6521],\n",
      "         [ 0.3821, -0.3124],\n",
      "         [ 0.0683,  0.6909],\n",
      "         [ 0.0683,  0.6909]],\n",
      "\n",
      "        [[ 0.0169, -0.3579],\n",
      "         [ 0.2599,  0.9101],\n",
      "         [-0.3859, -0.6488],\n",
      "         [-0.3859, -0.6488]],\n",
      "\n",
      "        [[-1.5266, -0.2734],\n",
      "         [-1.4775, -0.3137],\n",
      "         [-1.5703, -0.0657],\n",
      "         [-1.5703, -0.0657]],\n",
      "\n",
      "        [[-1.0515, -0.0299],\n",
      "         [-0.9949,  0.0368],\n",
      "         [-1.2190, -0.2373],\n",
      "         [-1.2190, -0.2373]],\n",
      "\n",
      "        [[-1.5266, -0.2734],\n",
      "         [-1.4775, -0.3137],\n",
      "         [-1.5703, -0.0657],\n",
      "         [-1.5703, -0.0657]],\n",
      "\n",
      "        [[-1.0515, -0.0299],\n",
      "         [-0.9949,  0.0368],\n",
      "         [-1.2190, -0.2373],\n",
      "         [-1.2190, -0.2373]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "K reshaped =  tensor([[[ 1.1857, -0.1569],\n",
      "         [ 0.9291, -0.1052],\n",
      "         [-0.9526,  0.8701],\n",
      "         [ 0.1841,  0.4583]],\n",
      "\n",
      "        [[-1.2063, -1.6157],\n",
      "         [-1.1116, -0.8156],\n",
      "         [-0.7294,  0.4585],\n",
      "         [-1.2785, -0.8344]],\n",
      "\n",
      "        [[ 1.1432, -0.1255],\n",
      "         [ 1.0645, -0.1449],\n",
      "         [-0.7640,  0.8184],\n",
      "         [ 0.7077,  0.1616]],\n",
      "\n",
      "        [[-1.2047, -1.6500],\n",
      "         [-1.1690, -1.0901],\n",
      "         [-0.8414,  0.1610],\n",
      "         [-1.2630, -1.4005]],\n",
      "\n",
      "        [[ 1.1186, -0.1094],\n",
      "         [ 1.1217, -0.1586],\n",
      "         [-0.4547,  0.7157],\n",
      "         [ 0.8171,  0.0876]],\n",
      "\n",
      "        [[-1.2010, -1.6613],\n",
      "         [-1.1933, -1.2344],\n",
      "         [-1.0012, -0.2426],\n",
      "         [-1.2299, -1.5140]],\n",
      "\n",
      "        [[ 1.1024, -0.0986],\n",
      "         [ 1.1397, -0.1609],\n",
      "         [-0.2437,  0.6332],\n",
      "         [ 0.8544,  0.0614]],\n",
      "\n",
      "        [[-1.2008, -1.6617],\n",
      "         [-1.2029, -1.2934],\n",
      "         [-1.0839, -0.4956],\n",
      "         [-1.2167, -1.5506]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "V reshaped =  tensor([[[-0.8540,  0.4412],\n",
      "         [-0.5160,  0.2915],\n",
      "         [ 1.1423, -1.3873],\n",
      "         [ 0.1516, -0.5990]],\n",
      "\n",
      "        [[ 1.5958,  0.0235],\n",
      "         [ 1.1008, -0.1689],\n",
      "         [ 0.0668,  0.0747],\n",
      "         [ 1.1377,  0.0801]],\n",
      "\n",
      "        [[-0.8267,  0.3937],\n",
      "         [-0.6615,  0.3793],\n",
      "         [ 0.9688, -1.2757],\n",
      "         [-0.3979, -0.0817]],\n",
      "\n",
      "        [[ 1.6096,  0.0514],\n",
      "         [ 1.2878, -0.1228],\n",
      "         [ 0.2995,  0.1035],\n",
      "         [ 1.4724,  0.1058]],\n",
      "\n",
      "        [[-0.8103,  0.3686],\n",
      "         [-0.7287,  0.4139],\n",
      "         [ 0.6925, -1.0715],\n",
      "         [-0.5228,  0.0437]],\n",
      "\n",
      "        [[ 1.6115,  0.0647],\n",
      "         [ 1.3819, -0.0929],\n",
      "         [ 0.6238,  0.1228],\n",
      "         [ 1.5239,  0.1132]],\n",
      "\n",
      "        [[-0.7971,  0.3513],\n",
      "         [-0.7519,  0.4225],\n",
      "         [ 0.4989, -0.9155],\n",
      "         [-0.5656,  0.0877]],\n",
      "\n",
      "        [[ 1.6103,  0.0706],\n",
      "         [ 1.4197, -0.0787],\n",
      "         [ 0.8192,  0.1310],\n",
      "         [ 1.5394,  0.1152]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "torch.Size([4, 2, 4, 2]) torch.Size([4, 2, 4, 2]) torch.Size([4, 2, 4, 2])\n",
      "\n",
      "scaled_dot_product_attention\n",
      "attn_output = \n",
      "tensor([[ 6.5749e-01, -9.5168e-01,  1.0835e+00, -1.8783e-04],\n",
      "        [-3.3335e-01, -4.7886e-02,  1.2493e+00,  3.1897e-02],\n",
      "        [ 1.2327e-01, -5.1876e-01,  1.3142e+00,  4.9786e-02],\n",
      "        [-5.6527e-02, -3.6330e-01,  1.3618e+00,  5.7953e-02],\n",
      "        [ 5.4819e-01, -8.4944e-01,  1.0002e+00,  2.4730e-03],\n",
      "        [-4.0554e-01,  2.4195e-02,  8.8225e-01,  5.0043e-02],\n",
      "        [ 9.8770e-02, -4.9476e-01,  1.3035e+00,  5.0351e-02],\n",
      "        [-7.5707e-02, -3.4405e-01,  1.3548e+00,  5.8435e-02],\n",
      "        [ 6.0846e-01, -9.0787e-01,  1.1870e+00, -2.1457e-03],\n",
      "        [-1.1545e-01, -2.5641e-01,  1.3201e+00,  3.0669e-02],\n",
      "        [ 1.7532e-01, -5.6977e-01,  1.3453e+00,  4.8354e-02],\n",
      "        [-1.4577e-02, -4.0543e-01,  1.3825e+00,  5.6664e-02],\n",
      "        [ 6.0846e-01, -9.0787e-01,  1.1870e+00, -2.1457e-03],\n",
      "        [-1.1545e-01, -2.5641e-01,  1.3201e+00,  3.0669e-02],\n",
      "        [ 1.7532e-01, -5.6977e-01,  1.3453e+00,  4.8354e-02],\n",
      "        [-1.4577e-02, -4.0543e-01,  1.3825e+00,  5.6664e-02]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "\n",
      "Decoder norm2(x + mha(x, mem)) = \n",
      "tensor([[[-0.2858, -0.2164, -1.1173,  1.6195],\n",
      "         [ 1.1334,  0.7102, -0.4160, -1.4276],\n",
      "         [-1.0883, -0.1067, -0.4247,  1.6198],\n",
      "         [-1.2969,  0.1097, -0.3012,  1.4883]],\n",
      "\n",
      "        [[-0.8590, -0.8159,  0.0649,  1.6100],\n",
      "         [-0.7303, -0.1379,  1.6732, -0.8050],\n",
      "         [-1.1742, -0.2405, -0.1765,  1.5912],\n",
      "         [-1.3590, -0.0270, -0.0789,  1.4649]],\n",
      "\n",
      "        [[ 0.9055,  0.9384, -1.4694, -0.3745],\n",
      "         [ 1.2056,  0.7663, -0.8987, -1.0732],\n",
      "         [-0.7594,  0.8426, -1.2056,  1.1224],\n",
      "         [-1.0253,  0.9915, -0.9743,  1.0081]],\n",
      "\n",
      "        [[ 0.9055,  0.9384, -1.4694, -0.3745],\n",
      "         [ 1.2056,  0.7663, -0.8987, -1.0732],\n",
      "         [-0.7594,  0.8426, -1.2056,  1.1224],\n",
      "         [-1.0253,  0.9915, -0.9743,  1.0081]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "\n",
      "Decoder norm3(x + ff(x)) = \n",
      "tensor([[[-0.1817, -0.1557, -1.2251,  1.5626],\n",
      "         [ 1.2129,  0.7285, -0.7322, -1.2092],\n",
      "         [-0.9571, -0.0470, -0.6332,  1.6373],\n",
      "         [-1.1657,  0.1545, -0.5206,  1.5317]],\n",
      "\n",
      "        [[-0.7679, -0.7204, -0.2002,  1.6885],\n",
      "         [-0.8601, -0.1252,  1.6672, -0.6819],\n",
      "         [-1.0553, -0.1689, -0.4147,  1.6388],\n",
      "         [-1.2418,  0.0306, -0.3227,  1.5339]],\n",
      "\n",
      "        [[ 0.8855,  0.8504, -1.5701, -0.1658],\n",
      "         [ 1.2219,  0.7352, -1.1387, -0.8183],\n",
      "         [-0.6285,  0.7902, -1.2989,  1.1372],\n",
      "         [-0.8875,  0.9400, -1.1050,  1.0525]],\n",
      "\n",
      "        [[ 0.8855,  0.8504, -1.5701, -0.1658],\n",
      "         [ 1.2219,  0.7352, -1.1387, -0.8183],\n",
      "         [-0.6285,  0.7902, -1.2989,  1.1372],\n",
      "         [-0.8875,  0.9400, -1.1050,  1.0525]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "\n",
      "\n",
      "################### Transformer Decoder start #####################\n",
      "\n",
      "query =  tensor([[[-0.1817, -0.1557, -1.2251,  1.5626],\n",
      "         [ 1.2129,  0.7285, -0.7322, -1.2092],\n",
      "         [-0.9571, -0.0470, -0.6332,  1.6373],\n",
      "         [-1.1657,  0.1545, -0.5206,  1.5317]],\n",
      "\n",
      "        [[-0.7679, -0.7204, -0.2002,  1.6885],\n",
      "         [-0.8601, -0.1252,  1.6672, -0.6819],\n",
      "         [-1.0553, -0.1689, -0.4147,  1.6388],\n",
      "         [-1.2418,  0.0306, -0.3227,  1.5339]],\n",
      "\n",
      "        [[ 0.8855,  0.8504, -1.5701, -0.1658],\n",
      "         [ 1.2219,  0.7352, -1.1387, -0.8183],\n",
      "         [-0.6285,  0.7902, -1.2989,  1.1372],\n",
      "         [-0.8875,  0.9400, -1.1050,  1.0525]],\n",
      "\n",
      "        [[ 0.8855,  0.8504, -1.5701, -0.1658],\n",
      "         [ 1.2219,  0.7352, -1.1387, -0.8183],\n",
      "         [-0.6285,  0.7902, -1.2989,  1.1372],\n",
      "         [-0.8875,  0.9400, -1.1050,  1.0525]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "query shape =  torch.Size([4, 4, 4])\n",
      "\n",
      " _in_projection_packed \n",
      "\n",
      "Q :-  tensor([[[-0.5553,  0.9256, -0.4678,  0.1593],\n",
      "         [ 0.8956, -0.2561,  0.2852,  0.8043],\n",
      "         [-0.8247,  0.8472, -0.4941, -0.2622],\n",
      "         [-0.8313,  0.8084, -0.4804, -0.3295]],\n",
      "\n",
      "        [[-0.8857,  0.6694, -0.4403, -0.4192],\n",
      "         [-0.1102, -0.6798,  0.2352, -0.7378],\n",
      "         [-0.8786,  0.7813, -0.4805, -0.3749],\n",
      "         [-0.8783,  0.7469, -0.4670, -0.4292]],\n",
      "\n",
      "        [[ 0.4914,  0.4020, -0.0490,  0.8781],\n",
      "         [ 0.7828,  0.0043,  0.1628,  0.8975],\n",
      "         [-0.4256,  0.9047, -0.4314,  0.1904],\n",
      "         [-0.4641,  0.8492, -0.4182,  0.0740]],\n",
      "\n",
      "        [[ 0.4914,  0.4020, -0.0490,  0.8781],\n",
      "         [ 0.7828,  0.0043,  0.1628,  0.8975],\n",
      "         [-0.4256,  0.9047, -0.4314,  0.1904],\n",
      "         [-0.4641,  0.8492, -0.4182,  0.0740]]], grad_fn=<SelectBackward0>)\n",
      "\n",
      "K :-  tensor([[[ 8.5511e-01, -1.5126e+00,  6.3593e-02, -1.1235e+00],\n",
      "         [ 3.0016e-01,  1.2458e+00, -3.6813e-01,  6.0396e-01],\n",
      "         [ 2.8953e-01, -1.5703e+00,  5.1512e-01, -1.1767e+00],\n",
      "         [ 8.0365e-02, -1.4443e+00,  7.0303e-01, -1.1547e+00]],\n",
      "\n",
      "        [[ 3.7764e-01, -1.7030e+00,  1.4749e-01, -9.9425e-01],\n",
      "         [-1.1657e+00,  6.3637e-01,  3.8749e-01,  6.0625e-01],\n",
      "         [ 1.9271e-01, -1.5869e+00,  5.1786e-01, -1.1308e+00],\n",
      "         [ 1.5799e-03, -1.4617e+00,  6.9338e-01, -1.1098e+00]],\n",
      "\n",
      "        [[ 7.3695e-01,  2.6388e-01, -1.2730e-01, -2.1296e-01],\n",
      "         [ 5.6512e-01,  8.7308e-01, -3.6257e-01,  3.0439e-01],\n",
      "         [ 3.5210e-01, -9.8841e-01,  6.6802e-01, -1.1019e+00],\n",
      "         [ 1.0795e-01, -8.8875e-01,  8.6206e-01, -1.0753e+00]],\n",
      "\n",
      "        [[ 7.3695e-01,  2.6388e-01, -1.2730e-01, -2.1296e-01],\n",
      "         [ 5.6512e-01,  8.7308e-01, -3.6257e-01,  3.0439e-01],\n",
      "         [ 3.5210e-01, -9.8841e-01,  6.6802e-01, -1.1019e+00],\n",
      "         [ 1.0795e-01, -8.8875e-01,  8.6206e-01, -1.0753e+00]]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "\n",
      "V :-  tensor([[[-0.0102, -1.1538,  0.0644, -0.7489],\n",
      "         [ 1.5632,  1.4298,  0.6304,  0.6306],\n",
      "         [-0.6144, -1.5222, -0.3597, -0.6491],\n",
      "         [-0.6421, -1.5142, -0.4702, -0.5102]],\n",
      "\n",
      "        [[-1.0505, -1.5623, -0.3176, -0.9077],\n",
      "         [-1.0448,  0.0767, -0.5875,  0.3881],\n",
      "         [-0.8175, -1.5809, -0.4290, -0.6751],\n",
      "         [-0.8267, -1.5642, -0.5269, -0.5400]],\n",
      "\n",
      "        [[ 1.5739,  0.6002,  0.5796,  0.2278],\n",
      "         [ 1.6533,  1.1704,  0.6827,  0.4505],\n",
      "         [ 0.3664, -0.9361, -0.1406, -0.1984],\n",
      "         [ 0.2481, -0.9704, -0.2830, -0.0788]],\n",
      "\n",
      "        [[ 1.5739,  0.6002,  0.5796,  0.2278],\n",
      "         [ 1.6533,  1.1704,  0.6827,  0.4505],\n",
      "         [ 0.3664, -0.9361, -0.1406, -0.1984],\n",
      "         [ 0.2481, -0.9704, -0.2830, -0.0788]]], grad_fn=<SelectBackward0>)\n",
      "\n",
      "tgt_len =  4\n",
      "bsz =  4\n",
      "num_heads =  2\n",
      "head_dim =  2\n",
      "\n",
      "tensor([[[-0.5553,  0.9256, -0.4678,  0.1593],\n",
      "         [ 0.8956, -0.2561,  0.2852,  0.8043],\n",
      "         [-0.8247,  0.8472, -0.4941, -0.2622],\n",
      "         [-0.8313,  0.8084, -0.4804, -0.3295]],\n",
      "\n",
      "        [[-0.8857,  0.6694, -0.4403, -0.4192],\n",
      "         [-0.1102, -0.6798,  0.2352, -0.7378],\n",
      "         [-0.8786,  0.7813, -0.4805, -0.3749],\n",
      "         [-0.8783,  0.7469, -0.4670, -0.4292]],\n",
      "\n",
      "        [[ 0.4914,  0.4020, -0.0490,  0.8781],\n",
      "         [ 0.7828,  0.0043,  0.1628,  0.8975],\n",
      "         [-0.4256,  0.9047, -0.4314,  0.1904],\n",
      "         [-0.4641,  0.8492, -0.4182,  0.0740]],\n",
      "\n",
      "        [[ 0.4914,  0.4020, -0.0490,  0.8781],\n",
      "         [ 0.7828,  0.0043,  0.1628,  0.8975],\n",
      "         [-0.4256,  0.9047, -0.4314,  0.1904],\n",
      "         [-0.4641,  0.8492, -0.4182,  0.0740]]], grad_fn=<SelectBackward0>) torch.Size([4, 4, 4])\n",
      "Q reshaped =  tensor([[[-0.5553,  0.9256],\n",
      "         [-0.8857,  0.6694],\n",
      "         [ 0.4914,  0.4020],\n",
      "         [ 0.4914,  0.4020]],\n",
      "\n",
      "        [[-0.4678,  0.1593],\n",
      "         [-0.4403, -0.4192],\n",
      "         [-0.0490,  0.8781],\n",
      "         [-0.0490,  0.8781]],\n",
      "\n",
      "        [[ 0.8956, -0.2561],\n",
      "         [-0.1102, -0.6798],\n",
      "         [ 0.7828,  0.0043],\n",
      "         [ 0.7828,  0.0043]],\n",
      "\n",
      "        [[ 0.2852,  0.8043],\n",
      "         [ 0.2352, -0.7378],\n",
      "         [ 0.1628,  0.8975],\n",
      "         [ 0.1628,  0.8975]],\n",
      "\n",
      "        [[-0.8247,  0.8472],\n",
      "         [-0.8786,  0.7813],\n",
      "         [-0.4256,  0.9047],\n",
      "         [-0.4256,  0.9047]],\n",
      "\n",
      "        [[-0.4941, -0.2622],\n",
      "         [-0.4805, -0.3749],\n",
      "         [-0.4314,  0.1904],\n",
      "         [-0.4314,  0.1904]],\n",
      "\n",
      "        [[-0.8313,  0.8084],\n",
      "         [-0.8783,  0.7469],\n",
      "         [-0.4641,  0.8492],\n",
      "         [-0.4641,  0.8492]],\n",
      "\n",
      "        [[-0.4804, -0.3295],\n",
      "         [-0.4670, -0.4292],\n",
      "         [-0.4182,  0.0740],\n",
      "         [-0.4182,  0.0740]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "K reshaped =  tensor([[[ 8.5511e-01, -1.5126e+00],\n",
      "         [ 3.7764e-01, -1.7030e+00],\n",
      "         [ 7.3695e-01,  2.6388e-01],\n",
      "         [ 7.3695e-01,  2.6388e-01]],\n",
      "\n",
      "        [[ 6.3593e-02, -1.1235e+00],\n",
      "         [ 1.4749e-01, -9.9425e-01],\n",
      "         [-1.2730e-01, -2.1296e-01],\n",
      "         [-1.2730e-01, -2.1296e-01]],\n",
      "\n",
      "        [[ 3.0016e-01,  1.2458e+00],\n",
      "         [-1.1657e+00,  6.3637e-01],\n",
      "         [ 5.6512e-01,  8.7308e-01],\n",
      "         [ 5.6512e-01,  8.7308e-01]],\n",
      "\n",
      "        [[-3.6813e-01,  6.0396e-01],\n",
      "         [ 3.8749e-01,  6.0625e-01],\n",
      "         [-3.6257e-01,  3.0439e-01],\n",
      "         [-3.6257e-01,  3.0439e-01]],\n",
      "\n",
      "        [[ 2.8953e-01, -1.5703e+00],\n",
      "         [ 1.9271e-01, -1.5869e+00],\n",
      "         [ 3.5210e-01, -9.8841e-01],\n",
      "         [ 3.5210e-01, -9.8841e-01]],\n",
      "\n",
      "        [[ 5.1512e-01, -1.1767e+00],\n",
      "         [ 5.1786e-01, -1.1308e+00],\n",
      "         [ 6.6802e-01, -1.1019e+00],\n",
      "         [ 6.6802e-01, -1.1019e+00]],\n",
      "\n",
      "        [[ 8.0365e-02, -1.4443e+00],\n",
      "         [ 1.5799e-03, -1.4617e+00],\n",
      "         [ 1.0795e-01, -8.8875e-01],\n",
      "         [ 1.0795e-01, -8.8875e-01]],\n",
      "\n",
      "        [[ 7.0303e-01, -1.1547e+00],\n",
      "         [ 6.9338e-01, -1.1098e+00],\n",
      "         [ 8.6206e-01, -1.0753e+00],\n",
      "         [ 8.6206e-01, -1.0753e+00]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "V reshaped =  tensor([[[-0.0102, -1.1538],\n",
      "         [-1.0505, -1.5623],\n",
      "         [ 1.5739,  0.6002],\n",
      "         [ 1.5739,  0.6002]],\n",
      "\n",
      "        [[ 0.0644, -0.7489],\n",
      "         [-0.3176, -0.9077],\n",
      "         [ 0.5796,  0.2278],\n",
      "         [ 0.5796,  0.2278]],\n",
      "\n",
      "        [[ 1.5632,  1.4298],\n",
      "         [-1.0448,  0.0767],\n",
      "         [ 1.6533,  1.1704],\n",
      "         [ 1.6533,  1.1704]],\n",
      "\n",
      "        [[ 0.6304,  0.6306],\n",
      "         [-0.5875,  0.3881],\n",
      "         [ 0.6827,  0.4505],\n",
      "         [ 0.6827,  0.4505]],\n",
      "\n",
      "        [[-0.6144, -1.5222],\n",
      "         [-0.8175, -1.5809],\n",
      "         [ 0.3664, -0.9361],\n",
      "         [ 0.3664, -0.9361]],\n",
      "\n",
      "        [[-0.3597, -0.6491],\n",
      "         [-0.4290, -0.6751],\n",
      "         [-0.1406, -0.1984],\n",
      "         [-0.1406, -0.1984]],\n",
      "\n",
      "        [[-0.6421, -1.5142],\n",
      "         [-0.8267, -1.5642],\n",
      "         [ 0.2481, -0.9704],\n",
      "         [ 0.2481, -0.9704]],\n",
      "\n",
      "        [[-0.4702, -0.5102],\n",
      "         [-0.5269, -0.5400],\n",
      "         [-0.2830, -0.0788],\n",
      "         [-0.2830, -0.0788]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "torch.Size([4, 2, 4, 2]) torch.Size([4, 2, 4, 2]) torch.Size([4, 2, 4, 2])\n",
      "\n",
      "scaled_dot_product_attention\n",
      "attn_output = \n",
      "tensor([[ 1.0743,  0.1374,  0.2574, -0.2546],\n",
      "        [ 1.3308,  1.1142,  0.2849,  0.4783],\n",
      "        [-0.0982, -1.2000, -0.2714, -0.4374],\n",
      "        [-0.1748, -1.2150, -0.3944, -0.3096],\n",
      "        [ 0.9070, -0.0114,  0.1984, -0.3459],\n",
      "        [ 0.7982,  0.8848,  0.3494,  0.4750],\n",
      "        [-0.1068, -1.2048, -0.2715, -0.4377],\n",
      "        [-0.1813, -1.2188, -0.3946, -0.3099],\n",
      "        [ 0.8333, -0.1001,  0.3156, -0.1640],\n",
      "        [ 1.3108,  1.1104,  0.2984,  0.4804],\n",
      "        [-0.0830, -1.1915, -0.2700, -0.4348],\n",
      "        [-0.1663, -1.2102, -0.3933, -0.3070],\n",
      "        [ 0.8333, -0.1001,  0.3156, -0.1640],\n",
      "        [ 1.3108,  1.1104,  0.2984,  0.4804],\n",
      "        [-0.0830, -1.1915, -0.2700, -0.4348],\n",
      "        [-0.1663, -1.2102, -0.3933, -0.3070]], grad_fn=<ViewBackward0>)\n",
      "\n",
      "x =  tensor([[[-0.1817, -0.1557, -1.2251,  1.5626],\n",
      "         [ 1.2129,  0.7285, -0.7322, -1.2092],\n",
      "         [-0.9571, -0.0470, -0.6332,  1.6373],\n",
      "         [-1.1657,  0.1545, -0.5206,  1.5317]],\n",
      "\n",
      "        [[-0.7679, -0.7204, -0.2002,  1.6885],\n",
      "         [-0.8601, -0.1252,  1.6672, -0.6819],\n",
      "         [-1.0553, -0.1689, -0.4147,  1.6388],\n",
      "         [-1.2418,  0.0306, -0.3227,  1.5339]],\n",
      "\n",
      "        [[ 0.8855,  0.8504, -1.5701, -0.1658],\n",
      "         [ 1.2219,  0.7352, -1.1387, -0.8183],\n",
      "         [-0.6285,  0.7902, -1.2989,  1.1372],\n",
      "         [-0.8875,  0.9400, -1.1050,  1.0525]],\n",
      "\n",
      "        [[ 0.8855,  0.8504, -1.5701, -0.1658],\n",
      "         [ 1.2219,  0.7352, -1.1387, -0.8183],\n",
      "         [-0.6285,  0.7902, -1.2989,  1.1372],\n",
      "         [-0.8875,  0.9400, -1.1050,  1.0525]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "\n",
      "sa_op =  tensor([[[ 0.2220, -0.9363,  0.2278, -0.3418],\n",
      "         [-1.0206, -2.0187, -0.3516,  0.5187],\n",
      "         [ 1.1546,  1.2667,  0.5531, -0.7793],\n",
      "         [ 1.0322,  1.3907,  0.4947, -0.6744]],\n",
      "\n",
      "        [[ 0.3680, -0.6529,  0.2892, -0.4269],\n",
      "         [-0.8392, -1.5429, -0.3663,  0.5037],\n",
      "         [ 1.1581,  1.2761,  0.5534, -0.7800],\n",
      "         [ 1.0350,  1.3979,  0.4950, -0.6750]],\n",
      "\n",
      "        [[ 0.3680, -0.6580,  0.2206, -0.3591],\n",
      "         [-1.0147, -2.0129, -0.3550,  0.5201],\n",
      "         [ 1.1474,  1.2488,  0.5517, -0.7768],\n",
      "         [ 1.0276,  1.3801,  0.4934, -0.6724]],\n",
      "\n",
      "        [[ 0.3680, -0.6580,  0.2206, -0.3591],\n",
      "         [-1.0147, -2.0129, -0.3550,  0.5201],\n",
      "         [ 1.1474,  1.2488,  0.5517, -0.7768],\n",
      "         [ 1.0276,  1.3801,  0.4934, -0.6724]]], grad_fn=<ViewBackward0>)\n",
      "\n",
      "temop =  tensor([[[ 0.0403, -1.0921, -0.9973,  1.2208],\n",
      "         [ 0.1922, -1.2901, -1.0838, -0.6905],\n",
      "         [ 0.1974,  1.2197, -0.0800,  0.8580],\n",
      "         [-0.1335,  1.5452, -0.0258,  0.8574]],\n",
      "\n",
      "        [[-0.3999, -1.3733,  0.0889,  1.2616],\n",
      "         [-1.6993, -1.6681,  1.3009, -0.1782],\n",
      "         [ 0.1028,  1.1073,  0.1387,  0.8588],\n",
      "         [-0.2068,  1.4285,  0.1723,  0.8589]],\n",
      "\n",
      "        [[ 1.2535,  0.1924, -1.3494, -0.5250],\n",
      "         [ 0.2072, -1.2778, -1.4937, -0.2983],\n",
      "         [ 0.5189,  2.0390, -0.7472,  0.3604],\n",
      "         [ 0.1401,  2.3201, -0.6115,  0.3801]],\n",
      "\n",
      "        [[ 1.2535,  0.1924, -1.3494, -0.5250],\n",
      "         [ 0.2072, -1.2778, -1.4937, -0.2983],\n",
      "         [ 0.5189,  2.0390, -0.7472,  0.3604],\n",
      "         [ 0.1401,  2.3201, -0.6115,  0.3801]]], grad_fn=<AddBackward0>)\n",
      "\n",
      "Decoder norm1(x + sa(x)) = \n",
      "tensor([[[ 0.2642, -0.9450, -0.8439,  1.5248],\n",
      "         [ 1.6026, -1.0072, -0.6439,  0.0485],\n",
      "         [-0.6810,  1.3004, -1.2188,  0.5994],\n",
      "         [-1.0119,  1.4347, -0.8550,  0.4322]],\n",
      "\n",
      "        [[-0.3101, -1.3361,  0.2051,  1.4411],\n",
      "         [-0.9190, -0.8938,  1.5036,  0.3092],\n",
      "         [-1.0202,  1.2616, -0.9387,  0.6972],\n",
      "         [-1.2244,  1.3759, -0.6216,  0.4701]],\n",
      "\n",
      "        [[ 1.4226,  0.3131, -1.2989, -0.4369],\n",
      "         [ 1.3227, -0.8057, -1.1152,  0.5982],\n",
      "         [-0.0241,  1.5082, -1.3003, -0.1838],\n",
      "         [-0.3856,  1.6300, -1.0806, -0.1637]],\n",
      "\n",
      "        [[ 1.4226,  0.3131, -1.2989, -0.4369],\n",
      "         [ 1.3227, -0.8057, -1.1152,  0.5982],\n",
      "         [-0.0241,  1.5082, -1.3003, -0.1838],\n",
      "         [-0.3856,  1.6300, -1.0806, -0.1637]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "\n",
      "Memory = \n",
      "tensor([[[-1.4990,  0.7444, -0.2987,  1.0533],\n",
      "         [-1.5121,  0.8727, -0.2916,  0.9310],\n",
      "         [-1.5148,  0.9319, -0.2861,  0.8690],\n",
      "         [-1.5182,  0.9572, -0.2769,  0.8379]],\n",
      "\n",
      "        [[-1.3058, -0.3143,  0.1455,  1.4747],\n",
      "         [-1.3926, -0.0321, -0.0105,  1.4351],\n",
      "         [-1.4340,  0.1398, -0.0900,  1.3842],\n",
      "         [-1.4516,  0.2181, -0.1197,  1.3531]],\n",
      "\n",
      "        [[-1.0356,  0.3077,  1.4970, -0.7692],\n",
      "         [-1.1819,  0.5317,  1.3520, -0.7017],\n",
      "         [-1.3785,  0.7521,  1.1311, -0.5047],\n",
      "         [-1.4763,  0.8709,  0.9656, -0.3603]],\n",
      "\n",
      "        [[-1.6791,  0.7794,  0.7400,  0.1597],\n",
      "         [-1.6386,  1.0445,  0.1429,  0.4511],\n",
      "         [-1.5924,  1.1025, -0.0088,  0.4986],\n",
      "         [-1.5740,  1.1191, -0.0613,  0.5162]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "\n",
      "query =  tensor([[[ 0.2642, -0.9450, -0.8439,  1.5248],\n",
      "         [ 1.6026, -1.0072, -0.6439,  0.0485],\n",
      "         [-0.6810,  1.3004, -1.2188,  0.5994],\n",
      "         [-1.0119,  1.4347, -0.8550,  0.4322]],\n",
      "\n",
      "        [[-0.3101, -1.3361,  0.2051,  1.4411],\n",
      "         [-0.9190, -0.8938,  1.5036,  0.3092],\n",
      "         [-1.0202,  1.2616, -0.9387,  0.6972],\n",
      "         [-1.2244,  1.3759, -0.6216,  0.4701]],\n",
      "\n",
      "        [[ 1.4226,  0.3131, -1.2989, -0.4369],\n",
      "         [ 1.3227, -0.8057, -1.1152,  0.5982],\n",
      "         [-0.0241,  1.5082, -1.3003, -0.1838],\n",
      "         [-0.3856,  1.6300, -1.0806, -0.1637]],\n",
      "\n",
      "        [[ 1.4226,  0.3131, -1.2989, -0.4369],\n",
      "         [ 1.3227, -0.8057, -1.1152,  0.5982],\n",
      "         [-0.0241,  1.5082, -1.3003, -0.1838],\n",
      "         [-0.3856,  1.6300, -1.0806, -0.1637]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "query shape =  torch.Size([4, 4, 4])\n",
      "\n",
      " _in_projection_packed \n",
      "\n",
      "Q :-  tensor([[[-0.4659, -0.3508,  0.9487,  0.2591],\n",
      "         [-0.5579,  0.6332, -0.2305, -0.3497],\n",
      "         [-0.2133, -1.3781,  0.1598,  0.6494],\n",
      "         [-0.0208, -1.3419,  0.1494,  0.6406]],\n",
      "\n",
      "        [[-0.0533,  0.0514,  1.2135,  0.1254],\n",
      "         [ 0.5612,  0.5420,  0.7237, -0.1340],\n",
      "         [-0.0728, -1.3923,  0.3352,  0.6847],\n",
      "         [ 0.0846, -1.3079,  0.2574,  0.6434]],\n",
      "\n",
      "        [[-0.6170, -0.0386, -0.7923, -0.1110],\n",
      "         [-0.6761,  0.1131,  0.0833, -0.0746],\n",
      "         [-0.2960, -1.0389, -0.5168,  0.4033],\n",
      "         [-0.1554, -1.1217, -0.4190,  0.4607]],\n",
      "\n",
      "        [[-0.6170, -0.0386, -0.7923, -0.1110],\n",
      "         [-0.6761,  0.1131,  0.0833, -0.0746],\n",
      "         [-0.2960, -1.0389, -0.5168,  0.4033],\n",
      "         [-0.1554, -1.1217, -0.4190,  0.4607]]], grad_fn=<ViewBackward0>)\n",
      "\n",
      "K :-  tensor([[[ 1.0633e-01, -4.0705e-01, -1.4893e+00,  9.0800e-02],\n",
      "         [ 1.1569e-01, -2.7922e-01, -1.5160e+00,  9.4637e-04],\n",
      "         [ 1.1936e-01, -2.1674e-01, -1.5244e+00, -4.2518e-02],\n",
      "         [ 1.1824e-01, -1.8760e-01, -1.5268e+00, -6.3185e-02]],\n",
      "\n",
      "        [[-1.5384e-01, -1.1188e+00, -9.6946e-01,  5.9878e-01],\n",
      "         [-7.2311e-02, -9.7401e-01, -1.1544e+00,  4.9039e-01],\n",
      "         [-2.7857e-02, -8.6947e-01, -1.2505e+00,  4.1432e-01],\n",
      "         [-9.9587e-03, -8.1718e-01, -1.2903e+00,  3.7650e-01]],\n",
      "\n",
      "        [[-5.8209e-01,  4.7425e-01, -2.2089e-01, -4.8540e-01],\n",
      "         [-5.1352e-01,  5.2494e-01, -4.4163e-01, -5.3692e-01],\n",
      "         [-4.1970e-01,  4.9509e-01, -7.3799e-01, -5.3825e-01],\n",
      "         [-3.5135e-01,  4.5811e-01, -9.1223e-01, -5.2276e-01]],\n",
      "\n",
      "        [[-2.8693e-01,  9.9020e-02, -1.1698e+00, -2.9807e-01],\n",
      "         [-3.5954e-02,  6.0603e-02, -1.4480e+00, -2.5660e-01],\n",
      "         [ 2.8756e-02,  6.5272e-02, -1.4875e+00, -2.5135e-01],\n",
      "         [ 5.0950e-02,  6.4899e-02, -1.4987e+00, -2.4784e-01]]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "\n",
      "V :-  tensor([[[-0.8077,  0.8756,  0.7175,  0.6995],\n",
      "         [-0.8188,  0.8050,  0.8073,  0.7543],\n",
      "         [-0.8215,  0.7684,  0.8483,  0.7792],\n",
      "         [-0.8209,  0.7497,  0.8668,  0.7937]],\n",
      "\n",
      "        [[-0.4783,  1.0478,  0.0053,  0.4314],\n",
      "         [-0.5962,  1.0538,  0.1976,  0.4999],\n",
      "         [-0.6570,  1.0384,  0.3142,  0.5439],\n",
      "         [-0.6815,  1.0259,  0.3677,  0.5664]],\n",
      "\n",
      "        [[ 0.1404, -0.4518,  0.4964,  1.1367],\n",
      "         [ 0.0051, -0.3751,  0.6596,  1.2106],\n",
      "         [-0.1818, -0.2067,  0.8219,  1.2694],\n",
      "         [-0.2978, -0.0899,  0.9044,  1.2782]],\n",
      "\n",
      "        [[-0.4691,  0.2696,  0.8557,  1.2235],\n",
      "         [-0.7091,  0.4969,  0.9798,  1.0498],\n",
      "         [-0.7541,  0.5336,  0.9993,  0.9890],\n",
      "         [-0.7685,  0.5466,  1.0033,  0.9657]]], grad_fn=<SelectBackward0>)\n",
      "\n",
      "tgt_len =  4\n",
      "bsz =  4\n",
      "num_heads =  2\n",
      "head_dim =  2\n",
      "\n",
      "tensor([[[-0.4659, -0.3508,  0.9487,  0.2591],\n",
      "         [-0.5579,  0.6332, -0.2305, -0.3497],\n",
      "         [-0.2133, -1.3781,  0.1598,  0.6494],\n",
      "         [-0.0208, -1.3419,  0.1494,  0.6406]],\n",
      "\n",
      "        [[-0.0533,  0.0514,  1.2135,  0.1254],\n",
      "         [ 0.5612,  0.5420,  0.7237, -0.1340],\n",
      "         [-0.0728, -1.3923,  0.3352,  0.6847],\n",
      "         [ 0.0846, -1.3079,  0.2574,  0.6434]],\n",
      "\n",
      "        [[-0.6170, -0.0386, -0.7923, -0.1110],\n",
      "         [-0.6761,  0.1131,  0.0833, -0.0746],\n",
      "         [-0.2960, -1.0389, -0.5168,  0.4033],\n",
      "         [-0.1554, -1.1217, -0.4190,  0.4607]],\n",
      "\n",
      "        [[-0.6170, -0.0386, -0.7923, -0.1110],\n",
      "         [-0.6761,  0.1131,  0.0833, -0.0746],\n",
      "         [-0.2960, -1.0389, -0.5168,  0.4033],\n",
      "         [-0.1554, -1.1217, -0.4190,  0.4607]]], grad_fn=<ViewBackward0>) torch.Size([4, 4, 4])\n",
      "Q reshaped =  tensor([[[-0.4659, -0.3508],\n",
      "         [-0.0533,  0.0514],\n",
      "         [-0.6170, -0.0386],\n",
      "         [-0.6170, -0.0386]],\n",
      "\n",
      "        [[ 0.9487,  0.2591],\n",
      "         [ 1.2135,  0.1254],\n",
      "         [-0.7923, -0.1110],\n",
      "         [-0.7923, -0.1110]],\n",
      "\n",
      "        [[-0.5579,  0.6332],\n",
      "         [ 0.5612,  0.5420],\n",
      "         [-0.6761,  0.1131],\n",
      "         [-0.6761,  0.1131]],\n",
      "\n",
      "        [[-0.2305, -0.3497],\n",
      "         [ 0.7237, -0.1340],\n",
      "         [ 0.0833, -0.0746],\n",
      "         [ 0.0833, -0.0746]],\n",
      "\n",
      "        [[-0.2133, -1.3781],\n",
      "         [-0.0728, -1.3923],\n",
      "         [-0.2960, -1.0389],\n",
      "         [-0.2960, -1.0389]],\n",
      "\n",
      "        [[ 0.1598,  0.6494],\n",
      "         [ 0.3352,  0.6847],\n",
      "         [-0.5168,  0.4033],\n",
      "         [-0.5168,  0.4033]],\n",
      "\n",
      "        [[-0.0208, -1.3419],\n",
      "         [ 0.0846, -1.3079],\n",
      "         [-0.1554, -1.1217],\n",
      "         [-0.1554, -1.1217]],\n",
      "\n",
      "        [[ 0.1494,  0.6406],\n",
      "         [ 0.2574,  0.6434],\n",
      "         [-0.4190,  0.4607],\n",
      "         [-0.4190,  0.4607]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "K reshaped =  tensor([[[ 1.0633e-01, -4.0705e-01],\n",
      "         [-1.5384e-01, -1.1188e+00],\n",
      "         [-5.8209e-01,  4.7425e-01],\n",
      "         [-2.8693e-01,  9.9020e-02]],\n",
      "\n",
      "        [[-1.4893e+00,  9.0800e-02],\n",
      "         [-9.6946e-01,  5.9878e-01],\n",
      "         [-2.2089e-01, -4.8540e-01],\n",
      "         [-1.1698e+00, -2.9807e-01]],\n",
      "\n",
      "        [[ 1.1569e-01, -2.7922e-01],\n",
      "         [-7.2311e-02, -9.7401e-01],\n",
      "         [-5.1352e-01,  5.2494e-01],\n",
      "         [-3.5954e-02,  6.0603e-02]],\n",
      "\n",
      "        [[-1.5160e+00,  9.4637e-04],\n",
      "         [-1.1544e+00,  4.9039e-01],\n",
      "         [-4.4163e-01, -5.3692e-01],\n",
      "         [-1.4480e+00, -2.5660e-01]],\n",
      "\n",
      "        [[ 1.1936e-01, -2.1674e-01],\n",
      "         [-2.7857e-02, -8.6947e-01],\n",
      "         [-4.1970e-01,  4.9509e-01],\n",
      "         [ 2.8756e-02,  6.5272e-02]],\n",
      "\n",
      "        [[-1.5244e+00, -4.2518e-02],\n",
      "         [-1.2505e+00,  4.1432e-01],\n",
      "         [-7.3799e-01, -5.3825e-01],\n",
      "         [-1.4875e+00, -2.5135e-01]],\n",
      "\n",
      "        [[ 1.1824e-01, -1.8760e-01],\n",
      "         [-9.9587e-03, -8.1718e-01],\n",
      "         [-3.5135e-01,  4.5811e-01],\n",
      "         [ 5.0950e-02,  6.4899e-02]],\n",
      "\n",
      "        [[-1.5268e+00, -6.3185e-02],\n",
      "         [-1.2903e+00,  3.7650e-01],\n",
      "         [-9.1223e-01, -5.2276e-01],\n",
      "         [-1.4987e+00, -2.4784e-01]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "V reshaped =  tensor([[[-0.8077,  0.8756],\n",
      "         [-0.4783,  1.0478],\n",
      "         [ 0.1404, -0.4518],\n",
      "         [-0.4691,  0.2696]],\n",
      "\n",
      "        [[ 0.7175,  0.6995],\n",
      "         [ 0.0053,  0.4314],\n",
      "         [ 0.4964,  1.1367],\n",
      "         [ 0.8557,  1.2235]],\n",
      "\n",
      "        [[-0.8188,  0.8050],\n",
      "         [-0.5962,  1.0538],\n",
      "         [ 0.0051, -0.3751],\n",
      "         [-0.7091,  0.4969]],\n",
      "\n",
      "        [[ 0.8073,  0.7543],\n",
      "         [ 0.1976,  0.4999],\n",
      "         [ 0.6596,  1.2106],\n",
      "         [ 0.9798,  1.0498]],\n",
      "\n",
      "        [[-0.8215,  0.7684],\n",
      "         [-0.6570,  1.0384],\n",
      "         [-0.1818, -0.2067],\n",
      "         [-0.7541,  0.5336]],\n",
      "\n",
      "        [[ 0.8483,  0.7792],\n",
      "         [ 0.3142,  0.5439],\n",
      "         [ 0.8219,  1.2694],\n",
      "         [ 0.9993,  0.9890]],\n",
      "\n",
      "        [[-0.8209,  0.7497],\n",
      "         [-0.6815,  1.0259],\n",
      "         [-0.2978, -0.0899],\n",
      "         [-0.7685,  0.5466]],\n",
      "\n",
      "        [[ 0.8668,  0.7937],\n",
      "         [ 0.3677,  0.5664],\n",
      "         [ 0.9044,  1.2782],\n",
      "         [ 1.0033,  0.9657]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "torch.Size([4, 2, 4, 2]) torch.Size([4, 2, 4, 2]) torch.Size([4, 2, 4, 2])\n",
      "\n",
      "scaled_dot_product_attention\n",
      "attn_output = \n",
      "tensor([[-0.4079,  0.4760,  0.4744,  0.8901],\n",
      "        [-0.4435,  0.3212,  0.6854,  0.8937],\n",
      "        [-0.6527,  0.7241,  0.7090,  0.8578],\n",
      "        [-0.6811,  0.7220,  0.7515,  0.8665],\n",
      "        [-0.3960,  0.4185,  0.4779,  0.9168],\n",
      "        [-0.5197,  0.4396,  0.6510,  0.9238],\n",
      "        [-0.6560,  0.7305,  0.7055,  0.8610],\n",
      "        [-0.6821,  0.7207,  0.7509,  0.8689],\n",
      "        [-0.3710,  0.3870,  0.5540,  0.8559],\n",
      "        [-0.4829,  0.4157,  0.6629,  0.8876],\n",
      "        [-0.6401,  0.6742,  0.7300,  0.8545],\n",
      "        [-0.6740,  0.6926,  0.7642,  0.8645],\n",
      "        [-0.3710,  0.3870,  0.5540,  0.8559],\n",
      "        [-0.4829,  0.4157,  0.6629,  0.8876],\n",
      "        [-0.6401,  0.6742,  0.7300,  0.8545],\n",
      "        [-0.6740,  0.6926,  0.7642,  0.8645]], grad_fn=<ViewBackward0>)\n",
      "\n",
      "Decoder norm2(x + mha(x, mem)) = \n",
      "tensor([[[-0.3964, -0.8837, -0.4188,  1.6989],\n",
      "         [ 1.3963, -1.3259, -0.4159,  0.3455],\n",
      "         [-1.1664,  1.3591, -0.7184,  0.5257],\n",
      "         [-1.3833,  1.3673, -0.3213,  0.3373]],\n",
      "\n",
      "        [[-0.8847, -1.0407,  0.5880,  1.3373],\n",
      "         [-1.2077, -0.5555,  1.4640,  0.2992],\n",
      "         [-1.4085,  1.2327, -0.4026,  0.5783],\n",
      "         [-1.5117,  1.2561, -0.1005,  0.3562]],\n",
      "\n",
      "        [[ 1.2055,  0.6579, -1.3663, -0.4971],\n",
      "         [ 0.9711, -0.9784, -1.0210,  1.0283],\n",
      "         [-0.5635,  1.6874, -0.8808, -0.2431],\n",
      "         [-0.8871,  1.6811, -0.5862, -0.2078]],\n",
      "\n",
      "        [[ 1.2055,  0.6579, -1.3663, -0.4971],\n",
      "         [ 0.9711, -0.9784, -1.0210,  1.0283],\n",
      "         [-0.5635,  1.6874, -0.8808, -0.2431],\n",
      "         [-0.8871,  1.6811, -0.5862, -0.2078]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "\n",
      "Decoder norm3(x + ff(x)) = \n",
      "tensor([[[-0.3315, -1.1871, -0.0556,  1.5741],\n",
      "         [ 1.2438, -1.5390,  0.0050,  0.2901],\n",
      "         [-1.3638,  1.2571, -0.4729,  0.5797],\n",
      "         [-1.5434,  1.2195, -0.0357,  0.3596]],\n",
      "\n",
      "        [[-0.7281, -1.2285,  0.8257,  1.1309],\n",
      "         [-1.0083, -0.7562,  1.5360,  0.2285],\n",
      "         [-1.5670,  1.0727, -0.1215,  0.6158],\n",
      "         [-1.6368,  1.0700,  0.1960,  0.3707]],\n",
      "\n",
      "        [[ 1.3602,  0.4868, -1.2451, -0.6019],\n",
      "         [ 0.9558, -1.3281, -0.6055,  0.9777],\n",
      "         [-0.7754,  1.7007, -0.6604, -0.2648],\n",
      "         [-1.0936,  1.6294, -0.3199, -0.2159]],\n",
      "\n",
      "        [[ 1.3602,  0.4868, -1.2451, -0.6019],\n",
      "         [ 0.9558, -1.3281, -0.6055,  0.9777],\n",
      "         [-0.7754,  1.7007, -0.6604, -0.2648],\n",
      "         [-1.0936,  1.6294, -0.3199, -0.2159]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "\n",
      "\n",
      "################### Transformer Decoder start #####################\n",
      "\n",
      "query =  tensor([[[-0.3315, -1.1871, -0.0556,  1.5741],\n",
      "         [ 1.2438, -1.5390,  0.0050,  0.2901],\n",
      "         [-1.3638,  1.2571, -0.4729,  0.5797],\n",
      "         [-1.5434,  1.2195, -0.0357,  0.3596]],\n",
      "\n",
      "        [[-0.7281, -1.2285,  0.8257,  1.1309],\n",
      "         [-1.0083, -0.7562,  1.5360,  0.2285],\n",
      "         [-1.5670,  1.0727, -0.1215,  0.6158],\n",
      "         [-1.6368,  1.0700,  0.1960,  0.3707]],\n",
      "\n",
      "        [[ 1.3602,  0.4868, -1.2451, -0.6019],\n",
      "         [ 0.9558, -1.3281, -0.6055,  0.9777],\n",
      "         [-0.7754,  1.7007, -0.6604, -0.2648],\n",
      "         [-1.0936,  1.6294, -0.3199, -0.2159]],\n",
      "\n",
      "        [[ 1.3602,  0.4868, -1.2451, -0.6019],\n",
      "         [ 0.9558, -1.3281, -0.6055,  0.9777],\n",
      "         [-0.7754,  1.7007, -0.6604, -0.2648],\n",
      "         [-1.0936,  1.6294, -0.3199, -0.2159]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "query shape =  torch.Size([4, 4, 4])\n",
      "\n",
      " _in_projection_packed \n",
      "\n",
      "Q :-  tensor([[[-0.9733, -0.7926, -0.6464,  1.4853],\n",
      "         [-1.3821, -1.0148,  0.1028,  0.8220],\n",
      "         [ 0.8139,  0.7579, -0.6454,  0.0845],\n",
      "         [ 1.1306,  0.7989, -0.3999, -0.2764]],\n",
      "\n",
      "        [[-0.3033, -0.6873, -0.1572,  0.7434],\n",
      "         [ 0.6444, -0.2684,  0.4399, -0.3980],\n",
      "         [ 0.9492,  0.6890, -0.5344, -0.0116],\n",
      "         [ 1.1928,  0.7351, -0.3154, -0.3358]],\n",
      "\n",
      "        [[-0.6829,  0.1346, -0.1171,  0.1234],\n",
      "         [-1.6379, -0.9677, -0.4659,  1.4896],\n",
      "         [ 0.9719,  1.0230, -0.3284, -0.5098],\n",
      "         [ 1.2033,  1.0271, -0.2458, -0.6398]],\n",
      "\n",
      "        [[-0.6829,  0.1346, -0.1171,  0.1234],\n",
      "         [-1.6379, -0.9677, -0.4659,  1.4896],\n",
      "         [ 0.9719,  1.0230, -0.3284, -0.5098],\n",
      "         [ 1.2033,  1.0271, -0.2458, -0.6398]]], grad_fn=<SelectBackward0>)\n",
      "\n",
      "K :-  tensor([[[-1.0034,  1.0744, -0.7728,  0.8970],\n",
      "         [-0.9310,  1.4014,  0.3277,  1.0211],\n",
      "         [ 0.3517, -0.8755, -0.6422, -0.8615],\n",
      "         [ 0.6100, -1.1097, -0.7042, -0.7626]],\n",
      "\n",
      "        [[-0.4630,  0.5708, -0.9104,  1.0742],\n",
      "         [ 0.3635, -0.2819, -0.7771,  0.8412],\n",
      "         [ 0.4313, -0.9290, -0.8032, -0.6631],\n",
      "         [ 0.6490, -1.1189, -0.7902, -0.6150]],\n",
      "\n",
      "        [[-0.2888,  0.3580,  1.0026, -0.6382],\n",
      "         [-1.2724,  1.5704,  0.0584,  0.8151],\n",
      "         [ 0.6609, -1.1372, -0.0341, -1.2650],\n",
      "         [ 0.7969, -1.2882, -0.2400, -1.1409]],\n",
      "\n",
      "        [[-0.2888,  0.3580,  1.0026, -0.6382],\n",
      "         [-1.2724,  1.5704,  0.0584,  0.8151],\n",
      "         [ 0.6609, -1.1372, -0.0341, -1.2650],\n",
      "         [ 0.7969, -1.2882, -0.2400, -1.1409]]], grad_fn=<SelectBackward0>)\n",
      "\n",
      "V :-  tensor([[[ 6.6196e-02, -4.4720e-01, -6.7959e-02, -5.9123e-01],\n",
      "         [-6.3370e-01,  5.4245e-01, -1.8536e-01, -3.7719e-01],\n",
      "         [ 6.7137e-01, -8.7894e-01,  2.8667e-01,  2.1902e-02],\n",
      "         [ 7.4202e-01, -8.0658e-01,  1.8082e-01,  1.6638e-01]],\n",
      "\n",
      "        [[ 2.2569e-01, -3.1785e-01, -2.7693e-01, -2.9299e-01],\n",
      "         [ 3.6996e-01, -9.9813e-02, -4.0565e-01,  1.6698e-01],\n",
      "         [ 7.4344e-01, -8.8585e-01,  1.9165e-01,  6.1184e-02],\n",
      "         [ 7.7031e-01, -7.8960e-01,  1.1398e-01,  1.8925e-01]],\n",
      "\n",
      "        [[-5.4121e-01,  4.1175e-01,  2.9330e-01, -7.3105e-02],\n",
      "         [-4.9070e-01,  1.1884e-01,  4.7237e-04, -6.2683e-01],\n",
      "         [ 4.4905e-01, -5.0311e-01,  3.4346e-01,  2.4678e-01],\n",
      "         [ 5.7793e-01, -5.6994e-01,  2.6584e-01,  3.0401e-01]],\n",
      "\n",
      "        [[-5.4121e-01,  4.1175e-01,  2.9330e-01, -7.3105e-02],\n",
      "         [-4.9070e-01,  1.1884e-01,  4.7237e-04, -6.2683e-01],\n",
      "         [ 4.4905e-01, -5.0311e-01,  3.4346e-01,  2.4678e-01],\n",
      "         [ 5.7793e-01, -5.6994e-01,  2.6584e-01,  3.0401e-01]]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "\n",
      "tgt_len =  4\n",
      "bsz =  4\n",
      "num_heads =  2\n",
      "head_dim =  2\n",
      "\n",
      "tensor([[[-0.9733, -0.7926, -0.6464,  1.4853],\n",
      "         [-1.3821, -1.0148,  0.1028,  0.8220],\n",
      "         [ 0.8139,  0.7579, -0.6454,  0.0845],\n",
      "         [ 1.1306,  0.7989, -0.3999, -0.2764]],\n",
      "\n",
      "        [[-0.3033, -0.6873, -0.1572,  0.7434],\n",
      "         [ 0.6444, -0.2684,  0.4399, -0.3980],\n",
      "         [ 0.9492,  0.6890, -0.5344, -0.0116],\n",
      "         [ 1.1928,  0.7351, -0.3154, -0.3358]],\n",
      "\n",
      "        [[-0.6829,  0.1346, -0.1171,  0.1234],\n",
      "         [-1.6379, -0.9677, -0.4659,  1.4896],\n",
      "         [ 0.9719,  1.0230, -0.3284, -0.5098],\n",
      "         [ 1.2033,  1.0271, -0.2458, -0.6398]],\n",
      "\n",
      "        [[-0.6829,  0.1346, -0.1171,  0.1234],\n",
      "         [-1.6379, -0.9677, -0.4659,  1.4896],\n",
      "         [ 0.9719,  1.0230, -0.3284, -0.5098],\n",
      "         [ 1.2033,  1.0271, -0.2458, -0.6398]]], grad_fn=<SelectBackward0>) torch.Size([4, 4, 4])\n",
      "Q reshaped =  tensor([[[-0.9733, -0.7926],\n",
      "         [-0.3033, -0.6873],\n",
      "         [-0.6829,  0.1346],\n",
      "         [-0.6829,  0.1346]],\n",
      "\n",
      "        [[-0.6464,  1.4853],\n",
      "         [-0.1572,  0.7434],\n",
      "         [-0.1171,  0.1234],\n",
      "         [-0.1171,  0.1234]],\n",
      "\n",
      "        [[-1.3821, -1.0148],\n",
      "         [ 0.6444, -0.2684],\n",
      "         [-1.6379, -0.9677],\n",
      "         [-1.6379, -0.9677]],\n",
      "\n",
      "        [[ 0.1028,  0.8220],\n",
      "         [ 0.4399, -0.3980],\n",
      "         [-0.4659,  1.4896],\n",
      "         [-0.4659,  1.4896]],\n",
      "\n",
      "        [[ 0.8139,  0.7579],\n",
      "         [ 0.9492,  0.6890],\n",
      "         [ 0.9719,  1.0230],\n",
      "         [ 0.9719,  1.0230]],\n",
      "\n",
      "        [[-0.6454,  0.0845],\n",
      "         [-0.5344, -0.0116],\n",
      "         [-0.3284, -0.5098],\n",
      "         [-0.3284, -0.5098]],\n",
      "\n",
      "        [[ 1.1306,  0.7989],\n",
      "         [ 1.1928,  0.7351],\n",
      "         [ 1.2033,  1.0271],\n",
      "         [ 1.2033,  1.0271]],\n",
      "\n",
      "        [[-0.3999, -0.2764],\n",
      "         [-0.3154, -0.3358],\n",
      "         [-0.2458, -0.6398],\n",
      "         [-0.2458, -0.6398]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "K reshaped =  tensor([[[-1.0034,  1.0744],\n",
      "         [-0.4630,  0.5708],\n",
      "         [-0.2888,  0.3580],\n",
      "         [-0.2888,  0.3580]],\n",
      "\n",
      "        [[-0.7728,  0.8970],\n",
      "         [-0.9104,  1.0742],\n",
      "         [ 1.0026, -0.6382],\n",
      "         [ 1.0026, -0.6382]],\n",
      "\n",
      "        [[-0.9310,  1.4014],\n",
      "         [ 0.3635, -0.2819],\n",
      "         [-1.2724,  1.5704],\n",
      "         [-1.2724,  1.5704]],\n",
      "\n",
      "        [[ 0.3277,  1.0211],\n",
      "         [-0.7771,  0.8412],\n",
      "         [ 0.0584,  0.8151],\n",
      "         [ 0.0584,  0.8151]],\n",
      "\n",
      "        [[ 0.3517, -0.8755],\n",
      "         [ 0.4313, -0.9290],\n",
      "         [ 0.6609, -1.1372],\n",
      "         [ 0.6609, -1.1372]],\n",
      "\n",
      "        [[-0.6422, -0.8615],\n",
      "         [-0.8032, -0.6631],\n",
      "         [-0.0341, -1.2650],\n",
      "         [-0.0341, -1.2650]],\n",
      "\n",
      "        [[ 0.6100, -1.1097],\n",
      "         [ 0.6490, -1.1189],\n",
      "         [ 0.7969, -1.2882],\n",
      "         [ 0.7969, -1.2882]],\n",
      "\n",
      "        [[-0.7042, -0.7626],\n",
      "         [-0.7902, -0.6150],\n",
      "         [-0.2400, -1.1409],\n",
      "         [-0.2400, -1.1409]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "V reshaped =  tensor([[[ 6.6196e-02, -4.4720e-01],\n",
      "         [ 2.2569e-01, -3.1785e-01],\n",
      "         [-5.4121e-01,  4.1175e-01],\n",
      "         [-5.4121e-01,  4.1175e-01]],\n",
      "\n",
      "        [[-6.7959e-02, -5.9123e-01],\n",
      "         [-2.7693e-01, -2.9299e-01],\n",
      "         [ 2.9330e-01, -7.3105e-02],\n",
      "         [ 2.9330e-01, -7.3105e-02]],\n",
      "\n",
      "        [[-6.3370e-01,  5.4245e-01],\n",
      "         [ 3.6996e-01, -9.9813e-02],\n",
      "         [-4.9070e-01,  1.1884e-01],\n",
      "         [-4.9070e-01,  1.1884e-01]],\n",
      "\n",
      "        [[-1.8536e-01, -3.7719e-01],\n",
      "         [-4.0565e-01,  1.6698e-01],\n",
      "         [ 4.7237e-04, -6.2683e-01],\n",
      "         [ 4.7237e-04, -6.2683e-01]],\n",
      "\n",
      "        [[ 6.7137e-01, -8.7894e-01],\n",
      "         [ 7.4344e-01, -8.8585e-01],\n",
      "         [ 4.4905e-01, -5.0311e-01],\n",
      "         [ 4.4905e-01, -5.0311e-01]],\n",
      "\n",
      "        [[ 2.8667e-01,  2.1902e-02],\n",
      "         [ 1.9165e-01,  6.1184e-02],\n",
      "         [ 3.4346e-01,  2.4678e-01],\n",
      "         [ 3.4346e-01,  2.4678e-01]],\n",
      "\n",
      "        [[ 7.4202e-01, -8.0658e-01],\n",
      "         [ 7.7031e-01, -7.8960e-01],\n",
      "         [ 5.7793e-01, -5.6994e-01],\n",
      "         [ 5.7793e-01, -5.6994e-01]],\n",
      "\n",
      "        [[ 1.8082e-01,  1.6638e-01],\n",
      "         [ 1.1398e-01,  1.8925e-01],\n",
      "         [ 2.6584e-01,  3.0401e-01],\n",
      "         [ 2.6584e-01,  3.0401e-01]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "torch.Size([4, 2, 4, 2]) torch.Size([4, 2, 4, 2]) torch.Size([4, 2, 4, 2])\n",
      "\n",
      "scaled_dot_product_attention\n",
      "attn_output = \n",
      "tensor([[-0.1915,  0.0039, -0.1509, -0.3984],\n",
      "        [-0.3391,  0.1680, -0.1461, -0.3722],\n",
      "        [ 0.5765, -0.6900,  0.2812,  0.1271],\n",
      "        [ 0.6656, -0.6819,  0.2050,  0.2392],\n",
      "        [-0.2173,  0.0420, -0.0568, -0.3412],\n",
      "        [-0.1029,  0.0956, -0.1318, -0.3990],\n",
      "        [ 0.5742, -0.6865,  0.2839,  0.1314],\n",
      "        [ 0.6649, -0.6811,  0.2066,  0.2407],\n",
      "        [-0.1583, -0.0444,  0.0256, -0.2834],\n",
      "        [-0.3838,  0.1807, -0.1690, -0.3244],\n",
      "        [ 0.5774, -0.6913,  0.2923,  0.1451],\n",
      "        [ 0.6664, -0.6830,  0.2105,  0.2444],\n",
      "        [-0.1583, -0.0444,  0.0256, -0.2834],\n",
      "        [-0.3838,  0.1807, -0.1690, -0.3244],\n",
      "        [ 0.5774, -0.6913,  0.2923,  0.1451],\n",
      "        [ 0.6664, -0.6830,  0.2105,  0.2444]], grad_fn=<ViewBackward0>)\n",
      "\n",
      "x =  tensor([[[-0.3315, -1.1871, -0.0556,  1.5741],\n",
      "         [ 1.2438, -1.5390,  0.0050,  0.2901],\n",
      "         [-1.3638,  1.2571, -0.4729,  0.5797],\n",
      "         [-1.5434,  1.2195, -0.0357,  0.3596]],\n",
      "\n",
      "        [[-0.7281, -1.2285,  0.8257,  1.1309],\n",
      "         [-1.0083, -0.7562,  1.5360,  0.2285],\n",
      "         [-1.5670,  1.0727, -0.1215,  0.6158],\n",
      "         [-1.6368,  1.0700,  0.1960,  0.3707]],\n",
      "\n",
      "        [[ 1.3602,  0.4868, -1.2451, -0.6019],\n",
      "         [ 0.9558, -1.3281, -0.6055,  0.9777],\n",
      "         [-0.7754,  1.7007, -0.6604, -0.2648],\n",
      "         [-1.0936,  1.6294, -0.3199, -0.2159]],\n",
      "\n",
      "        [[ 1.3602,  0.4868, -1.2451, -0.6019],\n",
      "         [ 0.9558, -1.3281, -0.6055,  0.9777],\n",
      "         [-0.7754,  1.7007, -0.6604, -0.2648],\n",
      "         [-1.0936,  1.6294, -0.3199, -0.2159]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "\n",
      "sa_op =  tensor([[[ 0.3609,  0.0085, -0.2833, -0.2899],\n",
      "         [ 0.4486, -0.1136, -0.5114, -0.4683],\n",
      "         [-0.6399,  0.5424,  1.1720,  0.8101],\n",
      "         [-0.6833,  0.5162,  1.1832,  0.9081]],\n",
      "\n",
      "        [[ 0.2976, -0.0116, -0.2589, -0.3025],\n",
      "         [ 0.3288, -0.0694, -0.2897, -0.2587],\n",
      "         [-0.6416,  0.5398,  1.1699,  0.8083],\n",
      "         [-0.6844,  0.5157,  1.1833,  0.9078]],\n",
      "\n",
      "        [[ 0.1758,  0.0611, -0.0823, -0.1951],\n",
      "         [ 0.4640, -0.1251, -0.5615, -0.4950],\n",
      "         [-0.6555,  0.5439,  1.1837,  0.8178],\n",
      "         [-0.6896,  0.5175,  1.1892,  0.9113]],\n",
      "\n",
      "        [[ 0.1758,  0.0611, -0.0823, -0.1951],\n",
      "         [ 0.4640, -0.1251, -0.5615, -0.4950],\n",
      "         [-0.6555,  0.5439,  1.1837,  0.8178],\n",
      "         [-0.6896,  0.5175,  1.1892,  0.9113]]], grad_fn=<ViewBackward0>)\n",
      "\n",
      "temop =  tensor([[[ 0.0295, -1.1785, -0.3389,  1.2842],\n",
      "         [ 1.6924, -1.6526, -0.5063, -0.1781],\n",
      "         [-2.0037,  1.7994,  0.6991,  1.3898],\n",
      "         [-2.2267,  1.7357,  1.1475,  1.2677]],\n",
      "\n",
      "        [[-0.4305, -1.2401,  0.5668,  0.8284],\n",
      "         [-0.6795, -0.8256,  1.2463, -0.0302],\n",
      "         [-2.2086,  1.6125,  1.0484,  1.4241],\n",
      "         [-2.3212,  1.5857,  1.3794,  1.2785]],\n",
      "\n",
      "        [[ 1.5360,  0.5480, -1.3275, -0.7970],\n",
      "         [ 1.4198, -1.4532, -1.1670,  0.4827],\n",
      "         [-1.4309,  2.2446,  0.5233,  0.5530],\n",
      "         [-1.7832,  2.1469,  0.8692,  0.6954]],\n",
      "\n",
      "        [[ 1.5360,  0.5480, -1.3275, -0.7970],\n",
      "         [ 1.4198, -1.4532, -1.1670,  0.4827],\n",
      "         [-1.4309,  2.2446,  0.5233,  0.5530],\n",
      "         [-1.7832,  2.1469,  0.8692,  0.6954]]], grad_fn=<AddBackward0>)\n",
      "\n",
      "Decoder norm1(x + sa(x)) = \n",
      "tensor([[[ 0.0907, -1.2720, -0.3248,  1.5061],\n",
      "         [ 1.5420, -1.2408, -0.2872, -0.0141],\n",
      "         [-1.6700,  0.8963,  0.1538,  0.6199],\n",
      "         [-1.7152,  0.7948,  0.4221,  0.4983]],\n",
      "\n",
      "        [[-0.4393, -1.4225,  0.7720,  1.0897],\n",
      "         [-0.7423, -0.9209,  1.6119,  0.0514],\n",
      "         [-1.7173,  0.7333,  0.3715,  0.6124],\n",
      "         [-1.7280,  0.6816,  0.5543,  0.4921]],\n",
      "\n",
      "        [[ 1.3751,  0.4964, -1.1716, -0.6999],\n",
      "         [ 1.3524, -1.0772, -0.8351,  0.5599],\n",
      "         [-1.4628,  1.3619,  0.0390,  0.0619],\n",
      "         [-1.5920,  1.1700,  0.2721,  0.1499]],\n",
      "\n",
      "        [[ 1.3751,  0.4964, -1.1716, -0.6999],\n",
      "         [ 1.3524, -1.0772, -0.8351,  0.5599],\n",
      "         [-1.4628,  1.3619,  0.0390,  0.0619],\n",
      "         [-1.5920,  1.1700,  0.2721,  0.1499]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "\n",
      "Memory = \n",
      "tensor([[[-1.4990,  0.7444, -0.2987,  1.0533],\n",
      "         [-1.5121,  0.8727, -0.2916,  0.9310],\n",
      "         [-1.5148,  0.9319, -0.2861,  0.8690],\n",
      "         [-1.5182,  0.9572, -0.2769,  0.8379]],\n",
      "\n",
      "        [[-1.3058, -0.3143,  0.1455,  1.4747],\n",
      "         [-1.3926, -0.0321, -0.0105,  1.4351],\n",
      "         [-1.4340,  0.1398, -0.0900,  1.3842],\n",
      "         [-1.4516,  0.2181, -0.1197,  1.3531]],\n",
      "\n",
      "        [[-1.0356,  0.3077,  1.4970, -0.7692],\n",
      "         [-1.1819,  0.5317,  1.3520, -0.7017],\n",
      "         [-1.3785,  0.7521,  1.1311, -0.5047],\n",
      "         [-1.4763,  0.8709,  0.9656, -0.3603]],\n",
      "\n",
      "        [[-1.6791,  0.7794,  0.7400,  0.1597],\n",
      "         [-1.6386,  1.0445,  0.1429,  0.4511],\n",
      "         [-1.5924,  1.1025, -0.0088,  0.4986],\n",
      "         [-1.5740,  1.1191, -0.0613,  0.5162]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "\n",
      "query =  tensor([[[ 0.0907, -1.2720, -0.3248,  1.5061],\n",
      "         [ 1.5420, -1.2408, -0.2872, -0.0141],\n",
      "         [-1.6700,  0.8963,  0.1538,  0.6199],\n",
      "         [-1.7152,  0.7948,  0.4221,  0.4983]],\n",
      "\n",
      "        [[-0.4393, -1.4225,  0.7720,  1.0897],\n",
      "         [-0.7423, -0.9209,  1.6119,  0.0514],\n",
      "         [-1.7173,  0.7333,  0.3715,  0.6124],\n",
      "         [-1.7280,  0.6816,  0.5543,  0.4921]],\n",
      "\n",
      "        [[ 1.3751,  0.4964, -1.1716, -0.6999],\n",
      "         [ 1.3524, -1.0772, -0.8351,  0.5599],\n",
      "         [-1.4628,  1.3619,  0.0390,  0.0619],\n",
      "         [-1.5920,  1.1700,  0.2721,  0.1499]],\n",
      "\n",
      "        [[ 1.3751,  0.4964, -1.1716, -0.6999],\n",
      "         [ 1.3524, -1.0772, -0.8351,  0.5599],\n",
      "         [-1.4628,  1.3619,  0.0390,  0.0619],\n",
      "         [-1.5920,  1.1700,  0.2721,  0.1499]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "query shape =  torch.Size([4, 4, 4])\n",
      "\n",
      " _in_projection_packed \n",
      "\n",
      "Q :-  tensor([[[ 0.6505, -0.7555,  0.2066, -1.3130],\n",
      "         [ 0.6665,  0.8729,  1.1768, -0.7487],\n",
      "         [-0.4851, -1.2526, -1.2221,  0.2998],\n",
      "         [-0.4946, -1.3277, -1.2484,  0.4374]],\n",
      "\n",
      "        [[ 0.4817, -1.2755, -0.1611, -0.6506],\n",
      "         [ 0.0731, -1.1684, -0.4404,  0.4048],\n",
      "         [-0.4562, -1.3765, -1.2418,  0.3408],\n",
      "         [-0.4701, -1.3943, -1.2481,  0.4561]],\n",
      "\n",
      "        [[ 0.0479,  1.6623,  0.8998, -0.1472],\n",
      "         [ 0.6985,  0.6869,  1.0458, -1.1642],\n",
      "         [-0.6692, -0.7448, -1.1303,  0.6570],\n",
      "         [-0.6324, -0.9752, -1.2023,  0.6592]],\n",
      "\n",
      "        [[ 0.0479,  1.6623,  0.8998, -0.1472],\n",
      "         [ 0.6985,  0.6869,  1.0458, -1.1642],\n",
      "         [-0.6692, -0.7448, -1.1303,  0.6570],\n",
      "         [-0.6324, -0.9752, -1.2023,  0.6592]]], grad_fn=<ViewBackward0>)\n",
      "\n",
      "K :-  tensor([[[ 0.6874, -0.1200, -0.2746,  0.3318],\n",
      "         [ 0.6493, -0.1354, -0.3315,  0.3368],\n",
      "         [ 0.6295, -0.1430, -0.3589,  0.3370],\n",
      "         [ 0.6218, -0.1494, -0.3763,  0.3303]],\n",
      "\n",
      "        [[ 0.9028, -0.1936, -0.1796, -0.1994],\n",
      "         [ 0.8705, -0.1625, -0.1811, -0.0217],\n",
      "         [ 0.8424, -0.1486, -0.1923,  0.0715],\n",
      "         [ 0.8282, -0.1449, -0.2023,  0.1076]],\n",
      "\n",
      "        [[ 0.3915, -0.7812, -1.4194, -1.4575],\n",
      "         [ 0.4156, -0.7570, -1.4155, -1.2972],\n",
      "         [ 0.4789, -0.7067, -1.3592, -1.0643],\n",
      "         [ 0.5123, -0.6591, -1.2920, -0.8930]],\n",
      "\n",
      "        [[ 0.6905, -0.5813, -1.1126, -0.6851],\n",
      "         [ 0.6264, -0.3479, -0.7501, -0.0747],\n",
      "         [ 0.5913, -0.2834, -0.6492,  0.0802],\n",
      "         [ 0.5789, -0.2605, -0.6125,  0.1336]]], grad_fn=<SelectBackward0>)\n",
      "\n",
      "V :-  tensor([[[-1.5826,  0.3213, -0.0496, -0.6436],\n",
      "         [-1.6129,  0.2409,  0.0304, -0.7287],\n",
      "         [-1.6227,  0.2022,  0.0681, -0.7682],\n",
      "         [-1.6252,  0.1881,  0.0813, -0.7888]],\n",
      "\n",
      "        [[-0.9879,  1.0144, -0.8156, -0.1586],\n",
      "         [-1.1971,  0.8444, -0.6158, -0.2780],\n",
      "         [-1.3062,  0.7367, -0.4937, -0.3554],\n",
      "         [-1.3514,  0.6878, -0.4393, -0.3936]],\n",
      "\n",
      "        [[-0.1349,  0.6391, -0.6576, -1.1149],\n",
      "         [-0.3790,  0.5466, -0.5262, -1.2165],\n",
      "         [-0.7067,  0.4742, -0.3975, -1.2913],\n",
      "         [-0.9014,  0.4228, -0.3113, -1.3081]],\n",
      "\n",
      "        [[-1.1816,  0.5703, -0.3969, -1.1904],\n",
      "         [-1.5170,  0.2660, -0.0295, -1.0651],\n",
      "         [-1.5697,  0.1740,  0.0729, -1.0198],\n",
      "         [-1.5852,  0.1432,  0.1070, -1.0014]]], grad_fn=<SelectBackward0>)\n",
      "\n",
      "tgt_len =  4\n",
      "bsz =  4\n",
      "num_heads =  2\n",
      "head_dim =  2\n",
      "\n",
      "tensor([[[ 0.6505, -0.7555,  0.2066, -1.3130],\n",
      "         [ 0.6665,  0.8729,  1.1768, -0.7487],\n",
      "         [-0.4851, -1.2526, -1.2221,  0.2998],\n",
      "         [-0.4946, -1.3277, -1.2484,  0.4374]],\n",
      "\n",
      "        [[ 0.4817, -1.2755, -0.1611, -0.6506],\n",
      "         [ 0.0731, -1.1684, -0.4404,  0.4048],\n",
      "         [-0.4562, -1.3765, -1.2418,  0.3408],\n",
      "         [-0.4701, -1.3943, -1.2481,  0.4561]],\n",
      "\n",
      "        [[ 0.0479,  1.6623,  0.8998, -0.1472],\n",
      "         [ 0.6985,  0.6869,  1.0458, -1.1642],\n",
      "         [-0.6692, -0.7448, -1.1303,  0.6570],\n",
      "         [-0.6324, -0.9752, -1.2023,  0.6592]],\n",
      "\n",
      "        [[ 0.0479,  1.6623,  0.8998, -0.1472],\n",
      "         [ 0.6985,  0.6869,  1.0458, -1.1642],\n",
      "         [-0.6692, -0.7448, -1.1303,  0.6570],\n",
      "         [-0.6324, -0.9752, -1.2023,  0.6592]]], grad_fn=<ViewBackward0>) torch.Size([4, 4, 4])\n",
      "Q reshaped =  tensor([[[ 0.6505, -0.7555],\n",
      "         [ 0.4817, -1.2755],\n",
      "         [ 0.0479,  1.6623],\n",
      "         [ 0.0479,  1.6623]],\n",
      "\n",
      "        [[ 0.2066, -1.3130],\n",
      "         [-0.1611, -0.6506],\n",
      "         [ 0.8998, -0.1472],\n",
      "         [ 0.8998, -0.1472]],\n",
      "\n",
      "        [[ 0.6665,  0.8729],\n",
      "         [ 0.0731, -1.1684],\n",
      "         [ 0.6985,  0.6869],\n",
      "         [ 0.6985,  0.6869]],\n",
      "\n",
      "        [[ 1.1768, -0.7487],\n",
      "         [-0.4404,  0.4048],\n",
      "         [ 1.0458, -1.1642],\n",
      "         [ 1.0458, -1.1642]],\n",
      "\n",
      "        [[-0.4851, -1.2526],\n",
      "         [-0.4562, -1.3765],\n",
      "         [-0.6692, -0.7448],\n",
      "         [-0.6692, -0.7448]],\n",
      "\n",
      "        [[-1.2221,  0.2998],\n",
      "         [-1.2418,  0.3408],\n",
      "         [-1.1303,  0.6570],\n",
      "         [-1.1303,  0.6570]],\n",
      "\n",
      "        [[-0.4946, -1.3277],\n",
      "         [-0.4701, -1.3943],\n",
      "         [-0.6324, -0.9752],\n",
      "         [-0.6324, -0.9752]],\n",
      "\n",
      "        [[-1.2484,  0.4374],\n",
      "         [-1.2481,  0.4561],\n",
      "         [-1.2023,  0.6592],\n",
      "         [-1.2023,  0.6592]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "K reshaped =  tensor([[[ 0.6874, -0.1200],\n",
      "         [ 0.9028, -0.1936],\n",
      "         [ 0.3915, -0.7812],\n",
      "         [ 0.6905, -0.5813]],\n",
      "\n",
      "        [[-0.2746,  0.3318],\n",
      "         [-0.1796, -0.1994],\n",
      "         [-1.4194, -1.4575],\n",
      "         [-1.1126, -0.6851]],\n",
      "\n",
      "        [[ 0.6493, -0.1354],\n",
      "         [ 0.8705, -0.1625],\n",
      "         [ 0.4156, -0.7570],\n",
      "         [ 0.6264, -0.3479]],\n",
      "\n",
      "        [[-0.3315,  0.3368],\n",
      "         [-0.1811, -0.0217],\n",
      "         [-1.4155, -1.2972],\n",
      "         [-0.7501, -0.0747]],\n",
      "\n",
      "        [[ 0.6295, -0.1430],\n",
      "         [ 0.8424, -0.1486],\n",
      "         [ 0.4789, -0.7067],\n",
      "         [ 0.5913, -0.2834]],\n",
      "\n",
      "        [[-0.3589,  0.3370],\n",
      "         [-0.1923,  0.0715],\n",
      "         [-1.3592, -1.0643],\n",
      "         [-0.6492,  0.0802]],\n",
      "\n",
      "        [[ 0.6218, -0.1494],\n",
      "         [ 0.8282, -0.1449],\n",
      "         [ 0.5123, -0.6591],\n",
      "         [ 0.5789, -0.2605]],\n",
      "\n",
      "        [[-0.3763,  0.3303],\n",
      "         [-0.2023,  0.1076],\n",
      "         [-1.2920, -0.8930],\n",
      "         [-0.6125,  0.1336]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "V reshaped =  tensor([[[-1.5826,  0.3213],\n",
      "         [-0.9879,  1.0144],\n",
      "         [-0.1349,  0.6391],\n",
      "         [-1.1816,  0.5703]],\n",
      "\n",
      "        [[-0.0496, -0.6436],\n",
      "         [-0.8156, -0.1586],\n",
      "         [-0.6576, -1.1149],\n",
      "         [-0.3969, -1.1904]],\n",
      "\n",
      "        [[-1.6129,  0.2409],\n",
      "         [-1.1971,  0.8444],\n",
      "         [-0.3790,  0.5466],\n",
      "         [-1.5170,  0.2660]],\n",
      "\n",
      "        [[ 0.0304, -0.7287],\n",
      "         [-0.6158, -0.2780],\n",
      "         [-0.5262, -1.2165],\n",
      "         [-0.0295, -1.0651]],\n",
      "\n",
      "        [[-1.6227,  0.2022],\n",
      "         [-1.3062,  0.7367],\n",
      "         [-0.7067,  0.4742],\n",
      "         [-1.5697,  0.1740]],\n",
      "\n",
      "        [[ 0.0681, -0.7682],\n",
      "         [-0.4937, -0.3554],\n",
      "         [-0.3975, -1.2913],\n",
      "         [ 0.0729, -1.0198]],\n",
      "\n",
      "        [[-1.6252,  0.1881],\n",
      "         [-1.3514,  0.6878],\n",
      "         [-0.9014,  0.4228],\n",
      "         [-1.5852,  0.1432]],\n",
      "\n",
      "        [[ 0.0813, -0.7888],\n",
      "         [-0.4393, -0.3936],\n",
      "         [-0.3113, -1.3081],\n",
      "         [ 0.1070, -1.0014]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "torch.Size([4, 2, 4, 2]) torch.Size([4, 2, 4, 2]) torch.Size([4, 2, 4, 2])\n",
      "\n",
      "scaled_dot_product_attention\n",
      "attn_output = \n",
      "tensor([[-0.9415,  0.6444, -0.5576, -0.9139],\n",
      "        [-1.2515,  0.4838, -0.3197, -0.7638],\n",
      "        [-1.2187,  0.3971, -0.1976, -0.9538],\n",
      "        [-1.3055,  0.3585, -0.1390, -0.9508],\n",
      "        [-0.8916,  0.6409, -0.5266, -0.8811],\n",
      "        [-1.0835,  0.4786, -0.2644, -0.8302],\n",
      "        [-1.2112,  0.3981, -0.1954, -0.9515],\n",
      "        [-1.3026,  0.3590, -0.1380, -0.9490],\n",
      "        [-1.0957,  0.6353, -0.4786, -0.6693],\n",
      "        [-1.2415,  0.4842, -0.3485, -0.8257],\n",
      "        [-1.2475,  0.3926, -0.1711, -0.9086],\n",
      "        [-1.3202,  0.3558, -0.1271, -0.9269],\n",
      "        [-1.0957,  0.6353, -0.4786, -0.6693],\n",
      "        [-1.2415,  0.4842, -0.3485, -0.8257],\n",
      "        [-1.2475,  0.3926, -0.1711, -0.9086],\n",
      "        [-1.3202,  0.3558, -0.1271, -0.9269]], grad_fn=<ViewBackward0>)\n",
      "\n",
      "Decoder norm2(x + mha(x, mem)) = \n",
      "tensor([[[ 0.0271, -0.8601, -0.7902,  1.6233],\n",
      "         [ 1.1779, -1.0652, -0.9141,  0.8014],\n",
      "         [-1.2365,  0.5136, -0.6250,  1.3478],\n",
      "         [-1.3444,  0.4428, -0.4408,  1.3425]],\n",
      "\n",
      "        [[-0.3587, -1.1458, -0.0923,  1.5968],\n",
      "         [-0.7606, -1.2006,  0.8118,  1.1493],\n",
      "         [-1.3118,  0.3944, -0.4641,  1.3814],\n",
      "         [-1.3813,  0.3534, -0.3342,  1.3621]],\n",
      "\n",
      "        [[ 0.9910,  0.4526, -1.6628,  0.2193],\n",
      "         [ 0.8853, -0.8085, -1.1697,  1.0929],\n",
      "         [-1.1982,  0.9930, -0.7798,  0.9850],\n",
      "         [-1.3257,  0.8173, -0.5958,  1.1042]],\n",
      "\n",
      "        [[ 0.9910,  0.4526, -1.6628,  0.2193],\n",
      "         [ 0.8853, -0.8085, -1.1697,  1.0929],\n",
      "         [-1.1982,  0.9930, -0.7798,  0.9850],\n",
      "         [-1.3257,  0.8173, -0.5958,  1.1042]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "\n",
      "Decoder norm3(x + ff(x)) = \n",
      "tensor([[[-0.1138, -1.1486, -0.3361,  1.5984],\n",
      "         [ 1.0943, -1.3582, -0.5473,  0.8112],\n",
      "         [-1.4614,  0.3338, -0.1836,  1.3112],\n",
      "         [-1.5289,  0.2500,  0.0140,  1.2648]],\n",
      "\n",
      "        [[-0.4406, -1.3158,  0.3624,  1.3940],\n",
      "         [-0.7173, -1.2418,  1.0899,  0.8692],\n",
      "         [-1.4987,  0.1977, -0.0086,  1.3095],\n",
      "         [-1.5387,  0.1515,  0.1246,  1.2625]],\n",
      "\n",
      "        [[ 1.0570,  0.3412, -1.6448,  0.2467],\n",
      "         [ 0.8070, -1.1386, -0.8335,  1.1652],\n",
      "         [-1.4695,  0.8911, -0.3738,  0.9522],\n",
      "         [-1.5573,  0.6726, -0.1621,  1.0469]],\n",
      "\n",
      "        [[ 1.0570,  0.3412, -1.6448,  0.2467],\n",
      "         [ 0.8070, -1.1386, -0.8335,  1.1652],\n",
      "         [-1.4695,  0.8911, -0.3738,  0.9522],\n",
      "         [-1.5573,  0.6726, -0.1621,  1.0469]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "\n",
      "tensor([[[ 0.3940, -0.5207,  0.5881, -0.0633, -0.3120,  0.5819, -0.4101,\n",
      "          -1.4635, -0.1699, -0.2797],\n",
      "         [ 0.5073,  0.1428,  0.3195, -0.0658, -0.9096, -0.1855, -0.1959,\n",
      "          -1.3236, -0.4518,  0.5941],\n",
      "         [-0.5948, -0.5942,  0.6483, -0.1120,  0.4457,  0.9227, -0.2608,\n",
      "          -0.6598,  0.2730, -0.9357],\n",
      "         [-0.5474, -0.6907,  0.6200, -0.1524,  0.4327,  0.9527, -0.1927,\n",
      "          -0.5951,  0.2499, -0.9856]],\n",
      "\n",
      "        [[ 0.4754, -0.8489,  0.4886, -0.2087, -0.3043,  0.6991, -0.1543,\n",
      "          -1.1643, -0.2179, -0.4919],\n",
      "         [ 0.3852, -1.0187,  0.3113, -0.3779, -0.3152,  0.6655,  0.2213,\n",
      "          -0.6228, -0.2607, -0.5969],\n",
      "         [-0.5107, -0.6982,  0.6293, -0.1455,  0.4188,  0.9588, -0.2144,\n",
      "          -0.6444,  0.2413, -0.9784],\n",
      "         [-0.4861, -0.7543,  0.6063, -0.1736,  0.4078,  0.9704, -0.1630,\n",
      "          -0.5902,  0.2254, -1.0049]],\n",
      "\n",
      "        [[-0.5983,  1.0723,  0.3436,  0.1102, -0.5498, -0.5710, -0.2827,\n",
      "          -0.8323, -0.1088,  0.8705],\n",
      "         [ 0.3852,  0.0908,  0.4625,  0.0084, -0.6836,  0.0219, -0.3930,\n",
      "          -1.4907, -0.3148,  0.3874],\n",
      "         [-0.9692, -0.2947,  0.5966, -0.0966,  0.5243,  0.7436, -0.1840,\n",
      "          -0.3591,  0.3570, -0.8197],\n",
      "         [-0.8282, -0.4762,  0.5989, -0.1316,  0.5109,  0.8432, -0.1617,\n",
      "          -0.4020,  0.3243, -0.9195]],\n",
      "\n",
      "        [[-0.5983,  1.0723,  0.3436,  0.1102, -0.5498, -0.5710, -0.2827,\n",
      "          -0.8323, -0.1088,  0.8705],\n",
      "         [ 0.3852,  0.0908,  0.4625,  0.0084, -0.6836,  0.0219, -0.3930,\n",
      "          -1.4907, -0.3148,  0.3874],\n",
      "         [-0.9692, -0.2947,  0.5966, -0.0966,  0.5243,  0.7436, -0.1840,\n",
      "          -0.3591,  0.3570, -0.8197],\n",
      "         [-0.8282, -0.4762,  0.5989, -0.1316,  0.5109,  0.8432, -0.1617,\n",
      "          -0.4020,  0.3243, -0.9195]]], grad_fn=<ViewBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sreevaatsav/.pyenv/versions/project_env/lib/python3.10/site-packages/torch/nn/modules/transformer.py:282: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=512, dropout=0):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.encoding = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1).float()\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(torch.log(torch.tensor(10000.0)) / d_model))\n",
    "        self.encoding[:, 0::2] = torch.sin(position * div_term)\n",
    "        self.encoding[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.encoding = self.encoding.unsqueeze(0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.encoding[:, :x.size(1)].detach()\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, src_vocab_size, tgt_vocab_size, d_model=4, nhead=2, num_encoder_layers=1, num_decoder_layers=1):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.src_embedding = nn.Embedding(src_vocab_size, d_model)\n",
    "        self.tgt_embedding = nn.Embedding(tgt_vocab_size, d_model)\n",
    "        self.positional_encoding = PositionalEncoding(d_model, dropout=0)\n",
    "        self.transformer = nn.Transformer(\n",
    "            d_model=d_model,\n",
    "            nhead=nhead,\n",
    "            num_encoder_layers=num_encoder_layers,\n",
    "            num_decoder_layers=num_decoder_layers,\n",
    "            dropout=0\n",
    "        )\n",
    "        self.fc = nn.Linear(d_model, tgt_vocab_size)\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "\n",
    "        print(\"Source embeddings :- \\n\")\n",
    "        print(self.src_embedding(src))\n",
    "        print()\n",
    "\n",
    "        print(\"Target embeddings :- \\n\")\n",
    "        print(self.tgt_embedding(tgt))\n",
    "        print()\n",
    "\n",
    "\n",
    "        src = self.src_embedding(src) + self.positional_encoding(src)\n",
    "        tgt = self.tgt_embedding(tgt) + self.positional_encoding(tgt)\n",
    "\n",
    "        print(\"Positional encoded source embeddings:\")\n",
    "        print(src)\n",
    "        print()\n",
    "\n",
    "        print(\"Positional encoded target embeddings:\")\n",
    "        print(tgt)\n",
    "        print()\n",
    "\n",
    "        output = self.transformer(src, tgt)\n",
    "        output = self.fc(output)\n",
    "        return output\n",
    "\n",
    "    \n",
    "\n",
    "src_vocab_size = 10  # Source language vocabulary size\n",
    "tgt_vocab_size = 10  # Target language vocabulary size\n",
    "d_model = 4  # Dimension of the model\n",
    "num_heads = 2\n",
    "num_encoder_layers = 3\n",
    "num_decoder_layers = 3\n",
    "\n",
    "need_weights = False\n",
    "\n",
    "src_mask = None\n",
    "tgt_mask = None\n",
    "memory_mask = None\n",
    "\n",
    "max_seq_len = 4\n",
    "\n",
    "\n",
    "model = TransformerModel(src_vocab_size, tgt_vocab_size, d_model=d_model, nhead=num_heads, num_encoder_layers = num_encoder_layers, num_decoder_layers=num_decoder_layers)\n",
    "# Source sentence in the source language\n",
    "src_sentence = torch.tensor([[0], [1], [2], [3]])  # Source sequence\n",
    "\n",
    "# Target sentence in the target language (translation of the source sentence)\n",
    "tgt_sentence = torch.tensor([[1], [0], [3], [3]])  # Target sequence\n",
    "\n",
    "# Forward pass\n",
    "output = model(src_sentence, tgt_sentence)\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_intermediate_outputs(src_sentence, tgt_sentence,d_model, model, num_encoder_layers , num_decoder_layers):\n",
    "\n",
    "    state_dict = model.state_dict()\n",
    "\n",
    "    pe_src_embeds, pe_tgt_embeds = get_embedding_outputs(src_sentence=src_sentence,  tgt_sentence=tgt_sentence, state_dict=state_dict, max_seq_len=max_seq_len, d_model = d_model)\n",
    "    print(\"###\"*25)\n",
    "    print(\"### Encoder Start ###\")\n",
    "    print()\n",
    "\n",
    "    x_enc = pe_src_embeds\n",
    "\n",
    "    for lno in range(num_encoder_layers):\n",
    "        attn_enc_output, src_len, head_dim, attn_weights = encoder_block_attn_output(x_enc, state_dict, layer_num = lno, need_weights = False, embed_dim = d_model, num_heads= num_heads, src_mask=None)\n",
    "\n",
    "        if lno == 0:\n",
    "            tgt_len, bsz, embed_dim = x_enc.shape\n",
    "\n",
    "        output_enc_final = encoder_block_post_attn_output(x_enc, attn_enc_output, state_dict, layer_num = lno , bsz = bsz, tgt_len = tgt_len)\n",
    "\n",
    "        x_enc = output_enc_final\n",
    "\n",
    "    \n",
    "    print(\"### Encoder Done ###\")\n",
    "    print(\"\\n\\n\\n\")\n",
    "    print(\"### Decoder Start ###\")\n",
    "\n",
    "    x_dec = pe_tgt_embeds\n",
    "    memory = x_enc\n",
    "\n",
    "    for lno in range(num_decoder_layers):\n",
    "\n",
    "        self_attn_dec, dec_sa_wts = decoder_block_self_attn_output(x_dec, state_dict, layer_num = lno, need_weights = False, tgt_mask=None)\n",
    "        x_dec = dec_post_self_attn(self_attn_dec, x_dec, state_dict, layer_num = lno)\n",
    "\n",
    "        attn_dec_mha_output, attn_dec_mha_wts = decoder_block_cross_attn_output(x_dec, memory, state_dict, layer_num = lno, tgt_len = tgt_len, src_len = src_len, head_dim = head_dim, need_weights = False, memory_mask=None)\n",
    "        final_op = decoder_block_post_attn_output(x_dec, attn_dec_mha_output, state_dict, layer_num = lno)\n",
    "\n",
    "        print(pe_tgt_embeds.shape, final_op.shape)\n",
    "        x_dec = final_op\n",
    "\n",
    "\n",
    "    final_op = feef_fwd_transformer(final_op, state_dict)\n",
    "\n",
    "    print(\"### Decoder Done ###\")\n",
    "\n",
    "\n",
    "    return final_op\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source sentence embedding\n",
      "Word index: 0, Embedding: tensor([-0.8011, -0.1107, -1.6151,  1.4762])\n",
      "Word index: 1, Embedding: tensor([-0.3233, -1.9852, -0.9699,  0.9073])\n",
      "Word index: 2, Embedding: tensor([-0.8814, -0.9857,  0.9917, -1.8681])\n",
      "Word index: 3, Embedding: tensor([ 0.3493, -0.4863,  0.4214, -0.6675])\n",
      "\n",
      "Target sentence embedding\n",
      "Word index: 1, Embedding: tensor([-0.2207, -0.4344, -0.1539, -0.3831])\n",
      "Word index: 0, Embedding: tensor([-1.3731, -0.7337, -0.0944, -0.0221])\n",
      "Word index: 3, Embedding: tensor([ 0.4218,  0.4912, -1.6193, -2.3457])\n",
      "Word index: 3, Embedding: tensor([ 0.4218,  0.4912, -1.6193, -2.3457])\n",
      "\n",
      "PE of src :\n",
      "tensor([[[0., 1., 0., 1.],\n",
      "         [1., 2., 1., 2.],\n",
      "         [2., 3., 2., 3.],\n",
      "         [3., 4., 3., 4.]]])\n",
      "\n",
      "PE of tgt :\n",
      "tensor([[[1., 2., 1., 2.],\n",
      "         [0., 1., 0., 1.],\n",
      "         [3., 4., 3., 4.],\n",
      "         [3., 4., 3., 4.]]])\n",
      "\n",
      "PE source embeddings : \n",
      "\n",
      "tensor([[[-0.8011,  0.8893, -1.6151,  2.4762],\n",
      "         [ 0.1989,  1.8893, -0.6151,  3.4762],\n",
      "         [ 1.1989,  2.8893,  0.3849,  4.4762],\n",
      "         [ 2.1989,  3.8893,  1.3849,  5.4762]],\n",
      "\n",
      "        [[-0.3233, -0.9852, -0.9699,  1.9073],\n",
      "         [ 0.6767,  0.0148,  0.0301,  2.9073],\n",
      "         [ 1.6767,  1.0148,  1.0301,  3.9073],\n",
      "         [ 2.6767,  2.0148,  2.0301,  4.9073]],\n",
      "\n",
      "        [[-0.8814,  0.0143,  0.9917, -0.8681],\n",
      "         [ 0.1186,  1.0143,  1.9917,  0.1319],\n",
      "         [ 1.1186,  2.0143,  2.9917,  1.1319],\n",
      "         [ 2.1186,  3.0143,  3.9917,  2.1319]],\n",
      "\n",
      "        [[ 0.3493,  0.5137,  0.4214,  0.3325],\n",
      "         [ 1.3493,  1.5137,  1.4214,  1.3325],\n",
      "         [ 2.3493,  2.5137,  2.4214,  2.3325],\n",
      "         [ 3.3493,  3.5137,  3.4214,  3.3325]]])\n",
      "\n",
      "PE target embeddings : \n",
      "\n",
      "tensor([[[ 0.7793,  1.5656,  0.8461,  1.6169],\n",
      "         [-0.2207,  0.5656, -0.1539,  0.6169],\n",
      "         [ 2.7793,  3.5656,  2.8461,  3.6169],\n",
      "         [ 2.7793,  3.5656,  2.8461,  3.6169]],\n",
      "\n",
      "        [[-0.3731,  1.2663,  0.9056,  1.9779],\n",
      "         [-1.3731,  0.2663, -0.0944,  0.9779],\n",
      "         [ 1.6269,  3.2663,  2.9056,  3.9779],\n",
      "         [ 1.6269,  3.2663,  2.9056,  3.9779]],\n",
      "\n",
      "        [[ 1.4218,  2.4912, -0.6193, -0.3457],\n",
      "         [ 0.4218,  1.4912, -1.6193, -1.3457],\n",
      "         [ 3.4218,  4.4912,  1.3807,  1.6543],\n",
      "         [ 3.4218,  4.4912,  1.3807,  1.6543]],\n",
      "\n",
      "        [[ 1.4218,  2.4912, -0.6193, -0.3457],\n",
      "         [ 0.4218,  1.4912, -1.6193, -1.3457],\n",
      "         [ 3.4218,  4.4912,  1.3807,  1.6543],\n",
      "         [ 3.4218,  4.4912,  1.3807,  1.6543]]])\n",
      "\n",
      "###########################################################################\n",
      "### Encoder Start ###\n",
      "\n",
      "torch.Size([4, 4, 4])\n",
      "Q_enc_0 = \n",
      "tensor([[[ 1.1692e+00, -3.4754e-01],\n",
      "         [ 5.6561e-02, -1.2562e+00],\n",
      "         [-5.9954e-01,  4.1997e-01],\n",
      "         [ 6.3646e-01,  4.8874e-03]],\n",
      "\n",
      "        [[-5.7117e-02,  1.0919e+00],\n",
      "         [-3.2538e-01,  1.1722e+00],\n",
      "         [-2.7642e-01, -1.8666e-01],\n",
      "         [ 4.0824e-02, -2.2408e-01]],\n",
      "\n",
      "        [[ 2.7108e+00, -5.8195e-01],\n",
      "         [ 1.5981e+00, -1.4906e+00],\n",
      "         [ 9.4200e-01,  1.8555e-01],\n",
      "         [ 2.1780e+00, -2.2953e-01]],\n",
      "\n",
      "        [[ 1.0914e-02,  6.3649e-01],\n",
      "         [-2.5735e-01,  7.1679e-01],\n",
      "         [-2.0838e-01, -6.4204e-01],\n",
      "         [ 1.0886e-01, -6.7947e-01]],\n",
      "\n",
      "        [[ 4.2523e+00, -8.1637e-01],\n",
      "         [ 3.1396e+00, -1.7250e+00],\n",
      "         [ 2.4835e+00, -4.8863e-02],\n",
      "         [ 3.7195e+00, -4.6394e-01]],\n",
      "\n",
      "        [[ 7.8946e-02,  1.8111e-01],\n",
      "         [-1.8931e-01,  2.6141e-01],\n",
      "         [-1.4035e-01, -1.0974e+00],\n",
      "         [ 1.7689e-01, -1.1348e+00]],\n",
      "\n",
      "        [[ 5.7938e+00, -1.0508e+00],\n",
      "         [ 4.6812e+00, -1.9595e+00],\n",
      "         [ 4.0251e+00, -2.8328e-01],\n",
      "         [ 5.2611e+00, -6.9836e-01]],\n",
      "\n",
      "        [[ 1.4698e-01, -2.7427e-01],\n",
      "         [-1.2128e-01, -1.9397e-01],\n",
      "         [-7.2321e-02, -1.5528e+00],\n",
      "         [ 2.4492e-01, -1.5902e+00]]])\n",
      "\n",
      "K_enc_0 = \n",
      "tensor([[[-1.0390,  0.2198],\n",
      "         [-0.1149,  0.7552],\n",
      "         [ 0.6616, -0.0217],\n",
      "         [-0.7741, -0.2753]],\n",
      "\n",
      "        [[ 0.4348, -1.2000],\n",
      "         [ 0.8130,  0.0533],\n",
      "         [-0.7709,  0.4205],\n",
      "         [-0.3118,  0.0204]],\n",
      "\n",
      "        [[-2.9655, -0.3558],\n",
      "         [-2.0415,  0.1795],\n",
      "         [-1.2650, -0.5974],\n",
      "         [-2.7006, -0.8510]],\n",
      "\n",
      "        [[-0.1874, -1.0216],\n",
      "         [ 0.1908,  0.2318],\n",
      "         [-1.3932,  0.5989],\n",
      "         [-0.9340,  0.1988]],\n",
      "\n",
      "        [[-4.8921, -0.9315],\n",
      "         [-3.9680, -0.3962],\n",
      "         [-3.1916, -1.1731],\n",
      "         [-4.6272, -1.4267]],\n",
      "\n",
      "        [[-0.8097, -0.8431],\n",
      "         [-0.4315,  0.4103],\n",
      "         [-2.0155,  0.7774],\n",
      "         [-1.5563,  0.3773]],\n",
      "\n",
      "        [[-6.8187, -1.5072],\n",
      "         [-5.8946, -0.9719],\n",
      "         [-5.1181, -1.7488],\n",
      "         [-6.5538, -2.0024]],\n",
      "\n",
      "        [[-1.4320, -0.6646],\n",
      "         [-1.0538,  0.5888],\n",
      "         [-2.6377,  0.9559],\n",
      "         [-2.1786,  0.5558]]])\n",
      "\n",
      "V_enc_0 = \n",
      "tensor([[[ 1.1734,  1.5581],\n",
      "         [ 0.9299,  0.6206],\n",
      "         [-0.6012, -0.2033],\n",
      "         [-0.2759,  0.1176]],\n",
      "\n",
      "        [[-0.6262, -1.3653],\n",
      "         [-0.9657, -0.2515],\n",
      "         [-0.0120, -0.3402],\n",
      "         [ 0.2829, -0.0716]],\n",
      "\n",
      "        [[ 0.5602,  1.8068],\n",
      "         [ 0.3167,  0.8693],\n",
      "         [-1.2144,  0.0454],\n",
      "         [-0.8891,  0.3664]],\n",
      "\n",
      "        [[-0.0203, -1.4007],\n",
      "         [-0.3598, -0.2869],\n",
      "         [ 0.5939, -0.3756],\n",
      "         [ 0.8888, -0.1071]],\n",
      "\n",
      "        [[-0.0530,  2.0556],\n",
      "         [-0.2965,  1.1181],\n",
      "         [-1.8276,  0.2942],\n",
      "         [-1.5023,  0.6151]],\n",
      "\n",
      "        [[ 0.5856, -1.4362],\n",
      "         [ 0.2461, -0.3224],\n",
      "         [ 1.1998, -0.4111],\n",
      "         [ 1.4947, -0.1425]],\n",
      "\n",
      "        [[-0.6662,  2.3044],\n",
      "         [-0.9096,  1.3669],\n",
      "         [-2.4408,  0.5430],\n",
      "         [-2.1154,  0.8639]],\n",
      "\n",
      "        [[ 1.1915, -1.4716],\n",
      "         [ 0.8520, -0.3578],\n",
      "         [ 1.8057, -0.4465],\n",
      "         [ 2.1006, -0.1780]]])\n",
      "\n",
      "torch.Size([4, 2, 4, 2]) torch.Size([4, 2, 4, 2]) torch.Size([4, 2, 2, 4])\n",
      "Attention weights = \n",
      "torch.Size([4, 2, 4, 4]) tensor([[[[-9.1301e-01, -2.8056e-01,  5.5229e-01, -5.7231e-01],\n",
      "          [-2.3683e-01, -6.7539e-01,  4.5741e-02,  2.1358e-01],\n",
      "          [ 5.0574e-01,  2.7296e-01, -2.8691e-01,  2.4640e-01],\n",
      "          [-4.6683e-01, -4.9094e-02,  2.9766e-01, -3.4931e-01]],\n",
      "\n",
      "         [[-9.4409e-01,  8.3511e-03,  3.5577e-01,  2.8324e-02],\n",
      "          [-1.0947e+00, -1.4284e-01,  5.2588e-01,  8.8619e-02],\n",
      "          [ 7.3401e-02, -1.6595e-01,  9.5188e-02,  5.8246e-02],\n",
      "          [ 2.0270e-01,  1.5016e-02, -8.8879e-02, -1.2228e-02]]],\n",
      "\n",
      "\n",
      "        [[[-5.5379e+00, -3.9869e+00, -2.1789e+00, -4.8264e+00],\n",
      "          [-2.9761e+00, -2.4961e+00, -7.9981e-01, -2.1548e+00],\n",
      "          [-2.0220e+00, -1.3363e+00, -9.2099e-01, -1.9105e+00],\n",
      "          [-4.5094e+00, -3.1731e+00, -1.8512e+00, -4.0211e+00]],\n",
      "\n",
      "         [[-4.6123e-01,  1.0581e-01,  2.5881e-01,  8.2287e-02],\n",
      "          [-4.8368e-01,  8.2785e-02,  5.5710e-01,  2.7075e-01],\n",
      "          [ 4.9141e-01, -1.3335e-01, -6.6626e-02,  4.7353e-02],\n",
      "          [ 4.7639e-01, -9.6695e-02, -3.9500e-01, -1.6743e-01]]],\n",
      "\n",
      "\n",
      "        [[[-1.4172e+01, -1.1702e+01, -8.9193e+00, -1.3090e+01],\n",
      "          [-9.7245e+00, -8.3259e+00, -5.6545e+00, -8.5324e+00],\n",
      "          [-8.5590e+00, -6.9547e+00, -5.5643e+00, -8.0766e+00],\n",
      "          [-1.2561e+01, -1.0306e+01, -8.0093e+00, -1.1702e+01]],\n",
      "\n",
      "         [[-1.5317e-01,  2.8455e-02, -1.2951e-02, -3.8556e-02],\n",
      "          [-4.7456e-02,  1.3360e-01,  4.1350e-01,  2.7808e-01],\n",
      "          [ 7.3460e-01, -2.7556e-01, -4.0325e-01, -1.3835e-01],\n",
      "          [ 5.7528e-01, -3.8321e-01, -8.7593e-01, -4.9744e-01]]],\n",
      "\n",
      "\n",
      "        [[[-2.6815e+01, -2.3427e+01, -1.9669e+01, -2.5362e+01],\n",
      "          [-2.0482e+01, -1.8165e+01, -1.4518e+01, -1.8919e+01],\n",
      "          [-1.9105e+01, -1.6582e+01, -1.4217e+01, -1.8252e+01],\n",
      "          [-2.4622e+01, -2.1449e+01, -1.8177e+01, -2.3392e+01]],\n",
      "\n",
      "         [[-1.9923e-02, -2.2370e-01, -4.5952e-01, -3.3420e-01],\n",
      "          [ 2.1396e-01,  9.6187e-03,  9.5105e-02,  1.1060e-01],\n",
      "          [ 8.0300e-01, -5.9257e-01, -9.1467e-01, -4.9885e-01],\n",
      "          [ 4.9937e-01, -8.4453e-01, -1.5317e+00, -1.0023e+00]]]])\n",
      "Final Encoder 0 Output :\n",
      "norm2(norm1(x + self_atten(x)) + feed_fwd_op)\n",
      "\n",
      "tensor([[[-1.0755,  0.7841, -0.8997,  1.1912],\n",
      "         [-1.0454,  0.8258, -0.9393,  1.1588],\n",
      "         [-1.0267,  0.8427, -0.9613,  1.1453],\n",
      "         [-1.0123,  0.8473, -0.9767,  1.1416]],\n",
      "\n",
      "        [[-0.8170, -0.4040, -0.4903,  1.7114],\n",
      "         [-0.8434, -0.1321, -0.6944,  1.6700],\n",
      "         [-0.8510,  0.0199, -0.7948,  1.6259],\n",
      "         [-0.8467,  0.0880, -0.8429,  1.6015]],\n",
      "\n",
      "        [[-1.0781,  0.3527,  1.4695, -0.7441],\n",
      "         [-1.3433,  0.5776,  1.2684, -0.5028],\n",
      "         [-1.5727,  0.8289,  0.9024, -0.1587],\n",
      "         [-1.6452,  0.9547,  0.6131,  0.0774]],\n",
      "\n",
      "        [[-1.6967,  0.8916,  0.3687,  0.4364],\n",
      "         [-1.4137,  1.0724, -0.4590,  0.8003],\n",
      "         [-1.2787,  1.0791, -0.6685,  0.8680],\n",
      "         [-1.2130,  1.0672, -0.7574,  0.9033]]], grad_fn=<AddBackward0>)\n",
      "\n",
      "torch.Size([4, 4, 4])\n",
      "Q_enc_1 = \n",
      "tensor([[[-0.9445,  1.5327],\n",
      "         [-0.7726,  0.8446],\n",
      "         [ 0.6795,  0.1554],\n",
      "         [-0.2673,  1.3727]],\n",
      "\n",
      "        [[ 0.2663,  0.2969],\n",
      "         [-0.0759,  0.3830],\n",
      "         [-0.9309, -1.4403],\n",
      "         [-0.4866, -0.8362]],\n",
      "\n",
      "        [[-0.9523,  1.5387],\n",
      "         [-0.8779,  1.0354],\n",
      "         [ 0.4678,  0.5289],\n",
      "         [-0.7114,  1.6307]],\n",
      "\n",
      "        [[ 0.3013,  0.3200],\n",
      "         [ 0.0637,  0.4557],\n",
      "         [-0.8691, -1.3933],\n",
      "         [ 0.0171, -0.1940]],\n",
      "\n",
      "        [[-0.9571,  1.5390],\n",
      "         [-0.9239,  1.1287],\n",
      "         [ 0.1571,  0.9684],\n",
      "         [-0.8049,  1.6360]],\n",
      "\n",
      "        [[ 0.3201,  0.3351],\n",
      "         [ 0.1372,  0.4866],\n",
      "         [-0.7145, -1.2186],\n",
      "         [ 0.1552, -0.0093]],\n",
      "\n",
      "        [[-0.9613,  1.5366],\n",
      "         [-0.9435,  1.1660],\n",
      "         [-0.0574,  1.2116],\n",
      "         [-0.8440,  1.6282]],\n",
      "\n",
      "        [[ 0.3323,  0.3481],\n",
      "         [ 0.1739,  0.5041],\n",
      "         [-0.5753, -1.0406],\n",
      "         [ 0.2134,  0.0750]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "K_enc_1 = \n",
      "tensor([[[-0.1676,  0.2603],\n",
      "         [-0.5094, -0.4906],\n",
      "         [ 0.8221, -0.0446],\n",
      "         [ 0.4731,  0.1674]],\n",
      "\n",
      "        [[-0.4620,  0.1106],\n",
      "         [-1.0858,  0.7254],\n",
      "         [ 0.5694,  0.2895],\n",
      "         [ 0.0338,  0.3213]],\n",
      "\n",
      "        [[-0.1651,  0.2971],\n",
      "         [-0.4843, -0.3098],\n",
      "         [ 0.8064,  0.0412],\n",
      "         [ 0.1653,  0.3686]],\n",
      "\n",
      "        [[-0.4332,  0.0689],\n",
      "         [-0.9901,  0.5718],\n",
      "         [ 0.4945,  0.2953],\n",
      "         [-0.1445,  0.0868]],\n",
      "\n",
      "        [[-0.1666,  0.3136],\n",
      "         [-0.4627, -0.2069],\n",
      "         [ 0.7230,  0.1551],\n",
      "         [ 0.0739,  0.4093]],\n",
      "\n",
      "        [[-0.4216,  0.0488],\n",
      "         [-0.9245,  0.4815],\n",
      "         [ 0.3646,  0.2718],\n",
      "         [-0.1876,  0.0186]],\n",
      "\n",
      "        [[-0.1709,  0.3207],\n",
      "         [-0.4539, -0.1587],\n",
      "         [ 0.6311,  0.2235],\n",
      "         [ 0.0287,  0.4186]],\n",
      "\n",
      "        [[-0.4187,  0.0387],\n",
      "         [-0.8926,  0.4369],\n",
      "         [ 0.2603,  0.2419],\n",
      "         [-0.2139, -0.0052]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "V_enc_1 = \n",
      "tensor([[[ 0.7822,  0.1640],\n",
      "         [ 1.5860, -0.0715],\n",
      "         [-0.3192, -1.3732],\n",
      "         [ 0.4230, -0.9020]],\n",
      "\n",
      "        [[ 0.4078,  0.4556],\n",
      "         [-0.0843,  0.0768],\n",
      "         [-0.5649, -0.8715],\n",
      "         [-0.0970, -0.2859]],\n",
      "\n",
      "        [[ 0.7322,  0.2052],\n",
      "         [ 1.4438,  0.0667],\n",
      "         [-0.1757, -1.3436],\n",
      "         [ 0.4963, -0.2409]],\n",
      "\n",
      "        [[ 0.4398,  0.4877],\n",
      "         [ 0.0720,  0.2266],\n",
      "         [-0.4614, -0.7649],\n",
      "         [ 0.2956,  0.2226]],\n",
      "\n",
      "        [[ 0.7096,  0.2285],\n",
      "         [ 1.3481,  0.1382],\n",
      "         [ 0.0135, -1.1919],\n",
      "         [ 0.4962, -0.0490]],\n",
      "\n",
      "        [[ 0.4559,  0.5048],\n",
      "         [ 0.1557,  0.3040],\n",
      "         [-0.2858, -0.5614],\n",
      "         [ 0.3919,  0.3537]],\n",
      "\n",
      "        [[ 0.7006,  0.2449],\n",
      "         [ 1.2996,  0.1757],\n",
      "         [ 0.1378, -1.0303],\n",
      "         [ 0.5046,  0.0350]],\n",
      "\n",
      "        [[ 0.4650,  0.5158],\n",
      "         [ 0.1955,  0.3415],\n",
      "         [-0.1543, -0.3975],\n",
      "         [ 0.4292,  0.4081]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "torch.Size([4, 2, 4, 2]) torch.Size([4, 2, 4, 2]) torch.Size([4, 2, 2, 4])\n",
      "Attention weights = \n",
      "torch.Size([4, 2, 4, 4]) tensor([[[[ 0.3941, -0.1915, -0.5974, -0.1345],\n",
      "          [ 0.2471, -0.0147, -0.4758, -0.1585],\n",
      "          [-0.0519, -0.2987,  0.3901,  0.2457],\n",
      "          [ 0.2844, -0.3799, -0.1987,  0.0731]],\n",
      "\n",
      "         [[-0.0638, -0.0522,  0.1680,  0.0738],\n",
      "          [ 0.0548,  0.2548,  0.0478,  0.0852],\n",
      "          [ 0.1915, -0.0241, -0.6696, -0.3495],\n",
      "          [ 0.0936, -0.0554, -0.3671, -0.2016]]],\n",
      "\n",
      "\n",
      "        [[[ 0.4344, -0.0110, -0.4982,  0.2898],\n",
      "          [ 0.3200,  0.0738, -0.4704,  0.1673],\n",
      "          [ 0.0565, -0.2760,  0.2821,  0.1925],\n",
      "          [ 0.4256, -0.1136, -0.3581,  0.3419]],\n",
      "\n",
      "         [[-0.0767, -0.0815,  0.1722, -0.0111],\n",
      "          [ 0.0027,  0.1397,  0.1174,  0.0215],\n",
      "          [ 0.1983,  0.0451, -0.5949,  0.0033],\n",
      "          [-0.0147, -0.0904, -0.0345, -0.0137]]],\n",
      "\n",
      "\n",
      "        [[[ 0.4540,  0.0879, -0.3205,  0.3954],\n",
      "          [ 0.3591,  0.1371, -0.3485,  0.2784],\n",
      "          [ 0.1963, -0.1931,  0.1865,  0.2885],\n",
      "          [ 0.4576,  0.0239, -0.2320,  0.4315]],\n",
      "\n",
      "         [[-0.0838, -0.0951,  0.1469, -0.0381],\n",
      "          [-0.0241,  0.0760,  0.1289, -0.0118],\n",
      "          [ 0.1709,  0.0522, -0.4184,  0.0788],\n",
      "          [-0.0466, -0.1046,  0.0382, -0.0207]]],\n",
      "\n",
      "\n",
      "        [[[ 0.4646,  0.1361, -0.1861,  0.4353],\n",
      "          [ 0.3784,  0.1720, -0.2368,  0.3260],\n",
      "          [ 0.2817, -0.1176,  0.1659,  0.3575],\n",
      "          [ 0.4712,  0.0881, -0.1193,  0.4648]],\n",
      "\n",
      "         [[-0.0889, -0.1022,  0.1207, -0.0515],\n",
      "          [-0.0377,  0.0460,  0.1182, -0.0282],\n",
      "          [ 0.1418,  0.0416, -0.2838,  0.0909],\n",
      "          [-0.0611, -0.1115,  0.0521, -0.0326]]]], grad_fn=<AddBackward0>)\n",
      "Final Encoder 1 Output :\n",
      "norm2(norm1(x + self_atten(x)) + feed_fwd_op)\n",
      "\n",
      "tensor([[[-1.0807,  0.3127, -0.7204,  1.4884],\n",
      "         [-1.1770,  0.4916, -0.6913,  1.3766],\n",
      "         [-1.2203,  0.5756, -0.6705,  1.3152],\n",
      "         [-1.2444,  0.6144, -0.6534,  1.2834]],\n",
      "\n",
      "        [[-0.6312, -0.8254, -0.2360,  1.6925],\n",
      "         [-0.8104, -0.5406, -0.3586,  1.7095],\n",
      "         [-0.9041, -0.3634, -0.4263,  1.6937],\n",
      "         [-0.9506, -0.2785, -0.4492,  1.6783]],\n",
      "\n",
      "        [[-0.8912, -0.1581,  1.6714, -0.6221],\n",
      "         [-1.1783,  0.1932,  1.5145, -0.5295],\n",
      "         [-1.4773,  0.5059,  1.2238, -0.2525],\n",
      "         [-1.6060,  0.6696,  0.9849, -0.0485]],\n",
      "\n",
      "        [[-1.7250,  0.4605,  0.5525,  0.7120],\n",
      "         [-1.5541,  0.7847, -0.1953,  0.9648],\n",
      "         [-1.4765,  0.8544, -0.3584,  0.9805],\n",
      "         [-1.4463,  0.8741, -0.4140,  0.9862]]], grad_fn=<AddBackward0>)\n",
      "\n",
      "torch.Size([4, 4, 4])\n",
      "Q_enc_2 = \n",
      "tensor([[[ 0.6110,  0.3636],\n",
      "         [ 0.6454,  0.4092],\n",
      "         [-0.4904, -0.0794],\n",
      "         [ 0.1242,  0.2314]],\n",
      "\n",
      "        [[-0.1023, -0.5693],\n",
      "         [-0.2229,  0.4976],\n",
      "         [-0.1931, -0.2476],\n",
      "         [-0.2016, -0.9703]],\n",
      "\n",
      "        [[ 0.5606,  0.3410],\n",
      "         [ 0.6599,  0.4157],\n",
      "         [-0.4474, -0.0528],\n",
      "         [ 0.3240,  0.2677]],\n",
      "\n",
      "        [[-0.0893, -0.7511],\n",
      "         [-0.2029,  0.2092],\n",
      "         [-0.1784, -0.6244],\n",
      "         [-0.1100, -1.1442]],\n",
      "\n",
      "        [[ 0.5328,  0.3285],\n",
      "         [ 0.6586,  0.4132],\n",
      "         [-0.3177,  0.0142],\n",
      "         [ 0.3544,  0.2652]],\n",
      "\n",
      "        [[-0.0830, -0.8362],\n",
      "         [-0.1879,  0.0350],\n",
      "         [-0.1692, -0.9654],\n",
      "         [-0.0840, -1.1684]],\n",
      "\n",
      "        [[ 0.5174,  0.3222],\n",
      "         [ 0.6537,  0.4105],\n",
      "         [-0.2151,  0.0601],\n",
      "         [ 0.3651,  0.2644]],\n",
      "\n",
      "        [[-0.0808, -0.8772],\n",
      "         [-0.1811, -0.0495],\n",
      "         [-0.1584, -1.1301],\n",
      "         [-0.0752, -1.1720]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "K_enc_2 = \n",
      "tensor([[[-0.4109, -0.0646],\n",
      "         [-0.2731, -0.4326],\n",
      "         [-0.9525, -0.0697],\n",
      "         [-1.1217, -0.0048]],\n",
      "\n",
      "        [[-0.7506,  1.1329],\n",
      "         [-0.2860,  0.3274],\n",
      "         [ 1.3044, -0.7023],\n",
      "         [ 0.2484,  0.6350]],\n",
      "\n",
      "        [[-0.4756, -0.0031],\n",
      "         [-0.3444, -0.3494],\n",
      "         [-1.0758,  0.0239],\n",
      "         [-0.8271,  0.0987]],\n",
      "\n",
      "        [[-0.7376,  1.2011],\n",
      "         [-0.4121,  0.5654],\n",
      "         [ 1.1355, -0.3696],\n",
      "         [-0.3636,  1.1228]],\n",
      "\n",
      "        [[-0.5070,  0.0265],\n",
      "         [-0.3812, -0.2948],\n",
      "         [-1.1683,  0.0912],\n",
      "         [-0.7403,  0.1262]],\n",
      "\n",
      "        [[-0.7252,  1.2273],\n",
      "         [-0.4809,  0.6982],\n",
      "         [ 0.8512,  0.0530],\n",
      "         [-0.4918,  1.2074]],\n",
      "\n",
      "        [[-0.5258,  0.0402],\n",
      "         [-0.4024, -0.2681],\n",
      "         [-1.1775,  0.1244],\n",
      "         [-0.7085,  0.1345]],\n",
      "\n",
      "        [[-0.7135,  1.2363],\n",
      "         [-0.5061,  0.7562],\n",
      "         [ 0.6322,  0.3188],\n",
      "         [-0.5351,  1.2335]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "V_enc_2 = \n",
      "tensor([[[ 0.0663, -0.7889],\n",
      "         [ 0.1208, -0.6337],\n",
      "         [-1.4102, -0.8487],\n",
      "         [-1.0516, -1.3639]],\n",
      "\n",
      "        [[-0.8201,  0.1853],\n",
      "         [-0.3947,  0.1440],\n",
      "         [-0.7254,  0.2074],\n",
      "         [-1.3707,  0.3279]],\n",
      "\n",
      "        [[-0.0229, -0.8402],\n",
      "         [ 0.0858, -0.7268],\n",
      "         [-1.4717, -1.0170],\n",
      "         [-0.5494, -1.1291]],\n",
      "\n",
      "        [[-0.9104,  0.1986],\n",
      "         [-0.5517,  0.1672],\n",
      "         [-0.9689,  0.2486],\n",
      "         [-1.2419,  0.2709]],\n",
      "\n",
      "        [[-0.0690, -0.8632],\n",
      "         [ 0.0640, -0.7708],\n",
      "         [-1.4376, -1.1936],\n",
      "         [-0.4262, -1.0452]],\n",
      "\n",
      "        [[-0.9517,  0.2047],\n",
      "         [-0.6362,  0.1784],\n",
      "         [-1.2128,  0.2909],\n",
      "         [-1.1832,  0.2508]],\n",
      "\n",
      "        [[-0.0968, -0.8776],\n",
      "         [ 0.0466, -0.7937],\n",
      "         [-1.3510, -1.2583],\n",
      "         [-0.3820, -1.0142]],\n",
      "\n",
      "        [[-0.9741,  0.2084],\n",
      "         [-0.6783,  0.1842],\n",
      "         [-1.3160,  0.3061],\n",
      "         [-1.1599,  0.2433]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "torch.Size([4, 2, 4, 2]) torch.Size([4, 2, 4, 2]) torch.Size([4, 2, 2, 4])\n",
      "Attention weights = \n",
      "torch.Size([4, 2, 4, 4]) tensor([[[[-0.1941, -0.2292, -0.4294, -0.4858],\n",
      "          [-0.2062, -0.2498, -0.4548, -0.5133],\n",
      "          [ 0.1461,  0.1190,  0.3342,  0.3893],\n",
      "          [-0.0466, -0.0948, -0.0950, -0.0993]],\n",
      "\n",
      "         [[-0.4018, -0.1111,  0.1884, -0.2736],\n",
      "          [ 0.5169,  0.1603, -0.4527,  0.1843],\n",
      "          [-0.0959, -0.0183, -0.0551, -0.1451],\n",
      "          [-0.6703, -0.1839,  0.2959, -0.4711]]],\n",
      "\n",
      "\n",
      "        [[[-0.1893, -0.2208, -0.4207, -0.3041],\n",
      "          [-0.2228, -0.2634, -0.4950, -0.3569],\n",
      "          [ 0.1506,  0.1220,  0.3395,  0.2580],\n",
      "          [-0.1096, -0.1451, -0.2420, -0.1708]],\n",
      "\n",
      "         [[-0.5913, -0.2743,  0.1247, -0.5733],\n",
      "          [ 0.2835,  0.1428, -0.2176,  0.2182],\n",
      "          [-0.4372, -0.1977,  0.0199, -0.4498],\n",
      "          [-0.9144, -0.4254,  0.2108, -0.8802]]],\n",
      "\n",
      "\n",
      "        [[[-0.1848, -0.2121, -0.4190, -0.2496],\n",
      "          [-0.2283, -0.2636, -0.5174, -0.3078],\n",
      "          [ 0.1142,  0.0827,  0.2634,  0.1676],\n",
      "          [-0.1221, -0.1508, -0.2757, -0.1618]],\n",
      "\n",
      "         [[-0.6831, -0.3846, -0.0813, -0.6851],\n",
      "          [ 0.1267,  0.0812, -0.1118,  0.0952],\n",
      "          [-0.7510, -0.4191, -0.1380, -0.7654],\n",
      "          [-0.9709, -0.5483, -0.0943, -0.9684]]],\n",
      "\n",
      "\n",
      "        [[[-0.1832, -0.2083, -0.4025, -0.2286],\n",
      "          [-0.2313, -0.2638, -0.5081, -0.2885],\n",
      "          [ 0.0817,  0.0498,  0.1844,  0.1135],\n",
      "          [-0.1282, -0.1540, -0.2807, -0.1578]],\n",
      "\n",
      "         [[-0.7261, -0.4401, -0.2339, -0.7346],\n",
      "          [ 0.0481,  0.0383, -0.0921,  0.0253],\n",
      "          [-0.9081, -0.5476, -0.3256, -0.9258],\n",
      "          [-0.9867, -0.5998, -0.2978, -0.9938]]]], grad_fn=<AddBackward0>)\n",
      "Final Encoder 2 Output :\n",
      "norm2(norm1(x + self_atten(x)) + feed_fwd_op)\n",
      "\n",
      "tensor([[[-1.4990,  0.7444, -0.2987,  1.0533],\n",
      "         [-1.5121,  0.8727, -0.2916,  0.9310],\n",
      "         [-1.5148,  0.9319, -0.2861,  0.8690],\n",
      "         [-1.5182,  0.9572, -0.2769,  0.8379]],\n",
      "\n",
      "        [[-1.3058, -0.3143,  0.1455,  1.4747],\n",
      "         [-1.3925, -0.0321, -0.0105,  1.4351],\n",
      "         [-1.4340,  0.1398, -0.0900,  1.3842],\n",
      "         [-1.4516,  0.2181, -0.1197,  1.3531]],\n",
      "\n",
      "        [[-1.0356,  0.3077,  1.4970, -0.7692],\n",
      "         [-1.1819,  0.5317,  1.3520, -0.7017],\n",
      "         [-1.3785,  0.7521,  1.1311, -0.5047],\n",
      "         [-1.4762,  0.8709,  0.9656, -0.3603]],\n",
      "\n",
      "        [[-1.6791,  0.7794,  0.7400,  0.1597],\n",
      "         [-1.6386,  1.0445,  0.1429,  0.4511],\n",
      "         [-1.5924,  1.1025, -0.0088,  0.4986],\n",
      "         [-1.5740,  1.1191, -0.0613,  0.5161]]], grad_fn=<AddBackward0>)\n",
      "\n",
      "### Encoder Done ###\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "### Decoder Start ###\n",
      "DEC SA\n",
      "torch.Size([1, 4, 4, 4]) torch.Size([1, 4, 4, 4]) torch.Size([1, 4, 4, 4])\n",
      "4 8 2\n",
      "Q_dec_0 = \n",
      "tensor([[[-0.7885,  1.1865],\n",
      "         [-0.7394,  0.5141],\n",
      "         [-0.8008,  1.8334],\n",
      "         [-0.8008,  1.8334]],\n",
      "\n",
      "        [[ 0.8490,  0.0050],\n",
      "         [ 0.1118,  0.7839],\n",
      "         [ 1.8169, -0.6278],\n",
      "         [ 1.8169, -0.6278]],\n",
      "\n",
      "        [[-0.3683,  0.3795],\n",
      "         [-0.3192, -0.2928],\n",
      "         [-0.3806,  1.0264],\n",
      "         [-0.3806,  1.0264]],\n",
      "\n",
      "        [[ 0.2103,  0.5547],\n",
      "         [-0.5269,  1.3336],\n",
      "         [ 1.1782, -0.0780],\n",
      "         [ 1.1782, -0.0780]],\n",
      "\n",
      "        [[-1.6290,  2.8004],\n",
      "         [-1.5799,  2.1281],\n",
      "         [-1.6413,  3.4473],\n",
      "         [-1.6413,  3.4473]],\n",
      "\n",
      "        [[ 2.1264, -1.0945],\n",
      "         [ 1.3892, -0.3156],\n",
      "         [ 3.0943, -1.7272],\n",
      "         [ 3.0943, -1.7272]],\n",
      "\n",
      "        [[-1.6290,  2.8004],\n",
      "         [-1.5799,  2.1281],\n",
      "         [-1.6413,  3.4473],\n",
      "         [-1.6413,  3.4473]],\n",
      "\n",
      "        [[ 2.1264, -1.0945],\n",
      "         [ 1.3892, -0.3156],\n",
      "         [ 3.0943, -1.7272],\n",
      "         [ 3.0943, -1.7272]]])\n",
      "\n",
      "K_dec_0 = \n",
      "tensor([[[-4.7693e-01,  2.5232e+00],\n",
      "         [ 1.0854e-01,  2.1713e+00],\n",
      "         [-1.8438e+00,  1.1542e+00],\n",
      "         [-1.8438e+00,  1.1542e+00]],\n",
      "\n",
      "        [[ 4.8025e-01, -9.4048e-01],\n",
      "         [ 5.0683e-01, -8.4301e-01],\n",
      "         [-1.0318e+00,  6.0348e-02],\n",
      "         [-1.0318e+00,  6.0348e-02]],\n",
      "\n",
      "        [[-1.9006e-01,  4.5797e-01],\n",
      "         [ 3.9540e-01,  1.0605e-01],\n",
      "         [-1.5569e+00, -9.1103e-01],\n",
      "         [-1.5569e+00, -9.1103e-01]],\n",
      "\n",
      "        [[-1.2757e-03, -1.8467e-01],\n",
      "         [ 2.5305e-02, -8.7211e-02],\n",
      "         [-1.5133e+00,  8.1615e-01],\n",
      "         [-1.5133e+00,  8.1615e-01]],\n",
      "\n",
      "        [[-1.0507e+00,  6.6537e+00],\n",
      "         [-4.6519e-01,  6.3017e+00],\n",
      "         [-2.4175e+00,  5.2847e+00],\n",
      "         [-2.4175e+00,  5.2847e+00]],\n",
      "\n",
      "        [[ 1.4433e+00, -2.4521e+00],\n",
      "         [ 1.4699e+00, -2.3546e+00],\n",
      "         [-6.8767e-02, -1.4513e+00],\n",
      "         [-6.8767e-02, -1.4513e+00]],\n",
      "\n",
      "        [[-1.0507e+00,  6.6537e+00],\n",
      "         [-4.6519e-01,  6.3017e+00],\n",
      "         [-2.4175e+00,  5.2847e+00],\n",
      "         [-2.4175e+00,  5.2847e+00]],\n",
      "\n",
      "        [[ 1.4433e+00, -2.4521e+00],\n",
      "         [ 1.4699e+00, -2.3546e+00],\n",
      "         [-6.8767e-02, -1.4513e+00],\n",
      "         [-6.8767e-02, -1.4513e+00]]])\n",
      "\n",
      "V_dec_0 = \n",
      "tensor([[[-2.4422e+00,  4.8449e-01],\n",
      "         [-1.9011e+00,  3.9233e-01],\n",
      "         [-1.5094e+00,  1.1968e+00],\n",
      "         [-1.5094e+00,  1.1968e+00]],\n",
      "\n",
      "        [[-1.7597e+00, -1.1242e-01],\n",
      "         [-1.6004e+00,  6.3111e-01],\n",
      "         [-6.0689e-03, -1.0117e+00],\n",
      "         [-6.0689e-03, -1.0117e+00]],\n",
      "\n",
      "        [[-3.6649e-01,  1.5799e-01],\n",
      "         [ 1.7460e-01,  6.5828e-02],\n",
      "         [ 5.6626e-01,  8.7033e-01],\n",
      "         [ 5.6626e-01,  8.7033e-01]],\n",
      "\n",
      "        [[-2.2292e-01,  3.3488e-01],\n",
      "         [-6.3583e-02,  1.0784e+00],\n",
      "         [ 1.5307e+00, -5.6444e-01],\n",
      "         [ 1.5307e+00, -5.6444e-01]],\n",
      "\n",
      "        [[-6.5935e+00,  1.1375e+00],\n",
      "         [-6.0525e+00,  1.0453e+00],\n",
      "         [-5.6608e+00,  1.8498e+00],\n",
      "         [-5.6608e+00,  1.8498e+00]],\n",
      "\n",
      "        [[-4.8332e+00, -1.0070e+00],\n",
      "         [-4.6739e+00, -2.6350e-01],\n",
      "         [-3.0796e+00, -1.9063e+00],\n",
      "         [-3.0796e+00, -1.9063e+00]],\n",
      "\n",
      "        [[-6.5935e+00,  1.1375e+00],\n",
      "         [-6.0525e+00,  1.0453e+00],\n",
      "         [-5.6608e+00,  1.8498e+00],\n",
      "         [-5.6608e+00,  1.8498e+00]],\n",
      "\n",
      "        [[-4.8332e+00, -1.0070e+00],\n",
      "         [-4.6739e+00, -2.6350e-01],\n",
      "         [-3.0796e+00, -1.9063e+00],\n",
      "         [-3.0796e+00, -1.9063e+00]]])\n",
      "\n",
      "torch.Size([4, 2, 4, 2]) torch.Size([4, 2, 4, 2]) torch.Size([4, 2, 2, 4])\n",
      "Attention weights = \n",
      "torch.Size([4, 2, 4, 4]) tensor([[[[ 2.3828e+00,  1.7611e+00,  1.9964e+00,  1.9964e+00],\n",
      "          [ 1.1667e+00,  7.3263e-01,  1.3836e+00,  1.3836e+00],\n",
      "          [ 3.5411e+00,  2.7534e+00,  2.5404e+00,  2.5404e+00],\n",
      "          [ 3.5411e+00,  2.7534e+00,  2.5404e+00,  2.5404e+00]],\n",
      "\n",
      "         [[ 2.8500e-01,  3.0130e-01, -6.1922e-01, -6.1922e-01],\n",
      "          [-4.8335e-01, -4.2723e-01, -4.8099e-02, -4.8099e-02],\n",
      "          [ 1.0345e+00,  1.0253e+00, -1.3524e+00, -1.3524e+00],\n",
      "          [ 1.0345e+00,  1.0253e+00, -1.3524e+00, -1.3524e+00]]],\n",
      "\n",
      "\n",
      "        [[[ 1.7240e-01, -7.4506e-02,  1.6095e-01,  1.6095e-01],\n",
      "          [-5.1930e-02, -1.1119e-01,  5.4001e-01,  5.4001e-01],\n",
      "          [ 3.8354e-01, -2.9439e-02, -2.4222e-01, -2.4222e-01],\n",
      "          [ 3.8354e-01, -2.9439e-02, -2.4222e-01, -2.4222e-01]],\n",
      "\n",
      "         [[-7.2627e-02, -3.0445e-02,  9.5082e-02,  9.5082e-02],\n",
      "          [-1.7368e-01, -9.1671e-02,  1.3335e+00,  1.3335e+00],\n",
      "          [ 9.1235e-03,  2.5892e-02, -1.3058e+00, -1.3058e+00],\n",
      "          [ 9.1235e-03,  2.5892e-02, -1.3058e+00, -1.3058e+00]]],\n",
      "\n",
      "\n",
      "        [[[ 1.4386e+01,  1.3014e+01,  1.3249e+01,  1.3249e+01],\n",
      "          [ 1.1186e+01,  1.0002e+01,  1.0653e+01,  1.0653e+01],\n",
      "          [ 1.7438e+01,  1.5901e+01,  1.5688e+01,  1.5688e+01],\n",
      "          [ 1.7438e+01,  1.5901e+01,  1.5688e+01,  1.5688e+01]],\n",
      "\n",
      "         [[ 4.0679e+00,  4.0324e+00,  1.0198e+00,  1.0198e+00],\n",
      "          [ 1.9649e+00,  1.9693e+00,  2.5631e-01,  2.5631e-01],\n",
      "          [ 6.1527e+00,  6.0919e+00,  1.6220e+00,  1.6220e+00],\n",
      "          [ 6.1527e+00,  6.0919e+00,  1.6220e+00,  1.6220e+00]]],\n",
      "\n",
      "\n",
      "        [[[ 1.4386e+01,  1.3014e+01,  1.3249e+01,  1.3249e+01],\n",
      "          [ 1.1186e+01,  1.0002e+01,  1.0653e+01,  1.0653e+01],\n",
      "          [ 1.7438e+01,  1.5901e+01,  1.5688e+01,  1.5688e+01],\n",
      "          [ 1.7438e+01,  1.5901e+01,  1.5688e+01,  1.5688e+01]],\n",
      "\n",
      "         [[ 4.0679e+00,  4.0324e+00,  1.0198e+00,  1.0198e+00],\n",
      "          [ 1.9649e+00,  1.9693e+00,  2.5631e-01,  2.5631e-01],\n",
      "          [ 6.1527e+00,  6.0919e+00,  1.6220e+00,  1.6220e+00],\n",
      "          [ 6.1527e+00,  6.0919e+00,  1.6220e+00,  1.6220e+00]]]])\n",
      "Decoder Self Attention = \n",
      "tensor([[-1.9041,  0.8017, -1.2000, -0.1027],\n",
      "        [ 0.2367,  0.5136,  0.7557,  0.0283],\n",
      "        [-6.2053,  1.3664, -4.6778, -0.7001],\n",
      "        [-6.2053,  1.3664, -4.6778, -0.7001],\n",
      "        [-1.7965,  0.8983, -0.6742, -0.4995],\n",
      "        [ 0.3320,  0.6057,  1.2172, -0.3231],\n",
      "        [-6.0853,  1.4632, -4.4972, -0.8291],\n",
      "        [-6.0853,  1.4632, -4.4972, -0.8291],\n",
      "        [-2.0167,  0.7045, -1.5389,  0.1503],\n",
      "        [ 0.1299,  0.4146,  0.2093,  0.4418],\n",
      "        [-6.3117,  1.2832, -4.7376, -0.6604],\n",
      "        [-6.3117,  1.2832, -4.7376, -0.6604],\n",
      "        [-2.0167,  0.7045, -1.5389,  0.1503],\n",
      "        [ 0.1299,  0.4146,  0.2093,  0.4418],\n",
      "        [-6.3117,  1.2832, -4.7376, -0.6604],\n",
      "        [-6.3117,  1.2832, -4.7376, -0.6604]])\n",
      "\n",
      "Decoder_0 norm1(x + sa(x))\n",
      "tensor([[[-1.5830,  0.8435, -0.1348,  0.8743],\n",
      "         [ 0.7494,  1.1113, -0.4446, -1.4161],\n",
      "         [-1.6079,  0.5910, -0.0152,  1.0321],\n",
      "         [-1.6079,  0.5910, -0.0152,  1.0321]],\n",
      "\n",
      "        [[-1.6881,  0.4854,  0.2920,  0.9107],\n",
      "         [-1.0089,  0.0246,  1.6095, -0.6252],\n",
      "         [-1.6320,  0.5056,  0.0908,  1.0357],\n",
      "         [-1.6320,  0.5056,  0.0908,  1.0357]],\n",
      "\n",
      "        [[-0.7740,  1.6410, -0.8411, -0.0259],\n",
      "         [ 0.5773,  1.3469, -0.9374, -0.9868],\n",
      "         [-1.4932,  1.0438, -0.3137,  0.7631],\n",
      "         [-1.4932,  1.0438, -0.3137,  0.7631]],\n",
      "\n",
      "        [[-0.7740,  1.6410, -0.8411, -0.0259],\n",
      "         [ 0.5773,  1.3469, -0.9374, -0.9868],\n",
      "         [-1.4932,  1.0438, -0.3137,  0.7631],\n",
      "         [-1.4932,  1.0438, -0.3137,  0.7631]]], grad_fn=<AddBackward0>)\n",
      "\n",
      "Q_dec_0 = \n",
      "tensor([[[-1.5499, -0.1699],\n",
      "         [-1.3632, -0.3399],\n",
      "         [-1.1641,  0.3842],\n",
      "         [-1.1641,  0.3842]],\n",
      "\n",
      "        [[-1.1429, -0.1188],\n",
      "         [-0.9174,  0.1513],\n",
      "         [-1.1776, -0.5957],\n",
      "         [-1.1776, -0.5957]],\n",
      "\n",
      "        [[ 0.6232,  0.6521],\n",
      "         [ 0.3821, -0.3124],\n",
      "         [ 0.0683,  0.6909],\n",
      "         [ 0.0683,  0.6909]],\n",
      "\n",
      "        [[ 0.0169, -0.3579],\n",
      "         [ 0.2599,  0.9101],\n",
      "         [-0.3859, -0.6488],\n",
      "         [-0.3859, -0.6488]],\n",
      "\n",
      "        [[-1.5266, -0.2734],\n",
      "         [-1.4775, -0.3137],\n",
      "         [-1.5703, -0.0657],\n",
      "         [-1.5703, -0.0657]],\n",
      "\n",
      "        [[-1.0515, -0.0299],\n",
      "         [-0.9949,  0.0368],\n",
      "         [-1.2190, -0.2373],\n",
      "         [-1.2190, -0.2373]],\n",
      "\n",
      "        [[-1.5266, -0.2734],\n",
      "         [-1.4775, -0.3137],\n",
      "         [-1.5703, -0.0657],\n",
      "         [-1.5703, -0.0657]],\n",
      "\n",
      "        [[-1.0515, -0.0299],\n",
      "         [-0.9949,  0.0368],\n",
      "         [-1.2190, -0.2373],\n",
      "         [-1.2190, -0.2373]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "K_dec_0 = \n",
      "tensor([[[ 1.1857, -0.1569],\n",
      "         [ 0.9291, -0.1052],\n",
      "         [-0.9526,  0.8701],\n",
      "         [ 0.1841,  0.4583]],\n",
      "\n",
      "        [[-1.2063, -1.6157],\n",
      "         [-1.1115, -0.8156],\n",
      "         [-0.7294,  0.4585],\n",
      "         [-1.2785, -0.8344]],\n",
      "\n",
      "        [[ 1.1432, -0.1255],\n",
      "         [ 1.0645, -0.1449],\n",
      "         [-0.7640,  0.8184],\n",
      "         [ 0.7077,  0.1616]],\n",
      "\n",
      "        [[-1.2047, -1.6500],\n",
      "         [-1.1690, -1.0901],\n",
      "         [-0.8414,  0.1610],\n",
      "         [-1.2630, -1.4005]],\n",
      "\n",
      "        [[ 1.1186, -0.1094],\n",
      "         [ 1.1217, -0.1586],\n",
      "         [-0.4547,  0.7157],\n",
      "         [ 0.8171,  0.0876]],\n",
      "\n",
      "        [[-1.2010, -1.6612],\n",
      "         [-1.1933, -1.2344],\n",
      "         [-1.0012, -0.2426],\n",
      "         [-1.2299, -1.5140]],\n",
      "\n",
      "        [[ 1.1024, -0.0986],\n",
      "         [ 1.1397, -0.1609],\n",
      "         [-0.2437,  0.6332],\n",
      "         [ 0.8544,  0.0614]],\n",
      "\n",
      "        [[-1.2008, -1.6617],\n",
      "         [-1.2028, -1.2934],\n",
      "         [-1.0839, -0.4956],\n",
      "         [-1.2167, -1.5506]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "V_dec_0 = \n",
      "tensor([[[-0.8539,  0.4412],\n",
      "         [-0.5160,  0.2915],\n",
      "         [ 1.1423, -1.3873],\n",
      "         [ 0.1516, -0.5990]],\n",
      "\n",
      "        [[ 1.5958,  0.0235],\n",
      "         [ 1.1008, -0.1689],\n",
      "         [ 0.0668,  0.0747],\n",
      "         [ 1.1377,  0.0801]],\n",
      "\n",
      "        [[-0.8267,  0.3937],\n",
      "         [-0.6615,  0.3793],\n",
      "         [ 0.9688, -1.2757],\n",
      "         [-0.3979, -0.0817]],\n",
      "\n",
      "        [[ 1.6096,  0.0514],\n",
      "         [ 1.2878, -0.1228],\n",
      "         [ 0.2995,  0.1035],\n",
      "         [ 1.4724,  0.1058]],\n",
      "\n",
      "        [[-0.8102,  0.3686],\n",
      "         [-0.7287,  0.4139],\n",
      "         [ 0.6925, -1.0715],\n",
      "         [-0.5228,  0.0437]],\n",
      "\n",
      "        [[ 1.6115,  0.0647],\n",
      "         [ 1.3819, -0.0929],\n",
      "         [ 0.6238,  0.1228],\n",
      "         [ 1.5239,  0.1132]],\n",
      "\n",
      "        [[-0.7971,  0.3513],\n",
      "         [-0.7519,  0.4225],\n",
      "         [ 0.4989, -0.9155],\n",
      "         [-0.5656,  0.0877]],\n",
      "\n",
      "        [[ 1.6103,  0.0706],\n",
      "         [ 1.4197, -0.0787],\n",
      "         [ 0.8192,  0.1310],\n",
      "         [ 1.5394,  0.1152]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "torch.Size([4, 2, 4, 2]) torch.Size([4, 2, 4, 2]) torch.Size([4, 2, 2, 4])\n",
      "Attention weights = \n",
      "torch.Size([4, 2, 4, 4]) tensor([[[[-1.2807, -1.0056,  0.9394, -0.2568],\n",
      "          [-1.1053, -0.8703,  0.7091, -0.2876],\n",
      "          [-1.0187, -0.7934,  1.0205, -0.0270],\n",
      "          [-1.0187, -0.7934,  1.0205, -0.0270]],\n",
      "\n",
      "         [[ 1.1106,  0.9669,  0.5510,  1.1034],\n",
      "          [ 0.6096,  0.6338,  0.5223,  0.7401],\n",
      "          [ 1.6850,  1.2691,  0.4142,  1.4160],\n",
      "          [ 1.6850,  1.2691,  0.4142,  1.4160]]],\n",
      "\n",
      "\n",
      "        [[[ 0.4459,  0.4023,  0.0407,  0.3864],\n",
      "          [ 0.3366,  0.3196, -0.3872,  0.1555],\n",
      "          [-0.0061, -0.0194,  0.3630,  0.1131],\n",
      "          [-0.0061, -0.0194,  0.3630,  0.1131]],\n",
      "\n",
      "         [[ 0.4032,  0.2619, -0.0508,  0.3394],\n",
      "          [-1.2832, -0.9163, -0.0510, -1.1334],\n",
      "          [ 1.0857,  0.8191,  0.1557,  0.9871],\n",
      "          [ 1.0857,  0.8191,  0.1557,  0.9871]]],\n",
      "\n",
      "\n",
      "        [[[-1.1864, -1.1802,  0.3525, -0.8989],\n",
      "          [-1.1444, -1.1367,  0.3163, -0.8731],\n",
      "          [-1.2370, -1.2382,  0.4716, -0.9113],\n",
      "          [-1.2370, -1.2382,  0.4716, -0.9113]],\n",
      "\n",
      "         [[ 0.9281,  0.9134,  0.7496,  0.9465],\n",
      "          [ 0.8017,  0.8074,  0.6981,  0.8259],\n",
      "          [ 1.3140,  1.2357,  0.9037,  1.3142],\n",
      "          [ 1.3140,  1.2357,  0.9037,  1.3142]]],\n",
      "\n",
      "\n",
      "        [[[-1.1709, -1.1992,  0.1407, -0.9341],\n",
      "          [-1.1298, -1.1550,  0.1142, -0.9062],\n",
      "          [-1.2195, -1.2581,  0.2412, -0.9515],\n",
      "          [-1.2195, -1.2581,  0.2412, -0.9515]],\n",
      "\n",
      "         [[ 0.9280,  0.9217,  0.8164,  0.9375],\n",
      "          [ 0.8016,  0.8126,  0.7497,  0.8156],\n",
      "          [ 1.3139,  1.2538,  1.0174,  1.3090],\n",
      "          [ 1.3139,  1.2538,  1.0174,  1.3090]]]], grad_fn=<AddBackward0>)\n",
      "Cross attention in decoder_0\n",
      "tensor([[ 6.5748e-01, -9.5167e-01,  1.0835e+00, -1.8772e-04],\n",
      "        [-3.3335e-01, -4.7886e-02,  1.2493e+00,  3.1897e-02],\n",
      "        [ 1.2327e-01, -5.1876e-01,  1.3142e+00,  4.9786e-02],\n",
      "        [-5.6529e-02, -3.6330e-01,  1.3618e+00,  5.7952e-02],\n",
      "        [ 5.4818e-01, -8.4943e-01,  1.0002e+00,  2.4731e-03],\n",
      "        [-4.0554e-01,  2.4197e-02,  8.8224e-01,  5.0043e-02],\n",
      "        [ 9.8768e-02, -4.9476e-01,  1.3035e+00,  5.0351e-02],\n",
      "        [-7.5708e-02, -3.4405e-01,  1.3547e+00,  5.8435e-02],\n",
      "        [ 6.0845e-01, -9.0786e-01,  1.1870e+00, -2.1456e-03],\n",
      "        [-1.1545e-01, -2.5640e-01,  1.3201e+00,  3.0669e-02],\n",
      "        [ 1.7531e-01, -5.6976e-01,  1.3453e+00,  4.8354e-02],\n",
      "        [-1.4579e-02, -4.0542e-01,  1.3825e+00,  5.6663e-02],\n",
      "        [ 6.0845e-01, -9.0786e-01,  1.1870e+00, -2.1456e-03],\n",
      "        [-1.1545e-01, -2.5640e-01,  1.3201e+00,  3.0669e-02],\n",
      "        [ 1.7531e-01, -5.6976e-01,  1.3453e+00,  4.8354e-02],\n",
      "        [-1.4579e-02, -4.0542e-01,  1.3825e+00,  5.6663e-02]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "\n",
      "norm2(x' + mha(x', mem)), \n",
      " where x' = Decoder_curr_layer norm1(x + sa(x))\n",
      "tensor([[[-0.2858, -0.2163, -1.1173,  1.6195],\n",
      "         [ 1.1334,  0.7102, -0.4160, -1.4276],\n",
      "         [-1.0883, -0.1067, -0.4247,  1.6198],\n",
      "         [-1.2969,  0.1097, -0.3012,  1.4883]],\n",
      "\n",
      "        [[-0.8590, -0.8159,  0.0649,  1.6100],\n",
      "         [-0.7303, -0.1379,  1.6732, -0.8050],\n",
      "         [-1.1742, -0.2405, -0.1765,  1.5912],\n",
      "         [-1.3590, -0.0270, -0.0789,  1.4649]],\n",
      "\n",
      "        [[ 0.9055,  0.9384, -1.4694, -0.3745],\n",
      "         [ 1.2056,  0.7663, -0.8987, -1.0732],\n",
      "         [-0.7594,  0.8426, -1.2056,  1.1224],\n",
      "         [-1.0253,  0.9915, -0.9743,  1.0081]],\n",
      "\n",
      "        [[ 0.9055,  0.9384, -1.4694, -0.3745],\n",
      "         [ 1.2056,  0.7663, -0.8987, -1.0732],\n",
      "         [-0.7594,  0.8426, -1.2056,  1.1224],\n",
      "         [-1.0253,  0.9915, -0.9743,  1.0081]]], grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "\n",
      "norm3(x'' + ff(x'')) \n",
      " where, x'' = Decoder_curr_layer norm2(x' + mha(x'))\n",
      "tensor([[[-0.1817, -0.1557, -1.2251,  1.5626],\n",
      "         [ 1.2128,  0.7285, -0.7322, -1.2092],\n",
      "         [-0.9571, -0.0470, -0.6332,  1.6373],\n",
      "         [-1.1657,  0.1545, -0.5206,  1.5317]],\n",
      "\n",
      "        [[-0.7679, -0.7204, -0.2002,  1.6885],\n",
      "         [-0.8601, -0.1252,  1.6672, -0.6819],\n",
      "         [-1.0553, -0.1689, -0.4147,  1.6388],\n",
      "         [-1.2418,  0.0306, -0.3227,  1.5339]],\n",
      "\n",
      "        [[ 0.8855,  0.8504, -1.5701, -0.1658],\n",
      "         [ 1.2219,  0.7352, -1.1387, -0.8183],\n",
      "         [-0.6286,  0.7902, -1.2989,  1.1372],\n",
      "         [-0.8875,  0.9400, -1.1050,  1.0525]],\n",
      "\n",
      "        [[ 0.8855,  0.8504, -1.5701, -0.1658],\n",
      "         [ 1.2219,  0.7352, -1.1387, -0.8183],\n",
      "         [-0.6286,  0.7902, -1.2989,  1.1372],\n",
      "         [-0.8875,  0.9400, -1.1050,  1.0525]]], grad_fn=<AddBackward0>)\n",
      "\n",
      "torch.Size([4, 4, 4]) torch.Size([4, 4, 4])\n",
      "DEC SA\n",
      "torch.Size([1, 4, 4, 4]) torch.Size([1, 4, 4, 4]) torch.Size([1, 4, 4, 4])\n",
      "4 8 2\n",
      "Q_dec_1 = \n",
      "tensor([[[-0.5553,  0.9256],\n",
      "         [-0.8857,  0.6694],\n",
      "         [ 0.4914,  0.4020],\n",
      "         [ 0.4914,  0.4020]],\n",
      "\n",
      "        [[-0.4678,  0.1593],\n",
      "         [-0.4403, -0.4192],\n",
      "         [-0.0490,  0.8781],\n",
      "         [-0.0490,  0.8781]],\n",
      "\n",
      "        [[ 0.8956, -0.2561],\n",
      "         [-0.1102, -0.6798],\n",
      "         [ 0.7828,  0.0043],\n",
      "         [ 0.7828,  0.0043]],\n",
      "\n",
      "        [[ 0.2852,  0.8043],\n",
      "         [ 0.2352, -0.7378],\n",
      "         [ 0.1628,  0.8975],\n",
      "         [ 0.1628,  0.8975]],\n",
      "\n",
      "        [[-0.8247,  0.8472],\n",
      "         [-0.8786,  0.7813],\n",
      "         [-0.4256,  0.9047],\n",
      "         [-0.4256,  0.9047]],\n",
      "\n",
      "        [[-0.4941, -0.2622],\n",
      "         [-0.4805, -0.3749],\n",
      "         [-0.4314,  0.1904],\n",
      "         [-0.4314,  0.1904]],\n",
      "\n",
      "        [[-0.8313,  0.8084],\n",
      "         [-0.8783,  0.7469],\n",
      "         [-0.4641,  0.8492],\n",
      "         [-0.4641,  0.8492]],\n",
      "\n",
      "        [[-0.4804, -0.3295],\n",
      "         [-0.4670, -0.4292],\n",
      "         [-0.4182,  0.0740],\n",
      "         [-0.4182,  0.0740]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "K_dec_1 = \n",
      "tensor([[[ 8.5510e-01, -1.5126e+00],\n",
      "         [ 3.7763e-01, -1.7030e+00],\n",
      "         [ 7.3695e-01,  2.6388e-01],\n",
      "         [ 7.3695e-01,  2.6388e-01]],\n",
      "\n",
      "        [[ 6.3604e-02, -1.1235e+00],\n",
      "         [ 1.4750e-01, -9.9425e-01],\n",
      "         [-1.2730e-01, -2.1296e-01],\n",
      "         [-1.2730e-01, -2.1296e-01]],\n",
      "\n",
      "        [[ 3.0015e-01,  1.2458e+00],\n",
      "         [-1.1657e+00,  6.3636e-01],\n",
      "         [ 5.6512e-01,  8.7307e-01],\n",
      "         [ 5.6512e-01,  8.7307e-01]],\n",
      "\n",
      "        [[-3.6812e-01,  6.0396e-01],\n",
      "         [ 3.8751e-01,  6.0623e-01],\n",
      "         [-3.6256e-01,  3.0439e-01],\n",
      "         [-3.6256e-01,  3.0439e-01]],\n",
      "\n",
      "        [[ 2.8952e-01, -1.5703e+00],\n",
      "         [ 1.9271e-01, -1.5869e+00],\n",
      "         [ 3.5208e-01, -9.8840e-01],\n",
      "         [ 3.5208e-01, -9.8840e-01]],\n",
      "\n",
      "        [[ 5.1513e-01, -1.1767e+00],\n",
      "         [ 5.1787e-01, -1.1308e+00],\n",
      "         [ 6.6803e-01, -1.1019e+00],\n",
      "         [ 6.6803e-01, -1.1019e+00]],\n",
      "\n",
      "        [[ 8.0359e-02, -1.4443e+00],\n",
      "         [ 1.5757e-03, -1.4617e+00],\n",
      "         [ 1.0794e-01, -8.8874e-01],\n",
      "         [ 1.0794e-01, -8.8874e-01]],\n",
      "\n",
      "        [[ 7.0303e-01, -1.1547e+00],\n",
      "         [ 6.9338e-01, -1.1098e+00],\n",
      "         [ 8.6206e-01, -1.0753e+00],\n",
      "         [ 8.6206e-01, -1.0753e+00]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "V_dec_1 = \n",
      "tensor([[[-0.0102, -1.1538],\n",
      "         [-1.0505, -1.5623],\n",
      "         [ 1.5739,  0.6002],\n",
      "         [ 1.5739,  0.6002]],\n",
      "\n",
      "        [[ 0.0644, -0.7489],\n",
      "         [-0.3176, -0.9077],\n",
      "         [ 0.5796,  0.2278],\n",
      "         [ 0.5796,  0.2278]],\n",
      "\n",
      "        [[ 1.5632,  1.4298],\n",
      "         [-1.0448,  0.0767],\n",
      "         [ 1.6533,  1.1704],\n",
      "         [ 1.6533,  1.1704]],\n",
      "\n",
      "        [[ 0.6304,  0.6306],\n",
      "         [-0.5875,  0.3881],\n",
      "         [ 0.6827,  0.4505],\n",
      "         [ 0.6827,  0.4505]],\n",
      "\n",
      "        [[-0.6144, -1.5222],\n",
      "         [-0.8175, -1.5809],\n",
      "         [ 0.3664, -0.9361],\n",
      "         [ 0.3664, -0.9361]],\n",
      "\n",
      "        [[-0.3598, -0.6490],\n",
      "         [-0.4290, -0.6751],\n",
      "         [-0.1406, -0.1984],\n",
      "         [-0.1406, -0.1984]],\n",
      "\n",
      "        [[-0.6421, -1.5142],\n",
      "         [-0.8267, -1.5642],\n",
      "         [ 0.2481, -0.9704],\n",
      "         [ 0.2481, -0.9704]],\n",
      "\n",
      "        [[-0.4702, -0.5102],\n",
      "         [-0.5269, -0.5400],\n",
      "         [-0.2830, -0.0788],\n",
      "         [-0.2830, -0.0788]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "torch.Size([4, 2, 4, 2]) torch.Size([4, 2, 4, 2]) torch.Size([4, 2, 2, 4])\n",
      "Attention weights = \n",
      "torch.Size([4, 2, 4, 4]) tensor([[[[-1.3257, -1.2628, -0.1167, -0.1167],\n",
      "          [-1.2516, -1.0426, -0.3366, -0.3366],\n",
      "          [-0.1328, -0.3528,  0.3311,  0.3311],\n",
      "          [-0.1328, -0.3528,  0.3311,  0.3311]],\n",
      "\n",
      "         [[-0.1476, -0.1608,  0.0181,  0.0181],\n",
      "          [ 0.3133,  0.2488,  0.1028,  0.1028],\n",
      "          [-0.6998, -0.6225, -0.1278, -0.1278],\n",
      "          [-0.6998, -0.6225, -0.1278, -0.1278]]],\n",
      "\n",
      "\n",
      "        [[[-0.0355, -0.8534,  0.1998,  0.1998],\n",
      "          [-0.6222, -0.2151, -0.4637, -0.4637],\n",
      "          [ 0.1700, -0.6433,  0.3155,  0.3155],\n",
      "          [ 0.1700, -0.6433,  0.3155,  0.3155]],\n",
      "\n",
      "         [[ 0.2692,  0.4229,  0.1000,  0.1000],\n",
      "          [-0.3763, -0.2518, -0.2191, -0.2191],\n",
      "          [ 0.3409,  0.4293,  0.1515,  0.1515],\n",
      "          [ 0.3409,  0.4293,  0.1515,  0.1515]]],\n",
      "\n",
      "\n",
      "        [[[-1.1095, -1.0630, -0.7974, -0.7974],\n",
      "          [-1.0474, -0.9964, -0.7648, -0.7648],\n",
      "          [-1.0916, -1.0731, -0.7382, -0.7382],\n",
      "          [-1.0916, -1.0731, -0.7382, -0.7382]],\n",
      "\n",
      "         [[ 0.0381,  0.0287, -0.0291, -0.0291],\n",
      "          [ 0.1369,  0.1238,  0.0651,  0.0651],\n",
      "          [-0.3155, -0.3102, -0.3521, -0.3521],\n",
      "          [-0.3155, -0.3102, -0.3521, -0.3521]]],\n",
      "\n",
      "\n",
      "        [[[-0.8729, -0.8365, -0.5715, -0.5715],\n",
      "          [-0.8127, -0.7730, -0.5364, -0.5364],\n",
      "          [-0.8936, -0.8782, -0.5691, -0.5691],\n",
      "          [-0.8936, -0.8782, -0.5691, -0.5691]],\n",
      "\n",
      "         [[ 0.0302,  0.0230, -0.0423, -0.0423],\n",
      "          [ 0.1183,  0.1078,  0.0417,  0.0417],\n",
      "          [-0.2683, -0.2631, -0.3112, -0.3112],\n",
      "          [-0.2683, -0.2631, -0.3112, -0.3112]]]], grad_fn=<AddBackward0>)\n",
      "Decoder Self Attention = \n",
      "tensor([[ 1.0743,  0.1374,  0.2574, -0.2546],\n",
      "        [ 1.3308,  1.1142,  0.2849,  0.4783],\n",
      "        [-0.0982, -1.2000, -0.2714, -0.4374],\n",
      "        [-0.1748, -1.2150, -0.3944, -0.3096],\n",
      "        [ 0.9070, -0.0114,  0.1983, -0.3459],\n",
      "        [ 0.7981,  0.8848,  0.3494,  0.4750],\n",
      "        [-0.1068, -1.2048, -0.2715, -0.4377],\n",
      "        [-0.1813, -1.2188, -0.3946, -0.3099],\n",
      "        [ 0.8333, -0.1001,  0.3156, -0.1640],\n",
      "        [ 1.3108,  1.1103,  0.2984,  0.4804],\n",
      "        [-0.0830, -1.1915, -0.2700, -0.4348],\n",
      "        [-0.1663, -1.2102, -0.3933, -0.3070],\n",
      "        [ 0.8333, -0.1001,  0.3156, -0.1640],\n",
      "        [ 1.3108,  1.1103,  0.2984,  0.4804],\n",
      "        [-0.0830, -1.1915, -0.2700, -0.4348],\n",
      "        [-0.1663, -1.2102, -0.3933, -0.3070]], grad_fn=<ViewBackward0>)\n",
      "\n",
      "Decoder_1 norm1(x + sa(x))\n",
      "tensor([[[ 0.2642, -0.9450, -0.8439,  1.5248],\n",
      "         [ 1.6026, -1.0072, -0.6439,  0.0485],\n",
      "         [-0.6811,  1.3005, -1.2188,  0.5994],\n",
      "         [-1.0119,  1.4347, -0.8550,  0.4322]],\n",
      "\n",
      "        [[-0.3101, -1.3360,  0.2051,  1.4411],\n",
      "         [-0.9190, -0.8938,  1.5036,  0.3092],\n",
      "         [-1.0202,  1.2617, -0.9387,  0.6972],\n",
      "         [-1.2244,  1.3759, -0.6216,  0.4701]],\n",
      "\n",
      "        [[ 1.4226,  0.3131, -1.2989, -0.4369],\n",
      "         [ 1.3227, -0.8057, -1.1152,  0.5982],\n",
      "         [-0.0241,  1.5082, -1.3003, -0.1838],\n",
      "         [-0.3856,  1.6300, -1.0806, -0.1637]],\n",
      "\n",
      "        [[ 1.4226,  0.3131, -1.2989, -0.4369],\n",
      "         [ 1.3227, -0.8057, -1.1152,  0.5982],\n",
      "         [-0.0241,  1.5082, -1.3003, -0.1838],\n",
      "         [-0.3856,  1.6300, -1.0806, -0.1637]]], grad_fn=<AddBackward0>)\n",
      "\n",
      "Q_dec_1 = \n",
      "tensor([[[-0.4659, -0.3508],\n",
      "         [-0.0533,  0.0514],\n",
      "         [-0.6170, -0.0386],\n",
      "         [-0.6170, -0.0386]],\n",
      "\n",
      "        [[ 0.9487,  0.2591],\n",
      "         [ 1.2135,  0.1254],\n",
      "         [-0.7923, -0.1110],\n",
      "         [-0.7923, -0.1110]],\n",
      "\n",
      "        [[-0.5579,  0.6332],\n",
      "         [ 0.5612,  0.5420],\n",
      "         [-0.6761,  0.1131],\n",
      "         [-0.6761,  0.1131]],\n",
      "\n",
      "        [[-0.2305, -0.3497],\n",
      "         [ 0.7237, -0.1340],\n",
      "         [ 0.0833, -0.0746],\n",
      "         [ 0.0833, -0.0746]],\n",
      "\n",
      "        [[-0.2133, -1.3781],\n",
      "         [-0.0728, -1.3923],\n",
      "         [-0.2960, -1.0389],\n",
      "         [-0.2960, -1.0389]],\n",
      "\n",
      "        [[ 0.1598,  0.6494],\n",
      "         [ 0.3352,  0.6847],\n",
      "         [-0.5168,  0.4033],\n",
      "         [-0.5168,  0.4033]],\n",
      "\n",
      "        [[-0.0208, -1.3419],\n",
      "         [ 0.0846, -1.3078],\n",
      "         [-0.1554, -1.1217],\n",
      "         [-0.1554, -1.1217]],\n",
      "\n",
      "        [[ 0.1494,  0.6406],\n",
      "         [ 0.2574,  0.6434],\n",
      "         [-0.4190,  0.4607],\n",
      "         [-0.4190,  0.4607]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "K_dec_1 = \n",
      "tensor([[[ 1.0633e-01, -4.0705e-01],\n",
      "         [-1.5384e-01, -1.1188e+00],\n",
      "         [-5.8209e-01,  4.7425e-01],\n",
      "         [-2.8693e-01,  9.9020e-02]],\n",
      "\n",
      "        [[-1.4893e+00,  9.0800e-02],\n",
      "         [-9.6946e-01,  5.9878e-01],\n",
      "         [-2.2089e-01, -4.8540e-01],\n",
      "         [-1.1698e+00, -2.9807e-01]],\n",
      "\n",
      "        [[ 1.1569e-01, -2.7922e-01],\n",
      "         [-7.2311e-02, -9.7400e-01],\n",
      "         [-5.1352e-01,  5.2494e-01],\n",
      "         [-3.5954e-02,  6.0602e-02]],\n",
      "\n",
      "        [[-1.5159e+00,  9.4658e-04],\n",
      "         [-1.1544e+00,  4.9039e-01],\n",
      "         [-4.4163e-01, -5.3692e-01],\n",
      "         [-1.4480e+00, -2.5660e-01]],\n",
      "\n",
      "        [[ 1.1936e-01, -2.1674e-01],\n",
      "         [-2.7857e-02, -8.6947e-01],\n",
      "         [-4.1970e-01,  4.9509e-01],\n",
      "         [ 2.8755e-02,  6.5271e-02]],\n",
      "\n",
      "        [[-1.5244e+00, -4.2518e-02],\n",
      "         [-1.2505e+00,  4.1432e-01],\n",
      "         [-7.3799e-01, -5.3825e-01],\n",
      "         [-1.4875e+00, -2.5135e-01]],\n",
      "\n",
      "        [[ 1.1824e-01, -1.8760e-01],\n",
      "         [-9.9592e-03, -8.1717e-01],\n",
      "         [-3.5135e-01,  4.5811e-01],\n",
      "         [ 5.0949e-02,  6.4899e-02]],\n",
      "\n",
      "        [[-1.5268e+00, -6.3184e-02],\n",
      "         [-1.2902e+00,  3.7650e-01],\n",
      "         [-9.1222e-01, -5.2276e-01],\n",
      "         [-1.4987e+00, -2.4784e-01]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "V_dec_1 = \n",
      "tensor([[[-0.8077,  0.8756],\n",
      "         [-0.4783,  1.0478],\n",
      "         [ 0.1404, -0.4518],\n",
      "         [-0.4691,  0.2696]],\n",
      "\n",
      "        [[ 0.7175,  0.6995],\n",
      "         [ 0.0053,  0.4314],\n",
      "         [ 0.4964,  1.1367],\n",
      "         [ 0.8557,  1.2235]],\n",
      "\n",
      "        [[-0.8188,  0.8050],\n",
      "         [-0.5962,  1.0538],\n",
      "         [ 0.0051, -0.3751],\n",
      "         [-0.7091,  0.4969]],\n",
      "\n",
      "        [[ 0.8073,  0.7543],\n",
      "         [ 0.1976,  0.4999],\n",
      "         [ 0.6596,  1.2106],\n",
      "         [ 0.9798,  1.0498]],\n",
      "\n",
      "        [[-0.8215,  0.7684],\n",
      "         [-0.6570,  1.0384],\n",
      "         [-0.1818, -0.2067],\n",
      "         [-0.7541,  0.5336]],\n",
      "\n",
      "        [[ 0.8483,  0.7792],\n",
      "         [ 0.3142,  0.5439],\n",
      "         [ 0.8219,  1.2694],\n",
      "         [ 0.9993,  0.9890]],\n",
      "\n",
      "        [[-0.8209,  0.7497],\n",
      "         [-0.6815,  1.0259],\n",
      "         [-0.2978, -0.0899],\n",
      "         [-0.7685,  0.5466]],\n",
      "\n",
      "        [[ 0.8668,  0.7937],\n",
      "         [ 0.3677,  0.5664],\n",
      "         [ 0.9044,  1.2782],\n",
      "         [ 1.0033,  0.9657]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "torch.Size([4, 2, 4, 2]) torch.Size([4, 2, 4, 2]) torch.Size([4, 2, 2, 4])\n",
      "Attention weights = \n",
      "torch.Size([4, 2, 4, 4]) tensor([[[[ 0.0659,  0.3282,  0.0741,  0.0700],\n",
      "          [-0.0188, -0.0349,  0.0392,  0.0144],\n",
      "          [-0.0353,  0.0976,  0.2410,  0.1225],\n",
      "          [-0.0353,  0.0976,  0.2410,  0.1225]],\n",
      "\n",
      "         [[-0.9824, -0.5406, -0.2371, -0.8393],\n",
      "          [-1.2699, -0.7788, -0.2326, -1.0302],\n",
      "          [ 0.8272,  0.4961,  0.1618,  0.6787],\n",
      "          [ 0.8272,  0.4961,  0.1618,  0.6787]]],\n",
      "\n",
      "\n",
      "        [[[-0.1707, -0.4076,  0.4376,  0.0413],\n",
      "          [-0.0611, -0.4020, -0.0026,  0.0090],\n",
      "          [-0.0776, -0.0433,  0.2874,  0.0220],\n",
      "          [-0.0776, -0.0433,  0.2874,  0.0220]],\n",
      "\n",
      "         [[ 0.2469,  0.0669,  0.2047,  0.2995],\n",
      "          [-0.7758, -0.6372, -0.1751, -0.7167],\n",
      "          [-0.0894, -0.0939,  0.0023, -0.0718],\n",
      "          [-0.0894, -0.0939,  0.0023, -0.0718]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1932,  0.8515, -0.4192, -0.0679],\n",
      "          [ 0.2072,  0.8574, -0.4658, -0.0657],\n",
      "          [ 0.1342,  0.6446, -0.2759, -0.0540],\n",
      "          [ 0.1342,  0.6446, -0.2759, -0.0540]],\n",
      "\n",
      "         [[-0.1918,  0.0490, -0.3306, -0.2835],\n",
      "          [-0.3819, -0.0958, -0.4355, -0.4743],\n",
      "          [ 0.5450,  0.5751,  0.1162,  0.4719],\n",
      "          [ 0.5450,  0.5751,  0.1162,  0.4719]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1763,  0.7756, -0.4295, -0.0623],\n",
      "          [ 0.1806,  0.7551, -0.4447, -0.0570],\n",
      "          [ 0.1358,  0.6492, -0.3247, -0.0571],\n",
      "          [ 0.1358,  0.6492, -0.3247, -0.0571]],\n",
      "\n",
      "         [[-0.1899,  0.0342, -0.3332, -0.2706],\n",
      "          [-0.3067, -0.0636, -0.4039, -0.3856],\n",
      "          [ 0.4317,  0.5049,  0.0999,  0.3632],\n",
      "          [ 0.4317,  0.5049,  0.0999,  0.3632]]]], grad_fn=<AddBackward0>)\n",
      "Cross attention in decoder_1\n",
      "tensor([[-0.4079,  0.4760,  0.4744,  0.8901],\n",
      "        [-0.4435,  0.3212,  0.6854,  0.8937],\n",
      "        [-0.6527,  0.7241,  0.7090,  0.8578],\n",
      "        [-0.6811,  0.7220,  0.7515,  0.8665],\n",
      "        [-0.3960,  0.4185,  0.4779,  0.9168],\n",
      "        [-0.5197,  0.4396,  0.6510,  0.9238],\n",
      "        [-0.6560,  0.7305,  0.7055,  0.8610],\n",
      "        [-0.6821,  0.7207,  0.7508,  0.8689],\n",
      "        [-0.3710,  0.3870,  0.5540,  0.8559],\n",
      "        [-0.4829,  0.4157,  0.6629,  0.8876],\n",
      "        [-0.6401,  0.6742,  0.7300,  0.8545],\n",
      "        [-0.6740,  0.6926,  0.7642,  0.8645],\n",
      "        [-0.3710,  0.3870,  0.5540,  0.8559],\n",
      "        [-0.4829,  0.4157,  0.6629,  0.8876],\n",
      "        [-0.6401,  0.6742,  0.7300,  0.8545],\n",
      "        [-0.6740,  0.6926,  0.7642,  0.8645]], grad_fn=<ViewBackward0>)\n",
      "\n",
      "norm2(x' + mha(x', mem)), \n",
      " where x' = Decoder_curr_layer norm1(x + sa(x))\n",
      "tensor([[[-0.3964, -0.8837, -0.4189,  1.6989],\n",
      "         [ 1.3963, -1.3259, -0.4159,  0.3455],\n",
      "         [-1.1664,  1.3591, -0.7184,  0.5257],\n",
      "         [-1.3833,  1.3673, -0.3213,  0.3373]],\n",
      "\n",
      "        [[-0.8847, -1.0407,  0.5880,  1.3373],\n",
      "         [-1.2077, -0.5555,  1.4640,  0.2992],\n",
      "         [-1.4085,  1.2327, -0.4026,  0.5783],\n",
      "         [-1.5117,  1.2561, -0.1005,  0.3562]],\n",
      "\n",
      "        [[ 1.2055,  0.6580, -1.3663, -0.4971],\n",
      "         [ 0.9711, -0.9784, -1.0210,  1.0283],\n",
      "         [-0.5635,  1.6874, -0.8808, -0.2431],\n",
      "         [-0.8871,  1.6811, -0.5862, -0.2078]],\n",
      "\n",
      "        [[ 1.2055,  0.6580, -1.3663, -0.4971],\n",
      "         [ 0.9711, -0.9784, -1.0210,  1.0283],\n",
      "         [-0.5635,  1.6874, -0.8808, -0.2431],\n",
      "         [-0.8871,  1.6811, -0.5862, -0.2078]]], grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "\n",
      "norm3(x'' + ff(x'')) \n",
      " where, x'' = Decoder_curr_layer norm2(x' + mha(x'))\n",
      "tensor([[[-0.3315, -1.1870, -0.0556,  1.5741],\n",
      "         [ 1.2439, -1.5390,  0.0050,  0.2901],\n",
      "         [-1.3638,  1.2571, -0.4729,  0.5797],\n",
      "         [-1.5434,  1.2195, -0.0357,  0.3596]],\n",
      "\n",
      "        [[-0.7281, -1.2285,  0.8257,  1.1309],\n",
      "         [-1.0083, -0.7562,  1.5360,  0.2285],\n",
      "         [-1.5670,  1.0727, -0.1215,  0.6158],\n",
      "         [-1.6368,  1.0700,  0.1960,  0.3707]],\n",
      "\n",
      "        [[ 1.3602,  0.4868, -1.2451, -0.6019],\n",
      "         [ 0.9558, -1.3281, -0.6055,  0.9777],\n",
      "         [-0.7755,  1.7007, -0.6604, -0.2648],\n",
      "         [-1.0936,  1.6294, -0.3199, -0.2159]],\n",
      "\n",
      "        [[ 1.3602,  0.4868, -1.2451, -0.6019],\n",
      "         [ 0.9558, -1.3281, -0.6055,  0.9777],\n",
      "         [-0.7755,  1.7007, -0.6604, -0.2648],\n",
      "         [-1.0936,  1.6294, -0.3199, -0.2159]]], grad_fn=<AddBackward0>)\n",
      "\n",
      "torch.Size([4, 4, 4]) torch.Size([4, 4, 4])\n",
      "DEC SA\n",
      "torch.Size([1, 4, 4, 4]) torch.Size([1, 4, 4, 4]) torch.Size([1, 4, 4, 4])\n",
      "4 8 2\n",
      "Q_dec_2 = \n",
      "tensor([[[-0.9733, -0.7926],\n",
      "         [-0.3033, -0.6873],\n",
      "         [-0.6829,  0.1347],\n",
      "         [-0.6829,  0.1347]],\n",
      "\n",
      "        [[-0.6464,  1.4853],\n",
      "         [-0.1572,  0.7434],\n",
      "         [-0.1171,  0.1234],\n",
      "         [-0.1171,  0.1234]],\n",
      "\n",
      "        [[-1.3821, -1.0148],\n",
      "         [ 0.6444, -0.2684],\n",
      "         [-1.6379, -0.9677],\n",
      "         [-1.6379, -0.9677]],\n",
      "\n",
      "        [[ 0.1028,  0.8220],\n",
      "         [ 0.4399, -0.3980],\n",
      "         [-0.4659,  1.4896],\n",
      "         [-0.4659,  1.4896]],\n",
      "\n",
      "        [[ 0.8140,  0.7579],\n",
      "         [ 0.9492,  0.6890],\n",
      "         [ 0.9719,  1.0230],\n",
      "         [ 0.9719,  1.0230]],\n",
      "\n",
      "        [[-0.6454,  0.0845],\n",
      "         [-0.5344, -0.0116],\n",
      "         [-0.3284, -0.5098],\n",
      "         [-0.3284, -0.5098]],\n",
      "\n",
      "        [[ 1.1306,  0.7989],\n",
      "         [ 1.1928,  0.7351],\n",
      "         [ 1.2033,  1.0271],\n",
      "         [ 1.2033,  1.0271]],\n",
      "\n",
      "        [[-0.3999, -0.2764],\n",
      "         [-0.3154, -0.3358],\n",
      "         [-0.2458, -0.6398],\n",
      "         [-0.2458, -0.6398]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "K_dec_2 = \n",
      "tensor([[[-1.0034,  1.0744],\n",
      "         [-0.4630,  0.5708],\n",
      "         [-0.2887,  0.3580],\n",
      "         [-0.2887,  0.3580]],\n",
      "\n",
      "        [[-0.7728,  0.8970],\n",
      "         [-0.9104,  1.0742],\n",
      "         [ 1.0026, -0.6382],\n",
      "         [ 1.0026, -0.6382]],\n",
      "\n",
      "        [[-0.9310,  1.4014],\n",
      "         [ 0.3635, -0.2819],\n",
      "         [-1.2724,  1.5704],\n",
      "         [-1.2724,  1.5704]],\n",
      "\n",
      "        [[ 0.3277,  1.0211],\n",
      "         [-0.7771,  0.8411],\n",
      "         [ 0.0584,  0.8151],\n",
      "         [ 0.0584,  0.8151]],\n",
      "\n",
      "        [[ 0.3517, -0.8755],\n",
      "         [ 0.4314, -0.9290],\n",
      "         [ 0.6609, -1.1372],\n",
      "         [ 0.6609, -1.1372]],\n",
      "\n",
      "        [[-0.6422, -0.8615],\n",
      "         [-0.8032, -0.6631],\n",
      "         [-0.0341, -1.2649],\n",
      "         [-0.0341, -1.2649]],\n",
      "\n",
      "        [[ 0.6100, -1.1097],\n",
      "         [ 0.6490, -1.1189],\n",
      "         [ 0.7969, -1.2882],\n",
      "         [ 0.7969, -1.2882]],\n",
      "\n",
      "        [[-0.7042, -0.7626],\n",
      "         [-0.7902, -0.6149],\n",
      "         [-0.2400, -1.1409],\n",
      "         [-0.2400, -1.1409]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "V_dec_2 = \n",
      "tensor([[[ 6.6203e-02, -4.4721e-01],\n",
      "         [ 2.2570e-01, -3.1786e-01],\n",
      "         [-5.4120e-01,  4.1174e-01],\n",
      "         [-5.4120e-01,  4.1174e-01]],\n",
      "\n",
      "        [[-6.7956e-02, -5.9123e-01],\n",
      "         [-2.7693e-01, -2.9299e-01],\n",
      "         [ 2.9330e-01, -7.3099e-02],\n",
      "         [ 2.9330e-01, -7.3099e-02]],\n",
      "\n",
      "        [[-6.3371e-01,  5.4245e-01],\n",
      "         [ 3.6996e-01, -9.9818e-02],\n",
      "         [-4.9070e-01,  1.1884e-01],\n",
      "         [-4.9070e-01,  1.1884e-01]],\n",
      "\n",
      "        [[-1.8535e-01, -3.7719e-01],\n",
      "         [-4.0565e-01,  1.6698e-01],\n",
      "         [ 4.7418e-04, -6.2683e-01],\n",
      "         [ 4.7418e-04, -6.2683e-01]],\n",
      "\n",
      "        [[ 6.7137e-01, -8.7893e-01],\n",
      "         [ 7.4344e-01, -8.8584e-01],\n",
      "         [ 4.4906e-01, -5.0311e-01],\n",
      "         [ 4.4906e-01, -5.0311e-01]],\n",
      "\n",
      "        [[ 2.8667e-01,  2.1910e-02],\n",
      "         [ 1.9165e-01,  6.1190e-02],\n",
      "         [ 3.4345e-01,  2.4679e-01],\n",
      "         [ 3.4345e-01,  2.4679e-01]],\n",
      "\n",
      "        [[ 7.4201e-01, -8.0657e-01],\n",
      "         [ 7.7031e-01, -7.8960e-01],\n",
      "         [ 5.7793e-01, -5.6994e-01],\n",
      "         [ 5.7793e-01, -5.6994e-01]],\n",
      "\n",
      "        [[ 1.8082e-01,  1.6638e-01],\n",
      "         [ 1.1398e-01,  1.8925e-01],\n",
      "         [ 2.6583e-01,  3.0401e-01],\n",
      "         [ 2.6583e-01,  3.0401e-01]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "torch.Size([4, 2, 4, 2]) torch.Size([4, 2, 4, 2]) torch.Size([4, 2, 2, 4])\n",
      "Attention weights = \n",
      "torch.Size([4, 2, 4, 4]) tensor([[[[ 8.8440e-02, -1.2374e-03, -1.9015e-03, -1.9015e-03],\n",
      "          [-3.0695e-01, -1.7810e-01, -1.1205e-01, -1.1205e-01],\n",
      "          [ 5.8678e-01,  2.7792e-01,  1.7351e-01,  1.7351e-01],\n",
      "          [ 5.8678e-01,  2.7792e-01,  1.7351e-01,  1.7351e-01]],\n",
      "\n",
      "         [[ 1.2953e+00,  1.5443e+00, -1.1285e+00, -1.1285e+00],\n",
      "          [ 5.5744e-01,  6.6591e-01, -4.4693e-01, -4.4693e-01],\n",
      "          [ 1.4222e-01,  1.6908e-01, -1.3866e-01, -1.3866e-01],\n",
      "          [ 1.4222e-01,  1.6908e-01, -1.3866e-01, -1.3866e-01]]],\n",
      "\n",
      "\n",
      "        [[[-9.5753e-02, -1.5298e-01,  1.1664e-01,  1.1664e-01],\n",
      "          [-6.9019e-01,  2.1915e-01, -8.7784e-01, -8.7784e-01],\n",
      "          [ 1.1927e-01, -2.2812e-01,  3.9905e-01,  3.9905e-01],\n",
      "          [ 1.1927e-01, -2.2812e-01,  3.9905e-01,  3.9905e-01]],\n",
      "\n",
      "         [[ 6.1730e-01,  4.3243e-01,  4.7801e-01,  4.7801e-01],\n",
      "          [-1.8546e-01, -4.7845e-01, -2.1125e-01, -2.1125e-01],\n",
      "          [ 9.6756e-01,  1.1420e+00,  8.3931e-01,  8.3931e-01],\n",
      "          [ 9.6756e-01,  1.1420e+00,  8.3931e-01,  8.3931e-01]]],\n",
      "\n",
      "\n",
      "        [[[-2.6680e-01, -2.4961e-01, -2.2908e-01, -2.2908e-01],\n",
      "          [-1.9052e-01, -1.6310e-01, -1.1048e-01, -1.1048e-01],\n",
      "          [-3.9163e-01, -3.7557e-01, -3.6843e-01, -3.6843e-01],\n",
      "          [-3.9163e-01, -3.7557e-01, -3.6843e-01, -3.6843e-01]],\n",
      "\n",
      "         [[ 2.4163e-01,  3.2693e-01, -5.9989e-02, -5.9989e-02],\n",
      "          [ 2.4974e-01,  3.0894e-01,  2.3257e-02,  2.3257e-02],\n",
      "          [ 4.5969e-01,  4.2557e-01,  4.6395e-01,  4.6395e-01],\n",
      "          [ 4.5969e-01,  4.2557e-01,  4.6395e-01,  4.6395e-01]]],\n",
      "\n",
      "\n",
      "        [[[-1.3918e-01, -1.1322e-01, -9.0644e-02, -9.0644e-02],\n",
      "          [-6.2286e-02, -3.4192e-02,  2.5227e-03,  2.5227e-03],\n",
      "          [-2.8686e-01, -2.6039e-01, -2.5752e-01, -2.5752e-01],\n",
      "          [-2.8686e-01, -2.6039e-01, -2.5752e-01, -2.5752e-01]],\n",
      "\n",
      "         [[ 3.4817e-01,  3.4365e-01,  2.9086e-01,  2.9086e-01],\n",
      "          [ 3.3810e-01,  3.2224e-01,  3.2439e-01,  3.2439e-01],\n",
      "          [ 4.6739e-01,  4.1556e-01,  5.5782e-01,  5.5782e-01],\n",
      "          [ 4.6739e-01,  4.1556e-01,  5.5782e-01,  5.5782e-01]]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "Decoder Self Attention = \n",
      "tensor([[-0.1915,  0.0039, -0.1509, -0.3984],\n",
      "        [-0.3391,  0.1680, -0.1461, -0.3722],\n",
      "        [ 0.5765, -0.6900,  0.2812,  0.1271],\n",
      "        [ 0.6656, -0.6819,  0.2050,  0.2392],\n",
      "        [-0.2173,  0.0420, -0.0568, -0.3412],\n",
      "        [-0.1029,  0.0956, -0.1318, -0.3990],\n",
      "        [ 0.5742, -0.6865,  0.2839,  0.1314],\n",
      "        [ 0.6649, -0.6811,  0.2066,  0.2407],\n",
      "        [-0.1583, -0.0444,  0.0256, -0.2834],\n",
      "        [-0.3838,  0.1807, -0.1690, -0.3244],\n",
      "        [ 0.5774, -0.6913,  0.2923,  0.1451],\n",
      "        [ 0.6664, -0.6830,  0.2105,  0.2444],\n",
      "        [-0.1583, -0.0444,  0.0256, -0.2834],\n",
      "        [-0.3838,  0.1807, -0.1690, -0.3244],\n",
      "        [ 0.5774, -0.6913,  0.2923,  0.1451],\n",
      "        [ 0.6664, -0.6830,  0.2105,  0.2444]], grad_fn=<ViewBackward0>)\n",
      "\n",
      "Decoder_2 norm1(x + sa(x))\n",
      "tensor([[[ 0.0907, -1.2719, -0.3248,  1.5061],\n",
      "         [ 1.5420, -1.2407, -0.2872, -0.0141],\n",
      "         [-1.6700,  0.8963,  0.1538,  0.6199],\n",
      "         [-1.7152,  0.7948,  0.4221,  0.4983]],\n",
      "\n",
      "        [[-0.4393, -1.4225,  0.7720,  1.0897],\n",
      "         [-0.7424, -0.9209,  1.6119,  0.0514],\n",
      "         [-1.7173,  0.7333,  0.3715,  0.6124],\n",
      "         [-1.7280,  0.6816,  0.5543,  0.4921]],\n",
      "\n",
      "        [[ 1.3751,  0.4964, -1.1716, -0.6999],\n",
      "         [ 1.3523, -1.0771, -0.8351,  0.5599],\n",
      "         [-1.4628,  1.3619,  0.0390,  0.0619],\n",
      "         [-1.5920,  1.1700,  0.2721,  0.1499]],\n",
      "\n",
      "        [[ 1.3751,  0.4964, -1.1716, -0.6999],\n",
      "         [ 1.3523, -1.0771, -0.8351,  0.5599],\n",
      "         [-1.4628,  1.3619,  0.0390,  0.0619],\n",
      "         [-1.5920,  1.1700,  0.2721,  0.1499]]], grad_fn=<AddBackward0>)\n",
      "\n",
      "Q_dec_2 = \n",
      "tensor([[[ 0.6504, -0.7556],\n",
      "         [ 0.4817, -1.2755],\n",
      "         [ 0.0479,  1.6623],\n",
      "         [ 0.0479,  1.6623]],\n",
      "\n",
      "        [[ 0.2066, -1.3130],\n",
      "         [-0.1611, -0.6506],\n",
      "         [ 0.8998, -0.1472],\n",
      "         [ 0.8998, -0.1472]],\n",
      "\n",
      "        [[ 0.6665,  0.8729],\n",
      "         [ 0.0731, -1.1684],\n",
      "         [ 0.6984,  0.6869],\n",
      "         [ 0.6984,  0.6869]],\n",
      "\n",
      "        [[ 1.1768, -0.7487],\n",
      "         [-0.4404,  0.4048],\n",
      "         [ 1.0458, -1.1642],\n",
      "         [ 1.0458, -1.1642]],\n",
      "\n",
      "        [[-0.4851, -1.2526],\n",
      "         [-0.4562, -1.3764],\n",
      "         [-0.6692, -0.7448],\n",
      "         [-0.6692, -0.7448]],\n",
      "\n",
      "        [[-1.2221,  0.2998],\n",
      "         [-1.2418,  0.3408],\n",
      "         [-1.1303,  0.6570],\n",
      "         [-1.1303,  0.6570]],\n",
      "\n",
      "        [[-0.4945, -1.3277],\n",
      "         [-0.4701, -1.3943],\n",
      "         [-0.6324, -0.9752],\n",
      "         [-0.6324, -0.9752]],\n",
      "\n",
      "        [[-1.2484,  0.4374],\n",
      "         [-1.2481,  0.4561],\n",
      "         [-1.2023,  0.6592],\n",
      "         [-1.2023,  0.6592]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "K_dec_2 = \n",
      "tensor([[[ 0.6874, -0.1200],\n",
      "         [ 0.9028, -0.1936],\n",
      "         [ 0.3915, -0.7812],\n",
      "         [ 0.6905, -0.5813]],\n",
      "\n",
      "        [[-0.2746,  0.3318],\n",
      "         [-0.1796, -0.1994],\n",
      "         [-1.4194, -1.4575],\n",
      "         [-1.1126, -0.6851]],\n",
      "\n",
      "        [[ 0.6493, -0.1354],\n",
      "         [ 0.8705, -0.1625],\n",
      "         [ 0.4156, -0.7570],\n",
      "         [ 0.6264, -0.3479]],\n",
      "\n",
      "        [[-0.3315,  0.3368],\n",
      "         [-0.1811, -0.0217],\n",
      "         [-1.4155, -1.2972],\n",
      "         [-0.7501, -0.0747]],\n",
      "\n",
      "        [[ 0.6295, -0.1430],\n",
      "         [ 0.8424, -0.1486],\n",
      "         [ 0.4789, -0.7067],\n",
      "         [ 0.5913, -0.2834]],\n",
      "\n",
      "        [[-0.3589,  0.3370],\n",
      "         [-0.1923,  0.0715],\n",
      "         [-1.3592, -1.0643],\n",
      "         [-0.6492,  0.0802]],\n",
      "\n",
      "        [[ 0.6218, -0.1494],\n",
      "         [ 0.8282, -0.1449],\n",
      "         [ 0.5123, -0.6591],\n",
      "         [ 0.5789, -0.2605]],\n",
      "\n",
      "        [[-0.3763,  0.3303],\n",
      "         [-0.2023,  0.1076],\n",
      "         [-1.2920, -0.8930],\n",
      "         [-0.6125,  0.1336]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "V_dec_2 = \n",
      "tensor([[[-1.5826,  0.3213],\n",
      "         [-0.9879,  1.0144],\n",
      "         [-0.1349,  0.6391],\n",
      "         [-1.1816,  0.5703]],\n",
      "\n",
      "        [[-0.0496, -0.6436],\n",
      "         [-0.8156, -0.1586],\n",
      "         [-0.6576, -1.1149],\n",
      "         [-0.3969, -1.1904]],\n",
      "\n",
      "        [[-1.6129,  0.2409],\n",
      "         [-1.1970,  0.8444],\n",
      "         [-0.3790,  0.5466],\n",
      "         [-1.5170,  0.2660]],\n",
      "\n",
      "        [[ 0.0304, -0.7287],\n",
      "         [-0.6158, -0.2780],\n",
      "         [-0.5262, -1.2165],\n",
      "         [-0.0295, -1.0651]],\n",
      "\n",
      "        [[-1.6227,  0.2022],\n",
      "         [-1.3062,  0.7367],\n",
      "         [-0.7067,  0.4742],\n",
      "         [-1.5697,  0.1740]],\n",
      "\n",
      "        [[ 0.0681, -0.7682],\n",
      "         [-0.4937, -0.3554],\n",
      "         [-0.3975, -1.2913],\n",
      "         [ 0.0729, -1.0198]],\n",
      "\n",
      "        [[-1.6252,  0.1881],\n",
      "         [-1.3514,  0.6878],\n",
      "         [-0.9014,  0.4228],\n",
      "         [-1.5852,  0.1432]],\n",
      "\n",
      "        [[ 0.0813, -0.7888],\n",
      "         [-0.4393, -0.3936],\n",
      "         [-0.3113, -1.3081],\n",
      "         [ 0.1070, -1.0014]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "torch.Size([4, 2, 4, 2]) torch.Size([4, 2, 4, 2]) torch.Size([4, 2, 2, 4])\n",
      "Attention weights = \n",
      "torch.Size([4, 2, 4, 4]) tensor([[[[ 0.3802,  0.5186,  0.5974,  0.6281],\n",
      "          [ 0.3423,  0.4821,  0.8380,  0.7595],\n",
      "          [-0.1177, -0.1970, -0.9049, -0.6599],\n",
      "          [-0.1177, -0.1970, -0.9049, -0.6599]],\n",
      "\n",
      "         [[-0.3482,  0.1589,  1.1458,  0.4736],\n",
      "          [-0.1214,  0.1122,  0.8323,  0.4420],\n",
      "          [-0.2093, -0.0935, -0.7515, -0.6366],\n",
      "          [-0.2093, -0.0935, -0.7515, -0.6366]]],\n",
      "\n",
      "\n",
      "        [[[ 0.2225,  0.3100, -0.2713,  0.0805],\n",
      "          [ 0.1454,  0.1792,  0.6469,  0.3198],\n",
      "          [ 0.2549,  0.3510, -0.1624,  0.1404],\n",
      "          [ 0.2549,  0.3510, -0.1624,  0.1404]],\n",
      "\n",
      "         [[-0.4541, -0.1392, -0.4912, -0.5846],\n",
      "          [ 0.1996,  0.0502,  0.0695,  0.2122],\n",
      "          [-0.5224, -0.1160,  0.0211, -0.4932],\n",
      "          [-0.5224, -0.1160,  0.0211, -0.4932]]],\n",
      "\n",
      "\n",
      "        [[[-0.0893, -0.1573,  0.4617,  0.0482],\n",
      "          [-0.0639, -0.1271,  0.5334,  0.0851],\n",
      "          [-0.2226, -0.3204,  0.1456, -0.1306],\n",
      "          [-0.2226, -0.3204,  0.1456, -0.1306]],\n",
      "\n",
      "         [[ 0.3816,  0.1813,  0.9489,  0.5780],\n",
      "          [ 0.3964,  0.1861,  0.9371,  0.5894],\n",
      "          [ 0.4434,  0.1869,  0.5919,  0.5561],\n",
      "          [ 0.4434,  0.1869,  0.5919,  0.5561]]],\n",
      "\n",
      "\n",
      "        [[[-0.0771, -0.1536,  0.4396,  0.0421],\n",
      "          [-0.0593, -0.1324,  0.4796,  0.0644],\n",
      "          [-0.1750, -0.2704,  0.2255, -0.0792],\n",
      "          [-0.1750, -0.2704,  0.2255, -0.0792]],\n",
      "\n",
      "         [[ 0.4343,  0.2119,  0.8642,  0.5820],\n",
      "          [ 0.4386,  0.2132,  0.8523,  0.5837],\n",
      "          [ 0.4738,  0.2222,  0.6822,  0.5830],\n",
      "          [ 0.4738,  0.2222,  0.6822,  0.5830]]]], grad_fn=<AddBackward0>)\n",
      "Cross attention in decoder_2\n",
      "tensor([[-0.9415,  0.6444, -0.5576, -0.9139],\n",
      "        [-1.2515,  0.4838, -0.3197, -0.7638],\n",
      "        [-1.2187,  0.3971, -0.1976, -0.9538],\n",
      "        [-1.3055,  0.3585, -0.1390, -0.9508],\n",
      "        [-0.8916,  0.6408, -0.5266, -0.8811],\n",
      "        [-1.0834,  0.4786, -0.2644, -0.8302],\n",
      "        [-1.2112,  0.3981, -0.1954, -0.9515],\n",
      "        [-1.3026,  0.3590, -0.1380, -0.9490],\n",
      "        [-1.0957,  0.6353, -0.4786, -0.6693],\n",
      "        [-1.2415,  0.4842, -0.3485, -0.8257],\n",
      "        [-1.2475,  0.3926, -0.1711, -0.9086],\n",
      "        [-1.3202,  0.3558, -0.1271, -0.9269],\n",
      "        [-1.0957,  0.6353, -0.4786, -0.6693],\n",
      "        [-1.2415,  0.4842, -0.3485, -0.8257],\n",
      "        [-1.2475,  0.3926, -0.1711, -0.9086],\n",
      "        [-1.3202,  0.3558, -0.1271, -0.9269]], grad_fn=<ViewBackward0>)\n",
      "\n",
      "norm2(x' + mha(x', mem)), \n",
      " where x' = Decoder_curr_layer norm1(x + sa(x))\n",
      "tensor([[[ 0.0271, -0.8601, -0.7902,  1.6233],\n",
      "         [ 1.1779, -1.0651, -0.9142,  0.8014],\n",
      "         [-1.2365,  0.5136, -0.6250,  1.3478],\n",
      "         [-1.3444,  0.4428, -0.4408,  1.3425]],\n",
      "\n",
      "        [[-0.3587, -1.1458, -0.0923,  1.5968],\n",
      "         [-0.7606, -1.2006,  0.8118,  1.1493],\n",
      "         [-1.3118,  0.3944, -0.4641,  1.3814],\n",
      "         [-1.3813,  0.3534, -0.3342,  1.3621]],\n",
      "\n",
      "        [[ 0.9909,  0.4526, -1.6628,  0.2193],\n",
      "         [ 0.8853, -0.8085, -1.1697,  1.0929],\n",
      "         [-1.1982,  0.9929, -0.7797,  0.9850],\n",
      "         [-1.3257,  0.8173, -0.5958,  1.1042]],\n",
      "\n",
      "        [[ 0.9909,  0.4526, -1.6628,  0.2193],\n",
      "         [ 0.8853, -0.8085, -1.1697,  1.0929],\n",
      "         [-1.1982,  0.9929, -0.7797,  0.9850],\n",
      "         [-1.3257,  0.8173, -0.5958,  1.1042]]], grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "\n",
      "norm3(x'' + ff(x'')) \n",
      " where, x'' = Decoder_curr_layer norm2(x' + mha(x'))\n",
      "tensor([[[-0.1138, -1.1485, -0.3361,  1.5984],\n",
      "         [ 1.0943, -1.3582, -0.5473,  0.8112],\n",
      "         [-1.4614,  0.3338, -0.1836,  1.3112],\n",
      "         [-1.5289,  0.2500,  0.0140,  1.2648]],\n",
      "\n",
      "        [[-0.4406, -1.3158,  0.3624,  1.3940],\n",
      "         [-0.7173, -1.2418,  1.0899,  0.8692],\n",
      "         [-1.4987,  0.1977, -0.0085,  1.3095],\n",
      "         [-1.5387,  0.1515,  0.1246,  1.2625]],\n",
      "\n",
      "        [[ 1.0570,  0.3412, -1.6448,  0.2467],\n",
      "         [ 0.8070, -1.1386, -0.8335,  1.1652],\n",
      "         [-1.4695,  0.8911, -0.3738,  0.9522],\n",
      "         [-1.5573,  0.6725, -0.1621,  1.0469]],\n",
      "\n",
      "        [[ 1.0570,  0.3412, -1.6448,  0.2467],\n",
      "         [ 0.8070, -1.1386, -0.8335,  1.1652],\n",
      "         [-1.4695,  0.8911, -0.3738,  0.9522],\n",
      "         [-1.5573,  0.6725, -0.1621,  1.0469]]], grad_fn=<AddBackward0>)\n",
      "\n",
      "torch.Size([4, 4, 4]) torch.Size([4, 4, 4])\n",
      "### Decoder Done ###\n"
     ]
    }
   ],
   "source": [
    "d_model = 4\n",
    "num_encoder_layers = 3\n",
    "num_decoder_layers = 3\n",
    "\n",
    "final_op = get_all_intermediate_outputs(src_sentence, tgt_sentence, model = model, num_encoder_layers = num_encoder_layers , num_decoder_layers = num_encoder_layers, d_model=d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.3940, -0.5207,  0.5881, -0.0633, -0.3120,  0.5819, -0.4101,\n",
       "          -1.4635, -0.1699, -0.2797],\n",
       "         [ 0.5073,  0.1428,  0.3195, -0.0658, -0.9096, -0.1855, -0.1959,\n",
       "          -1.3236, -0.4518,  0.5941],\n",
       "         [-0.5948, -0.5942,  0.6483, -0.1120,  0.4457,  0.9227, -0.2608,\n",
       "          -0.6598,  0.2730, -0.9357],\n",
       "         [-0.5474, -0.6907,  0.6200, -0.1524,  0.4327,  0.9527, -0.1927,\n",
       "          -0.5951,  0.2499, -0.9856]],\n",
       "\n",
       "        [[ 0.4754, -0.8489,  0.4886, -0.2087, -0.3043,  0.6991, -0.1543,\n",
       "          -1.1643, -0.2179, -0.4919],\n",
       "         [ 0.3852, -1.0187,  0.3113, -0.3779, -0.3152,  0.6655,  0.2213,\n",
       "          -0.6228, -0.2607, -0.5969],\n",
       "         [-0.5107, -0.6982,  0.6293, -0.1455,  0.4188,  0.9588, -0.2144,\n",
       "          -0.6444,  0.2413, -0.9784],\n",
       "         [-0.4861, -0.7543,  0.6063, -0.1736,  0.4078,  0.9704, -0.1630,\n",
       "          -0.5902,  0.2254, -1.0049]],\n",
       "\n",
       "        [[-0.5983,  1.0723,  0.3436,  0.1102, -0.5498, -0.5710, -0.2827,\n",
       "          -0.8323, -0.1088,  0.8705],\n",
       "         [ 0.3852,  0.0908,  0.4625,  0.0084, -0.6836,  0.0219, -0.3930,\n",
       "          -1.4907, -0.3148,  0.3874],\n",
       "         [-0.9692, -0.2947,  0.5966, -0.0966,  0.5243,  0.7436, -0.1840,\n",
       "          -0.3591,  0.3570, -0.8197],\n",
       "         [-0.8282, -0.4762,  0.5989, -0.1316,  0.5109,  0.8432, -0.1617,\n",
       "          -0.4020,  0.3243, -0.9195]],\n",
       "\n",
       "        [[-0.5983,  1.0723,  0.3436,  0.1102, -0.5498, -0.5710, -0.2827,\n",
       "          -0.8323, -0.1088,  0.8705],\n",
       "         [ 0.3852,  0.0908,  0.4625,  0.0084, -0.6836,  0.0219, -0.3930,\n",
       "          -1.4907, -0.3148,  0.3874],\n",
       "         [-0.9692, -0.2947,  0.5966, -0.0966,  0.5243,  0.7436, -0.1840,\n",
       "          -0.3591,  0.3570, -0.8197],\n",
       "         [-0.8282, -0.4762,  0.5989, -0.1316,  0.5109,  0.8432, -0.1617,\n",
       "          -0.4020,  0.3243, -0.9195]]], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.3940, -0.5207,  0.5881, -0.0633, -0.3120,  0.5819, -0.4101,\n",
       "          -1.4635, -0.1699, -0.2797],\n",
       "         [ 0.5072,  0.1428,  0.3195, -0.0658, -0.9096, -0.1855, -0.1959,\n",
       "          -1.3236, -0.4518,  0.5941],\n",
       "         [-0.5948, -0.5942,  0.6483, -0.1120,  0.4457,  0.9227, -0.2608,\n",
       "          -0.6598,  0.2730, -0.9357],\n",
       "         [-0.5474, -0.6907,  0.6200, -0.1524,  0.4327,  0.9527, -0.1927,\n",
       "          -0.5951,  0.2499, -0.9856]],\n",
       "\n",
       "        [[ 0.4754, -0.8489,  0.4886, -0.2087, -0.3043,  0.6991, -0.1543,\n",
       "          -1.1643, -0.2179, -0.4919],\n",
       "         [ 0.3852, -1.0187,  0.3113, -0.3779, -0.3152,  0.6655,  0.2213,\n",
       "          -0.6228, -0.2607, -0.5969],\n",
       "         [-0.5107, -0.6982,  0.6293, -0.1455,  0.4188,  0.9588, -0.2144,\n",
       "          -0.6444,  0.2413, -0.9784],\n",
       "         [-0.4861, -0.7543,  0.6063, -0.1736,  0.4078,  0.9704, -0.1630,\n",
       "          -0.5902,  0.2254, -1.0049]],\n",
       "\n",
       "        [[-0.5984,  1.0723,  0.3436,  0.1102, -0.5498, -0.5710, -0.2827,\n",
       "          -0.8323, -0.1088,  0.8705],\n",
       "         [ 0.3852,  0.0908,  0.4625,  0.0084, -0.6836,  0.0219, -0.3930,\n",
       "          -1.4907, -0.3148,  0.3874],\n",
       "         [-0.9692, -0.2947,  0.5966, -0.0966,  0.5243,  0.7436, -0.1840,\n",
       "          -0.3591,  0.3570, -0.8197],\n",
       "         [-0.8282, -0.4762,  0.5989, -0.1316,  0.5109,  0.8432, -0.1617,\n",
       "          -0.4020,  0.3243, -0.9195]],\n",
       "\n",
       "        [[-0.5984,  1.0723,  0.3436,  0.1102, -0.5498, -0.5710, -0.2827,\n",
       "          -0.8323, -0.1088,  0.8705],\n",
       "         [ 0.3852,  0.0908,  0.4625,  0.0084, -0.6836,  0.0219, -0.3930,\n",
       "          -1.4907, -0.3148,  0.3874],\n",
       "         [-0.9692, -0.2947,  0.5966, -0.0966,  0.5243,  0.7436, -0.1840,\n",
       "          -0.3591,  0.3570, -0.8197],\n",
       "         [-0.8282, -0.4762,  0.5989, -0.1316,  0.5109,  0.8432, -0.1617,\n",
       "          -0.4020,  0.3243, -0.9195]]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "my_project_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
