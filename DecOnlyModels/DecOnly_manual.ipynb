{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decoder only transformer architecture\n",
    "\n",
    "## Manual computation of the intermediate outputs (Verification)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intialising the same transformer model as used in the other notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import copy \n",
    "\n",
    "torch.manual_seed(6)\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=512, dropout=0):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.encoding = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1).float()\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(torch.log(torch.tensor(10000.0)) / d_model))\n",
    "        self.encoding[:, 0::2] = torch.sin(position * div_term)\n",
    "        self.encoding[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.encoding = self.encoding.unsqueeze(0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.encoding[:, :x.shape[1]].detach()\n",
    "    \n",
    "    \n",
    "\n",
    "class DecoderOnlyTransformer(nn.Module):\n",
    "    def __init__(self,num_layers, d_model, nhead, dim_feedforward, vocab_size, max_seq_len, dropout = 0):\n",
    "        super(DecoderOnlyTransformer, self).__init__()\n",
    "\n",
    "        self.src_embedding = nn.Embedding(vocab_size, d_model)\n",
    "\n",
    "        self.positional_encoding = PositionalEncoding(d_model = d_model, dropout=0, max_len=max_seq_len)\n",
    "\n",
    "        self.decoder_layer = nn.TransformerEncoderLayer(d_model = d_model, nhead = nhead, dim_feedforward = dim_feedforward, dropout = dropout)\n",
    "        self.layers = nn.ModuleList([copy.deepcopy(self.decoder_layer) for _ in range(num_layers)])\n",
    "\n",
    "        self.fc = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    \n",
    "    def generate_square_subsequent_mask(self, tgt):\n",
    "        seq_length = tgt.size(0)\n",
    "        nopeak_mask = (torch.triu(torch.ones(seq_length, seq_length), diagonal=1)).bool()\n",
    "\n",
    "        return nopeak_mask\n",
    "    \n",
    "        # mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
    "        # mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "        # return mask\n",
    "\n",
    "    def forward(self, src, src_mask=None):\n",
    "\n",
    "        src_embed = self.src_embedding(src)\n",
    "        pe_src = self.positional_encoding(src)\n",
    "        \n",
    "        pe_src = pe_src.transpose(0,1)\n",
    "\n",
    "        src = src_embed + pe_src\n",
    "\n",
    "        for layer in self.layers:\n",
    "\n",
    "            src_mask = self.generate_square_subsequent_mask(src)\n",
    "            src = layer(src, src_mask)\n",
    "\n",
    "\n",
    "        op = self.fc(src)\n",
    "        return op\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "\n",
    "vocab_size = 20  # Source language vocabulary size\n",
    "d_model = 6  # Dimension of the model\n",
    "num_heads = 2\n",
    "d_ff = 4\n",
    "num_layers = 2\n",
    "\n",
    "\n",
    "max_seq_len = 5\n",
    "model = DecoderOnlyTransformer(d_model=d_model, vocab_size=vocab_size, num_layers=num_layers , nhead=num_heads, max_seq_len = max_seq_len-1, dim_feedforward=d_ff)\n",
    "\n",
    "\n",
    "batch_size = 10\n",
    "\n",
    "# Generate random sample data\n",
    "src_data = torch.randint(1, vocab_size, (max_seq_len , batch_size))  # (seq_length, batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "state_dict = model.state_dict()\n",
    "\n",
    "state_dict1 = copy.deepcopy(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 10]), torch.Size([5, 10]))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_data[:-1, :].shape , src_data.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions to get the intermediate outputs "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function to fetch word embeddings with help of token indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def look_up_table(sentence, vocab_embeds, embedding):\n",
    "\n",
    "    for i in range(sentence.size(0)):\n",
    "        for j in range(sentence.size(1)):\n",
    "            \n",
    "            # Get the index for the current word token index in the sequence\n",
    "            word_index = sentence[i, j].item()\n",
    "\n",
    "            if word_index < 0 or word_index >= vocab_embeds.size(0):\n",
    "                raise ValueError(f\"Invalid word index: {word_index}\")\n",
    "\n",
    "            # Lookup the corresponding embedding vector for the word\n",
    "            embedding[i, j, :] = vocab_embeds[word_index, :]\n",
    "\n",
    "            print(f\"Word index: {word_index}, Embedding: {vocab_embeds[word_index, :]}\")\n",
    "    print()\n",
    "\n",
    "    return embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embeddings and Positional encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding_outputs(src_data, max_seq_len, state_dict, d_model):\n",
    "\n",
    "    src_vocab_embeds = state_dict[\"src_embedding.weight\"]\n",
    "\n",
    "    src_embedding = torch.zeros(src_data.size(0), src_data.size(1), d_model)\n",
    "    print(\"Source sentence embedding\")\n",
    "    src_embedding =  look_up_table(src_data, src_vocab_embeds, src_embedding)\n",
    "    print(src_embedding.shape)\n",
    "\n",
    "    pe = PositionalEncoding(d_model = d_model, dropout=0, max_len=max_seq_len)\n",
    "\n",
    "    print(\"PE of src data:\")\n",
    "    print(pe(src_data).transpose(0,1))\n",
    "    print()\n",
    "\n",
    "    pe_src_embeds = src_embedding + pe(src_data).transpose(0,1)\n",
    "\n",
    "    print(\"PE source embeddings : \\n\")\n",
    "    print(pe_src_embeds)\n",
    "    print()\n",
    "\n",
    "    return pe_src_embeds\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder function to display the intermediate outputs and get the final outputs from the decoder\n",
    "\n",
    "### Masked self attention \n",
    "\n",
    "#### Functions to perform the attention calculation with Q,K and V matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def atten_product_needs_wts_false(Q, V, K, bsz, head_dim, src_len, tgt_len, embed_dim, attn_mask):\n",
    "\n",
    "\n",
    "    # *** For multi-head attention ***\n",
    "    #  (bsz*num_heads, src_len , head_dim) -> (bsz, num_heads, tgt_len, head_dim)\n",
    "    Q1 = Q.view(bsz, num_heads, tgt_len, head_dim)\n",
    "    K1 = K.view(bsz, num_heads, src_len, head_dim)\n",
    "    V1 = V.view(bsz, num_heads, src_len, head_dim)\n",
    "\n",
    "\n",
    "    L, S = Q1.size(-2), K1.size(-2)\n",
    "\n",
    "    scale_factor = 1 / math.sqrt(Q1.size(-1)) \n",
    "    # scale_factor = 1\n",
    "    attn_bias = torch.zeros(L, S, dtype=Q1.dtype)\n",
    "\n",
    "    if attn_mask is not None:\n",
    "        if attn_mask.dtype == torch.bool:\n",
    "            # attn_mask.masked_fill_(attn_mask.logical_not(), float(\"-inf\"))\n",
    "\n",
    "            masked_tensor = attn_mask.float().masked_fill(attn_mask, float('-inf'))\n",
    "            masked_tensor = masked_tensor.masked_fill(~attn_mask, 0)\n",
    "            attn_mask = masked_tensor\n",
    "\n",
    "            print(\"Attnetion mask infunction = \")\n",
    "            print(attn_mask)\n",
    "            print()\n",
    "            attn_bias = attn_bias.unsqueeze(0).unsqueeze(0)\n",
    "            attn_bias += attn_mask\n",
    "\n",
    "        else:\n",
    "            attn_bias += attn_mask\n",
    "            attn_bias = attn_bias.unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "\n",
    "    # print(\"Attnetion bias = \", attn_bias.shape)\n",
    "    # print(attn_bias)\n",
    "    # print()\n",
    "            \n",
    "\n",
    "    # (bsz, num_heads, tgt_len, head_dim) @ (bsz, num_heads, head_dim, tgt_len) -> (bsz, num_heads, tgt_len, tgt_len) \n",
    "    attn_weight = Q1 @ K1.transpose(-2, -1) * scale_factor\n",
    "    attn_weight += attn_bias\n",
    "\n",
    "\n",
    "    # print(\"INTERMEDIATE PDT = \", attn_weight)\n",
    "\n",
    "\n",
    "    # (bsz, num_heads, tgt_len, tgt_len) \n",
    "    attn_weight = torch.softmax(attn_weight, dim=-1)\n",
    "\n",
    "    # print(\"ATTN PDT = \", attn_weight)\n",
    "\n",
    "    sum_last_dim = attn_weight.sum(dim=-1)\n",
    "    tolerance = 1e-6  \n",
    "    assert torch.allclose(sum_last_dim, torch.ones_like(sum_last_dim), atol=tolerance), \"Attention weights sum is not approximately equal to 1\"\n",
    "\n",
    "    # (bsz, num_heads, tgt_len, tgt_len) @ (bsz, num_heads, tgt_len, head_dim) -> (bsz, num_heads, tgt_len, head_dim) \n",
    "    attn_output = attn_weight @ V1\n",
    "\n",
    "    # print(\"Dot product attention  = \")\n",
    "    # print(attn_weight.shape, attn_weight)\n",
    "\n",
    "    # print(attn_output.shape)\n",
    "    # print(bsz, tgt_len, embed_dim)\n",
    "    \n",
    "    # (bsz*tgt_len, embed_dim)\n",
    "    attn_output = attn_output.permute(2, 0, 1, 3).contiguous().view(bsz * tgt_len, embed_dim)\n",
    "\n",
    "    print(\"Attention output = \")\n",
    "    print(attn_weight.shape, attn_weight)\n",
    "\n",
    "    return attn_output\n",
    "\n",
    "\n",
    "\n",
    "def atten_product_needs_wts_true(Q, K, V, bsz, tgt_len, embed_dim, attn_mask):\n",
    "\n",
    "    # *** For multi-head attention ***\n",
    "    #  (bsz*num_heads, src_len , head_dim)\n",
    "    \n",
    "    B, Nt, E = Q.shape\n",
    "\n",
    "    Q_scaled = Q / math.sqrt(E)\n",
    "\n",
    "    if attn_mask is not None:\n",
    "        temp_pdt_matrix = torch.baddbmm(attn_mask, Q_scaled, K.transpose(-2, -1))\n",
    "    else:\n",
    "        temp_pdt_matrix = torch.bmm(Q_scaled, K.transpose(-2, -1))\n",
    "\n",
    "    attn_wt_matrix = torch.nn.functional.softmax(temp_pdt_matrix, dim=-1)\n",
    "\n",
    "    attn_output = torch.bmm(attn_wt_matrix, V)\n",
    "\n",
    "    attn_output = attn_output.transpose(0, 1).contiguous().view(tgt_len * bsz, embed_dim)\n",
    "\n",
    "\n",
    "    sum_last_dim = attn_wt_matrix.sum(dim=-1)\n",
    "\n",
    "    tolerance = 1e-6  \n",
    "    assert torch.allclose(sum_last_dim, torch.ones_like(sum_last_dim), atol=tolerance), \"Attention weights sum is not approximately equal to 1\"\n",
    "\n",
    "\n",
    "    print(\"Encoder Attention output = \")\n",
    "    print(attn_output)\n",
    "    print()\n",
    "\n",
    "    return attn_output, attn_wt_matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function to get the Q,K,V matrices from the model's intialised weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_qkv(query, key, value ,W, b):\n",
    "\n",
    "    # embed_dim\n",
    "    E = query.size(-1)\n",
    "\n",
    "    if key is value:\n",
    "        if query is key:\n",
    "            \n",
    "            # (src_len, bsz, embed_dim) @ (embed_dim*num_heads, embed_dim).T -> (src_len, bsz, embed_dim*num_heads)\n",
    "            tempop1 = query@W.T\n",
    "\n",
    "            # reshape to 3, E and not E, 3 is deliberate for better memory coalescing and keeping same order as chunk()\n",
    "            tempop1 = tempop1.unflatten(-1, (3, E)).unsqueeze(0).transpose(0, -2).squeeze(-2).contiguous()\n",
    "\n",
    "            # (src_len, bsz, embed_dim)\n",
    "            return tempop1[0], tempop1[1], tempop1[2]\n",
    "        \n",
    "\n",
    "        else:\n",
    "\n",
    "            # (embed_dim*1, embed_dim)\n",
    "            # (embed_dim*2, embed_dim)\n",
    "            W_q, W_kv = W.split([E, E * 2])\n",
    "\n",
    "            if b is None:\n",
    "                b_q = b_kv = None\n",
    "            else:\n",
    "                b_q, b_kv = b.split([E, E * 2])\n",
    "\n",
    "            # (src_len, bsz, embed_dim) @ (embed_dim*1, embed_dim).T -> (src_len, bsz, embed_dim)\n",
    "            q_matmul = query@W_q.T\n",
    "\n",
    "            # (src_len, bsz, embed_dim) @ (embed_dim*2, embed_dim).T -> (src_len, bsz, embed_dim*2)\n",
    "            kv_matmul = key@W_kv.T\n",
    "\n",
    "            kv_matmul = kv_matmul.unflatten(-1, (2, E)).unsqueeze(0).transpose(0, -2).squeeze(-2).contiguous()\n",
    "\n",
    "            # (src_len, bsz, embed_dim)\n",
    "            return q_matmul, kv_matmul[0], kv_matmul[1]\n",
    "\n",
    "    else:\n",
    "\n",
    "        W_q, W_k, W_v = W.chunk(3)\n",
    "        if b is None:\n",
    "            b_q = b_k = b_v = None\n",
    "        else:\n",
    "            b_q, b_k, b_v = b.chunk(3)\n",
    "\n",
    "\n",
    "        q_matmul = query@W_q.T\n",
    "        k_matmul = key@W_k.T\n",
    "        v_matmul = value@W_v.T\n",
    "\n",
    "        return q_matmul, k_matmul, v_matmul\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decoder block's self attention output function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoder_block_self_attn_output(x, state_dict, layer_num, num_heads, tgt_mask = None,need_weights = False):\n",
    "\n",
    "    # (tgt_len, bsz, embed_dim)\n",
    "    query_dec = key_dec = value_dec = x\n",
    "\n",
    "    tgt_len, bsz, embed_dim = x.shape\n",
    " \n",
    "    # (embed_dim*num_heads, embed_dim)\n",
    "    W_dec = state_dict[\"layers.{}.self_attn.in_proj_weight\".format(layer_num)]\n",
    "    b_dec = state_dict[\"layers.{}.self_attn.in_proj_bias\".format(layer_num)]\n",
    "\n",
    "\n",
    "    head_dim = embed_dim//num_heads\n",
    "\n",
    "    # (tgt_len, bsz, embed_dim)\n",
    "    Q_dec,K_dec,V_dec = get_qkv(query_dec, key_dec, value_dec ,W_dec, b_dec)\n",
    "    \n",
    "    # Q_dec = Q_dec.unsqueeze(0)\n",
    "    # K_dec = K_dec.unsqueeze(0)\n",
    "    # V_dec = V_dec.unsqueeze(0)\n",
    "\n",
    "    # (1, tgt_len, bsz, embed_dim)\n",
    "    # print(Q_dec.shape, K_dec.shape , V_dec.shape)\n",
    "    # print(tgt_len, bsz * num_heads, head_dim)\n",
    "\n",
    "    # (1, tgt_len, bsz, embed_dim) -> ( bsz*num_heads, tgt_len , head_dim )\n",
    "    Q_dec = Q_dec.reshape(tgt_len, bsz * num_heads, head_dim).transpose(0, 1)\n",
    "    K_dec = K_dec.reshape(K_dec.shape[0], bsz * num_heads, head_dim).transpose(0, 1)\n",
    "    V_dec = V_dec.reshape(V_dec.shape[0], bsz * num_heads, head_dim).transpose(0, 1)\n",
    "\n",
    "    print(\"Q_dec_{} = \".format(layer_num))\n",
    "    print(Q_dec)\n",
    "    print()\n",
    "\n",
    "    print(\"K_dec_{} = \".format(layer_num))\n",
    "    print(K_dec)\n",
    "    print()\n",
    "\n",
    "    print(\"V_dec_{} = \".format(layer_num))\n",
    "    print(V_dec)\n",
    "    print()\n",
    "\n",
    "    src_len = K_dec.size(1)\n",
    "\n",
    "\n",
    "    attn_mask = tgt_mask\n",
    "    if attn_mask is not None:\n",
    "\n",
    "        # Ensuring attn_mask's dim is 3\n",
    "        if attn_mask.dim() == 2:\n",
    "            correct_2d_size = (tgt_len, src_len)\n",
    "            if attn_mask.shape != correct_2d_size:\n",
    "                raise RuntimeError(f\"The shape of the 2D attn_mask is {attn_mask.shape}, but should be {correct_2d_size}.\")\n",
    "            attn_mask = attn_mask.unsqueeze(0)\n",
    "        elif attn_mask.dim() == 3:\n",
    "            correct_3d_size = (bsz * num_heads, tgt_len, src_len)\n",
    "            if attn_mask.shape != correct_3d_size:\n",
    "                raise RuntimeError(f\"The shape of the 3D attn_mask is {attn_mask.shape}, but should be {correct_3d_size}.\")\n",
    "        else:\n",
    "            raise RuntimeError(f\"attn_mask's dimension {attn_mask.dim()} is not supported\")\n",
    "\n",
    "    # attn_mask can be either (L,S) or (N*num_heads, L, S)\n",
    "    # if attn_mask's shape is (1, L, S) we need to unsqueeze to (1, 1, L, S)\n",
    "    # in order to match the input for SDPA of (N, num_heads, L, S)\n",
    "    if attn_mask is not None:\n",
    "        if attn_mask.size(0) == 1 and attn_mask.dim() == 3:\n",
    "            attn_mask = attn_mask.unsqueeze(0)\n",
    "        else:\n",
    "            attn_mask = attn_mask.view(bsz, num_heads, -1, src_len)\n",
    "\n",
    "    \n",
    "    \n",
    "    if need_weights is False:\n",
    "        attn_output = atten_product_needs_wts_false(Q = Q_dec, V = V_dec, K = K_dec, bsz = bsz, head_dim=head_dim, src_len=src_len, tgt_len=tgt_len, attn_mask = attn_mask, embed_dim=embed_dim)\n",
    "\n",
    "        print(\"Decoder Self Attention = \")\n",
    "        print(attn_output)\n",
    "        print()\n",
    "\n",
    "        op_dec_1 = torch.matmul(attn_output, state_dict[\"layers.{}.self_attn.out_proj.weight\".format(layer_num)].t()) + state_dict[\"layers.{}.self_attn.out_proj.bias\".format(layer_num)]\n",
    "        attn_dec_output = op_dec_1.view(tgt_len, bsz, attn_output.size(1))\n",
    "\n",
    "        return attn_dec_output, None\n",
    "    \n",
    "    else:\n",
    "\n",
    "        attn_dec_output,attn_wt_matrix_dec = atten_product_needs_wts_true(Q=Q_dec, K=K_dec, V=V_dec, bsz=bsz, tgt_len=tgt_len, attn_mask = attn_mask, embed_dim=embed_dim)\n",
    "\n",
    "        print(\"Decoder Attention output = \")\n",
    "        print(attn_wt_matrix_dec)\n",
    "        print()\n",
    "\n",
    "        op_dec_1 = torch.matmul(attn_dec_output, state_dict[\"layers.{}.self_attn.out_proj.weight\".format(layer_num)].t()) + state_dict[\"layers.{}.self_attn.out_proj.bias\".format(layer_num)]\n",
    "        attn_dec_output = op_dec_1.view(tgt_len, bsz, attn_dec_output.size(1))\n",
    "    \n",
    "\n",
    "        return attn_dec_output, attn_wt_matrix_dec\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Post self attention in decoder\n",
    "\n",
    "#### Function to perform the linear layer calculations after deccoder's self attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dec_post_self_attn(x, attn_dec_output, state_dict, layer_num, bsz, tgt_len):\n",
    "\n",
    "    # (bsz*src_len , embed_dim) @ (embed_dim , embed_dim).T -> (bsz*src_len , embed_dim)\n",
    "    # op_dec_1 = torch.matmul(attn_dec_output, state_dict[\"layers.{}.self_attn.out_proj.weight\".format(layer_num)].t()) + state_dict[\"layers.{}.self_attn.out_proj.bias\".format(layer_num)]\n",
    "\n",
    "    # print(op_dec_1.shape)\n",
    "\n",
    "    # # (bsz*src_len , embed_dim) -> (src_len, bsz, embed_dim)\n",
    "\n",
    "    # print()\n",
    "\n",
    "    # attn_dec_output = op_dec_1.view(tgt_len, bsz, attn_dec_output.size(1))\n",
    "\n",
    "\n",
    "    # Here src is the original passed inputs to the 1st transformer encoder layer which \n",
    "    # are pe_src_embeds \n",
    "\n",
    "    # (src_len, bsz, embed_dim)\n",
    "    output_dec_1 = attn_dec_output + x\n",
    "\n",
    "    #  (src_len, bsz, embed_dim) @ (embed_dim) -> (src_len, bsz, embed_dim) \n",
    "    linear_result_dec_1 = output_dec_1*state_dict[\"layers.{}.norm1.weight\".format(layer_num)] + state_dict[\"layers.{}.norm1.bias\".format(layer_num)]\n",
    "\n",
    "    # Layer normalization from Torch's implementation\n",
    "    layernorm_dec_1 = torch.nn.LayerNorm(normalized_shape=linear_result_dec_1.shape[2:])\n",
    "    linear_op_dec_1 = layernorm_dec_1(linear_result_dec_1)\n",
    "\n",
    "\n",
    "    # Manual Layer Normalization\n",
    "    x = linear_result_dec_1\n",
    "\n",
    "    # Obtained layer norm weights and biases (learnable)\n",
    "    w = layernorm_dec_1.weight\n",
    "    b = layernorm_dec_1.bias\n",
    "\n",
    "    linear_result_dec_1f = w*x + b\n",
    "\n",
    "    epsilon = 1e-5  \n",
    "    mean = linear_result_dec_1f.mean(dim=-1, keepdim=True)\n",
    "    std = linear_result_dec_1f.std(dim=-1, unbiased=False, keepdim=True)\n",
    "    normalized_result_dec_1 = (linear_result_dec_1f - mean) / (std + epsilon) * w + b\n",
    "\n",
    "\n",
    "    op_dec_1 = torch.matmul(normalized_result_dec_1, state_dict[\"layers.{}.linear1.weight\".format(layer_num)].t()) + state_dict[\"layers.{}.linear1.bias\".format(layer_num)]\n",
    "    op_dec_1_relu = torch.nn.functional.relu(op_dec_1)\n",
    "    op_dec_2 = torch.matmul(op_dec_1_relu, state_dict[\"layers.{}.linear2.weight\".format(layer_num)].t()) + state_dict[\"layers.{}.linear2.bias\".format(layer_num)]\n",
    "\n",
    "\n",
    "    output_dec_2 = op_dec_2 + linear_op_dec_1\n",
    "    output_dec_2_norm = output_dec_2*state_dict[\"layers.{}.norm2.weight\".format(layer_num)] + state_dict[\"layers.{}.norm2.bias\".format(layer_num)]\n",
    "\n",
    "    # Layer normalization from Torch's implementation\n",
    "    layernorm_dec_final = torch.nn.LayerNorm(normalized_shape=output_dec_2_norm.shape[2:])\n",
    "    output_dec_final = layernorm_dec_final(output_dec_2_norm)\n",
    "\n",
    "\n",
    "    # Manual Layer Normalization \n",
    "    x = output_dec_2_norm\n",
    "\n",
    "    # Obtained layer norm weights and biases (learnable)\n",
    "    w = layernorm_dec_final.weight\n",
    "    b = layernorm_dec_final.bias\n",
    "\n",
    "    linear_result_dec_2 = w*x + b\n",
    "\n",
    "    epsilon = 1e-5  \n",
    "    mean = linear_result_dec_2.mean(dim=-1, keepdim=True)\n",
    "    std = linear_result_dec_2.std(dim=-1, unbiased=False, keepdim=True)\n",
    "    output_dec_final = (linear_result_dec_2 - mean) / (std + epsilon) * w + b\n",
    "\n",
    "    print(\"Final Encoder {} Output :\".format(layer_num))\n",
    "    print(\"norm2(norm1(x + self_atten(x)) + feed_fwd_op)\\n\")\n",
    "    print(output_dec_final)\n",
    "    print()\n",
    "\n",
    "    # (src_len, bsz, embed_dim) \n",
    "    return output_dec_final\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function to perform the linear layer calculations after transformer blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feef_fwd_transformer(dec_output_final, state_dict):\n",
    "\n",
    "    # (tgt_len, bsz, embed_dim) @ (vocab_size, embed_dim).T -> (tgt_len, bsz, vocab_size)\n",
    "    final_op = dec_output_final@state_dict[\"fc.weight\"].T + state_dict[\"fc.bias\"]\n",
    "\n",
    "    return final_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_intermediate_outputs_mask(src_data ,d_model, state_dict , num_decoder_layers, tgt_mask, max_seq_len, d_ff):\n",
    "\n",
    "    pe_src_embeds = get_embedding_outputs(src_data=src_data,  state_dict=state_dict, max_seq_len=max_seq_len, d_model = d_model)\n",
    "    print(\"###\"*25)\n",
    "    print(\"### Decoder Start ###\")\n",
    "    print()\n",
    "\n",
    "    x_dec = pe_src_embeds\n",
    "    for lno in range(num_decoder_layers):\n",
    "        attn_dec_output, attn_wt_matrix = decoder_block_self_attn_output(x_dec, state_dict, layer_num = lno, num_heads=num_heads, need_weights = False, tgt_mask=tgt_mask)\n",
    "\n",
    "\n",
    "        if lno == 0:\n",
    "            tgt_len, bsz, embed_dim = x_dec.shape\n",
    "\n",
    "        output_dec_final = dec_post_self_attn(x_dec, attn_dec_output, state_dict, layer_num = lno , bsz = bsz, tgt_len = tgt_len)\n",
    "\n",
    "        x_dec = output_dec_final\n",
    "\n",
    "    \n",
    "    print(\"### Decoder End ###\")\n",
    "    \n",
    "    final_op = feef_fwd_transformer(x_dec, state_dict)\n",
    "\n",
    "    return final_op\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import functional as F\n",
    "\n",
    "need_weights = False\n",
    "\n",
    "src_mask = None\n",
    "\n",
    "tgt_mask = None\n",
    "\n",
    "memory_mask = None\n",
    "\n",
    "embed_dim = 6\n",
    "\n",
    "num_heads = 2\n",
    "\n",
    "max_seq_len = 5\n",
    "\n",
    "num_decoder_layers = 2\n",
    " \n",
    "\n",
    "def generate_square_subsequent_mask(self, tgt):\n",
    "        seq_length = tgt.size(0)\n",
    "        nopeak_mask = (torch.triu(torch.ones(seq_length, seq_length), diagonal=1)).bool()\n",
    "\n",
    "        return nopeak_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_mask(src):\n",
    "    seq_length = src.size(0)\n",
    "    nopeak_mask = (torch.triu(torch.ones(seq_length, seq_length), diagonal=1)).bool()\n",
    "    # tgt_mask = tgt_mask & nopeak_mask\n",
    "    return nopeak_mask\n",
    "\n",
    "tgt_mask = generate_mask(src_data[:-1, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source sentence embedding\n",
      "Word index: 2, Embedding: tensor([ 0.1198,  1.2377,  1.1168, -0.2473, -1.3527, -1.6959])\n",
      "Word index: 15, Embedding: tensor([-1.8817, -0.0497, -1.0450, -0.9565,  0.0335,  0.7101])\n",
      "Word index: 9, Embedding: tensor([ 0.5433, -0.3952, -0.4462,  0.7440,  1.5210,  3.4105])\n",
      "Word index: 16, Embedding: tensor([ 1.6459, -1.3602,  0.3446,  0.5199, -2.6133, -1.6965])\n",
      "Word index: 5, Embedding: tensor([ 0.9463, -0.8437, -0.6136,  0.0316, -0.4927,  0.2484])\n",
      "Word index: 4, Embedding: tensor([ 0.7502, -0.5855, -0.1734,  0.1835,  1.3894,  1.5863])\n",
      "Word index: 12, Embedding: tensor([-1.6293, -0.5497, -0.4798, -0.4997, -1.0670,  1.1149])\n",
      "Word index: 11, Embedding: tensor([ 1.1108,  1.2899, -1.4782,  2.5672, -0.4731,  0.3356])\n",
      "Word index: 7, Embedding: tensor([-0.2897,  0.0525,  0.5229,  2.3022, -1.4689, -1.5867])\n",
      "Word index: 10, Embedding: tensor([-1.5312, -1.2341,  1.8197, -0.5515, -0.5692,  0.9200])\n",
      "Word index: 12, Embedding: tensor([-1.6293, -0.5497, -0.4798, -0.4997, -1.0670,  1.1149])\n",
      "Word index: 18, Embedding: tensor([ 0.0930, -0.6661,  0.6080, -0.7300,  1.3750,  0.6596])\n",
      "Word index: 12, Embedding: tensor([-1.6293, -0.5497, -0.4798, -0.4997, -1.0670,  1.1149])\n",
      "Word index: 2, Embedding: tensor([ 0.1198,  1.2377,  1.1168, -0.2473, -1.3527, -1.6959])\n",
      "Word index: 9, Embedding: tensor([ 0.5433, -0.3952, -0.4462,  0.7440,  1.5210,  3.4105])\n",
      "Word index: 2, Embedding: tensor([ 0.1198,  1.2377,  1.1168, -0.2473, -1.3527, -1.6959])\n",
      "Word index: 10, Embedding: tensor([-1.5312, -1.2341,  1.8197, -0.5515, -0.5692,  0.9200])\n",
      "Word index: 14, Embedding: tensor([ 0.8419, -0.4000,  1.0395,  0.3582, -0.2460,  2.3025])\n",
      "Word index: 15, Embedding: tensor([-1.8817, -0.0497, -1.0450, -0.9565,  0.0335,  0.7101])\n",
      "Word index: 13, Embedding: tensor([-0.1407,  0.8058, -0.0933,  0.6871, -0.8383,  0.0009])\n",
      "Word index: 1, Embedding: tensor([-0.3160, -2.1152,  0.3223, -1.2633,  0.3500,  0.3081])\n",
      "Word index: 9, Embedding: tensor([ 0.5433, -0.3952, -0.4462,  0.7440,  1.5210,  3.4105])\n",
      "Word index: 17, Embedding: tensor([-0.2282,  0.2800, -0.7015,  1.0367, -0.6037, -1.2788])\n",
      "Word index: 19, Embedding: tensor([ 0.4766, -1.0163,  0.1804,  0.1083, -0.7548,  0.2443])\n",
      "Word index: 16, Embedding: tensor([ 1.6459, -1.3602,  0.3446,  0.5199, -2.6133, -1.6965])\n",
      "Word index: 5, Embedding: tensor([ 0.9463, -0.8437, -0.6136,  0.0316, -0.4927,  0.2484])\n",
      "Word index: 7, Embedding: tensor([-0.2897,  0.0525,  0.5229,  2.3022, -1.4689, -1.5867])\n",
      "Word index: 19, Embedding: tensor([ 0.4766, -1.0163,  0.1804,  0.1083, -0.7548,  0.2443])\n",
      "Word index: 17, Embedding: tensor([-0.2282,  0.2800, -0.7015,  1.0367, -0.6037, -1.2788])\n",
      "Word index: 4, Embedding: tensor([ 0.7502, -0.5855, -0.1734,  0.1835,  1.3894,  1.5863])\n",
      "Word index: 5, Embedding: tensor([ 0.9463, -0.8437, -0.6136,  0.0316, -0.4927,  0.2484])\n",
      "Word index: 1, Embedding: tensor([-0.3160, -2.1152,  0.3223, -1.2633,  0.3500,  0.3081])\n",
      "Word index: 4, Embedding: tensor([ 0.7502, -0.5855, -0.1734,  0.1835,  1.3894,  1.5863])\n",
      "Word index: 12, Embedding: tensor([-1.6293, -0.5497, -0.4798, -0.4997, -1.0670,  1.1149])\n",
      "Word index: 6, Embedding: tensor([ 0.4397,  0.1124,  0.6408,  0.4412, -0.1023,  0.7924])\n",
      "Word index: 5, Embedding: tensor([ 0.9463, -0.8437, -0.6136,  0.0316, -0.4927,  0.2484])\n",
      "Word index: 7, Embedding: tensor([-0.2897,  0.0525,  0.5229,  2.3022, -1.4689, -1.5867])\n",
      "Word index: 1, Embedding: tensor([-0.3160, -2.1152,  0.3223, -1.2633,  0.3500,  0.3081])\n",
      "Word index: 10, Embedding: tensor([-1.5312, -1.2341,  1.8197, -0.5515, -0.5692,  0.9200])\n",
      "Word index: 9, Embedding: tensor([ 0.5433, -0.3952, -0.4462,  0.7440,  1.5210,  3.4105])\n",
      "\n",
      "torch.Size([4, 10, 6])\n",
      "PE of src data:\n",
      "tensor([[[ 0.0000,  1.0000,  0.0000,  1.0000,  0.0000,  1.0000]],\n",
      "\n",
      "        [[ 0.8415,  0.5403,  0.0464,  0.9989,  0.0022,  1.0000]],\n",
      "\n",
      "        [[ 0.9093, -0.4161,  0.0927,  0.9957,  0.0043,  1.0000]],\n",
      "\n",
      "        [[ 0.1411, -0.9900,  0.1388,  0.9903,  0.0065,  1.0000]]])\n",
      "\n",
      "PE source embeddings : \n",
      "\n",
      "tensor([[[ 0.1198,  2.2377,  1.1168,  0.7527, -1.3527, -0.6959],\n",
      "         [-1.8817,  0.9503, -1.0450,  0.0435,  0.0335,  1.7101],\n",
      "         [ 0.5433,  0.6048, -0.4462,  1.7440,  1.5210,  4.4105],\n",
      "         [ 1.6459, -0.3602,  0.3446,  1.5199, -2.6133, -0.6965],\n",
      "         [ 0.9463,  0.1563, -0.6136,  1.0316, -0.4927,  1.2484],\n",
      "         [ 0.7502,  0.4145, -0.1734,  1.1835,  1.3894,  2.5863],\n",
      "         [-1.6293,  0.4503, -0.4798,  0.5003, -1.0670,  2.1149],\n",
      "         [ 1.1108,  2.2899, -1.4782,  3.5672, -0.4731,  1.3356],\n",
      "         [-0.2897,  1.0525,  0.5229,  3.3022, -1.4689, -0.5867],\n",
      "         [-1.5312, -0.2341,  1.8197,  0.4485, -0.5692,  1.9200]],\n",
      "\n",
      "        [[-0.7879, -0.0094, -0.4334,  0.4992, -1.0648,  2.1149],\n",
      "         [ 0.9344, -0.1258,  0.6544,  0.2689,  1.3772,  1.6596],\n",
      "         [-0.7879, -0.0094, -0.4334,  0.4992, -1.0648,  2.1149],\n",
      "         [ 0.9613,  1.7780,  1.1632,  0.7516, -1.3505, -0.6959],\n",
      "         [ 1.3848,  0.1451, -0.3998,  1.7429,  1.5231,  4.4105],\n",
      "         [ 0.9613,  1.7780,  1.1632,  0.7516, -1.3505, -0.6959],\n",
      "         [-0.6897, -0.6938,  1.8661,  0.4474, -0.5671,  1.9200],\n",
      "         [ 1.6834,  0.1403,  1.0859,  1.3571, -0.2438,  3.3025],\n",
      "         [-1.0402,  0.4906, -0.9986,  0.0424,  0.0357,  1.7101],\n",
      "         [ 0.7008,  1.3461, -0.0469,  1.6860, -0.8362,  1.0009]],\n",
      "\n",
      "        [[ 0.5933, -2.5314,  0.4150, -0.2676,  0.3543,  1.3081],\n",
      "         [ 1.4526, -0.8113, -0.3535,  1.7397,  1.5253,  4.4105],\n",
      "         [ 0.6811, -0.1362, -0.6088,  2.0324, -0.5994, -0.2788],\n",
      "         [ 1.3859, -1.4325,  0.2731,  1.1040, -0.7505,  1.2443],\n",
      "         [ 2.5552, -1.7763,  0.4373,  1.5156, -2.6090, -0.6965],\n",
      "         [ 1.8556, -1.2598, -0.5209,  1.0273, -0.4884,  1.2484],\n",
      "         [ 0.6196, -0.3636,  0.6156,  3.2979, -1.4646, -0.5867],\n",
      "         [ 1.3859, -1.4325,  0.2731,  1.1040, -0.7505,  1.2443],\n",
      "         [ 0.6811, -0.1362, -0.6088,  2.0324, -0.5994, -0.2788],\n",
      "         [ 1.6595, -1.0016, -0.0807,  1.1792,  1.3937,  2.5863]],\n",
      "\n",
      "        [[ 1.0874, -1.8337, -0.4748,  1.0219, -0.4862,  1.2484],\n",
      "         [-0.1749, -3.1052,  0.4611, -0.2730,  0.3564,  1.3081],\n",
      "         [ 0.8913, -1.5755, -0.0346,  1.1738,  1.3958,  2.5863],\n",
      "         [-1.4882, -1.5397, -0.3410,  0.4906, -1.0605,  2.1149],\n",
      "         [ 0.5808, -0.8776,  0.7796,  1.4315, -0.0958,  1.7924],\n",
      "         [ 1.0874, -1.8337, -0.4748,  1.0219, -0.4862,  1.2484],\n",
      "         [-0.1485, -0.9375,  0.6617,  3.2925, -1.4624, -0.5867],\n",
      "         [-0.1749, -3.1052,  0.4611, -0.2730,  0.3564,  1.3081],\n",
      "         [-1.3901, -2.2241,  1.9585,  0.4388, -0.5628,  1.9200],\n",
      "         [ 0.6844, -1.3851, -0.3074,  1.7343,  1.5274,  4.4105]]])\n",
      "\n",
      "###########################################################################\n",
      "### Decoder Start ###\n",
      "\n",
      "Q_dec_0 = \n",
      "tensor([[[-1.4552,  0.5793,  1.1043],\n",
      "         [-0.4717,  0.8204, -0.3588],\n",
      "         [ 1.6194, -0.1178, -1.2677],\n",
      "         [ 1.0439,  0.4257, -0.1776]],\n",
      "\n",
      "        [[-0.2463, -0.9246,  0.0794],\n",
      "         [ 1.0109,  1.6849, -0.4990],\n",
      "         [-0.2970,  0.9185, -0.1552],\n",
      "         [-0.1077,  1.3278, -0.1918]],\n",
      "\n",
      "        [[-0.9463,  0.2863, -0.5349],\n",
      "         [ 1.1548, -0.0225, -0.8391],\n",
      "         [ 1.8677,  0.6317, -1.2890],\n",
      "         [ 1.5479, -0.4095, -1.7861]],\n",
      "\n",
      "        [[ 1.5734,  1.4188, -0.4736],\n",
      "         [ 0.0722,  0.0465,  0.0289],\n",
      "         [ 0.9781,  2.0269, -0.3672],\n",
      "         [-0.1849,  1.3902, -0.3299]],\n",
      "\n",
      "        [[ 0.9250,  0.6758, -1.1312],\n",
      "         [-0.4717,  0.8204, -0.3588],\n",
      "         [-0.1710,  0.1705,  0.7718],\n",
      "         [ 1.7276, -0.0578, -1.3506]],\n",
      "\n",
      "        [[ 1.5125,  1.9137, -0.4423],\n",
      "         [ 1.0109,  1.6849, -0.4990],\n",
      "         [-0.1649,  0.6675, -0.0899],\n",
      "         [ 0.3316,  1.4646, -0.2622]],\n",
      "\n",
      "        [[-0.3673,  1.0187,  1.5554],\n",
      "         [-0.9296,  0.6939,  1.2640],\n",
      "         [ 0.9377,  0.5774, -0.0895],\n",
      "         [-0.1260,  0.3699, -1.1947]],\n",
      "\n",
      "        [[-0.9398,  0.1094,  0.0388],\n",
      "         [-0.5838, -1.0991,  0.2025],\n",
      "         [-0.3567,  0.9347, -0.1394],\n",
      "         [ 0.9261,  2.4443, -0.7218]],\n",
      "\n",
      "        [[ 0.1726,  0.7615,  0.4986],\n",
      "         [ 1.4505,  0.7904, -0.9715],\n",
      "         [ 0.5754,  0.9746,  1.3975],\n",
      "         [ 0.6947,  0.2654, -0.7723]],\n",
      "\n",
      "        [[ 0.3146,  0.7429, -0.0921],\n",
      "         [ 1.1751,  1.7392, -0.3192],\n",
      "         [-1.4742,  0.2226,  0.1139],\n",
      "         [ 0.0086,  1.1295, -0.3056]],\n",
      "\n",
      "        [[ 0.8563,  0.2781, -0.6744],\n",
      "         [-0.9296,  0.6939,  1.2640],\n",
      "         [ 1.1153,  0.7174,  0.3408],\n",
      "         [ 1.0439,  0.4257, -0.1776]],\n",
      "\n",
      "        [[ 0.7538,  0.8798, -0.1626],\n",
      "         [-0.5838, -1.0991,  0.2025],\n",
      "         [-0.2198,  0.8561, -0.0170],\n",
      "         [-0.1077,  1.3278, -0.1918]],\n",
      "\n",
      "        [[-0.9973,  0.7058, -0.5184],\n",
      "         [ 0.1234,  0.1942, -1.4879],\n",
      "         [-0.5224,  0.1084,  0.6557],\n",
      "         [-0.5938, -0.1834,  0.1373]],\n",
      "\n",
      "        [[ 1.3484,  1.8594, -0.6221],\n",
      "         [ 0.1284,  1.0253, -0.4746],\n",
      "         [-0.6820,  0.8819, -0.2641],\n",
      "         [-0.5698,  1.3535, -0.4389]],\n",
      "\n",
      "        [[-0.9163,  1.0306,  1.6244],\n",
      "         [ 0.9064,  1.1511, -0.4595],\n",
      "         [ 0.9377,  0.5774, -0.0895],\n",
      "         [ 1.5479, -0.4095, -1.7861]],\n",
      "\n",
      "        [[ 0.8897,  1.1200, -0.1834],\n",
      "         [ 0.2545,  0.9119, -0.2173],\n",
      "         [-0.3567,  0.9347, -0.1394],\n",
      "         [-0.1849,  1.3902, -0.3299]],\n",
      "\n",
      "        [[-1.4651,  0.1525,  0.8135],\n",
      "         [-0.4207,  0.4009, -0.3753],\n",
      "         [-0.1710,  0.1705,  0.7718],\n",
      "         [ 0.4690, -0.2563, -2.3238]],\n",
      "\n",
      "        [[-0.1476,  0.7686, -0.3392],\n",
      "         [ 1.2359,  1.2443, -0.3505],\n",
      "         [-0.1649,  0.6675, -0.0899],\n",
      "         [ 0.0436,  1.7847, -0.6973]],\n",
      "\n",
      "        [[-0.4022,  0.0795, -1.6476],\n",
      "         [-0.6015,  0.8501,  0.8121],\n",
      "         [ 1.7990,  0.2340, -0.8322],\n",
      "         [ 1.7962,  0.3400, -1.8074]],\n",
      "\n",
      "        [[ 0.4659,  1.1998, -0.5977],\n",
      "         [ 0.3264,  0.4492, -0.1213],\n",
      "         [ 0.2194,  0.9930, -0.0875],\n",
      "         [ 1.0903,  2.4986, -0.5420]]])\n",
      "\n",
      "K_dec_0 = \n",
      "tensor([[[-0.7554,  1.7286, -0.8978],\n",
      "         [-1.2473,  0.9376,  1.1841],\n",
      "         [-0.6435, -1.0317,  0.6396],\n",
      "         [-0.9058, -0.4974,  0.3891]],\n",
      "\n",
      "        [[ 1.0559,  0.9876, -0.9088],\n",
      "         [-1.3301,  0.4544, -0.7485],\n",
      "         [-1.2680, -0.5859,  1.5287],\n",
      "         [-1.7719, -0.1159,  0.4630]],\n",
      "\n",
      "        [[-0.2158,  0.7585,  1.4128],\n",
      "         [-0.4551, -0.1594,  0.2473],\n",
      "         [-1.5484, -0.0611,  1.3556],\n",
      "         [-0.6931, -1.2807,  0.9873]],\n",
      "\n",
      "        [[-0.6221,  0.3783, -1.4847],\n",
      "         [-0.1606, -0.2279,  0.2952],\n",
      "         [-2.1476, -0.0749, -0.4453],\n",
      "         [-1.5567, -0.5022,  1.6273]],\n",
      "\n",
      "        [[-1.4214,  0.6571,  1.4936],\n",
      "         [-1.2473,  0.9376,  1.1841],\n",
      "         [-0.4345,  0.0176, -0.3898],\n",
      "         [-0.8836, -0.7651,  0.8993]],\n",
      "\n",
      "        [[-1.6008,  0.3131, -1.3995],\n",
      "         [-1.3301,  0.4544, -0.7485],\n",
      "         [-0.8830,  0.5938, -0.7997],\n",
      "         [-1.6327, -0.2086,  0.2832]],\n",
      "\n",
      "        [[-1.3343,  0.8064, -1.0069],\n",
      "         [-0.8047,  1.4784, -1.1399],\n",
      "         [-1.3003, -0.0599,  0.0913],\n",
      "         [-1.3746,  0.2206,  1.6358]],\n",
      "\n",
      "        [[-0.7735,  0.6453,  0.1352],\n",
      "         [ 0.9175,  0.7486, -0.4527],\n",
      "         [-1.4286,  0.0767,  0.5807],\n",
      "         [-2.0272,  0.3891, -0.1518]],\n",
      "\n",
      "        [[-0.7292,  0.4698,  0.1795],\n",
      "         [-1.4707,  0.4070,  1.2515],\n",
      "         [-1.4613,  0.0881, -1.1450],\n",
      "         [-1.5041,  0.1916,  0.4196]],\n",
      "\n",
      "        [[-0.9364,  0.1884, -0.5898],\n",
      "         [-1.7391,  0.0741, -0.9433],\n",
      "         [-1.3203,  0.2574,  1.0893],\n",
      "         [-1.2258,  0.4304,  0.1108]],\n",
      "\n",
      "        [[-0.7070,  0.2021,  0.6897],\n",
      "         [-0.8047,  1.4784, -1.1399],\n",
      "         [-0.8561, -0.2485,  0.0415],\n",
      "         [-0.9058, -0.4974,  0.3891]],\n",
      "\n",
      "        [[-0.7972,  0.0957, -0.7696],\n",
      "         [ 0.9175,  0.7486, -0.4527],\n",
      "         [-1.4832, -0.1995,  0.3643],\n",
      "         [-1.7719, -0.1159,  0.4630]],\n",
      "\n",
      "        [[-1.1980,  1.1878,  1.4262],\n",
      "         [-1.8970,  0.7462,  0.7866],\n",
      "         [-1.4836,  0.4451, -0.7665],\n",
      "         [-1.5333,  0.1961, -0.4188]],\n",
      "\n",
      "        [[-1.1918,  0.6934, -1.2046],\n",
      "         [-0.7302,  0.6129,  0.4804],\n",
      "         [-1.0995,  1.3892, -0.7659],\n",
      "         [-1.3881,  1.4728, -0.6673]],\n",
      "\n",
      "        [[-0.9475,  1.4121, -0.1886],\n",
      "         [-2.2158,  1.1479,  0.4586],\n",
      "         [-1.3003, -0.0599,  0.0913],\n",
      "         [-0.6931, -1.2807,  0.9873]],\n",
      "\n",
      "        [[-1.0630,  1.2727, -2.9474],\n",
      "         [-1.1509,  0.3941, -0.0855],\n",
      "         [-1.4286,  0.0767,  0.5807],\n",
      "         [-1.5567, -0.5022,  1.6273]],\n",
      "\n",
      "        [[-1.3567,  1.1633, -0.6284],\n",
      "         [-0.2651,  0.5083,  1.1707],\n",
      "         [-0.4345,  0.0176, -0.3898],\n",
      "         [-2.0243,  0.0292,  1.2384]],\n",
      "\n",
      "        [[-0.5527,  1.7772, -1.7201],\n",
      "         [-0.7605,  0.1393, -1.0286],\n",
      "         [-0.8830,  0.5938, -0.7997],\n",
      "         [-1.4273,  0.5476,  1.0771]],\n",
      "\n",
      "        [[-1.8477,  0.9964,  1.0288],\n",
      "         [-1.0838,  1.2662, -0.1494],\n",
      "         [-0.8340, -0.5162,  0.5516],\n",
      "         [-1.5980, -0.3101,  1.7033]],\n",
      "\n",
      "        [[-0.5919,  0.8519,  0.0243],\n",
      "         [-0.4420,  0.8095, -1.2754],\n",
      "         [-1.3440, -0.2922,  0.1845],\n",
      "         [-2.4362,  0.0088, -0.3467]]])\n",
      "\n",
      "V_dec_0 = \n",
      "tensor([[[ 9.9826e-01,  1.1588e-01,  1.0535e+00],\n",
      "         [-1.2033e-01,  1.2405e+00,  8.6981e-02],\n",
      "         [-4.5990e-01, -6.0168e-01, -5.5067e-01],\n",
      "         [-8.3972e-01, -6.8078e-01, -4.2016e-01]],\n",
      "\n",
      "        [[ 4.7661e-01, -8.0867e-01, -1.3011e+00],\n",
      "         [ 3.9194e-01,  2.3666e-01,  1.1554e+00],\n",
      "         [-1.6172e-01,  1.3981e+00,  1.1582e+00],\n",
      "         [ 4.3451e-01,  1.7881e-01,  1.7117e+00]],\n",
      "\n",
      "        [[-2.0628e-01,  1.8654e+00, -8.2431e-02],\n",
      "         [-7.3881e-01, -6.5794e-01,  3.2566e-01],\n",
      "         [-2.1049e+00, -5.7002e-01,  1.5704e-01],\n",
      "         [-2.7632e-01, -3.6041e-01, -9.6534e-01]],\n",
      "\n",
      "        [[-2.3835e-01, -1.2486e-01,  8.0209e-01],\n",
      "         [-3.2283e-01,  1.5291e-01,  7.0044e-01],\n",
      "         [-8.3138e-02, -2.2078e-01,  3.0860e+00],\n",
      "         [-5.6613e-01,  1.8912e+00,  1.3184e+00]],\n",
      "\n",
      "        [[-1.8096e+00,  2.2229e-01,  4.4281e-01],\n",
      "         [-1.2033e-01,  1.2405e+00,  8.6981e-02],\n",
      "         [-5.0765e-01, -8.0769e-01, -2.7915e-01],\n",
      "         [-1.4173e+00, -8.3921e-01, -3.8387e-01]],\n",
      "\n",
      "        [[-2.4140e-01, -6.3873e-01,  2.6339e+00],\n",
      "         [ 3.9194e-01,  2.3666e-01,  1.1554e+00],\n",
      "         [ 1.8098e-01, -1.0631e+00,  9.4828e-01],\n",
      "         [-4.9601e-01,  3.4702e-01,  2.2125e+00]],\n",
      "\n",
      "        [[ 3.6765e-01, -9.3427e-01,  3.6855e-01],\n",
      "         [ 7.6500e-01, -4.1629e-01,  1.1082e+00],\n",
      "         [-5.1538e-01, -8.7015e-01, -2.8922e-02],\n",
      "         [ 1.1502e-03,  1.2217e+00, -6.6814e-01]],\n",
      "\n",
      "        [[ 1.3489e+00, -5.9482e-01,  1.1218e-01],\n",
      "         [ 7.4823e-01, -8.1928e-01, -1.1521e+00],\n",
      "         [ 5.3999e-01,  2.3188e-01,  1.2887e+00],\n",
      "         [-1.2583e-01,  1.1584e+00,  1.6186e+00]],\n",
      "\n",
      "        [[-7.2794e-01, -1.2974e-01,  2.8027e-01],\n",
      "         [-2.0428e+00, -3.0987e-01,  4.9750e-01],\n",
      "         [ 7.2291e-02, -1.7266e+00,  8.2789e-02],\n",
      "         [-4.0084e-01, -6.0786e-01, -2.4693e-03]],\n",
      "\n",
      "        [[ 6.8065e-01, -7.3234e-01,  1.0993e+00],\n",
      "         [ 3.0223e-02, -6.4934e-01,  2.7830e+00],\n",
      "         [ 1.5072e+00, -1.7686e-01,  5.6428e-01],\n",
      "         [-1.6253e-01,  3.6745e-01,  1.2956e+00]],\n",
      "\n",
      "        [[-1.3055e+00, -2.8817e-01,  3.1656e-01],\n",
      "         [ 7.6500e-01, -4.1629e-01,  1.1082e+00],\n",
      "         [-1.0233e+00, -9.2205e-01, -5.4934e-03],\n",
      "         [-8.3972e-01, -6.8078e-01, -4.2016e-01]],\n",
      "\n",
      "        [[-2.4987e-01, -5.6413e-01,  1.6002e+00],\n",
      "         [ 7.4823e-01, -8.1928e-01, -1.1521e+00],\n",
      "         [ 8.3892e-01, -3.1438e-01,  1.5514e+00],\n",
      "         [ 4.3451e-01,  1.7881e-01,  1.7117e+00]],\n",
      "\n",
      "        [[ 1.1293e-01,  1.7727e+00,  3.2291e-02],\n",
      "         [ 6.5525e-01,  3.0311e-01,  1.7769e-01],\n",
      "         [ 1.2854e-01, -1.3842e+00, -2.9655e-01],\n",
      "         [ 3.1211e-01, -1.1429e+00, -7.1122e-01]],\n",
      "\n",
      "        [[ 1.2031e-01,  2.4727e-01,  1.0063e+00],\n",
      "         [-4.4895e-01,  1.4830e+00,  3.9679e-01],\n",
      "         [-8.6533e-02, -8.1345e-01,  8.3516e-01],\n",
      "         [-4.9094e-01, -3.2026e-01,  9.9539e-01]],\n",
      "\n",
      "        [[-1.3803e+00, -3.4877e-01,  5.7254e-01],\n",
      "         [-7.2173e-01, -4.9650e-01,  9.9179e-01],\n",
      "         [-5.1538e-01, -8.7015e-01, -2.8922e-02],\n",
      "         [-2.7632e-01, -3.6041e-01, -9.6534e-01]],\n",
      "\n",
      "        [[ 6.4705e-01, -2.9206e+00,  1.7819e+00],\n",
      "         [ 6.0714e-01, -1.2490e-02,  1.4317e+00],\n",
      "         [ 5.3999e-01,  2.3188e-01,  1.2887e+00],\n",
      "         [-5.6613e-01,  1.8912e+00,  1.3184e+00]],\n",
      "\n",
      "        [[ 4.2389e-01, -5.9185e-01, -1.0787e-02],\n",
      "         [-4.3955e-01,  1.3332e+00, -2.7741e-02],\n",
      "         [-5.0765e-01, -8.0769e-01, -2.7915e-01],\n",
      "         [ 7.7673e-01,  2.8424e-01, -5.7743e-01]],\n",
      "\n",
      "        [[-2.4479e-01, -1.2314e+00,  3.8306e-01],\n",
      "         [ 3.3270e-02, -1.3547e-01,  9.5114e-01],\n",
      "         [ 1.8098e-01, -1.0631e+00,  9.4828e-01],\n",
      "         [-9.6672e-01,  2.4048e+00,  8.6006e-01]],\n",
      "\n",
      "        [[ 8.8852e-01,  8.3528e-01,  1.2300e-01],\n",
      "         [-2.9830e-01, -6.6192e-02,  6.7677e-01],\n",
      "         [-1.6009e+00, -1.0805e+00,  3.0794e-02],\n",
      "         [-1.9214e+00, -3.2874e-01, -2.5762e-01]],\n",
      "\n",
      "        [[-7.2057e-01,  1.4936e+00,  2.4773e-01],\n",
      "         [ 5.8927e-01, -1.1738e+00,  5.9630e-01],\n",
      "         [-9.1605e-02, -1.4617e-01,  2.0523e+00],\n",
      "         [-4.8755e-01,  2.7241e-01,  3.2463e+00]]])\n",
      "\n",
      "Attnetion mask infunction = \n",
      "tensor([[[[0., -inf, -inf, -inf],\n",
      "          [0., 0., -inf, -inf],\n",
      "          [0., 0., 0., -inf],\n",
      "          [0., 0., 0., 0.]]]])\n",
      "\n",
      "Attention output = \n",
      "torch.Size([10, 2, 4, 4]) tensor([[[[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.6619, 0.3381, 0.0000, 0.0000],\n",
      "          [0.6329, 0.0919, 0.2752, 0.0000],\n",
      "          [0.4130, 0.2042, 0.1915, 0.1913]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.8763, 0.1237, 0.0000, 0.0000],\n",
      "          [0.3790, 0.4240, 0.1970, 0.0000],\n",
      "          [0.4064, 0.3077, 0.1073, 0.1786]]],\n",
      "\n",
      "\n",
      "        [[[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.3972, 0.6028, 0.0000, 0.0000],\n",
      "          [0.4000, 0.5264, 0.0736, 0.0000],\n",
      "          [0.1570, 0.5240, 0.0615, 0.2575]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.4918, 0.5082, 0.0000, 0.0000],\n",
      "          [0.6108, 0.2674, 0.1218, 0.0000],\n",
      "          [0.4185, 0.1745, 0.2809, 0.1261]]],\n",
      "\n",
      "\n",
      "        [[[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.4627, 0.5373, 0.0000, 0.0000],\n",
      "          [0.4448, 0.3915, 0.1637, 0.0000],\n",
      "          [0.0581, 0.0871, 0.6894, 0.1655]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.4731, 0.5269, 0.0000, 0.0000],\n",
      "          [0.3329, 0.3312, 0.3358, 0.0000],\n",
      "          [0.2564, 0.2758, 0.3407, 0.1271]]],\n",
      "\n",
      "\n",
      "        [[[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.5280, 0.4720, 0.0000, 0.0000],\n",
      "          [0.2942, 0.4937, 0.2121, 0.0000],\n",
      "          [0.3638, 0.4428, 0.1414, 0.0520]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.6691, 0.3309, 0.0000, 0.0000],\n",
      "          [0.3854, 0.3016, 0.3131, 0.0000],\n",
      "          [0.1882, 0.6869, 0.0494, 0.0756]]],\n",
      "\n",
      "\n",
      "        [[[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.7775, 0.2225, 0.0000, 0.0000],\n",
      "          [0.3323, 0.5955, 0.0722, 0.0000],\n",
      "          [0.2979, 0.1359, 0.3782, 0.1880]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.6443, 0.3557, 0.0000, 0.0000],\n",
      "          [0.2238, 0.4266, 0.3496, 0.0000],\n",
      "          [0.2636, 0.2594, 0.2046, 0.2724]]],\n",
      "\n",
      "\n",
      "        [[[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.6838, 0.3162, 0.0000, 0.0000],\n",
      "          [0.3603, 0.4006, 0.2391, 0.0000],\n",
      "          [0.2380, 0.3705, 0.2081, 0.1834]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.7221, 0.2779, 0.0000, 0.0000],\n",
      "          [0.3290, 0.3643, 0.3067, 0.0000],\n",
      "          [0.2529, 0.3621, 0.1856, 0.1993]]],\n",
      "\n",
      "\n",
      "        [[[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.3893, 0.6107, 0.0000, 0.0000],\n",
      "          [0.4173, 0.3934, 0.1893, 0.0000],\n",
      "          [0.2301, 0.2913, 0.2308, 0.2477]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.6166, 0.3834, 0.0000, 0.0000],\n",
      "          [0.3443, 0.2131, 0.4426, 0.0000],\n",
      "          [0.2113, 0.1112, 0.3159, 0.3617]]],\n",
      "\n",
      "\n",
      "        [[[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.7332, 0.2668, 0.0000, 0.0000],\n",
      "          [0.5144, 0.2292, 0.2564, 0.0000],\n",
      "          [0.3765, 0.0662, 0.2916, 0.2657]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.6973, 0.3027, 0.0000, 0.0000],\n",
      "          [0.5184, 0.2610, 0.2207, 0.0000],\n",
      "          [0.6256, 0.1809, 0.1272, 0.0664]]],\n",
      "\n",
      "\n",
      "        [[[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.6914, 0.3086, 0.0000, 0.0000],\n",
      "          [0.2643, 0.4960, 0.2397, 0.0000],\n",
      "          [0.4315, 0.0572, 0.4765, 0.0348]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.8123, 0.1877, 0.0000, 0.0000],\n",
      "          [0.4658, 0.2438, 0.2904, 0.0000],\n",
      "          [0.6989, 0.0973, 0.1414, 0.0624]]],\n",
      "\n",
      "\n",
      "        [[[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.6649, 0.3351, 0.0000, 0.0000],\n",
      "          [0.1254, 0.5063, 0.3683, 0.0000],\n",
      "          [0.0772, 0.6145, 0.2700, 0.0383]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.4730, 0.5270, 0.0000, 0.0000],\n",
      "          [0.3952, 0.4198, 0.1850, 0.0000],\n",
      "          [0.3609, 0.5604, 0.0410, 0.0376]]]])\n",
      "Decoder Self Attention = \n",
      "tensor([[ 9.9826e-01,  1.1588e-01,  1.0535e+00,  4.7661e-01, -8.0867e-01,\n",
      "         -1.3011e+00],\n",
      "        [-2.0628e-01,  1.8654e+00, -8.2431e-02, -2.3835e-01, -1.2486e-01,\n",
      "          8.0209e-01],\n",
      "        [-1.8096e+00,  2.2229e-01,  4.4281e-01, -2.4140e-01, -6.3873e-01,\n",
      "          2.6339e+00],\n",
      "        [ 3.6765e-01, -9.3427e-01,  3.6855e-01,  1.3489e+00, -5.9482e-01,\n",
      "          1.1218e-01],\n",
      "        [-7.2794e-01, -1.2974e-01,  2.8027e-01,  6.8065e-01, -7.3234e-01,\n",
      "          1.0993e+00],\n",
      "        [-1.3055e+00, -2.8817e-01,  3.1656e-01, -2.4987e-01, -5.6413e-01,\n",
      "          1.6002e+00],\n",
      "        [ 1.1293e-01,  1.7727e+00,  3.2291e-02,  1.2031e-01,  2.4727e-01,\n",
      "          1.0063e+00],\n",
      "        [-1.3803e+00, -3.4877e-01,  5.7254e-01,  6.4705e-01, -2.9206e+00,\n",
      "          1.7819e+00],\n",
      "        [ 4.2389e-01, -5.9185e-01, -1.0787e-02, -2.4479e-01, -1.2314e+00,\n",
      "          3.8306e-01],\n",
      "        [ 8.8852e-01,  8.3528e-01,  1.2300e-01, -7.2057e-01,  1.4936e+00,\n",
      "          2.4773e-01],\n",
      "        [ 6.2011e-01,  4.9608e-01,  7.2676e-01,  4.6613e-01, -6.7932e-01,\n",
      "         -9.9714e-01],\n",
      "        [-5.2727e-01,  3.4440e-01,  1.6355e-01, -2.8128e-01,  1.6291e-02,\n",
      "          7.5043e-01],\n",
      "        [-9.0192e-01,  7.6941e-01,  2.5162e-01,  9.2331e-02, -1.7745e-01,\n",
      "          1.8548e+00],\n",
      "        [ 5.5521e-01, -6.8977e-01,  7.1768e-01,  1.1502e+00, -6.6909e-01,\n",
      "         -3.0614e-01],\n",
      "        [-1.0206e+00, -1.6983e-01,  3.2861e-01,  4.4932e-01, -7.0282e-01,\n",
      "          1.6982e+00],\n",
      "        [-6.5085e-01, -3.2868e-01,  5.6686e-01,  2.7456e-02, -6.3502e-01,\n",
      "          8.3549e-01],\n",
      "        [ 4.4411e-01,  8.7526e-01,  1.2108e-01, -9.7940e-02,  7.2106e-01,\n",
      "          7.7263e-01],\n",
      "        [-1.2046e+00, -3.8818e-01,  6.8439e-01,  6.3497e-01, -2.0404e+00,\n",
      "          1.6759e+00],\n",
      "        [ 1.5741e-01,  2.2750e-03, -1.6019e-02, -1.9260e-01, -1.0257e+00,\n",
      "          4.8970e-01],\n",
      "        [ 4.9083e-01,  5.3321e-01,  3.0856e-01, -3.0233e-02,  8.7800e-02,\n",
      "          4.3144e-01],\n",
      "        [ 4.9420e-01,  2.1751e-02,  5.2324e-01,  3.1495e-01,  6.9270e-02,\n",
      "          2.2487e-01],\n",
      "        [-6.2629e-01,  3.5793e-01,  1.5000e-01, -2.4203e-01, -6.2279e-02,\n",
      "          1.0532e+00],\n",
      "        [-9.3508e-01,  4.5238e-01,  1.8532e-01,  1.1023e-01, -4.9129e-01,\n",
      "          1.5781e+00],\n",
      "        [ 3.7652e-01, -6.6495e-01,  6.4940e-01,  9.1454e-01, -4.0370e-01,\n",
      "          9.9244e-02],\n",
      "        [-1.4532e+00, -3.5230e-01,  3.9537e-01,  6.9210e-01, -5.0275e-01,\n",
      "          1.6306e+00],\n",
      "        [-4.0869e-01, -4.9105e-01,  5.5665e-01,  4.4769e-01, -5.8048e-01,\n",
      "          5.8262e-01],\n",
      "        [ 3.2924e-01,  5.9695e-01,  2.7240e-02, -9.2551e-02,  4.1197e-02,\n",
      "          8.0067e-01],\n",
      "        [-1.0075e+00, -5.1632e-01,  5.1444e-01,  6.1301e-01, -1.4661e+00,\n",
      "          1.5817e+00],\n",
      "        [-2.2767e-01,  3.1115e-01, -8.3533e-02, -5.3348e-02, -9.1533e-01,\n",
      "          6.8572e-01],\n",
      "        [-6.2932e-01, -3.2680e-01,  3.6941e-01, -5.4347e-02,  7.0513e-02,\n",
      "          7.2785e-01],\n",
      "        [ 1.3900e-01,  5.5737e-02,  2.6703e-01,  3.7454e-01, -7.3843e-02,\n",
      "          2.5679e-01],\n",
      "        [-6.2008e-01, -1.7967e-01, -8.1173e-02, -2.5083e-01,  1.5089e-01,\n",
      "          1.4910e+00],\n",
      "        [-7.0000e-01, -5.7470e-01, -2.2267e-01,  4.4797e-02, -4.1657e-01,\n",
      "          1.5984e+00],\n",
      "        [ 3.9969e-01, -5.8368e-01,  5.8598e-01,  7.8495e-01, -5.7573e-01,\n",
      "         -5.8434e-01],\n",
      "        [-5.4253e-01, -8.4796e-01,  1.8196e-01,  4.5143e-01, -2.9758e-01,\n",
      "          1.4800e+00],\n",
      "        [-3.9431e-01, -5.3957e-01,  4.0771e-01,  4.5008e-01, -4.6205e-01,\n",
      "          6.1683e-01],\n",
      "        [ 3.2387e-01, -1.0630e-01, -1.8545e-01, -2.2939e-01, -1.5561e-01,\n",
      "          8.8052e-01],\n",
      "        [-7.9118e-01, -5.1365e-01,  1.6306e-02,  5.4569e-01, -1.6743e+00,\n",
      "          1.6250e+00],\n",
      "        [-5.7072e-02, -5.5414e-01, -1.5935e-01, -2.0263e-01, -8.7386e-01,\n",
      "          5.4804e-01],\n",
      "        [-6.2056e-01, -2.8055e-01,  4.2382e-01,  4.8039e-02, -1.1442e-01,\n",
      "          6.2996e-01]])\n",
      "\n",
      "Final Encoder 0 Output :\n",
      "norm2(norm1(x + self_atten(x)) + feed_fwd_op)\n",
      "\n",
      "tensor([[[-0.0326,  1.7392, -0.2331,  0.7041, -1.2516, -0.9259],\n",
      "         [-0.9507,  0.9642, -1.1234, -0.7091,  0.2984,  1.5206],\n",
      "         [-0.2402, -1.0165, -0.7649, -0.4848,  0.5649,  1.9416],\n",
      "         [ 1.4589, -0.2760, -0.0505,  0.9832, -1.6000, -0.5157],\n",
      "         [ 1.2051, -1.0559, -1.0730,  0.3423, -0.7080,  1.2895],\n",
      "         [-0.1103, -1.3751, -0.6142, -0.4670,  0.9609,  1.6057],\n",
      "         [-0.6938,  0.7205, -0.9127, -0.3655, -0.6489,  1.9004],\n",
      "         [ 0.0479,  0.1756, -1.4313,  1.9098, -0.4611, -0.2410],\n",
      "         [-0.5913,  0.5724, -0.0042,  1.8889, -0.8498, -1.0161],\n",
      "         [-1.2468,  0.3232,  0.5049, -0.4374, -0.9001,  1.7562]],\n",
      "\n",
      "        [[-0.6665,  0.1599, -1.1635,  0.6118, -0.7441,  1.8024],\n",
      "         [ 0.4361, -1.2489, -0.0906, -1.3202,  0.9961,  1.2275],\n",
      "         [-0.2635, -0.3130, -0.3963, -0.4429, -0.7883,  2.2040],\n",
      "         [ 0.8773,  1.2288,  0.1087,  0.4236, -1.4749, -1.1635],\n",
      "         [ 0.1498, -1.1532, -0.9436, -0.3592,  0.4572,  1.8491],\n",
      "         [ 0.8549,  1.0077,  0.7676,  0.0741, -1.3853, -1.3190],\n",
      "         [-0.5318, -0.6230,  0.9102, -0.7098, -0.8535,  1.8078],\n",
      "         [ 0.5117, -1.6991,  0.4789, -0.2073, -0.5812,  1.4970],\n",
      "         [-1.3023,  0.6131, -1.1757, -0.0883,  0.4151,  1.5380],\n",
      "         [ 0.2884,  1.1179, -1.3051,  0.8727, -1.4165,  0.4426]],\n",
      "\n",
      "        [[ 0.6224, -1.8781,  0.0631, -0.5594,  0.5342,  1.2178],\n",
      "         [ 0.0754, -1.2188, -0.8798, -0.3358,  0.5318,  1.8273],\n",
      "         [ 1.1716, -1.2042, -0.6291,  1.5455, -0.4085, -0.4754],\n",
      "         [ 1.1030, -1.7516, -0.2668,  0.5887, -0.6253,  0.9520],\n",
      "         [ 1.7270, -1.3856,  0.5061,  0.1522, -0.8989, -0.1007],\n",
      "         [ 1.2951, -1.7338, -0.5984,  0.2586, -0.1531,  0.9317],\n",
      "         [ 0.3664, -0.4340, -0.0459,  1.9558, -1.1979, -0.6443],\n",
      "         [ 0.9830, -2.0558,  0.5046,  0.0904, -0.2251,  0.7028],\n",
      "         [ 0.4728, -0.7170, -0.8595,  2.0124, -0.2797, -0.6291],\n",
      "         [ 0.5361, -1.6558, -0.6426, -0.3450,  0.7231,  1.3842]],\n",
      "\n",
      "        [[ 0.8343, -1.7325, -0.6701,  0.3055, -0.0625,  1.3252],\n",
      "         [ 0.2021, -1.8091,  0.5980, -0.7961,  0.7215,  1.0836],\n",
      "         [ 0.1598, -1.7972, -0.1352, -0.4417,  0.9083,  1.3061],\n",
      "         [-0.8401, -0.9482, -0.4497,  0.7193, -0.3635,  1.8823],\n",
      "         [ 0.3084, -1.9931,  0.6501,  0.2034, -0.3337,  1.1649],\n",
      "         [ 0.8183, -1.9582, -0.3300,  0.3236,  0.0139,  1.1323],\n",
      "         [-0.2472, -0.7398,  0.4074,  2.0321, -0.8696, -0.5829],\n",
      "         [ 0.1481, -1.9125,  0.7654, -0.6336,  0.8069,  0.8257],\n",
      "         [-0.9226, -1.4322,  1.3245, -0.1581,  0.0411,  1.1474],\n",
      "         [-0.1833, -1.3685, -0.7172, -0.1074,  0.5783,  1.7981]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "\n",
      "Q_dec_1 = \n",
      "tensor([[[-1.3781e+00,  5.1813e-01,  1.3760e+00],\n",
      "         [-4.7169e-01,  7.3859e-01, -1.9685e-02],\n",
      "         [ 1.4304e+00, -2.1024e-02, -9.5361e-01],\n",
      "         [ 1.1283e+00,  3.2774e-01, -3.6110e-01]],\n",
      "\n",
      "        [[ 4.3719e-02, -4.3613e-01,  6.2692e-02],\n",
      "         [ 1.1268e+00,  1.6428e+00, -4.1897e-01],\n",
      "         [-1.1123e-01,  6.1287e-01, -5.6832e-02],\n",
      "         [ 8.3040e-02,  1.1509e+00, -1.4177e-01]],\n",
      "\n",
      "        [[-4.3700e-01,  4.4051e-01, -1.9086e-01],\n",
      "         [ 1.3393e+00, -4.3944e-02, -9.4057e-01],\n",
      "         [ 9.5549e-01,  2.1994e-01, -7.7460e-01],\n",
      "         [ 1.3360e+00, -2.6279e-01, -1.3199e+00]],\n",
      "\n",
      "        [[ 1.2806e+00,  6.9407e-01, -1.9148e-01],\n",
      "         [ 1.1753e-01,  1.6679e-01,  5.2950e-02],\n",
      "         [ 6.3572e-01,  1.2028e+00, -2.0233e-01],\n",
      "         [-1.7304e-01,  4.1972e-01, -8.0005e-02]],\n",
      "\n",
      "        [[ 7.9707e-01,  1.9901e-01, -9.1764e-01],\n",
      "         [ 7.2358e-02,  8.9758e-01, -4.1059e-01],\n",
      "         [ 5.4724e-01,  2.1521e-02,  5.8433e-01],\n",
      "         [ 1.3356e+00, -2.3668e-01, -1.1932e+00]],\n",
      "\n",
      "        [[ 7.6717e-01,  1.2094e+00, -2.4325e-01],\n",
      "         [ 8.4002e-01,  1.2383e+00, -3.2330e-01],\n",
      "         [-5.3835e-01,  5.6694e-01,  1.3831e-02],\n",
      "         [ 1.3155e-01,  8.2879e-01, -1.3288e-01]],\n",
      "\n",
      "        [[-5.4085e-02,  7.2404e-01,  1.2304e+00],\n",
      "         [-8.7326e-01,  6.6265e-01,  1.5799e+00],\n",
      "         [ 9.8242e-01,  4.4145e-01, -9.5808e-02],\n",
      "         [ 8.0717e-02,  2.6890e-01, -8.7322e-01]],\n",
      "\n",
      "        [[-6.6776e-01, -2.7851e-02,  1.0782e-01],\n",
      "         [-4.8866e-01, -8.6226e-01,  2.3445e-01],\n",
      "         [-2.5390e-01,  9.8174e-01, -1.1659e-01],\n",
      "         [ 8.0794e-01,  1.8845e+00, -5.1907e-01]],\n",
      "\n",
      "        [[ 7.4226e-01,  8.3716e-01,  3.9787e-01],\n",
      "         [ 9.3411e-01,  2.9618e-01, -6.8035e-01],\n",
      "         [ 9.2652e-01,  4.3650e-01,  3.8104e-01],\n",
      "         [ 9.8563e-01,  7.5682e-02, -9.8719e-01]],\n",
      "\n",
      "        [[ 1.8515e-01,  1.0094e+00, -7.0447e-02],\n",
      "         [ 6.5000e-01,  1.1852e+00, -1.8936e-01],\n",
      "         [-9.4375e-01, -1.5170e-01,  1.6270e-01],\n",
      "         [-2.5792e-01,  1.0013e+00, -2.4689e-01]],\n",
      "\n",
      "        [[ 1.0946e+00, -1.0728e-01, -1.0887e+00],\n",
      "         [-7.1064e-01,  4.9673e-01,  1.2664e+00],\n",
      "         [ 1.2345e+00,  3.6527e-01, -6.4699e-02],\n",
      "         [ 1.2298e+00,  1.5752e-01, -5.3797e-01]],\n",
      "\n",
      "        [[ 5.2300e-01,  1.0331e+00, -1.8161e-01],\n",
      "         [-7.6066e-01, -1.1733e+00,  2.7189e-01],\n",
      "         [-2.0101e-01,  7.9038e-01, -1.4940e-02],\n",
      "         [-1.1301e-01,  1.0469e+00, -1.3518e-01]],\n",
      "\n",
      "        [[-5.4566e-01,  8.9094e-01, -9.5757e-03],\n",
      "         [ 1.0984e-01,  5.6886e-01, -9.5788e-01],\n",
      "         [-4.0300e-01,  1.3028e-01,  6.5632e-01],\n",
      "         [-3.8267e-01, -2.5232e-01,  1.7327e-02]],\n",
      "\n",
      "        [[ 1.2001e+00,  1.0701e+00, -2.9798e-01],\n",
      "         [ 3.4974e-01,  8.1802e-01, -3.2877e-01],\n",
      "         [-4.2141e-01,  6.5188e-01, -1.5597e-01],\n",
      "         [-4.0409e-01,  8.5074e-01, -2.8191e-01]],\n",
      "\n",
      "        [[-5.2678e-01,  1.3848e-01,  8.5531e-01],\n",
      "         [ 9.3994e-01,  4.3216e-01, -7.5886e-01],\n",
      "         [ 1.2590e+00,  8.6138e-02, -6.0310e-01],\n",
      "         [ 1.3427e+00, -4.3630e-01, -1.3913e+00]],\n",
      "\n",
      "        [[ 3.3431e-01,  9.8680e-01, -1.6508e-01],\n",
      "         [-1.0957e-01,  9.1405e-01, -2.0373e-01],\n",
      "         [-5.6725e-01,  5.5608e-01, -5.7594e-02],\n",
      "         [-3.0797e-01,  3.5977e-01, -7.8190e-02]],\n",
      "\n",
      "        [[-1.1109e+00, -1.6422e-01,  5.8392e-01],\n",
      "         [-4.5939e-01,  1.9167e-01, -4.6421e-01],\n",
      "         [ 3.4275e-02, -1.7647e-01,  5.6213e-01],\n",
      "         [ 4.7220e-01, -3.2326e-01, -1.6572e+00]],\n",
      "\n",
      "        [[-9.4642e-02,  4.1346e-01, -1.9464e-01],\n",
      "         [ 1.3399e+00,  1.1663e+00, -3.3310e-01],\n",
      "         [-2.3476e-01,  8.0617e-01, -1.0134e-01],\n",
      "         [ 1.8459e-03,  8.7695e-01, -3.7521e-01]],\n",
      "\n",
      "        [[-6.3250e-01,  5.8950e-01, -7.3835e-01],\n",
      "         [-9.4174e-01,  1.0354e+00,  1.3541e+00],\n",
      "         [ 1.3364e+00,  2.7064e-02, -7.7053e-01],\n",
      "         [ 9.0991e-01,  6.0667e-02, -9.8917e-01]],\n",
      "\n",
      "        [[ 8.1995e-01,  9.6314e-01, -4.2060e-01],\n",
      "         [ 5.9917e-01,  6.3284e-01, -9.7872e-02],\n",
      "         [ 2.1747e-01,  8.8538e-01, -8.4088e-02],\n",
      "         [ 6.2611e-01,  1.3640e+00, -2.7551e-01]]],\n",
      "       grad_fn=<TransposeBackward0>)\n",
      "\n",
      "K_dec_1 = \n",
      "tensor([[[-0.1119,  1.1423, -0.6369],\n",
      "         [-0.7245,  0.6704,  1.1038],\n",
      "         [-0.2786, -0.8776,  0.5809],\n",
      "         [-0.4944, -0.6482,  0.5847]],\n",
      "\n",
      "        [[ 0.5296,  0.6890, -1.1547],\n",
      "         [-1.3231,  0.3234, -1.0808],\n",
      "         [-0.9167, -0.6925,  1.2226],\n",
      "         [-1.5077, -0.4165,  0.5709]],\n",
      "\n",
      "        [[ 0.1935,  0.5455,  1.0715],\n",
      "         [ 0.2043, -0.7960,  0.6466],\n",
      "         [-0.1727, -0.5471,  1.0751],\n",
      "         [-0.2898, -0.8495,  0.5943]],\n",
      "\n",
      "        [[-0.2175, -0.1550, -0.9511],\n",
      "         [-0.3529, -0.9456,  1.1317],\n",
      "         [-1.2307, -0.5506,  0.2779],\n",
      "         [-0.5842, -0.6240,  1.3850]],\n",
      "\n",
      "        [[-0.1975, -0.4030,  1.1978],\n",
      "         [-0.9606,  0.6643,  1.1422],\n",
      "         [-0.2120, -0.6701, -0.4535],\n",
      "         [-0.1098, -0.9954,  0.7972]],\n",
      "\n",
      "        [[-1.1105, -0.4961,  0.1925],\n",
      "         [-1.0936, -0.0931, -0.0694],\n",
      "         [-1.0707,  0.1159,  0.0551],\n",
      "         [-0.9546, -0.6313,  0.9391]],\n",
      "\n",
      "        [[-0.6974,  0.3815, -0.7662],\n",
      "         [-0.2016,  0.9528, -1.0243],\n",
      "         [-0.8043, -0.4391,  0.2462],\n",
      "         [-0.9807,  0.1138,  1.2303]],\n",
      "\n",
      "        [[-0.5353,  0.2679,  0.1170],\n",
      "         [ 0.5849,  0.4081, -0.4155],\n",
      "         [-1.4600, -0.2132,  0.7059],\n",
      "         [-1.5889,  0.2942, -0.3802]],\n",
      "\n",
      "        [[-0.5508, -0.1283,  0.4033],\n",
      "         [-0.1766, -0.4914,  1.0603],\n",
      "         [-0.6543, -0.2892, -0.5610],\n",
      "         [-1.0432, -0.4435,  0.5275]],\n",
      "\n",
      "        [[-1.4486, -0.3444,  0.1769],\n",
      "         [-1.2332, -0.5640,  0.2495],\n",
      "         [-0.5720, -0.2845,  1.2369],\n",
      "         [-1.2254, -0.1202,  1.0927]],\n",
      "\n",
      "        [[ 0.0117, -0.8124,  1.0395],\n",
      "         [-0.2824,  0.8810, -1.1333],\n",
      "         [-0.3888, -0.7114,  0.2495],\n",
      "         [-0.5374, -0.7785,  0.4849]],\n",
      "\n",
      "        [[-1.0150, -0.6088,  0.4777],\n",
      "         [ 0.8716,  0.3434,  0.0822],\n",
      "         [-1.3241, -0.5058,  0.7682],\n",
      "         [-1.4262, -0.3958,  0.8083]],\n",
      "\n",
      "        [[-0.5320,  0.9631,  1.1052],\n",
      "         [-1.2860,  0.6829,  0.9186],\n",
      "         [-0.7458,  0.1510, -0.4782],\n",
      "         [-0.8528, -0.0223, -0.2709]],\n",
      "\n",
      "        [[-0.7015,  0.0711, -0.8718],\n",
      "         [-0.6398,  0.0439,  0.6273],\n",
      "         [-0.8447,  0.7769, -0.4746],\n",
      "         [-0.8535,  0.9126, -0.3553]],\n",
      "\n",
      "        [[-0.0181, -0.0063, -0.0452],\n",
      "         [-1.0852, -0.1526,  0.6081],\n",
      "         [-0.7317, -0.6881,  0.1389],\n",
      "         [-0.2651, -0.9671,  0.4822]],\n",
      "\n",
      "        [[-0.9781,  0.5698, -1.3623],\n",
      "         [-1.1764, -0.2623,  1.0809],\n",
      "         [-1.0324, -0.3566,  1.3308],\n",
      "         [-0.5238, -0.5532,  1.4369]],\n",
      "\n",
      "        [[-0.3856,  0.4469, -0.4335],\n",
      "         [ 0.0701,  0.3375,  1.2171],\n",
      "         [-0.0963, -0.5330, -0.3324],\n",
      "         [-1.0137, -0.1719,  0.7948]],\n",
      "\n",
      "        [[-0.2103,  1.0533, -1.1310],\n",
      "         [-0.5889,  0.0733, -1.1481],\n",
      "         [-1.0310,  0.4519, -0.6228],\n",
      "         [-0.6402,  0.1386,  0.9020]],\n",
      "\n",
      "        [[-1.1339,  1.1147,  1.0652],\n",
      "         [-0.3995,  1.0426,  0.0677],\n",
      "         [-0.0355, -0.9296,  0.7522],\n",
      "         [-0.2741, -0.6066,  1.1325]],\n",
      "\n",
      "        [[-0.4630,  0.3773, -0.2694],\n",
      "         [-0.6305,  0.4435, -1.3463],\n",
      "         [-1.1227, -0.7079,  0.7363],\n",
      "         [-1.3156, -0.4128,  0.2509]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "V_dec_1 = \n",
      "tensor([[[ 4.6698e-01,  3.2411e-01,  5.3707e-01],\n",
      "         [-4.9679e-01,  1.1741e+00, -5.8056e-02],\n",
      "         [-5.6392e-01, -3.6462e-01, -3.4581e-01],\n",
      "         [-8.8239e-01, -3.4651e-01, -4.1199e-01]],\n",
      "\n",
      "        [[ 6.0436e-01, -1.1584e+00, -6.9683e-01],\n",
      "         [ 4.2574e-01, -2.9254e-01,  1.3367e+00],\n",
      "         [-3.8931e-03,  1.0160e+00,  9.4195e-01],\n",
      "         [ 3.6679e-01,  3.3864e-01,  1.5334e+00]],\n",
      "\n",
      "        [[-4.1654e-01,  1.5464e+00,  2.1587e-01],\n",
      "         [-5.6456e-01,  3.0220e-02, -1.1766e-01],\n",
      "         [-9.1086e-01,  3.8300e-01, -3.5564e-01],\n",
      "         [-2.2370e-01, -2.8270e-01, -3.4010e-01]],\n",
      "\n",
      "        [[ 1.9595e-01, -2.2454e-01,  4.8697e-01],\n",
      "         [ 2.8743e-03,  9.4248e-01,  5.2786e-01],\n",
      "         [ 9.8383e-02,  4.7698e-01,  1.4343e+00],\n",
      "         [-3.4126e-01,  1.3847e+00,  5.6114e-01]],\n",
      "\n",
      "        [[-7.7605e-01,  6.3052e-01, -3.0359e-01],\n",
      "         [-2.2397e-01,  1.1636e+00,  2.4792e-01],\n",
      "         [-6.6492e-01, -1.2360e+00, -5.3717e-01],\n",
      "         [-6.6334e-01, -1.9110e-01, -5.3216e-01]],\n",
      "\n",
      "        [[ 7.0039e-04,  5.8858e-01,  1.3055e+00],\n",
      "         [ 6.5775e-01,  5.4410e-01,  9.3482e-01],\n",
      "         [ 2.4520e-01, -6.1445e-01,  1.0635e+00],\n",
      "         [-3.3116e-01,  9.7593e-01,  1.1016e+00]],\n",
      "\n",
      "        [[ 3.3171e-03, -7.8858e-01,  2.5447e-01],\n",
      "         [ 4.8617e-01, -2.3450e-01,  6.8704e-01],\n",
      "         [-5.9298e-01, -6.2050e-01, -2.9882e-01],\n",
      "         [-3.7268e-01,  7.1614e-01, -4.5919e-01]],\n",
      "\n",
      "        [[ 1.0437e+00, -6.1058e-01,  1.8415e-01],\n",
      "         [ 9.7390e-01, -9.6661e-01, -8.7724e-01],\n",
      "         [ 5.3842e-01,  3.3521e-01,  1.2933e+00],\n",
      "         [-1.6170e-01,  5.1801e-01,  1.5253e+00]],\n",
      "\n",
      "        [[-9.0910e-01, -1.1748e-01, -4.8495e-02],\n",
      "         [-9.3094e-01,  4.1186e-01, -3.0478e-01],\n",
      "         [-7.3867e-02, -1.1109e+00,  1.0264e-01],\n",
      "         [-9.0201e-02, -3.9566e-01, -3.8058e-01]],\n",
      "\n",
      "        [[ 9.9914e-01, -2.2404e-01,  1.4218e+00],\n",
      "         [ 1.9608e-01,  4.1384e-01,  1.4328e+00],\n",
      "         [ 7.8003e-01,  3.4227e-01,  2.3482e-01],\n",
      "         [-6.2222e-02,  1.2068e+00,  9.4981e-01]],\n",
      "\n",
      "        [[-8.2966e-01,  2.3242e-01, -4.8740e-01],\n",
      "         [ 8.0312e-01, -3.8541e-01,  7.3867e-01],\n",
      "         [-8.6767e-01, -6.6373e-01, -3.1514e-01],\n",
      "         [-7.4213e-01, -5.5475e-01, -4.8335e-01]],\n",
      "\n",
      "        [[-2.5802e-01,  7.0168e-01,  1.2673e+00],\n",
      "         [ 8.0181e-01, -4.8679e-01, -1.2660e+00],\n",
      "         [ 5.5042e-01,  2.0538e-01,  1.3229e+00],\n",
      "         [ 1.8790e-01,  5.5495e-01,  1.4050e+00]],\n",
      "\n",
      "        [[-2.5243e-01,  1.5808e+00,  3.5968e-01],\n",
      "         [ 4.9800e-01,  9.0673e-01,  2.8999e-01],\n",
      "         [ 4.4732e-02, -7.6123e-01, -3.3226e-01],\n",
      "         [ 2.5356e-01, -7.3742e-01, -5.7941e-01]],\n",
      "\n",
      "        [[ 6.2665e-01, -9.8842e-02,  7.0524e-01],\n",
      "         [ 2.4698e-01,  1.3640e+00,  2.3080e-01],\n",
      "         [ 1.3944e-01, -5.8866e-01,  5.8509e-01],\n",
      "         [-4.2809e-01, -1.1611e-01,  5.7760e-01]],\n",
      "\n",
      "        [[-6.9050e-01, -2.1413e-01, -4.6959e-01],\n",
      "         [-1.2305e-01, -8.5139e-02, -9.2767e-02],\n",
      "         [-2.6243e-01, -8.3562e-01, -3.1694e-01],\n",
      "         [-1.4855e-01, -4.7247e-01, -4.4089e-01]],\n",
      "\n",
      "        [[ 1.2521e-01, -1.3477e+00,  1.1438e+00],\n",
      "         [ 3.5121e-01,  1.1386e+00,  8.7730e-01],\n",
      "         [ 1.6627e-01,  9.4596e-01,  8.1257e-01],\n",
      "         [-5.2353e-01,  1.4247e+00,  4.9217e-01]],\n",
      "\n",
      "        [[ 3.5586e-01, -2.5437e-01, -2.8441e-01],\n",
      "         [-4.9736e-01,  1.4181e+00, -1.3883e-01],\n",
      "         [-6.2201e-01, -9.6148e-01, -6.9570e-01],\n",
      "         [ 4.9470e-01,  1.8672e-01, -3.6355e-01]],\n",
      "\n",
      "        [[-3.1621e-01, -8.0169e-01,  6.1818e-02],\n",
      "         [-1.8313e-01, -1.9800e-01,  8.8475e-01],\n",
      "         [-1.0855e-01, -9.1750e-01,  1.1144e+00],\n",
      "         [-7.4557e-01,  1.6914e+00,  3.4810e-01]],\n",
      "\n",
      "        [[ 5.6480e-01,  1.4797e+00,  3.4368e-01],\n",
      "         [-3.3058e-01,  6.6681e-01,  3.8623e-01],\n",
      "         [-9.3647e-01, -1.8030e-01, -4.4896e-01],\n",
      "         [-8.2717e-01,  3.4032e-01, -5.0106e-01]],\n",
      "\n",
      "        [[ 1.3007e-01,  8.7192e-01,  1.4827e-01],\n",
      "         [ 1.0915e+00, -1.2909e+00,  5.7202e-01],\n",
      "         [ 3.6091e-02,  5.7514e-01,  1.3209e+00],\n",
      "         [-1.4564e-01,  6.0663e-01,  1.4975e+00]]],\n",
      "       grad_fn=<TransposeBackward0>)\n",
      "\n",
      "Attnetion mask infunction = \n",
      "tensor([[[[0., -inf, -inf, -inf],\n",
      "          [0., 0., -inf, -inf],\n",
      "          [0., 0., 0., -inf],\n",
      "          [0., 0., 0., 0.]]]])\n",
      "\n",
      "Attention output = \n",
      "torch.Size([10, 2, 4, 4]) tensor([[[[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.5135, 0.4865, 0.0000, 0.0000],\n",
      "          [0.5920, 0.1377, 0.2704, 0.0000],\n",
      "          [0.4288, 0.1830, 0.2036, 0.1846]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.8278, 0.1722, 0.0000, 0.0000],\n",
      "          [0.3832, 0.3783, 0.2385, 0.0000],\n",
      "          [0.4171, 0.2975, 0.1279, 0.1575]]],\n",
      "\n",
      "\n",
      "        [[[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.4321, 0.5679, 0.0000, 0.0000],\n",
      "          [0.3655, 0.3750, 0.2595, 0.0000],\n",
      "          [0.2074, 0.3545, 0.1841, 0.2540]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.5054, 0.4946, 0.0000, 0.0000],\n",
      "          [0.5306, 0.2286, 0.2408, 0.0000],\n",
      "          [0.2824, 0.2147, 0.2682, 0.2347]]],\n",
      "\n",
      "\n",
      "        [[[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.3695, 0.6305, 0.0000, 0.0000],\n",
      "          [0.4256, 0.3326, 0.2419, 0.0000],\n",
      "          [0.1606, 0.0801, 0.5138, 0.2455]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.4146, 0.5854, 0.0000, 0.0000],\n",
      "          [0.2996, 0.3393, 0.3611, 0.0000],\n",
      "          [0.2226, 0.2757, 0.3024, 0.1994]]],\n",
      "\n",
      "\n",
      "        [[[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.5663, 0.4337, 0.0000, 0.0000],\n",
      "          [0.3052, 0.4744, 0.2204, 0.0000],\n",
      "          [0.3179, 0.4049, 0.1672, 0.1100]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.6125, 0.3875, 0.0000, 0.0000],\n",
      "          [0.3584, 0.3413, 0.3003, 0.0000],\n",
      "          [0.2295, 0.5288, 0.0740, 0.1677]]],\n",
      "\n",
      "\n",
      "        [[[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.5296, 0.4704, 0.0000, 0.0000],\n",
      "          [0.3308, 0.4261, 0.2431, 0.0000],\n",
      "          [0.2408, 0.2016, 0.3905, 0.1672]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.5193, 0.4807, 0.0000, 0.0000],\n",
      "          [0.3855, 0.3518, 0.2627, 0.0000],\n",
      "          [0.2784, 0.2350, 0.2175, 0.2691]]],\n",
      "\n",
      "\n",
      "        [[[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.7276, 0.2724, 0.0000, 0.0000],\n",
      "          [0.3281, 0.4124, 0.2595, 0.0000],\n",
      "          [0.2157, 0.4009, 0.2093, 0.1740]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.8228, 0.1772, 0.0000, 0.0000],\n",
      "          [0.3004, 0.3740, 0.3256, 0.0000],\n",
      "          [0.2074, 0.3363, 0.2202, 0.2361]]],\n",
      "\n",
      "\n",
      "        [[[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.5092, 0.4908, 0.0000, 0.0000],\n",
      "          [0.3802, 0.4134, 0.2063, 0.0000],\n",
      "          [0.2168, 0.2663, 0.2518, 0.2650]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.5708, 0.4292, 0.0000, 0.0000],\n",
      "          [0.3170, 0.2700, 0.4130, 0.0000],\n",
      "          [0.2187, 0.1666, 0.2998, 0.3149]]],\n",
      "\n",
      "\n",
      "        [[[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.7113, 0.2887, 0.0000, 0.0000],\n",
      "          [0.5253, 0.1912, 0.2835, 0.0000],\n",
      "          [0.3928, 0.1054, 0.2313, 0.2705]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.6713, 0.3287, 0.0000, 0.0000],\n",
      "          [0.4091, 0.3081, 0.2828, 0.0000],\n",
      "          [0.3163, 0.2468, 0.2333, 0.2036]]],\n",
      "\n",
      "\n",
      "        [[[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.6400, 0.3600, 0.0000, 0.0000],\n",
      "          [0.2569, 0.4480, 0.2951, 0.0000],\n",
      "          [0.3691, 0.0879, 0.4353, 0.1078]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.7210, 0.2790, 0.0000, 0.0000],\n",
      "          [0.4020, 0.2684, 0.3296, 0.0000],\n",
      "          [0.3737, 0.2283, 0.2467, 0.1513]]],\n",
      "\n",
      "\n",
      "        [[[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.7725, 0.2275, 0.0000, 0.0000],\n",
      "          [0.1577, 0.4326, 0.4097, 0.0000],\n",
      "          [0.1429, 0.3706, 0.2832, 0.2034]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.4932, 0.5068, 0.0000, 0.0000],\n",
      "          [0.3890, 0.4152, 0.1958, 0.0000],\n",
      "          [0.3539, 0.4165, 0.1011, 0.1285]]]], grad_fn=<SoftmaxBackward0>)\n",
      "Decoder Self Attention = \n",
      "tensor([[ 4.6698e-01,  3.2411e-01,  5.3707e-01,  6.0436e-01, -1.1584e+00,\n",
      "         -6.9683e-01],\n",
      "        [-4.1654e-01,  1.5464e+00,  2.1587e-01,  1.9595e-01, -2.2454e-01,\n",
      "          4.8697e-01],\n",
      "        [-7.7605e-01,  6.3052e-01, -3.0359e-01,  7.0039e-04,  5.8858e-01,\n",
      "          1.3055e+00],\n",
      "        [ 3.3171e-03, -7.8858e-01,  2.5447e-01,  1.0437e+00, -6.1058e-01,\n",
      "          1.8415e-01],\n",
      "        [-9.0910e-01, -1.1748e-01, -4.8495e-02,  9.9914e-01, -2.2404e-01,\n",
      "          1.4218e+00],\n",
      "        [-8.2966e-01,  2.3242e-01, -4.8740e-01, -2.5802e-01,  7.0168e-01,\n",
      "          1.2673e+00],\n",
      "        [-2.5243e-01,  1.5808e+00,  3.5968e-01,  6.2665e-01, -9.8842e-02,\n",
      "          7.0524e-01],\n",
      "        [-6.9050e-01, -2.1413e-01, -4.6959e-01,  1.2521e-01, -1.3477e+00,\n",
      "          1.1438e+00],\n",
      "        [ 3.5586e-01, -2.5437e-01, -2.8441e-01, -3.1621e-01, -8.0169e-01,\n",
      "          6.1818e-02],\n",
      "        [ 5.6480e-01,  1.4797e+00,  3.4368e-01,  1.3007e-01,  8.7192e-01,\n",
      "          1.4827e-01],\n",
      "        [-1.8581e-03,  7.3761e-01,  2.4756e-01,  5.7360e-01, -1.0093e+00,\n",
      "         -3.4661e-01],\n",
      "        [-5.0060e-01,  6.8539e-01,  2.6470e-02,  1.0046e-01,  3.5265e-01,\n",
      "          5.0720e-01],\n",
      "        [-4.2795e-01,  9.6663e-01,  4.4143e-02,  3.8537e-01,  5.6254e-01,\n",
      "          1.0885e+00],\n",
      "        [ 2.1272e-01, -5.4830e-01,  4.4206e-01,  1.0166e+00, -7.4853e-01,\n",
      "         -2.2711e-01],\n",
      "        [-9.1937e-01,  1.3154e-01, -1.6906e-01,  6.1313e-01,  8.2574e-02,\n",
      "          1.4271e+00],\n",
      "        [-3.8486e-01,  6.4115e-02, -1.5340e-01, -7.0252e-02,  4.9112e-01,\n",
      "          8.1845e-01],\n",
      "        [ 1.1591e-01,  1.2500e+00,  3.2547e-01,  4.6368e-01,  5.2906e-01,\n",
      "          5.0159e-01],\n",
      "        [-5.2670e-01, -1.7690e-01, -3.6082e-01,  1.9950e-01, -5.3034e-01,\n",
      "          1.0562e+00],\n",
      "        [ 4.8698e-02,  3.4773e-01, -2.3200e-01, -2.7908e-01, -6.3326e-01,\n",
      "          2.9142e-01],\n",
      "        [ 3.6107e-01,  1.2947e+00,  3.5336e-01,  6.1733e-01, -2.2413e-01,\n",
      "          3.6301e-01],\n",
      "        [ 5.5571e-02,  2.5491e-01,  2.1643e-01,  3.9170e-01, -3.1221e-01,\n",
      "          4.6333e-01],\n",
      "        [-6.0034e-01,  6.7590e-01, -5.7523e-02,  1.2832e-01,  2.1115e-01,\n",
      "          7.2442e-01],\n",
      "        [-5.6557e-01,  3.5633e-01, -1.7668e-01,  3.1194e-01,  1.3905e-01,\n",
      "          1.0923e+00],\n",
      "        [ 1.0101e-01, -4.8867e-01,  3.3778e-01,  8.6812e-01, -4.4805e-01,\n",
      "          1.5502e-01],\n",
      "        [-7.1537e-01, -1.3341e-01, -1.2096e-01,  6.5905e-01,  1.4916e-01,\n",
      "          1.1138e+00],\n",
      "        [-1.6616e-01, -2.5492e-01,  6.2936e-02,  4.0155e-01,  9.5619e-02,\n",
      "          3.3794e-01],\n",
      "        [ 1.1912e-01,  8.1891e-01,  1.8810e-01,  3.2291e-01,  9.3872e-02,\n",
      "          5.2750e-01],\n",
      "        [-4.6063e-01, -3.6565e-01, -3.5426e-01,  2.0646e-01,  6.7045e-02,\n",
      "          9.6802e-01],\n",
      "        [-3.1491e-01,  2.8622e-01, -3.4055e-01, -2.1204e-01, -6.7782e-01,\n",
      "          6.2966e-01],\n",
      "        [-4.3761e-01,  4.4795e-01,  3.7366e-02,  5.1085e-01, -8.4121e-02,\n",
      "          5.5382e-01],\n",
      "        [-1.6841e-01,  2.1565e-01,  7.3186e-02,  4.3600e-01, -3.8688e-01,\n",
      "          4.6908e-01],\n",
      "        [-5.1102e-01,  3.3020e-01, -1.4878e-01,  2.2461e-03,  5.9184e-01,\n",
      "          7.6727e-01],\n",
      "        [-6.4707e-01, -4.8758e-01, -4.3557e-01,  1.8964e-01,  2.8979e-01,\n",
      "          1.0895e+00],\n",
      "        [ 5.7797e-02, -3.7060e-01,  2.5863e-01,  7.6725e-01, -5.3957e-01,\n",
      "         -7.0074e-02],\n",
      "        [-4.5047e-01, -4.4518e-01, -9.6659e-02,  4.7716e-01,  4.3404e-01,\n",
      "          1.0393e+00],\n",
      "        [-1.6772e-01, -3.3990e-01,  4.0958e-02,  3.8169e-01,  1.5808e-01,\n",
      "          4.6009e-01],\n",
      "        [ 1.5635e-01,  1.9711e-01, -8.2012e-02,  8.5177e-02, -7.3400e-03,\n",
      "          5.4996e-01],\n",
      "        [-3.8506e-01, -4.1418e-01, -3.8679e-01,  5.8485e-02,  3.6559e-01,\n",
      "          8.6808e-01],\n",
      "        [-1.2981e-01, -3.6758e-01, -4.5916e-01, -2.9958e-01, -3.1514e-01,\n",
      "          5.5264e-01],\n",
      "        [-4.7523e-01,  4.7666e-01, -3.6820e-02,  4.8562e-01, -9.3003e-02,\n",
      "          6.1667e-01]], grad_fn=<ViewBackward0>)\n",
      "\n",
      "Final Encoder 1 Output :\n",
      "norm2(norm1(x + self_atten(x)) + feed_fwd_op)\n",
      "\n",
      "tensor([[[ 6.6781e-02,  1.7533e+00, -7.9001e-01,  7.9493e-01, -9.1361e-01,\n",
      "          -9.1140e-01],\n",
      "         [-3.1363e-01,  7.9776e-01, -1.2249e+00, -1.1842e+00,  4.5765e-01,\n",
      "           1.4673e+00],\n",
      "         [ 1.9029e-01, -7.1964e-01, -6.0745e-01, -1.2778e+00,  6.9312e-01,\n",
      "           1.7215e+00],\n",
      "         [ 1.7361e+00, -4.8693e-01, -2.1036e-01,  8.2795e-01, -1.3052e+00,\n",
      "          -5.6151e-01],\n",
      "         [ 1.3593e+00, -1.2247e+00, -7.2519e-01, -5.7567e-01, -1.6343e-01,\n",
      "           1.3297e+00],\n",
      "         [ 2.0113e-01, -9.9010e-01, -3.8770e-01, -1.2922e+00,  9.9851e-01,\n",
      "           1.4704e+00],\n",
      "         [ 1.2275e-02,  6.4883e-01, -1.1967e+00, -9.2305e-01, -3.2685e-01,\n",
      "           1.7855e+00],\n",
      "         [-1.9400e-01, -2.7816e-01, -1.2114e+00,  1.9921e+00,  2.9365e-01,\n",
      "          -6.0219e-01],\n",
      "         [-7.6969e-01,  8.4527e-01, -1.1051e-01,  1.7023e+00, -3.9057e-01,\n",
      "          -1.2768e+00],\n",
      "         [-4.3576e-01,  8.2373e-01, -5.0884e-01, -9.1167e-01, -8.0819e-01,\n",
      "           1.8407e+00]],\n",
      "\n",
      "        [[-5.6691e-01,  1.6578e-01, -1.4753e+00,  3.9207e-01, -3.1113e-01,\n",
      "           1.7955e+00],\n",
      "         [ 6.9242e-01, -8.3491e-01, -3.1198e-01, -1.5963e+00,  8.3184e-01,\n",
      "           1.2190e+00],\n",
      "         [ 2.7969e-01, -1.5534e-01, -5.9893e-01, -1.1638e+00, -3.7296e-01,\n",
      "           2.0113e+00],\n",
      "         [ 1.1581e+00,  1.1571e+00, -2.7770e-01,  4.3400e-01, -1.2384e+00,\n",
      "          -1.2331e+00],\n",
      "         [ 5.2621e-01, -1.0385e+00, -6.2155e-01, -1.1186e+00,  6.4153e-01,\n",
      "           1.6110e+00],\n",
      "         [ 1.2389e+00,  1.0738e+00,  6.0566e-01, -7.0711e-01, -1.1236e+00,\n",
      "          -1.0876e+00],\n",
      "         [ 1.3651e-01, -2.7867e-01,  1.3378e-01, -1.3143e+00, -6.2001e-01,\n",
      "           1.9427e+00],\n",
      "         [ 4.8712e-01, -1.6042e+00,  6.7742e-01, -9.2504e-01, -4.4316e-03,\n",
      "           1.3691e+00],\n",
      "         [-1.1811e+00,  7.4686e-01, -1.2010e+00, -4.6451e-01,  7.4649e-01,\n",
      "           1.3533e+00],\n",
      "         [ 5.8713e-01,  1.1572e+00, -1.7049e+00,  2.5571e-01, -9.5394e-01,\n",
      "           6.5878e-01]],\n",
      "\n",
      "        [[ 6.9288e-01, -1.5906e+00, -8.2472e-02, -9.7995e-01,  7.5449e-01,\n",
      "           1.2056e+00],\n",
      "         [ 2.8903e-01, -9.3819e-01, -7.9559e-01, -9.8457e-01,  7.2323e-01,\n",
      "           1.7061e+00],\n",
      "         [ 1.6310e+00, -1.6592e+00, -5.7445e-01,  4.9561e-01,  1.0527e-01,\n",
      "           1.7405e-03],\n",
      "         [ 1.0941e+00, -1.8460e+00, -3.5958e-01,  2.0911e-01, -1.8706e-01,\n",
      "           1.0895e+00],\n",
      "         [ 1.7734e+00, -1.3384e+00,  4.8925e-01, -7.9515e-01, -3.6749e-01,\n",
      "           2.3835e-01],\n",
      "         [ 1.1807e+00, -1.6582e+00, -5.8523e-01, -3.5156e-01,  2.6559e-01,\n",
      "           1.1487e+00],\n",
      "         [ 8.7755e-01, -4.4066e-01, -5.7755e-01,  1.7512e+00, -1.2167e+00,\n",
      "          -3.9384e-01],\n",
      "         [ 9.2657e-01, -1.8357e+00,  6.4012e-01, -7.8964e-01,  2.3097e-01,\n",
      "           8.2766e-01],\n",
      "         [ 2.9487e-01, -1.1750e+00, -8.0758e-01,  1.7959e+00,  5.1549e-01,\n",
      "          -6.2366e-01],\n",
      "         [ 6.2461e-01, -1.3183e+00, -5.9372e-01, -9.4465e-01,  8.5553e-01,\n",
      "           1.3766e+00]],\n",
      "\n",
      "        [[ 7.4514e-01, -1.6433e+00, -6.1553e-01, -3.3400e-01,  4.0091e-01,\n",
      "           1.4468e+00],\n",
      "         [ 4.9096e-01, -1.3095e+00,  3.3559e-01, -1.4249e+00,  7.5267e-01,\n",
      "           1.1552e+00],\n",
      "         [ 3.7861e-01, -1.4527e+00,  8.1534e-02, -1.1715e+00,  9.6923e-01,\n",
      "           1.1948e+00],\n",
      "         [-6.9834e-01, -1.0294e+00, -6.7540e-01,  5.4984e-01, -6.7488e-02,\n",
      "           1.9208e+00],\n",
      "         [ 6.0403e-01, -1.8043e+00,  6.2682e-01, -6.6191e-01, -9.1142e-03,\n",
      "           1.2444e+00],\n",
      "         [ 7.9746e-01, -1.8327e+00, -2.9753e-01, -3.3326e-01,  3.7606e-01,\n",
      "           1.2900e+00],\n",
      "         [-1.5695e-01, -8.7207e-01,  4.0578e-01,  2.0156e+00, -7.9184e-01,\n",
      "          -6.0048e-01],\n",
      "         [ 4.0635e-01, -1.4872e+00,  6.9511e-01, -1.3067e+00,  8.4877e-01,\n",
      "           8.4370e-01],\n",
      "         [-8.7842e-01, -1.2262e+00,  1.3646e+00, -7.5909e-01,  4.6432e-01,\n",
      "           1.0348e+00],\n",
      "         [ 1.1551e-01, -1.1434e+00, -6.8527e-01, -7.8797e-01,  7.7045e-01,\n",
      "           1.7307e+00]]], grad_fn=<AddBackward0>)\n",
      "\n",
      "### Decoder End ###\n"
     ]
    }
   ],
   "source": [
    "final_op = get_all_intermediate_outputs_mask(src_data[:-1, :], state_dict = state_dict1 , num_decoder_layers = num_decoder_layers, d_model=d_model,  d_ff = d_ff, tgt_mask = tgt_mask, max_seq_len = src_data[:-1, :].shape[0])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "my_project_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
