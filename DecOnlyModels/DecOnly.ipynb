{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decoder only transformer architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import copy \n",
    "\n",
    "torch.manual_seed(6)\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=512, dropout=0):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.encoding = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1).float()\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(torch.log(torch.tensor(10000.0)) / d_model))\n",
    "        self.encoding[:, 0::2] = torch.sin(position * div_term)\n",
    "        self.encoding[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.encoding = self.encoding.unsqueeze(0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.encoding[:, :x.size(1)].detach()\n",
    "    \n",
    "\n",
    "class DecoderOnlyTransformer(nn.Module):\n",
    "    def __init__(self,num_layers, d_model, nhead, dim_feedforward, vocab_size, max_seq_len, dropout = 0):\n",
    "        super(DecoderOnlyTransformer, self).__init__()\n",
    "\n",
    "        self.src_embedding = nn.Embedding(vocab_size, d_model)\n",
    "\n",
    "        self.positional_encoding = PositionalEncoding(d_model, dropout=0, max_len=max_seq_len)\n",
    "\n",
    "        self.decoder_layer = nn.TransformerEncoderLayer(d_model = d_model, nhead = nhead, dim_feedforward = dim_feedforward, dropout = dropout)\n",
    "        self.layers = nn.ModuleList([copy.deepcopy(self.decoder_layer) for _ in range(num_layers)])\n",
    "\n",
    "        self.fc = nn.Linear(d_model, vocab_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=-1)\n",
    "\n",
    "    \n",
    "    def generate_square_subsequent_mask(self, tgt):\n",
    "        seq_length = tgt.size(0)\n",
    "        nopeak_mask = (torch.triu(torch.ones(seq_length, seq_length), diagonal=1)).bool()\n",
    "\n",
    "        return nopeak_mask\n",
    "    \n",
    "        # mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
    "        # mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "        # return mask\n",
    "\n",
    "    def forward(self, src, src_mask=None):\n",
    "\n",
    "\n",
    "        src = self.src_embedding(src) + self.positional_encoding(src)\n",
    "        # tgt = self.tgt_embedding(tgt) + self.positional_encoding(tgt)\n",
    "\n",
    "        for layer in self.layers:\n",
    "\n",
    "            src_mask = self.generate_square_subsequent_mask(src)\n",
    "            src = layer(src, src_mask)\n",
    "\n",
    "\n",
    "        op = self.fc(src)\n",
    "        op = self.softmax(op)\n",
    "        return op\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MASK =  tensor([[0., -inf, -inf, -inf, -inf],\n",
      "        [0., 0., -inf, -inf, -inf],\n",
      "        [0., 0., 0., -inf, -inf],\n",
      "        [0., 0., 0., 0., -inf],\n",
      "        [0., 0., 0., 0., 0.]])\n",
      "query =  tensor([[[-0.3136, -0.2602, -0.7884, -0.0438, -0.7863,  1.0629]],\n",
      "\n",
      "        [[ 0.3295,  1.3264, -0.4806,  2.1032,  2.5485,  1.3006]],\n",
      "\n",
      "        [[-0.9152,  2.5446, -0.2570, -0.3999,  0.7615,  1.8896]],\n",
      "\n",
      "        [[ 0.0160,  1.4019,  1.9538,  0.5540,  1.7102,  1.8944]],\n",
      "\n",
      "        [[ 0.0160,  1.4019,  1.9538,  0.5540,  1.7102,  1.8944]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "query shape =  torch.Size([5, 1, 6])\n",
      "MASK1 =  tensor([[0., -inf, -inf, -inf, -inf],\n",
      "        [0., 0., -inf, -inf, -inf],\n",
      "        [0., 0., 0., -inf, -inf],\n",
      "        [0., 0., 0., 0., -inf],\n",
      "        [0., 0., 0., 0., 0.]]) False\n",
      "\n",
      " _in_projection_packed \n",
      "\n",
      "Shapes =  torch.Size([5, 1, 6]) torch.Size([5, 1, 6]) torch.Size([5, 1, 6])\n",
      "\n",
      "Q :-  tensor([[[-0.2299,  0.4724, -0.1934, -0.0980,  0.8416, -0.6709]],\n",
      "\n",
      "        [[ 0.9476, -0.1598,  0.7058,  0.7810,  0.4146,  0.0997]],\n",
      "\n",
      "        [[ 0.4198, -0.6475,  0.0381, -0.1186,  1.0708, -0.8162]],\n",
      "\n",
      "        [[ 1.1683, -0.9125,  0.6359,  1.3842, -0.0489,  0.4018]],\n",
      "\n",
      "        [[ 1.1683, -0.9125,  0.6359,  1.3842, -0.0489,  0.4018]]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "\n",
      "K :-  tensor([[[-0.6781, -0.0356, -0.1597,  0.0840, -0.7201,  0.4009]],\n",
      "\n",
      "        [[ 1.3188, -0.9810, -1.0095,  0.6863, -0.4830,  0.1385]],\n",
      "\n",
      "        [[ 1.1382, -1.5106, -1.3268,  1.3087, -1.2448,  1.2459]],\n",
      "\n",
      "        [[ 2.0055, -0.8687, -1.4308,  0.2725,  0.3879,  0.7202]],\n",
      "\n",
      "        [[ 2.0055, -0.8687, -1.4308,  0.2725,  0.3879,  0.7202]]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "\n",
      "V :-  tensor([[[-0.8656,  0.7165,  0.7485, -0.3927,  0.1334,  0.5312]],\n",
      "\n",
      "        [[ 1.2018, -1.0112,  0.0525,  0.8201,  0.4797, -0.4047]],\n",
      "\n",
      "        [[ 0.0485,  0.0298,  1.4139, -0.7822, -0.2792, -0.0081]],\n",
      "\n",
      "        [[ 1.6701, -1.0816, -0.1200, -0.1437,  0.7112, -1.2349]],\n",
      "\n",
      "        [[ 1.6701, -1.0816, -0.1200, -0.1437,  0.7112, -1.2349]]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "\n",
      "tgt_len =  5\n",
      "bsz =  1\n",
      "num_heads =  2\n",
      "head_dim =  3\n",
      "\n",
      "MASK2 =  tensor([[[0., -inf, -inf, -inf, -inf],\n",
      "         [0., 0., -inf, -inf, -inf],\n",
      "         [0., 0., 0., -inf, -inf],\n",
      "         [0., 0., 0., 0., -inf],\n",
      "         [0., 0., 0., 0., 0.]]])\n",
      "Attention mask NOT NONE =  tensor([[[[0., -inf, -inf, -inf, -inf],\n",
      "          [0., 0., -inf, -inf, -inf],\n",
      "          [0., 0., 0., -inf, -inf],\n",
      "          [0., 0., 0., 0., -inf],\n",
      "          [0., 0., 0., 0., 0.]]]])\n",
      "\n",
      "SHAPES RIGHT BEFORE ATTENTION :-  torch.Size([1, 2, 5, 3]) torch.Size([1, 2, 5, 3]) torch.Size([1, 2, 5, 3])\n",
      "\n",
      "Q reshaped =  tensor([[[[-0.2299,  0.4724, -0.1934],\n",
      "          [ 0.9476, -0.1598,  0.7058],\n",
      "          [ 0.4198, -0.6475,  0.0381],\n",
      "          [ 1.1683, -0.9125,  0.6359],\n",
      "          [ 1.1683, -0.9125,  0.6359]],\n",
      "\n",
      "         [[-0.0980,  0.8416, -0.6709],\n",
      "          [ 0.7810,  0.4146,  0.0997],\n",
      "          [-0.1186,  1.0708, -0.8162],\n",
      "          [ 1.3842, -0.0489,  0.4018],\n",
      "          [ 1.3842, -0.0489,  0.4018]]]], grad_fn=<ViewBackward0>)\n",
      "\n",
      "K reshaped =  tensor([[[[-0.6781, -0.0356, -0.1597],\n",
      "          [ 1.3188, -0.9810, -1.0095],\n",
      "          [ 1.1382, -1.5106, -1.3268],\n",
      "          [ 2.0055, -0.8687, -1.4308],\n",
      "          [ 2.0055, -0.8687, -1.4308]],\n",
      "\n",
      "         [[ 0.0840, -0.7201,  0.4009],\n",
      "          [ 0.6863, -0.4830,  0.1385],\n",
      "          [ 1.3087, -1.2448,  1.2459],\n",
      "          [ 0.2725,  0.3879,  0.7202],\n",
      "          [ 0.2725,  0.3879,  0.7202]]]], grad_fn=<ViewBackward0>)\n",
      "\n",
      "V reshaped =  tensor([[[[-0.8656,  0.7165,  0.7485],\n",
      "          [ 1.2018, -1.0112,  0.0525],\n",
      "          [ 0.0485,  0.0298,  1.4139],\n",
      "          [ 1.6701, -1.0816, -0.1200],\n",
      "          [ 1.6701, -1.0816, -0.1200]],\n",
      "\n",
      "         [[-0.3927,  0.1334,  0.5312],\n",
      "          [ 0.8201,  0.4797, -0.4047],\n",
      "          [-0.7822, -0.2792, -0.0081],\n",
      "          [-0.1437,  0.7112, -1.2349],\n",
      "          [-0.1437,  0.7112, -1.2349]]]], grad_fn=<ViewBackward0>)\n",
      "\n",
      "Attention mask =  tensor([[[[0., -inf, -inf, -inf, -inf],\n",
      "          [0., 0., -inf, -inf, -inf],\n",
      "          [0., 0., 0., -inf, -inf],\n",
      "          [0., 0., 0., 0., -inf],\n",
      "          [0., 0., 0., 0., 0.]]]])\n",
      "scaled_dot_product_attention output = \n",
      "tensor([[[[-0.8656,  0.7165,  0.7485],\n",
      "          [ 0.5755, -0.4878,  0.2634],\n",
      "          [ 0.3371, -0.2541,  0.7773],\n",
      "          [ 0.9062, -0.6251,  0.4329],\n",
      "          [ 1.1094, -0.7465,  0.2858]],\n",
      "\n",
      "         [[-0.3927,  0.1334,  0.5312],\n",
      "          [ 0.3079,  0.3335, -0.0095],\n",
      "          [ 0.1070,  0.2263,  0.0069],\n",
      "          [-0.2705,  0.1148, -0.2291],\n",
      "          [-0.2518,  0.2027, -0.3775]]]], grad_fn=<UnsafeViewBackward0>)\n",
      "\n",
      "x + self_atten(x) = \n",
      "tensor([[[-1.2024, -0.1572, -0.6165,  0.7536, -0.5416,  1.5726]],\n",
      "\n",
      "        [[ 0.4759,  1.4758, -0.4398,  1.7247,  2.5807,  1.0182]],\n",
      "\n",
      "        [[-1.1017,  2.8390, -0.1301, -0.3549,  0.8854,  1.8580]],\n",
      "\n",
      "        [[ 0.1758,  1.5237,  2.0019,  0.2782,  1.8240,  1.8170]],\n",
      "\n",
      "        [[ 0.3116,  1.4632,  1.9825,  0.1266,  1.7852,  1.6967]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "\n",
      "norm1(x + self_atten(x)) = \n",
      "tensor([[[-1.2598, -0.1349, -0.6293,  0.8455, -0.5486,  1.7271]],\n",
      "\n",
      "        [[-0.6940,  0.3521, -1.6520,  0.6125,  1.5081, -0.1266]],\n",
      "\n",
      "        [[-1.3060,  1.6055, -0.5881, -0.7542,  0.1621,  0.8807]],\n",
      "\n",
      "        [[-1.4565,  0.3375,  0.9739, -1.3201,  0.7372,  0.7279]],\n",
      "\n",
      "        [[-1.2528,  0.3222,  1.0324, -1.5058,  0.7626,  0.6415]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "\n",
      "feed_fwd_op =  tensor([[[-0.4217, -0.4637,  0.2577, -0.3430,  0.1919, -0.4070]],\n",
      "\n",
      "        [[-0.2964, -0.1890,  0.0853, -0.1570,  0.8086, -0.7134]],\n",
      "\n",
      "        [[-0.4022, -0.4693,  0.2952, -0.2985,  0.0943, -0.3949]],\n",
      "\n",
      "        [[-0.4114, -0.4146,  0.2769, -0.3245,  0.0264, -0.4325]],\n",
      "\n",
      "        [[-0.4114, -0.4146,  0.2769, -0.3245,  0.0264, -0.4325]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "\n",
      "norm2(norm1(x + self_atten(x)) + feed_fwd_op) = \n",
      "tensor([[[-1.5921, -0.4302, -0.1866,  0.7512, -0.1708,  1.6284]],\n",
      "\n",
      "        [[-0.7182,  0.1887, -1.1712,  0.4186,  1.8819, -0.5999]],\n",
      "\n",
      "        [[-1.5832,  1.3945, -0.1016, -0.8970,  0.4736,  0.7137]],\n",
      "\n",
      "        [[-1.4187,  0.1168,  1.2553, -1.2272,  0.8376,  0.4362]],\n",
      "\n",
      "        [[-1.2384,  0.1032,  1.2996, -1.3802,  0.8555,  0.3604]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "\n",
      "##################################\n",
      "MASK =  tensor([[0., -inf, -inf, -inf, -inf],\n",
      "        [0., 0., -inf, -inf, -inf],\n",
      "        [0., 0., 0., -inf, -inf],\n",
      "        [0., 0., 0., 0., -inf],\n",
      "        [0., 0., 0., 0., 0.]])\n",
      "query =  tensor([[[-1.5921, -0.4302, -0.1866,  0.7512, -0.1708,  1.6284]],\n",
      "\n",
      "        [[-0.7182,  0.1887, -1.1712,  0.4186,  1.8819, -0.5999]],\n",
      "\n",
      "        [[-1.5832,  1.3945, -0.1016, -0.8970,  0.4736,  0.7137]],\n",
      "\n",
      "        [[-1.4187,  0.1168,  1.2553, -1.2272,  0.8376,  0.4362]],\n",
      "\n",
      "        [[-1.2384,  0.1032,  1.2996, -1.3802,  0.8555,  0.3604]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "query shape =  torch.Size([5, 1, 6])\n",
      "MASK1 =  tensor([[0., -inf, -inf, -inf, -inf],\n",
      "        [0., 0., -inf, -inf, -inf],\n",
      "        [0., 0., 0., -inf, -inf],\n",
      "        [0., 0., 0., 0., -inf],\n",
      "        [0., 0., 0., 0., 0.]]) False\n",
      "\n",
      " _in_projection_packed \n",
      "\n",
      "Shapes =  torch.Size([5, 1, 6]) torch.Size([5, 1, 6]) torch.Size([5, 1, 6])\n",
      "\n",
      "Q :-  tensor([[[ 0.2463,  0.8544, -0.5304,  0.7188,  0.0367, -0.8007]],\n",
      "\n",
      "        [[ 0.6275,  0.1355,  0.3227, -0.2305, -0.4644,  0.3833]],\n",
      "\n",
      "        [[ 0.3527, -0.3148, -0.2515, -0.2403,  0.0623, -0.4727]],\n",
      "\n",
      "        [[ 0.8841, -0.4587,  0.1395,  0.4842, -0.8412,  0.4947]],\n",
      "\n",
      "        [[ 0.8975, -0.5610,  0.2432,  0.4362, -0.8005,  0.6078]]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "\n",
      "K :-  tensor([[[-0.5072,  0.1176, -1.2011, -0.8957, -0.1332,  0.5932]],\n",
      "\n",
      "        [[ 0.2727, -0.1501, -0.2672,  0.0925,  0.2915, -0.3647]],\n",
      "\n",
      "        [[ 0.5373, -0.7141, -1.0254,  0.3749, -0.3029,  0.7895]],\n",
      "\n",
      "        [[ 0.7981,  0.0061, -0.9278, -0.4925,  1.1276,  0.4137]],\n",
      "\n",
      "        [[ 0.8588,  0.0048, -0.8075, -0.3915,  1.1650,  0.3739]]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "\n",
      "V :-  tensor([[[-0.6195, -0.4504,  0.0134, -0.0373,  0.6043, -0.1903]],\n",
      "\n",
      "        [[ 0.2657, -1.0612, -0.7328,  1.0932, -0.0414, -0.1390]],\n",
      "\n",
      "        [[-0.2172, -0.4341,  0.4190, -0.3907, -0.2956, -0.2094]],\n",
      "\n",
      "        [[ 0.5003, -1.1043, -0.8276,  0.0026,  0.3200, -0.9732]],\n",
      "\n",
      "        [[ 0.5657, -1.0284, -0.8252, -0.0113,  0.3037, -0.9589]]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "\n",
      "tgt_len =  5\n",
      "bsz =  1\n",
      "num_heads =  2\n",
      "head_dim =  3\n",
      "\n",
      "MASK2 =  tensor([[[0., -inf, -inf, -inf, -inf],\n",
      "         [0., 0., -inf, -inf, -inf],\n",
      "         [0., 0., 0., -inf, -inf],\n",
      "         [0., 0., 0., 0., -inf],\n",
      "         [0., 0., 0., 0., 0.]]])\n",
      "Attention mask NOT NONE =  tensor([[[[0., -inf, -inf, -inf, -inf],\n",
      "          [0., 0., -inf, -inf, -inf],\n",
      "          [0., 0., 0., -inf, -inf],\n",
      "          [0., 0., 0., 0., -inf],\n",
      "          [0., 0., 0., 0., 0.]]]])\n",
      "\n",
      "SHAPES RIGHT BEFORE ATTENTION :-  torch.Size([1, 2, 5, 3]) torch.Size([1, 2, 5, 3]) torch.Size([1, 2, 5, 3])\n",
      "\n",
      "Q reshaped =  tensor([[[[ 0.2463,  0.8544, -0.5304],\n",
      "          [ 0.6275,  0.1355,  0.3227],\n",
      "          [ 0.3527, -0.3148, -0.2515],\n",
      "          [ 0.8841, -0.4587,  0.1395],\n",
      "          [ 0.8975, -0.5610,  0.2432]],\n",
      "\n",
      "         [[ 0.7188,  0.0367, -0.8007],\n",
      "          [-0.2305, -0.4644,  0.3833],\n",
      "          [-0.2403,  0.0623, -0.4727],\n",
      "          [ 0.4842, -0.8412,  0.4947],\n",
      "          [ 0.4362, -0.8005,  0.6078]]]], grad_fn=<ViewBackward0>)\n",
      "\n",
      "K reshaped =  tensor([[[[-0.5072,  0.1176, -1.2011],\n",
      "          [ 0.2727, -0.1501, -0.2672],\n",
      "          [ 0.5373, -0.7141, -1.0254],\n",
      "          [ 0.7981,  0.0061, -0.9278],\n",
      "          [ 0.8588,  0.0048, -0.8075]],\n",
      "\n",
      "         [[-0.8957, -0.1332,  0.5932],\n",
      "          [ 0.0925,  0.2915, -0.3647],\n",
      "          [ 0.3749, -0.3029,  0.7895],\n",
      "          [-0.4925,  1.1276,  0.4137],\n",
      "          [-0.3915,  1.1650,  0.3739]]]], grad_fn=<ViewBackward0>)\n",
      "\n",
      "V reshaped =  tensor([[[[-0.6195, -0.4504,  0.0134],\n",
      "          [ 0.2657, -1.0612, -0.7328],\n",
      "          [-0.2172, -0.4341,  0.4190],\n",
      "          [ 0.5003, -1.1043, -0.8276],\n",
      "          [ 0.5657, -1.0284, -0.8252]],\n",
      "\n",
      "         [[-0.0373,  0.6043, -0.1903],\n",
      "          [ 1.0932, -0.0414, -0.1390],\n",
      "          [-0.3907, -0.2956, -0.2094],\n",
      "          [ 0.0026,  0.3200, -0.9732],\n",
      "          [-0.0113,  0.3037, -0.9589]]]], grad_fn=<ViewBackward0>)\n",
      "\n",
      "Attention mask =  tensor([[[[0., -inf, -inf, -inf, -inf],\n",
      "          [0., 0., -inf, -inf, -inf],\n",
      "          [0., 0., 0., -inf, -inf],\n",
      "          [0., 0., 0., 0., -inf],\n",
      "          [0., 0., 0., 0., 0.]]]])\n",
      "scaled_dot_product_attention output = \n",
      "tensor([[[[-0.6195, -0.4504,  0.0134],\n",
      "          [-0.0820, -0.8213, -0.4397],\n",
      "          [-0.1837, -0.6326, -0.0536],\n",
      "          [ 0.0571, -0.7907, -0.2949],\n",
      "          [ 0.1780, -0.8469, -0.4179]],\n",
      "\n",
      "         [[-0.0373,  0.6043, -0.1903],\n",
      "          [ 0.4009,  0.3540, -0.1704],\n",
      "          [ 0.3099,  0.1099, -0.1754],\n",
      "          [ 0.0536,  0.0672, -0.2996],\n",
      "          [ 0.0350,  0.1026, -0.3877]]]], grad_fn=<UnsafeViewBackward0>)\n",
      "\n",
      "x + self_atten(x) = \n",
      "tensor([[[-1.9347, -0.3697,  0.0435,  0.7488, -0.4337,  1.1488]],\n",
      "\n",
      "        [[-0.4534,  0.2387, -1.1078, -0.1455,  1.5482, -1.2813]],\n",
      "\n",
      "        [[-1.4736,  1.5848, -0.0096, -1.1498,  0.2136,  0.2670]],\n",
      "\n",
      "        [[-1.1499,  0.1765,  1.3058, -1.6292,  0.5659, -0.0256]],\n",
      "\n",
      "        [[-0.8800,  0.1064,  1.3304, -1.8717,  0.5626, -0.1583]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "\n",
      "norm1(x + self_atten(x)) = \n",
      "tensor([[[-1.8257, -0.2400,  0.1787,  0.8933, -0.3049,  1.2986]],\n",
      "\n",
      "        [[-0.2692,  0.4666, -0.9648,  0.0582,  1.8586, -1.1493]],\n",
      "\n",
      "        [[-1.3712,  1.6699,  0.0845, -1.0492,  0.3064,  0.3595]],\n",
      "\n",
      "        [[-1.0291,  0.3042,  1.4393, -1.5109,  0.6956,  0.1010]],\n",
      "\n",
      "        [[-0.7128,  0.2527,  1.4508, -1.6835,  0.6992, -0.0064]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "\n",
      "feed_fwd_op =  tensor([[[-0.4000, -0.5224,  0.2862, -0.3092,  0.2129, -0.3586]],\n",
      "\n",
      "        [[-0.2845, -0.1312,  0.1672,  0.1165,  0.9966, -0.8389]],\n",
      "\n",
      "        [[-0.4184, -0.4394,  0.2969, -0.2880,  0.0858, -0.4273]],\n",
      "\n",
      "        [[-0.4114, -0.4146,  0.2769, -0.3245,  0.0264, -0.4325]],\n",
      "\n",
      "        [[-0.4227, -0.4074,  0.2850, -0.3058,  0.0432, -0.4480]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "\n",
      "norm2(norm1(x + self_atten(x)) + feed_fwd_op) = \n",
      "tensor([[[-1.9219, -0.5459,  0.6082,  0.7202,  0.0846,  1.0548]],\n",
      "\n",
      "        [[-0.3763,  0.2233, -0.5408,  0.1149,  1.9228, -1.3438]],\n",
      "\n",
      "        [[-1.5203,  1.3653,  0.5540, -1.0880,  0.5643,  0.1248]],\n",
      "\n",
      "        [[-1.0157,  0.0851,  1.5969, -1.3425,  0.7740, -0.0978]],\n",
      "\n",
      "        [[-0.7657,  0.0451,  1.6081, -1.4716,  0.7868, -0.2027]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "\n",
      "##################################\n"
     ]
    }
   ],
   "source": [
    "\n",
    "vocab_size = 10  # Source language vocabulary size\n",
    "d_model = 6  # Dimension of the model\n",
    "num_heads = 2\n",
    "d_ff = 4\n",
    "\n",
    "max_seq_len = 5\n",
    "# max_seq_len = max_seq_len,\n",
    "model = DecoderOnlyTransformer(d_model=d_model, vocab_size=vocab_size, num_layers=2 , nhead=num_heads, max_seq_len = max_seq_len, dim_feedforward=d_ff)\n",
    "\n",
    "batch_size = 1\n",
    "input_tensor = torch.randint(0, vocab_size, (max_seq_len, batch_size))\n",
    "\n",
    "# Source token indexes from src vocabulary\n",
    "# src_sentence = torch.tensor([[0, 2], [1, 0], [2, 2], [3, 5]])\n",
    "\n",
    "# Forward pass\n",
    "output = model(input_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 1, 10])\n"
     ]
    }
   ],
   "source": [
    "print(output.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 1])\n"
     ]
    }
   ],
   "source": [
    "predicted_indices = output.argmax(dim=-1)\n",
    "\n",
    "print(predicted_indices.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[5],\n",
       "        [2],\n",
       "        [5],\n",
       "        [5],\n",
       "        [5]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 880 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"The model has {count_parameters(model):,} trainable parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.8532, -2.9262, -2.7423, -2.5471, -2.8962, -1.5334, -1.9506, -2.2096,\n",
       "        -2.5007, -3.1717], grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[0, 0, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training part of the decoder only architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(6)\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=512, dropout=0):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.encoding = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1).float()\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(torch.log(torch.tensor(10000.0)) / d_model))\n",
    "        self.encoding[:, 0::2] = torch.sin(position * div_term)\n",
    "        self.encoding[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.encoding = self.encoding.unsqueeze(0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.encoding[:, :x.shape[1]].detach()\n",
    "    \n",
    "    \n",
    "\n",
    "class DecoderOnlyTransformer(nn.Module):\n",
    "    def __init__(self,num_layers, d_model, nhead, dim_feedforward, vocab_size, max_seq_len, dropout = 0):\n",
    "        super(DecoderOnlyTransformer, self).__init__()\n",
    "\n",
    "        self.src_embedding = nn.Embedding(vocab_size, d_model)\n",
    "\n",
    "        self.positional_encoding = PositionalEncoding(d_model = d_model, dropout=0, max_len=max_seq_len)\n",
    "\n",
    "        self.decoder_layer = nn.TransformerEncoderLayer(d_model = d_model, nhead = nhead, dim_feedforward = dim_feedforward, dropout = dropout)\n",
    "        self.layers = nn.ModuleList([copy.deepcopy(self.decoder_layer) for _ in range(num_layers)])\n",
    "\n",
    "        self.fc = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    \n",
    "    def generate_square_subsequent_mask(self, tgt):\n",
    "        seq_length = tgt.size(0)\n",
    "        nopeak_mask = (torch.triu(torch.ones(seq_length, seq_length), diagonal=1)).bool()\n",
    "\n",
    "        return nopeak_mask\n",
    "    \n",
    "        # mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
    "        # mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "        # return mask\n",
    "\n",
    "    def forward(self, src, src_mask=None):\n",
    "\n",
    "        src_embed = self.src_embedding(src)\n",
    "        pe_src = self.positional_encoding(src)\n",
    "        \n",
    "        pe_src = pe_src.transpose(0,1)\n",
    "\n",
    "        src = src_embed + pe_src\n",
    "\n",
    "        for layer in self.layers:\n",
    "\n",
    "            src_mask = self.generate_square_subsequent_mask(src)\n",
    "            src = layer(src, src_mask)\n",
    "\n",
    "\n",
    "        op = self.fc(src)\n",
    "        return op\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "\n",
    "vocab_size = 20  # Source language vocabulary size\n",
    "d_model = 6  # Dimension of the model\n",
    "num_heads = 2\n",
    "d_ff = 4\n",
    "num_layers = 2\n",
    "\n",
    "\n",
    "max_seq_len = 5\n",
    "model = DecoderOnlyTransformer(d_model=d_model, vocab_size=vocab_size, num_layers=num_layers , nhead=num_heads, max_seq_len = max_seq_len-1, dim_feedforward=d_ff)\n",
    "\n",
    "\n",
    "batch_size = 10\n",
    "\n",
    "# Generate random sample data\n",
    "src_data = torch.randint(1, vocab_size, (max_seq_len , batch_size))  # (seq_length, batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "state_dict = model.state_dict()\n",
    "\n",
    "state_dict1 = copy.deepcopy(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 10]), torch.Size([5, 10]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_data[:-1, :].shape , src_data.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FWD PASS START\n",
      "\n",
      "src_data shape =  torch.Size([5, 10])\n",
      "MASK =  tensor([[0., -inf, -inf, -inf],\n",
      "        [0., 0., -inf, -inf],\n",
      "        [0., 0., 0., -inf],\n",
      "        [0., 0., 0., 0.]])\n",
      "query =  tensor([[[ 0.1198,  2.2377,  1.1168,  0.7527, -1.3527, -0.6959],\n",
      "         [-1.8817,  0.9503, -1.0450,  0.0435,  0.0335,  1.7101],\n",
      "         [ 0.5433,  0.6048, -0.4462,  1.7440,  1.5210,  4.4105],\n",
      "         [ 1.6459, -0.3602,  0.3446,  1.5199, -2.6133, -0.6965],\n",
      "         [ 0.9463,  0.1563, -0.6136,  1.0316, -0.4927,  1.2484],\n",
      "         [ 0.7502,  0.4145, -0.1734,  1.1835,  1.3894,  2.5863],\n",
      "         [-1.6293,  0.4503, -0.4798,  0.5003, -1.0670,  2.1149],\n",
      "         [ 1.1108,  2.2899, -1.4782,  3.5672, -0.4731,  1.3356],\n",
      "         [-0.2897,  1.0525,  0.5229,  3.3022, -1.4689, -0.5867],\n",
      "         [-1.5312, -0.2341,  1.8197,  0.4485, -0.5692,  1.9200]],\n",
      "\n",
      "        [[-0.7879, -0.0094, -0.4334,  0.4992, -1.0648,  2.1149],\n",
      "         [ 0.9344, -0.1258,  0.6544,  0.2689,  1.3772,  1.6596],\n",
      "         [-0.7879, -0.0094, -0.4334,  0.4992, -1.0648,  2.1149],\n",
      "         [ 0.9613,  1.7780,  1.1632,  0.7516, -1.3505, -0.6959],\n",
      "         [ 1.3848,  0.1451, -0.3998,  1.7429,  1.5231,  4.4105],\n",
      "         [ 0.9613,  1.7780,  1.1632,  0.7516, -1.3505, -0.6959],\n",
      "         [-0.6897, -0.6938,  1.8661,  0.4474, -0.5671,  1.9200],\n",
      "         [ 1.6834,  0.1403,  1.0859,  1.3571, -0.2438,  3.3025],\n",
      "         [-1.0402,  0.4906, -0.9986,  0.0424,  0.0357,  1.7101],\n",
      "         [ 0.7008,  1.3461, -0.0469,  1.6860, -0.8362,  1.0009]],\n",
      "\n",
      "        [[ 0.5933, -2.5314,  0.4150, -0.2676,  0.3543,  1.3081],\n",
      "         [ 1.4526, -0.8113, -0.3535,  1.7397,  1.5253,  4.4105],\n",
      "         [ 0.6811, -0.1362, -0.6088,  2.0324, -0.5994, -0.2788],\n",
      "         [ 1.3859, -1.4325,  0.2731,  1.1040, -0.7505,  1.2443],\n",
      "         [ 2.5552, -1.7763,  0.4373,  1.5156, -2.6090, -0.6965],\n",
      "         [ 1.8556, -1.2598, -0.5209,  1.0273, -0.4884,  1.2484],\n",
      "         [ 0.6196, -0.3636,  0.6156,  3.2979, -1.4646, -0.5867],\n",
      "         [ 1.3859, -1.4325,  0.2731,  1.1040, -0.7505,  1.2443],\n",
      "         [ 0.6811, -0.1362, -0.6088,  2.0324, -0.5994, -0.2788],\n",
      "         [ 1.6595, -1.0016, -0.0807,  1.1792,  1.3937,  2.5863]],\n",
      "\n",
      "        [[ 1.0874, -1.8337, -0.4748,  1.0219, -0.4862,  1.2484],\n",
      "         [-0.1749, -3.1052,  0.4611, -0.2730,  0.3564,  1.3081],\n",
      "         [ 0.8913, -1.5755, -0.0346,  1.1738,  1.3958,  2.5863],\n",
      "         [-1.4882, -1.5397, -0.3410,  0.4906, -1.0605,  2.1149],\n",
      "         [ 0.5808, -0.8776,  0.7796,  1.4315, -0.0958,  1.7924],\n",
      "         [ 1.0874, -1.8337, -0.4748,  1.0219, -0.4862,  1.2484],\n",
      "         [-0.1485, -0.9375,  0.6617,  3.2925, -1.4624, -0.5867],\n",
      "         [-0.1749, -3.1052,  0.4611, -0.2730,  0.3564,  1.3081],\n",
      "         [-1.3901, -2.2241,  1.9585,  0.4388, -0.5628,  1.9200],\n",
      "         [ 0.6844, -1.3851, -0.3074,  1.7343,  1.5274,  4.4105]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "query shape =  torch.Size([4, 10, 6])\n",
      "MASK1 =  tensor([[0., -inf, -inf, -inf],\n",
      "        [0., 0., -inf, -inf],\n",
      "        [0., 0., 0., -inf],\n",
      "        [0., 0., 0., 0.]]) False\n",
      "\n",
      " _in_projection_packed \n",
      "\n",
      "Shapes =  torch.Size([4, 10, 6]) torch.Size([4, 10, 6]) torch.Size([4, 10, 6])\n",
      "\n",
      "Q :-  tensor([[[-1.4552,  0.5793,  1.1043, -0.2463, -0.9246,  0.0794],\n",
      "         [-0.9463,  0.2863, -0.5349,  1.5734,  1.4188, -0.4736],\n",
      "         [ 0.9250,  0.6758, -1.1312,  1.5125,  1.9137, -0.4423],\n",
      "         [-0.3673,  1.0187,  1.5554, -0.9398,  0.1094,  0.0388],\n",
      "         [ 0.1726,  0.7615,  0.4986,  0.3146,  0.7429, -0.0921],\n",
      "         [ 0.8563,  0.2781, -0.6744,  0.7538,  0.8798, -0.1626],\n",
      "         [-0.9973,  0.7058, -0.5184,  1.3484,  1.8594, -0.6221],\n",
      "         [-0.9163,  1.0306,  1.6244,  0.8897,  1.1200, -0.1834],\n",
      "         [-1.4651,  0.1525,  0.8135, -0.1476,  0.7686, -0.3392],\n",
      "         [-0.4022,  0.0795, -1.6476,  0.4659,  1.1998, -0.5977]],\n",
      "\n",
      "        [[-0.4717,  0.8204, -0.3588,  1.0109,  1.6849, -0.4990],\n",
      "         [ 1.1548, -0.0225, -0.8391,  0.0722,  0.0465,  0.0289],\n",
      "         [-0.4717,  0.8204, -0.3588,  1.0109,  1.6849, -0.4990],\n",
      "         [-0.9296,  0.6939,  1.2640, -0.5838, -1.0991,  0.2025],\n",
      "         [ 1.4505,  0.7904, -0.9715,  1.1751,  1.7392, -0.3192],\n",
      "         [-0.9296,  0.6939,  1.2640, -0.5838, -1.0991,  0.2025],\n",
      "         [ 0.1234,  0.1942, -1.4879,  0.1284,  1.0253, -0.4746],\n",
      "         [ 0.9064,  1.1511, -0.4595,  0.2545,  0.9119, -0.2173],\n",
      "         [-0.4207,  0.4009, -0.3753,  1.2359,  1.2443, -0.3505],\n",
      "         [-0.6015,  0.8501,  0.8121,  0.3264,  0.4492, -0.1213]],\n",
      "\n",
      "        [[ 1.6194, -0.1178, -1.2677, -0.2970,  0.9185, -0.1552],\n",
      "         [ 1.8677,  0.6317, -1.2890,  0.9781,  2.0269, -0.3672],\n",
      "         [-0.1710,  0.1705,  0.7718, -0.1649,  0.6675, -0.0899],\n",
      "         [ 0.9377,  0.5774, -0.0895, -0.3567,  0.9347, -0.1394],\n",
      "         [ 0.5754,  0.9746,  1.3975, -1.4742,  0.2226,  0.1139],\n",
      "         [ 1.1153,  0.7174,  0.3408, -0.2198,  0.8561, -0.0170],\n",
      "         [-0.5224,  0.1084,  0.6557, -0.6820,  0.8819, -0.2641],\n",
      "         [ 0.9377,  0.5774, -0.0895, -0.3567,  0.9347, -0.1394],\n",
      "         [-0.1710,  0.1705,  0.7718, -0.1649,  0.6675, -0.0899],\n",
      "         [ 1.7990,  0.2340, -0.8322,  0.2194,  0.9930, -0.0875]],\n",
      "\n",
      "        [[ 1.0439,  0.4257, -0.1776, -0.1077,  1.3278, -0.1918],\n",
      "         [ 1.5479, -0.4095, -1.7861, -0.1849,  1.3902, -0.3299],\n",
      "         [ 1.7276, -0.0578, -1.3506,  0.3316,  1.4646, -0.2622],\n",
      "         [-0.1260,  0.3699, -1.1947,  0.9261,  2.4443, -0.7218],\n",
      "         [ 0.6947,  0.2654, -0.7723,  0.0086,  1.1295, -0.3056],\n",
      "         [ 1.0439,  0.4257, -0.1776, -0.1077,  1.3278, -0.1918],\n",
      "         [-0.5938, -0.1834,  0.1373, -0.5698,  1.3535, -0.4389],\n",
      "         [ 1.5479, -0.4095, -1.7861, -0.1849,  1.3902, -0.3299],\n",
      "         [ 0.4690, -0.2563, -2.3238,  0.0436,  1.7847, -0.6973],\n",
      "         [ 1.7962,  0.3400, -1.8074,  1.0903,  2.4986, -0.5420]]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "\n",
      "K :-  tensor([[[-0.7554,  1.7286, -0.8978,  1.0559,  0.9876, -0.9088],\n",
      "         [-0.2158,  0.7585,  1.4128, -0.6221,  0.3783, -1.4847],\n",
      "         [-1.4214,  0.6571,  1.4936, -1.6008,  0.3131, -1.3995],\n",
      "         [-1.3343,  0.8064, -1.0069, -0.7735,  0.6453,  0.1352],\n",
      "         [-0.7292,  0.4698,  0.1795, -0.9364,  0.1884, -0.5898],\n",
      "         [-0.7070,  0.2021,  0.6897, -0.7972,  0.0957, -0.7696],\n",
      "         [-1.1980,  1.1878,  1.4262, -1.1918,  0.6934, -1.2046],\n",
      "         [-0.9475,  1.4121, -0.1886, -1.0630,  1.2727, -2.9474],\n",
      "         [-1.3567,  1.1633, -0.6284, -0.5527,  1.7772, -1.7201],\n",
      "         [-1.8477,  0.9964,  1.0288, -0.5919,  0.8519,  0.0243]],\n",
      "\n",
      "        [[-1.2473,  0.9376,  1.1841, -1.3301,  0.4544, -0.7485],\n",
      "         [-0.4551, -0.1594,  0.2473, -0.1606, -0.2279,  0.2952],\n",
      "         [-1.2473,  0.9376,  1.1841, -1.3301,  0.4544, -0.7485],\n",
      "         [-0.8047,  1.4784, -1.1399,  0.9175,  0.7486, -0.4527],\n",
      "         [-1.4707,  0.4070,  1.2515, -1.7391,  0.0741, -0.9433],\n",
      "         [-0.8047,  1.4784, -1.1399,  0.9175,  0.7486, -0.4527],\n",
      "         [-1.8970,  0.7462,  0.7866, -0.7302,  0.6129,  0.4804],\n",
      "         [-2.2158,  1.1479,  0.4586, -1.1509,  0.3941, -0.0855],\n",
      "         [-0.2651,  0.5083,  1.1707, -0.7605,  0.1393, -1.0286],\n",
      "         [-1.0838,  1.2662, -0.1494, -0.4420,  0.8095, -1.2754]],\n",
      "\n",
      "        [[-0.6435, -1.0317,  0.6396, -1.2680, -0.5859,  1.5287],\n",
      "         [-1.5484, -0.0611,  1.3556, -2.1476, -0.0749, -0.4453],\n",
      "         [-0.4345,  0.0176, -0.3898, -0.8830,  0.5938, -0.7997],\n",
      "         [-1.3003, -0.0599,  0.0913, -1.4286,  0.0767,  0.5807],\n",
      "         [-1.4613,  0.0881, -1.1450, -1.3203,  0.2574,  1.0893],\n",
      "         [-0.8561, -0.2485,  0.0415, -1.4832, -0.1995,  0.3643],\n",
      "         [-1.4836,  0.4451, -0.7665, -1.0995,  1.3892, -0.7659],\n",
      "         [-1.3003, -0.0599,  0.0913, -1.4286,  0.0767,  0.5807],\n",
      "         [-0.4345,  0.0176, -0.3898, -0.8830,  0.5938, -0.7997],\n",
      "         [-0.8340, -0.5162,  0.5516, -1.3440, -0.2922,  0.1845]],\n",
      "\n",
      "        [[-0.9058, -0.4974,  0.3891, -1.7719, -0.1159,  0.4630],\n",
      "         [-0.6931, -1.2807,  0.9873, -1.5567, -0.5022,  1.6273],\n",
      "         [-0.8836, -0.7651,  0.8993, -1.6327, -0.2086,  0.2832],\n",
      "         [-1.3746,  0.2206,  1.6358, -2.0272,  0.3891, -0.1518],\n",
      "         [-1.5041,  0.1916,  0.4196, -1.2258,  0.4304,  0.1108],\n",
      "         [-0.9058, -0.4974,  0.3891, -1.7719, -0.1159,  0.4630],\n",
      "         [-1.5333,  0.1961, -0.4188, -1.3881,  1.4728, -0.6673],\n",
      "         [-0.6931, -1.2807,  0.9873, -1.5567, -0.5022,  1.6273],\n",
      "         [-2.0243,  0.0292,  1.2384, -1.4273,  0.5476,  1.0771],\n",
      "         [-1.5980, -0.3101,  1.7033, -2.4362,  0.0088, -0.3467]]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "\n",
      "V :-  tensor([[[ 9.9826e-01,  1.1588e-01,  1.0535e+00,  4.7661e-01, -8.0867e-01,\n",
      "          -1.3011e+00],\n",
      "         [-2.0628e-01,  1.8654e+00, -8.2431e-02, -2.3835e-01, -1.2486e-01,\n",
      "           8.0209e-01],\n",
      "         [-1.8096e+00,  2.2229e-01,  4.4281e-01, -2.4140e-01, -6.3873e-01,\n",
      "           2.6339e+00],\n",
      "         [ 3.6765e-01, -9.3427e-01,  3.6855e-01,  1.3489e+00, -5.9482e-01,\n",
      "           1.1218e-01],\n",
      "         [-7.2794e-01, -1.2974e-01,  2.8027e-01,  6.8065e-01, -7.3234e-01,\n",
      "           1.0993e+00],\n",
      "         [-1.3055e+00, -2.8817e-01,  3.1656e-01, -2.4987e-01, -5.6413e-01,\n",
      "           1.6002e+00],\n",
      "         [ 1.1293e-01,  1.7727e+00,  3.2291e-02,  1.2031e-01,  2.4727e-01,\n",
      "           1.0063e+00],\n",
      "         [-1.3803e+00, -3.4877e-01,  5.7254e-01,  6.4705e-01, -2.9206e+00,\n",
      "           1.7819e+00],\n",
      "         [ 4.2389e-01, -5.9185e-01, -1.0787e-02, -2.4479e-01, -1.2314e+00,\n",
      "           3.8306e-01],\n",
      "         [ 8.8852e-01,  8.3528e-01,  1.2300e-01, -7.2057e-01,  1.4936e+00,\n",
      "           2.4773e-01]],\n",
      "\n",
      "        [[-1.2033e-01,  1.2405e+00,  8.6981e-02,  3.9194e-01,  2.3666e-01,\n",
      "           1.1554e+00],\n",
      "         [-7.3881e-01, -6.5794e-01,  3.2566e-01, -3.2283e-01,  1.5291e-01,\n",
      "           7.0044e-01],\n",
      "         [-1.2033e-01,  1.2405e+00,  8.6981e-02,  3.9194e-01,  2.3666e-01,\n",
      "           1.1554e+00],\n",
      "         [ 7.6500e-01, -4.1629e-01,  1.1082e+00,  7.4823e-01, -8.1928e-01,\n",
      "          -1.1521e+00],\n",
      "         [-2.0428e+00, -3.0987e-01,  4.9750e-01,  3.0223e-02, -6.4934e-01,\n",
      "           2.7830e+00],\n",
      "         [ 7.6500e-01, -4.1629e-01,  1.1082e+00,  7.4823e-01, -8.1928e-01,\n",
      "          -1.1521e+00],\n",
      "         [ 6.5525e-01,  3.0311e-01,  1.7769e-01, -4.4895e-01,  1.4830e+00,\n",
      "           3.9679e-01],\n",
      "         [-7.2173e-01, -4.9650e-01,  9.9179e-01,  6.0714e-01, -1.2490e-02,\n",
      "           1.4317e+00],\n",
      "         [-4.3955e-01,  1.3332e+00, -2.7741e-02,  3.3270e-02, -1.3547e-01,\n",
      "           9.5114e-01],\n",
      "         [-2.9830e-01, -6.6192e-02,  6.7677e-01,  5.8927e-01, -1.1738e+00,\n",
      "           5.9630e-01]],\n",
      "\n",
      "        [[-4.5990e-01, -6.0168e-01, -5.5067e-01, -1.6172e-01,  1.3981e+00,\n",
      "           1.1582e+00],\n",
      "         [-2.1049e+00, -5.7002e-01,  1.5704e-01, -8.3138e-02, -2.2078e-01,\n",
      "           3.0860e+00],\n",
      "         [-5.0765e-01, -8.0769e-01, -2.7915e-01,  1.8098e-01, -1.0631e+00,\n",
      "           9.4828e-01],\n",
      "         [-5.1538e-01, -8.7015e-01, -2.8922e-02,  5.3999e-01,  2.3188e-01,\n",
      "           1.2887e+00],\n",
      "         [ 7.2291e-02, -1.7266e+00,  8.2789e-02,  1.5072e+00, -1.7686e-01,\n",
      "           5.6428e-01],\n",
      "         [-1.0233e+00, -9.2205e-01, -5.4934e-03,  8.3892e-01, -3.1438e-01,\n",
      "           1.5514e+00],\n",
      "         [ 1.2854e-01, -1.3842e+00, -2.9655e-01, -8.6533e-02, -8.1345e-01,\n",
      "           8.3516e-01],\n",
      "         [-5.1538e-01, -8.7015e-01, -2.8922e-02,  5.3999e-01,  2.3188e-01,\n",
      "           1.2887e+00],\n",
      "         [-5.0765e-01, -8.0769e-01, -2.7915e-01,  1.8098e-01, -1.0631e+00,\n",
      "           9.4828e-01],\n",
      "         [-1.6009e+00, -1.0805e+00,  3.0794e-02, -9.1605e-02, -1.4617e-01,\n",
      "           2.0523e+00]],\n",
      "\n",
      "        [[-8.3972e-01, -6.8078e-01, -4.2016e-01,  4.3451e-01,  1.7881e-01,\n",
      "           1.7117e+00],\n",
      "         [-2.7632e-01, -3.6041e-01, -9.6534e-01, -5.6613e-01,  1.8912e+00,\n",
      "           1.3184e+00],\n",
      "         [-1.4173e+00, -8.3921e-01, -3.8387e-01, -4.9601e-01,  3.4702e-01,\n",
      "           2.2125e+00],\n",
      "         [ 1.1502e-03,  1.2217e+00, -6.6814e-01, -1.2583e-01,  1.1584e+00,\n",
      "           1.6186e+00],\n",
      "         [-4.0084e-01, -6.0786e-01, -2.4693e-03, -1.6253e-01,  3.6745e-01,\n",
      "           1.2956e+00],\n",
      "         [-8.3972e-01, -6.8078e-01, -4.2016e-01,  4.3451e-01,  1.7881e-01,\n",
      "           1.7117e+00],\n",
      "         [ 3.1211e-01, -1.1429e+00, -7.1122e-01, -4.9094e-01, -3.2026e-01,\n",
      "           9.9539e-01],\n",
      "         [-2.7632e-01, -3.6041e-01, -9.6534e-01, -5.6613e-01,  1.8912e+00,\n",
      "           1.3184e+00],\n",
      "         [ 7.7673e-01,  2.8424e-01, -5.7743e-01, -9.6672e-01,  2.4048e+00,\n",
      "           8.6006e-01],\n",
      "         [-1.9214e+00, -3.2874e-01, -2.5762e-01, -4.8755e-01,  2.7241e-01,\n",
      "           3.2463e+00]]], grad_fn=<SelectBackward0>)\n",
      "\n",
      "tgt_len =  4\n",
      "bsz =  10\n",
      "num_heads =  2\n",
      "head_dim =  3\n",
      "\n",
      "MASK2 =  tensor([[[0., -inf, -inf, -inf],\n",
      "         [0., 0., -inf, -inf],\n",
      "         [0., 0., 0., -inf],\n",
      "         [0., 0., 0., 0.]]])\n",
      "Attention mask NOT NONE =  tensor([[[[0., -inf, -inf, -inf],\n",
      "          [0., 0., -inf, -inf],\n",
      "          [0., 0., 0., -inf],\n",
      "          [0., 0., 0., 0.]]]])\n",
      "\n",
      "SHAPES RIGHT BEFORE ATTENTION :-  torch.Size([10, 2, 4, 3]) torch.Size([10, 2, 4, 3]) torch.Size([10, 2, 4, 3])\n",
      "\n",
      "Q reshaped =  tensor([[[[-1.4552,  0.5793,  1.1043],\n",
      "          [-0.4717,  0.8204, -0.3588],\n",
      "          [ 1.6194, -0.1178, -1.2677],\n",
      "          [ 1.0439,  0.4257, -0.1776]],\n",
      "\n",
      "         [[-0.2463, -0.9246,  0.0794],\n",
      "          [ 1.0109,  1.6849, -0.4990],\n",
      "          [-0.2970,  0.9185, -0.1552],\n",
      "          [-0.1077,  1.3278, -0.1918]]],\n",
      "\n",
      "\n",
      "        [[[-0.9463,  0.2863, -0.5349],\n",
      "          [ 1.1548, -0.0225, -0.8391],\n",
      "          [ 1.8677,  0.6317, -1.2890],\n",
      "          [ 1.5479, -0.4095, -1.7861]],\n",
      "\n",
      "         [[ 1.5734,  1.4188, -0.4736],\n",
      "          [ 0.0722,  0.0465,  0.0289],\n",
      "          [ 0.9781,  2.0269, -0.3672],\n",
      "          [-0.1849,  1.3902, -0.3299]]],\n",
      "\n",
      "\n",
      "        [[[ 0.9250,  0.6758, -1.1312],\n",
      "          [-0.4717,  0.8204, -0.3588],\n",
      "          [-0.1710,  0.1705,  0.7718],\n",
      "          [ 1.7276, -0.0578, -1.3506]],\n",
      "\n",
      "         [[ 1.5125,  1.9137, -0.4423],\n",
      "          [ 1.0109,  1.6849, -0.4990],\n",
      "          [-0.1649,  0.6675, -0.0899],\n",
      "          [ 0.3316,  1.4646, -0.2622]]],\n",
      "\n",
      "\n",
      "        [[[-0.3673,  1.0187,  1.5554],\n",
      "          [-0.9296,  0.6939,  1.2640],\n",
      "          [ 0.9377,  0.5774, -0.0895],\n",
      "          [-0.1260,  0.3699, -1.1947]],\n",
      "\n",
      "         [[-0.9398,  0.1094,  0.0388],\n",
      "          [-0.5838, -1.0991,  0.2025],\n",
      "          [-0.3567,  0.9347, -0.1394],\n",
      "          [ 0.9261,  2.4443, -0.7218]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1726,  0.7615,  0.4986],\n",
      "          [ 1.4505,  0.7904, -0.9715],\n",
      "          [ 0.5754,  0.9746,  1.3975],\n",
      "          [ 0.6947,  0.2654, -0.7723]],\n",
      "\n",
      "         [[ 0.3146,  0.7429, -0.0921],\n",
      "          [ 1.1751,  1.7392, -0.3192],\n",
      "          [-1.4742,  0.2226,  0.1139],\n",
      "          [ 0.0086,  1.1295, -0.3056]]],\n",
      "\n",
      "\n",
      "        [[[ 0.8563,  0.2781, -0.6744],\n",
      "          [-0.9296,  0.6939,  1.2640],\n",
      "          [ 1.1153,  0.7174,  0.3408],\n",
      "          [ 1.0439,  0.4257, -0.1776]],\n",
      "\n",
      "         [[ 0.7538,  0.8798, -0.1626],\n",
      "          [-0.5838, -1.0991,  0.2025],\n",
      "          [-0.2198,  0.8561, -0.0170],\n",
      "          [-0.1077,  1.3278, -0.1918]]],\n",
      "\n",
      "\n",
      "        [[[-0.9973,  0.7058, -0.5184],\n",
      "          [ 0.1234,  0.1942, -1.4879],\n",
      "          [-0.5224,  0.1084,  0.6557],\n",
      "          [-0.5938, -0.1834,  0.1373]],\n",
      "\n",
      "         [[ 1.3484,  1.8594, -0.6221],\n",
      "          [ 0.1284,  1.0253, -0.4746],\n",
      "          [-0.6820,  0.8819, -0.2641],\n",
      "          [-0.5698,  1.3535, -0.4389]]],\n",
      "\n",
      "\n",
      "        [[[-0.9163,  1.0306,  1.6244],\n",
      "          [ 0.9064,  1.1511, -0.4595],\n",
      "          [ 0.9377,  0.5774, -0.0895],\n",
      "          [ 1.5479, -0.4095, -1.7861]],\n",
      "\n",
      "         [[ 0.8897,  1.1200, -0.1834],\n",
      "          [ 0.2545,  0.9119, -0.2173],\n",
      "          [-0.3567,  0.9347, -0.1394],\n",
      "          [-0.1849,  1.3902, -0.3299]]],\n",
      "\n",
      "\n",
      "        [[[-1.4651,  0.1525,  0.8135],\n",
      "          [-0.4207,  0.4009, -0.3753],\n",
      "          [-0.1710,  0.1705,  0.7718],\n",
      "          [ 0.4690, -0.2563, -2.3238]],\n",
      "\n",
      "         [[-0.1476,  0.7686, -0.3392],\n",
      "          [ 1.2359,  1.2443, -0.3505],\n",
      "          [-0.1649,  0.6675, -0.0899],\n",
      "          [ 0.0436,  1.7847, -0.6973]]],\n",
      "\n",
      "\n",
      "        [[[-0.4022,  0.0795, -1.6476],\n",
      "          [-0.6015,  0.8501,  0.8121],\n",
      "          [ 1.7990,  0.2340, -0.8322],\n",
      "          [ 1.7962,  0.3400, -1.8074]],\n",
      "\n",
      "         [[ 0.4659,  1.1998, -0.5977],\n",
      "          [ 0.3264,  0.4492, -0.1213],\n",
      "          [ 0.2194,  0.9930, -0.0875],\n",
      "          [ 1.0903,  2.4986, -0.5420]]]], grad_fn=<ViewBackward0>)\n",
      "\n",
      "K reshaped =  tensor([[[[-0.7554,  1.7286, -0.8978],\n",
      "          [-1.2473,  0.9376,  1.1841],\n",
      "          [-0.6435, -1.0317,  0.6396],\n",
      "          [-0.9058, -0.4974,  0.3891]],\n",
      "\n",
      "         [[ 1.0559,  0.9876, -0.9088],\n",
      "          [-1.3301,  0.4544, -0.7485],\n",
      "          [-1.2680, -0.5859,  1.5287],\n",
      "          [-1.7719, -0.1159,  0.4630]]],\n",
      "\n",
      "\n",
      "        [[[-0.2158,  0.7585,  1.4128],\n",
      "          [-0.4551, -0.1594,  0.2473],\n",
      "          [-1.5484, -0.0611,  1.3556],\n",
      "          [-0.6931, -1.2807,  0.9873]],\n",
      "\n",
      "         [[-0.6221,  0.3783, -1.4847],\n",
      "          [-0.1606, -0.2279,  0.2952],\n",
      "          [-2.1476, -0.0749, -0.4453],\n",
      "          [-1.5567, -0.5022,  1.6273]]],\n",
      "\n",
      "\n",
      "        [[[-1.4214,  0.6571,  1.4936],\n",
      "          [-1.2473,  0.9376,  1.1841],\n",
      "          [-0.4345,  0.0176, -0.3898],\n",
      "          [-0.8836, -0.7651,  0.8993]],\n",
      "\n",
      "         [[-1.6008,  0.3131, -1.3995],\n",
      "          [-1.3301,  0.4544, -0.7485],\n",
      "          [-0.8830,  0.5938, -0.7997],\n",
      "          [-1.6327, -0.2086,  0.2832]]],\n",
      "\n",
      "\n",
      "        [[[-1.3343,  0.8064, -1.0069],\n",
      "          [-0.8047,  1.4784, -1.1399],\n",
      "          [-1.3003, -0.0599,  0.0913],\n",
      "          [-1.3746,  0.2206,  1.6358]],\n",
      "\n",
      "         [[-0.7735,  0.6453,  0.1352],\n",
      "          [ 0.9175,  0.7486, -0.4527],\n",
      "          [-1.4286,  0.0767,  0.5807],\n",
      "          [-2.0272,  0.3891, -0.1518]]],\n",
      "\n",
      "\n",
      "        [[[-0.7292,  0.4698,  0.1795],\n",
      "          [-1.4707,  0.4070,  1.2515],\n",
      "          [-1.4613,  0.0881, -1.1450],\n",
      "          [-1.5041,  0.1916,  0.4196]],\n",
      "\n",
      "         [[-0.9364,  0.1884, -0.5898],\n",
      "          [-1.7391,  0.0741, -0.9433],\n",
      "          [-1.3203,  0.2574,  1.0893],\n",
      "          [-1.2258,  0.4304,  0.1108]]],\n",
      "\n",
      "\n",
      "        [[[-0.7070,  0.2021,  0.6897],\n",
      "          [-0.8047,  1.4784, -1.1399],\n",
      "          [-0.8561, -0.2485,  0.0415],\n",
      "          [-0.9058, -0.4974,  0.3891]],\n",
      "\n",
      "         [[-0.7972,  0.0957, -0.7696],\n",
      "          [ 0.9175,  0.7486, -0.4527],\n",
      "          [-1.4832, -0.1995,  0.3643],\n",
      "          [-1.7719, -0.1159,  0.4630]]],\n",
      "\n",
      "\n",
      "        [[[-1.1980,  1.1878,  1.4262],\n",
      "          [-1.8970,  0.7462,  0.7866],\n",
      "          [-1.4836,  0.4451, -0.7665],\n",
      "          [-1.5333,  0.1961, -0.4188]],\n",
      "\n",
      "         [[-1.1918,  0.6934, -1.2046],\n",
      "          [-0.7302,  0.6129,  0.4804],\n",
      "          [-1.0995,  1.3892, -0.7659],\n",
      "          [-1.3881,  1.4728, -0.6673]]],\n",
      "\n",
      "\n",
      "        [[[-0.9475,  1.4121, -0.1886],\n",
      "          [-2.2158,  1.1479,  0.4586],\n",
      "          [-1.3003, -0.0599,  0.0913],\n",
      "          [-0.6931, -1.2807,  0.9873]],\n",
      "\n",
      "         [[-1.0630,  1.2727, -2.9474],\n",
      "          [-1.1509,  0.3941, -0.0855],\n",
      "          [-1.4286,  0.0767,  0.5807],\n",
      "          [-1.5567, -0.5022,  1.6273]]],\n",
      "\n",
      "\n",
      "        [[[-1.3567,  1.1633, -0.6284],\n",
      "          [-0.2651,  0.5083,  1.1707],\n",
      "          [-0.4345,  0.0176, -0.3898],\n",
      "          [-2.0243,  0.0292,  1.2384]],\n",
      "\n",
      "         [[-0.5527,  1.7772, -1.7201],\n",
      "          [-0.7605,  0.1393, -1.0286],\n",
      "          [-0.8830,  0.5938, -0.7997],\n",
      "          [-1.4273,  0.5476,  1.0771]]],\n",
      "\n",
      "\n",
      "        [[[-1.8477,  0.9964,  1.0288],\n",
      "          [-1.0838,  1.2662, -0.1494],\n",
      "          [-0.8340, -0.5162,  0.5516],\n",
      "          [-1.5980, -0.3101,  1.7033]],\n",
      "\n",
      "         [[-0.5919,  0.8519,  0.0243],\n",
      "          [-0.4420,  0.8095, -1.2754],\n",
      "          [-1.3440, -0.2922,  0.1845],\n",
      "          [-2.4362,  0.0088, -0.3467]]]], grad_fn=<ViewBackward0>)\n",
      "\n",
      "V reshaped =  tensor([[[[ 9.9826e-01,  1.1588e-01,  1.0535e+00],\n",
      "          [-1.2033e-01,  1.2405e+00,  8.6981e-02],\n",
      "          [-4.5990e-01, -6.0168e-01, -5.5067e-01],\n",
      "          [-8.3972e-01, -6.8078e-01, -4.2016e-01]],\n",
      "\n",
      "         [[ 4.7661e-01, -8.0867e-01, -1.3011e+00],\n",
      "          [ 3.9194e-01,  2.3666e-01,  1.1554e+00],\n",
      "          [-1.6172e-01,  1.3981e+00,  1.1582e+00],\n",
      "          [ 4.3451e-01,  1.7881e-01,  1.7117e+00]]],\n",
      "\n",
      "\n",
      "        [[[-2.0628e-01,  1.8654e+00, -8.2431e-02],\n",
      "          [-7.3881e-01, -6.5794e-01,  3.2566e-01],\n",
      "          [-2.1049e+00, -5.7002e-01,  1.5704e-01],\n",
      "          [-2.7632e-01, -3.6041e-01, -9.6534e-01]],\n",
      "\n",
      "         [[-2.3835e-01, -1.2486e-01,  8.0209e-01],\n",
      "          [-3.2283e-01,  1.5291e-01,  7.0044e-01],\n",
      "          [-8.3138e-02, -2.2078e-01,  3.0860e+00],\n",
      "          [-5.6613e-01,  1.8912e+00,  1.3184e+00]]],\n",
      "\n",
      "\n",
      "        [[[-1.8096e+00,  2.2229e-01,  4.4281e-01],\n",
      "          [-1.2033e-01,  1.2405e+00,  8.6981e-02],\n",
      "          [-5.0765e-01, -8.0769e-01, -2.7915e-01],\n",
      "          [-1.4173e+00, -8.3921e-01, -3.8387e-01]],\n",
      "\n",
      "         [[-2.4140e-01, -6.3873e-01,  2.6339e+00],\n",
      "          [ 3.9194e-01,  2.3666e-01,  1.1554e+00],\n",
      "          [ 1.8098e-01, -1.0631e+00,  9.4828e-01],\n",
      "          [-4.9601e-01,  3.4702e-01,  2.2125e+00]]],\n",
      "\n",
      "\n",
      "        [[[ 3.6765e-01, -9.3427e-01,  3.6855e-01],\n",
      "          [ 7.6500e-01, -4.1629e-01,  1.1082e+00],\n",
      "          [-5.1538e-01, -8.7015e-01, -2.8922e-02],\n",
      "          [ 1.1502e-03,  1.2217e+00, -6.6814e-01]],\n",
      "\n",
      "         [[ 1.3489e+00, -5.9482e-01,  1.1218e-01],\n",
      "          [ 7.4823e-01, -8.1928e-01, -1.1521e+00],\n",
      "          [ 5.3999e-01,  2.3188e-01,  1.2887e+00],\n",
      "          [-1.2583e-01,  1.1584e+00,  1.6186e+00]]],\n",
      "\n",
      "\n",
      "        [[[-7.2794e-01, -1.2974e-01,  2.8027e-01],\n",
      "          [-2.0428e+00, -3.0987e-01,  4.9750e-01],\n",
      "          [ 7.2291e-02, -1.7266e+00,  8.2789e-02],\n",
      "          [-4.0084e-01, -6.0786e-01, -2.4693e-03]],\n",
      "\n",
      "         [[ 6.8065e-01, -7.3234e-01,  1.0993e+00],\n",
      "          [ 3.0223e-02, -6.4934e-01,  2.7830e+00],\n",
      "          [ 1.5072e+00, -1.7686e-01,  5.6428e-01],\n",
      "          [-1.6253e-01,  3.6745e-01,  1.2956e+00]]],\n",
      "\n",
      "\n",
      "        [[[-1.3055e+00, -2.8817e-01,  3.1656e-01],\n",
      "          [ 7.6500e-01, -4.1629e-01,  1.1082e+00],\n",
      "          [-1.0233e+00, -9.2205e-01, -5.4934e-03],\n",
      "          [-8.3972e-01, -6.8078e-01, -4.2016e-01]],\n",
      "\n",
      "         [[-2.4987e-01, -5.6413e-01,  1.6002e+00],\n",
      "          [ 7.4823e-01, -8.1928e-01, -1.1521e+00],\n",
      "          [ 8.3892e-01, -3.1438e-01,  1.5514e+00],\n",
      "          [ 4.3451e-01,  1.7881e-01,  1.7117e+00]]],\n",
      "\n",
      "\n",
      "        [[[ 1.1293e-01,  1.7727e+00,  3.2291e-02],\n",
      "          [ 6.5525e-01,  3.0311e-01,  1.7769e-01],\n",
      "          [ 1.2854e-01, -1.3842e+00, -2.9655e-01],\n",
      "          [ 3.1211e-01, -1.1429e+00, -7.1122e-01]],\n",
      "\n",
      "         [[ 1.2031e-01,  2.4727e-01,  1.0063e+00],\n",
      "          [-4.4895e-01,  1.4830e+00,  3.9679e-01],\n",
      "          [-8.6533e-02, -8.1345e-01,  8.3516e-01],\n",
      "          [-4.9094e-01, -3.2026e-01,  9.9539e-01]]],\n",
      "\n",
      "\n",
      "        [[[-1.3803e+00, -3.4877e-01,  5.7254e-01],\n",
      "          [-7.2173e-01, -4.9650e-01,  9.9179e-01],\n",
      "          [-5.1538e-01, -8.7015e-01, -2.8922e-02],\n",
      "          [-2.7632e-01, -3.6041e-01, -9.6534e-01]],\n",
      "\n",
      "         [[ 6.4705e-01, -2.9206e+00,  1.7819e+00],\n",
      "          [ 6.0714e-01, -1.2490e-02,  1.4317e+00],\n",
      "          [ 5.3999e-01,  2.3188e-01,  1.2887e+00],\n",
      "          [-5.6613e-01,  1.8912e+00,  1.3184e+00]]],\n",
      "\n",
      "\n",
      "        [[[ 4.2389e-01, -5.9185e-01, -1.0787e-02],\n",
      "          [-4.3955e-01,  1.3332e+00, -2.7741e-02],\n",
      "          [-5.0765e-01, -8.0769e-01, -2.7915e-01],\n",
      "          [ 7.7673e-01,  2.8424e-01, -5.7743e-01]],\n",
      "\n",
      "         [[-2.4479e-01, -1.2314e+00,  3.8306e-01],\n",
      "          [ 3.3270e-02, -1.3547e-01,  9.5114e-01],\n",
      "          [ 1.8098e-01, -1.0631e+00,  9.4828e-01],\n",
      "          [-9.6672e-01,  2.4048e+00,  8.6006e-01]]],\n",
      "\n",
      "\n",
      "        [[[ 8.8852e-01,  8.3528e-01,  1.2300e-01],\n",
      "          [-2.9830e-01, -6.6192e-02,  6.7677e-01],\n",
      "          [-1.6009e+00, -1.0805e+00,  3.0794e-02],\n",
      "          [-1.9214e+00, -3.2874e-01, -2.5762e-01]],\n",
      "\n",
      "         [[-7.2057e-01,  1.4936e+00,  2.4773e-01],\n",
      "          [ 5.8927e-01, -1.1738e+00,  5.9630e-01],\n",
      "          [-9.1605e-02, -1.4617e-01,  2.0523e+00],\n",
      "          [-4.8755e-01,  2.7241e-01,  3.2463e+00]]]], grad_fn=<ViewBackward0>)\n",
      "\n",
      "Attention mask =  tensor([[[[0., -inf, -inf, -inf],\n",
      "          [0., 0., -inf, -inf],\n",
      "          [0., 0., 0., -inf],\n",
      "          [0., 0., 0., 0.]]]])\n",
      "scaled_dot_product_attention output = \n",
      "tensor([[[[ 9.9826e-01,  1.1588e-01,  1.0535e+00],\n",
      "          [ 6.2011e-01,  4.9608e-01,  7.2676e-01],\n",
      "          [ 4.9420e-01,  2.1751e-02,  5.2324e-01],\n",
      "          [ 1.3900e-01,  5.5737e-02,  2.6703e-01]],\n",
      "\n",
      "         [[ 4.7661e-01, -8.0867e-01, -1.3011e+00],\n",
      "          [ 4.6613e-01, -6.7932e-01, -9.9714e-01],\n",
      "          [ 3.1495e-01,  6.9270e-02,  2.2487e-01],\n",
      "          [ 3.7454e-01, -7.3843e-02,  2.5679e-01]]],\n",
      "\n",
      "\n",
      "        [[[-2.0628e-01,  1.8654e+00, -8.2431e-02],\n",
      "          [-5.2727e-01,  3.4440e-01,  1.6355e-01],\n",
      "          [-6.2629e-01,  3.5793e-01,  1.5000e-01],\n",
      "          [-6.2008e-01, -1.7967e-01, -8.1173e-02]],\n",
      "\n",
      "         [[-2.3835e-01, -1.2486e-01,  8.0209e-01],\n",
      "          [-2.8128e-01,  1.6291e-02,  7.5043e-01],\n",
      "          [-2.4203e-01, -6.2279e-02,  1.0532e+00],\n",
      "          [-2.5083e-01,  1.5089e-01,  1.4910e+00]]],\n",
      "\n",
      "\n",
      "        [[[-1.8096e+00,  2.2229e-01,  4.4281e-01],\n",
      "          [-9.0192e-01,  7.6941e-01,  2.5162e-01],\n",
      "          [-9.3508e-01,  4.5238e-01,  1.8532e-01],\n",
      "          [-7.0000e-01, -5.7470e-01, -2.2267e-01]],\n",
      "\n",
      "         [[-2.4140e-01, -6.3873e-01,  2.6339e+00],\n",
      "          [ 9.2331e-02, -1.7745e-01,  1.8548e+00],\n",
      "          [ 1.1023e-01, -4.9129e-01,  1.5781e+00],\n",
      "          [ 4.4797e-02, -4.1657e-01,  1.5984e+00]]],\n",
      "\n",
      "\n",
      "        [[[ 3.6765e-01, -9.3427e-01,  3.6855e-01],\n",
      "          [ 5.5520e-01, -6.8977e-01,  7.1768e-01],\n",
      "          [ 3.7652e-01, -6.6495e-01,  6.4940e-01],\n",
      "          [ 3.9969e-01, -5.8368e-01,  5.8598e-01]],\n",
      "\n",
      "         [[ 1.3489e+00, -5.9482e-01,  1.1218e-01],\n",
      "          [ 1.1502e+00, -6.6909e-01, -3.0614e-01],\n",
      "          [ 9.1454e-01, -4.0370e-01,  9.9244e-02],\n",
      "          [ 7.8495e-01, -5.7573e-01, -5.8434e-01]]],\n",
      "\n",
      "\n",
      "        [[[-7.2794e-01, -1.2974e-01,  2.8027e-01],\n",
      "          [-1.0206e+00, -1.6983e-01,  3.2861e-01],\n",
      "          [-1.4532e+00, -3.5230e-01,  3.9537e-01],\n",
      "          [-5.4253e-01, -8.4796e-01,  1.8196e-01]],\n",
      "\n",
      "         [[ 6.8065e-01, -7.3234e-01,  1.0993e+00],\n",
      "          [ 4.4932e-01, -7.0282e-01,  1.6982e+00],\n",
      "          [ 6.9210e-01, -5.0275e-01,  1.6306e+00],\n",
      "          [ 4.5143e-01, -2.9758e-01,  1.4800e+00]]],\n",
      "\n",
      "\n",
      "        [[[-1.3055e+00, -2.8817e-01,  3.1656e-01],\n",
      "          [-6.5085e-01, -3.2868e-01,  5.6686e-01],\n",
      "          [-4.0869e-01, -4.9105e-01,  5.5665e-01],\n",
      "          [-3.9431e-01, -5.3957e-01,  4.0771e-01]],\n",
      "\n",
      "         [[-2.4987e-01, -5.6413e-01,  1.6002e+00],\n",
      "          [ 2.7456e-02, -6.3502e-01,  8.3549e-01],\n",
      "          [ 4.4769e-01, -5.8048e-01,  5.8262e-01],\n",
      "          [ 4.5008e-01, -4.6205e-01,  6.1683e-01]]],\n",
      "\n",
      "\n",
      "        [[[ 1.1293e-01,  1.7727e+00,  3.2291e-02],\n",
      "          [ 4.4411e-01,  8.7526e-01,  1.2108e-01],\n",
      "          [ 3.2924e-01,  5.9695e-01,  2.7240e-02],\n",
      "          [ 3.2387e-01, -1.0630e-01, -1.8545e-01]],\n",
      "\n",
      "         [[ 1.2031e-01,  2.4727e-01,  1.0063e+00],\n",
      "          [-9.7940e-02,  7.2106e-01,  7.7263e-01],\n",
      "          [-9.2551e-02,  4.1197e-02,  8.0067e-01],\n",
      "          [-2.2939e-01, -1.5561e-01,  8.8052e-01]]],\n",
      "\n",
      "\n",
      "        [[[-1.3803e+00, -3.4877e-01,  5.7254e-01],\n",
      "          [-1.2046e+00, -3.8818e-01,  6.8439e-01],\n",
      "          [-1.0075e+00, -5.1632e-01,  5.1444e-01],\n",
      "          [-7.9118e-01, -5.1365e-01,  1.6306e-02]],\n",
      "\n",
      "         [[ 6.4705e-01, -2.9206e+00,  1.7819e+00],\n",
      "          [ 6.3497e-01, -2.0404e+00,  1.6759e+00],\n",
      "          [ 6.1301e-01, -1.4661e+00,  1.5817e+00],\n",
      "          [ 5.4569e-01, -1.6743e+00,  1.6250e+00]]],\n",
      "\n",
      "\n",
      "        [[[ 4.2389e-01, -5.9185e-01, -1.0787e-02],\n",
      "          [ 1.5741e-01,  2.2750e-03, -1.6019e-02],\n",
      "          [-2.2767e-01,  3.1115e-01, -8.3533e-02],\n",
      "          [-5.7072e-02, -5.5414e-01, -1.5935e-01]],\n",
      "\n",
      "         [[-2.4479e-01, -1.2314e+00,  3.8306e-01],\n",
      "          [-1.9260e-01, -1.0257e+00,  4.8970e-01],\n",
      "          [-5.3348e-02, -9.1533e-01,  6.8572e-01],\n",
      "          [-2.0263e-01, -8.7386e-01,  5.4804e-01]]],\n",
      "\n",
      "\n",
      "        [[[ 8.8852e-01,  8.3528e-01,  1.2300e-01],\n",
      "          [ 4.9083e-01,  5.3321e-01,  3.0856e-01],\n",
      "          [-6.2932e-01, -3.2680e-01,  3.6941e-01],\n",
      "          [-6.2056e-01, -2.8055e-01,  4.2382e-01]],\n",
      "\n",
      "         [[-7.2057e-01,  1.4936e+00,  2.4773e-01],\n",
      "          [-3.0233e-02,  8.7800e-02,  4.3144e-01],\n",
      "          [-5.4347e-02,  7.0513e-02,  7.2785e-01],\n",
      "          [ 4.8039e-02, -1.1442e-01,  6.2996e-01]]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n",
      "\n",
      "x + self_atten(x) = \n",
      "tensor([[[-0.3122,  1.9212,  0.0869,  1.3286, -1.9496, -1.3372],\n",
      "         [-1.8587,  0.8548, -1.2832, -0.8033,  0.0267,  1.8937],\n",
      "         [ 0.9247, -0.4033,  0.4080,  1.0661,  1.6230,  3.9563],\n",
      "         [ 1.8626, -0.7520,  0.0310,  1.6646, -3.1393, -1.3132],\n",
      "         [ 1.1443, -0.5630, -0.4601,  0.6572, -0.7151,  0.7957],\n",
      "         [ 0.9035, -0.3118,  0.5518,  0.9530,  1.5317,  2.2057],\n",
      "         [-1.3001,  0.4726, -0.9063, -0.3908, -1.2827,  2.2726],\n",
      "         [ 0.6552,  0.6134, -0.8899,  3.2961, -0.5785, -0.0356],\n",
      "         [-0.7053,  0.9423,  0.8982,  3.7471, -1.2839, -1.2515],\n",
      "         [-1.0856,  0.5945,  1.5112,  0.4073, -0.5912,  2.3533]],\n",
      "\n",
      "        [[-1.1385, -0.3267, -1.3197,  0.7006, -1.5458,  1.7185],\n",
      "         [ 1.0577, -0.3433,  0.8548,  0.0228,  1.4332,  1.6389],\n",
      "         [-0.3682, -0.5461, -0.1891, -0.2599, -1.1592,  1.9330],\n",
      "         [ 1.0116,  1.3425,  0.5827,  1.0695, -1.9661, -1.3896],\n",
      "         [ 1.6914, -0.6527,  0.0184,  1.3043,  1.3921,  3.9054],\n",
      "         [ 0.9905,  1.1435,  1.4117,  0.7895, -1.4453, -1.2019],\n",
      "         [-0.2518, -0.3746,  1.6029,  0.0728, -0.7240,  2.0606],\n",
      "         [ 1.5502, -1.2607,  1.5156,  1.1142, -0.4534,  2.1856],\n",
      "         [-1.3595,  0.3083, -0.7642,  0.1412,  0.1650,  1.2426],\n",
      "         [ 0.8510,  1.4365, -0.3315,  1.5936, -1.0232,  0.8390]],\n",
      "\n",
      "        [[ 0.8082, -2.5746,  0.0441, -0.1777,  0.0077,  1.0063],\n",
      "         [ 1.6314, -1.0929, -0.0714,  1.4071,  1.5857,  4.3355],\n",
      "         [ 0.9176, -0.7420, -0.2754,  1.4410, -0.6291, -0.5461],\n",
      "         [ 1.5742, -1.7945, -0.1029,  1.3406, -1.2608,  0.6469],\n",
      "         [ 2.9917, -2.7522,  0.8595,  0.9866, -2.8146, -1.0975],\n",
      "         [ 1.9570, -1.8574, -0.4728,  1.0828, -0.7384,  0.7264],\n",
      "         [ 0.8041, -0.2364,  0.5593,  3.0359, -1.5141, -0.6820],\n",
      "         [ 1.4373, -2.5268,  0.7068,  0.8848, -0.9324,  0.3561],\n",
      "         [ 0.4706, -0.4489, -0.3804,  1.8209, -0.4949, -0.5993],\n",
      "         [ 1.8962, -1.3432,  0.1839,  1.1538,  1.3525,  2.4227]],\n",
      "\n",
      "        [[ 1.2443, -1.9731, -0.6879,  0.9429, -0.7254,  1.0535],\n",
      "         [ 0.1975, -3.2239,  1.0819, -0.5395,  0.5360,  1.2381],\n",
      "         [ 1.1501, -1.8769,  0.7492,  0.9459,  1.5978,  2.2926],\n",
      "         [-1.5851, -1.8929, -0.8170,  0.8536, -1.5028,  1.6047],\n",
      "         [ 1.0054, -1.3036,  1.2859,  1.3284, -0.1842,  1.3300],\n",
      "         [ 1.2337, -2.3373, -0.3605,  1.0451, -0.6872,  0.8077],\n",
      "         [-0.0479, -0.7420,  0.9891,  3.2996, -1.3160, -0.7912],\n",
      "         [-0.2489, -3.9780,  1.0905, -0.5401,  0.3770,  0.4831],\n",
      "         [-1.6328, -2.3766,  2.4692,  0.6532, -0.3254,  1.5045],\n",
      "         [ 0.8538, -1.8156, -0.1217,  1.7060,  1.4372,  4.1794]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "\n",
      "norm1(x + self_atten(x)) = \n",
      "tensor([[[-0.1971,  1.4421,  0.0959,  1.0072, -1.3988, -0.9493],\n",
      "         [-1.2994,  0.8199, -0.8499, -0.4751,  0.1731,  1.6313],\n",
      "         [-0.2489, -1.2278, -0.6298, -0.1447,  0.2658,  1.9855],\n",
      "         [ 1.2360, -0.2762,  0.1766,  1.1215, -1.6571, -0.6008],\n",
      "         [ 1.3516, -0.9534, -0.8145,  0.6940, -1.1587,  0.8811],\n",
      "         [-0.0880, -1.6430, -0.5381, -0.0247,  0.7157,  1.5781],\n",
      "         [-0.8837,  0.5264, -0.5705, -0.1605, -0.8698,  1.9581],\n",
      "         [ 0.1060,  0.0755, -1.0230,  2.0358, -0.7955, -0.3988],\n",
      "         [-0.6239,  0.3136,  0.2885,  1.9097, -0.9531, -0.9347],\n",
      "         [-1.3855,  0.0539,  0.8393, -0.1065, -0.9620,  1.5608]],\n",
      "\n",
      "        [[-0.6942, -0.0069, -0.8477,  0.8630, -1.0391,  1.7248],\n",
      "         [ 0.3911, -1.5636,  0.1081, -1.0527,  0.9151,  1.2021],\n",
      "         [-0.2804, -0.4653, -0.0944, -0.1679, -1.1022,  2.1102],\n",
      "         [ 0.6984,  0.9542,  0.3667,  0.7431, -1.6041, -1.1584],\n",
      "         [ 0.2886, -1.3419, -0.8750,  0.0193,  0.0804,  1.8285],\n",
      "         [ 0.6156,  0.7483,  0.9812,  0.4411, -1.4987, -1.2875],\n",
      "         [-0.6192, -0.7363,  1.1492, -0.3098, -1.0695,  1.5856],\n",
      "         [ 0.6360, -1.6710,  0.6076,  0.2781, -1.0084,  1.1575],\n",
      "         [-1.5896,  0.4263, -0.8700,  0.2244,  0.2531,  1.5557],\n",
      "         [ 0.3087,  0.9316, -0.9495,  1.0987, -1.6854,  0.2959]],\n",
      "\n",
      "        [[ 0.8178, -2.0764,  0.1640, -0.0257,  0.1330,  0.9873],\n",
      "         [ 0.1972, -1.4201, -0.8137,  0.0640,  0.1701,  1.8025],\n",
      "         [ 1.0593, -0.9162, -0.3608,  1.6824, -0.7818, -0.6830],\n",
      "         [ 1.1984, -1.4806, -0.1353,  1.0126, -1.0561,  0.4610],\n",
      "         [ 1.5594, -1.1580,  0.5506,  0.6108, -1.1875, -0.3752],\n",
      "         [ 1.4501, -1.5549, -0.4640,  0.7614, -0.6733,  0.4806],\n",
      "         [ 0.3320, -0.3933,  0.1614,  1.8880, -1.2841, -0.7040],\n",
      "         [ 1.0841, -1.8804,  0.5378,  0.6709, -0.6881,  0.2755],\n",
      "         [ 0.4744, -0.5915, -0.5120,  2.0397, -0.6448, -0.7659],\n",
      "         [ 0.7724, -1.8561, -0.6170,  0.1700,  0.3312,  1.1995]],\n",
      "\n",
      "        [[ 1.0699, -1.6435, -0.5597,  0.8156, -0.5913,  0.9089],\n",
      "         [ 0.2095, -2.0605,  0.7963, -0.2794,  0.4341,  0.9000],\n",
      "         [ 0.2613, -2.0630, -0.0465,  0.1045,  0.6051,  1.1386],\n",
      "         [-0.7787, -1.0118, -0.1972,  1.0677, -0.7164,  1.6364],\n",
      "         [ 0.4308, -1.8910,  0.7128,  0.7556, -0.7654,  0.7572],\n",
      "         [ 1.0299, -1.8357, -0.2494,  0.8786, -0.5116,  0.6881],\n",
      "         [-0.1803, -0.6275,  0.4878,  1.9765, -0.9973, -0.6592],\n",
      "         [ 0.1333, -2.1209,  0.9429, -0.0427,  0.5116,  0.5758],\n",
      "         [-0.9933, -1.4327,  1.4299,  0.3571, -0.2210,  0.8600],\n",
      "         [-0.1019, -1.5642, -0.6363,  0.3649,  0.2176,  1.7198]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "\n",
      "feed_fwd_op =  tensor([[[ 0.4025,  0.4060, -0.0764, -0.1188,  0.4740,  0.3265],\n",
      "         [ 0.4614,  0.4337, -0.1767, -0.0989,  0.3532,  0.2301],\n",
      "         [ 0.3038,  0.4135,  0.0972, -0.0743,  0.6904,  0.5118],\n",
      "         [ 0.2469,  0.4181,  0.1395, -0.0063,  0.7753,  0.5574],\n",
      "         [ 0.1664,  0.4358,  0.2816,  0.0473,  0.9544,  0.7129],\n",
      "         [ 0.3205,  0.4273,  0.1936, -0.1512,  0.7433,  0.6192],\n",
      "         [ 0.4096,  0.4848, -0.1335,  0.0311,  0.4430,  0.2922],\n",
      "         [ 0.2725,  0.4146,  0.1098, -0.0313,  0.7295,  0.5250],\n",
      "         [ 0.3871,  0.3954, -0.0482, -0.1307,  0.5062,  0.3526],\n",
      "         [ 0.4617,  0.4403, -0.1810, -0.0863,  0.3513,  0.2277]],\n",
      "\n",
      "        [[ 0.3309,  0.4801, -0.0188,  0.0677,  0.5971,  0.4112],\n",
      "         [ 0.4248,  0.4200,  0.0953, -0.1738,  0.5521,  0.5341],\n",
      "         [ 0.3240,  0.4588,  0.0037,  0.0301,  0.6152,  0.4279],\n",
      "         [ 0.3481,  0.4016,  0.0035, -0.0958,  0.5807,  0.4090],\n",
      "         [ 0.2544,  0.4257,  0.1936, -0.0463,  0.8068,  0.6175],\n",
      "         [ 0.3908,  0.3948, -0.0531, -0.1339,  0.4992,  0.3473],\n",
      "         [ 0.3762,  0.4077, -0.0400, -0.1002,  0.5248,  0.3652],\n",
      "         [ 0.2555,  0.4339,  0.2523, -0.0789,  0.8477,  0.6827],\n",
      "         [ 0.4426,  0.4252, -0.1449, -0.1047,  0.3918,  0.2608],\n",
      "         [ 0.2997,  0.4719,  0.0304,  0.0698,  0.6596,  0.4603]],\n",
      "\n",
      "        [[ 0.2847,  0.4449,  0.3269, -0.1641,  0.8730,  0.7661],\n",
      "         [ 0.2637,  0.4270,  0.2014, -0.0652,  0.8035,  0.6264],\n",
      "         [ 0.2457,  0.4427,  0.3178, -0.0975,  0.9039,  0.7550],\n",
      "         [ 0.2005,  0.4485,  0.3677, -0.0517,  0.9832,  0.8093],\n",
      "         [ 0.2521,  0.4358,  0.2670, -0.0812,  0.8615,  0.6989],\n",
      "         [ 0.1956,  0.4517,  0.3913, -0.0564,  1.0048,  0.8354],\n",
      "         [ 0.2907,  0.4212,  0.1549, -0.0838,  0.7442,  0.5756],\n",
      "         [ 0.2580,  0.4499,  0.3679, -0.1432,  0.9279,  0.8109],\n",
      "         [ 0.2776,  0.4334,  0.2449, -0.1100,  0.8212,  0.6750],\n",
      "         [ 0.2561,  0.4451,  0.3336, -0.1222,  0.9053,  0.7728]],\n",
      "\n",
      "        [[ 0.1971,  0.4509,  0.3851, -0.0554,  0.9989,  0.8285],\n",
      "         [ 0.3680,  0.4314,  0.2157, -0.2382,  0.7133,  0.6448],\n",
      "         [ 0.3187,  0.4412,  0.2947, -0.2012,  0.8172,  0.7312],\n",
      "         [ 0.2979,  0.4266,  0.0934, -0.0424,  0.6959,  0.5121],\n",
      "         [ 0.2772,  0.4431,  0.3152, -0.1460,  0.8719,  0.7530],\n",
      "         [ 0.2171,  0.4547,  0.4095, -0.0999,  0.9971,  0.8560],\n",
      "         [ 0.3351,  0.4212,  0.1470, -0.1502,  0.6958,  0.5679],\n",
      "         [ 0.3849,  0.4346,  0.2356, -0.2754,  0.7113,  0.6673],\n",
      "         [ 0.4270,  0.4070,  0.0293, -0.2344,  0.5233,  0.4395],\n",
      "         [ 0.2801,  0.4305,  0.2235, -0.1028,  0.8035,  0.6514]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "\n",
      "norm2(norm1(x + self_atten(x)) + feed_fwd_op) = \n",
      "tensor([[[-0.0326,  1.7392, -0.2331,  0.7041, -1.2516, -0.9259],\n",
      "         [-0.9507,  0.9642, -1.1234, -0.7091,  0.2984,  1.5206],\n",
      "         [-0.2402, -1.0165, -0.7649, -0.4848,  0.5649,  1.9416],\n",
      "         [ 1.4589, -0.2760, -0.0505,  0.9832, -1.6000, -0.5157],\n",
      "         [ 1.2051, -1.0559, -1.0730,  0.3423, -0.7080,  1.2895],\n",
      "         [-0.1103, -1.3751, -0.6142, -0.4670,  0.9609,  1.6057],\n",
      "         [-0.6938,  0.7205, -0.9127, -0.3655, -0.6489,  1.9005],\n",
      "         [ 0.0479,  0.1756, -1.4313,  1.9099, -0.4611, -0.2410],\n",
      "         [-0.5913,  0.5725, -0.0042,  1.8889, -0.8498, -1.0161],\n",
      "         [-1.2468,  0.3232,  0.5049, -0.4374, -0.9001,  1.7563]],\n",
      "\n",
      "        [[-0.6665,  0.1599, -1.1635,  0.6118, -0.7441,  1.8024],\n",
      "         [ 0.4361, -1.2489, -0.0906, -1.3202,  0.9961,  1.2275],\n",
      "         [-0.2635, -0.3130, -0.3963, -0.4429, -0.7883,  2.2041],\n",
      "         [ 0.8773,  1.2288,  0.1087,  0.4236, -1.4749, -1.1635],\n",
      "         [ 0.1498, -1.1532, -0.9436, -0.3592,  0.4572,  1.8491],\n",
      "         [ 0.8550,  1.0077,  0.7676,  0.0741, -1.3853, -1.3190],\n",
      "         [-0.5318, -0.6230,  0.9102, -0.7098, -0.8535,  1.8078],\n",
      "         [ 0.5117, -1.6991,  0.4789, -0.2073, -0.5812,  1.4970],\n",
      "         [-1.3023,  0.6131, -1.1757, -0.0883,  0.4151,  1.5381],\n",
      "         [ 0.2884,  1.1179, -1.3051,  0.8727, -1.4165,  0.4426]],\n",
      "\n",
      "        [[ 0.6224, -1.8781,  0.0631, -0.5594,  0.5342,  1.2178],\n",
      "         [ 0.0754, -1.2188, -0.8798, -0.3358,  0.5318,  1.8273],\n",
      "         [ 1.1716, -1.2042, -0.6291,  1.5455, -0.4085, -0.4754],\n",
      "         [ 1.1030, -1.7516, -0.2668,  0.5887, -0.6253,  0.9521],\n",
      "         [ 1.7270, -1.3856,  0.5061,  0.1522, -0.8989, -0.1007],\n",
      "         [ 1.2951, -1.7338, -0.5984,  0.2586, -0.1531,  0.9317],\n",
      "         [ 0.3664, -0.4340, -0.0459,  1.9558, -1.1979, -0.6443],\n",
      "         [ 0.9830, -2.0558,  0.5046,  0.0904, -0.2251,  0.7028],\n",
      "         [ 0.4728, -0.7170, -0.8595,  2.0124, -0.2797, -0.6291],\n",
      "         [ 0.5361, -1.6558, -0.6426, -0.3450,  0.7231,  1.3842]],\n",
      "\n",
      "        [[ 0.8343, -1.7325, -0.6701,  0.3055, -0.0625,  1.3252],\n",
      "         [ 0.2021, -1.8091,  0.5980, -0.7961,  0.7215,  1.0837],\n",
      "         [ 0.1598, -1.7972, -0.1352, -0.4417,  0.9083,  1.3061],\n",
      "         [-0.8401, -0.9482, -0.4497,  0.7193, -0.3635,  1.8823],\n",
      "         [ 0.3084, -1.9931,  0.6501,  0.2034, -0.3337,  1.1649],\n",
      "         [ 0.8183, -1.9582, -0.3300,  0.3236,  0.0139,  1.1323],\n",
      "         [-0.2473, -0.7398,  0.4074,  2.0321, -0.8696, -0.5829],\n",
      "         [ 0.1481, -1.9125,  0.7654, -0.6336,  0.8069,  0.8257],\n",
      "         [-0.9226, -1.4322,  1.3245, -0.1581,  0.0411,  1.1474],\n",
      "         [-0.1833, -1.3686, -0.7172, -0.1074,  0.5783,  1.7981]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "\n",
      "##################################\n",
      "MASK =  tensor([[0., -inf, -inf, -inf],\n",
      "        [0., 0., -inf, -inf],\n",
      "        [0., 0., 0., -inf],\n",
      "        [0., 0., 0., 0.]])\n",
      "query =  tensor([[[-0.0326,  1.7392, -0.2331,  0.7041, -1.2516, -0.9259],\n",
      "         [-0.9507,  0.9642, -1.1234, -0.7091,  0.2984,  1.5206],\n",
      "         [-0.2402, -1.0165, -0.7649, -0.4848,  0.5649,  1.9416],\n",
      "         [ 1.4589, -0.2760, -0.0505,  0.9832, -1.6000, -0.5157],\n",
      "         [ 1.2051, -1.0559, -1.0730,  0.3423, -0.7080,  1.2895],\n",
      "         [-0.1103, -1.3751, -0.6142, -0.4670,  0.9609,  1.6057],\n",
      "         [-0.6938,  0.7205, -0.9127, -0.3655, -0.6489,  1.9005],\n",
      "         [ 0.0479,  0.1756, -1.4313,  1.9099, -0.4611, -0.2410],\n",
      "         [-0.5913,  0.5725, -0.0042,  1.8889, -0.8498, -1.0161],\n",
      "         [-1.2468,  0.3232,  0.5049, -0.4374, -0.9001,  1.7563]],\n",
      "\n",
      "        [[-0.6665,  0.1599, -1.1635,  0.6118, -0.7441,  1.8024],\n",
      "         [ 0.4361, -1.2489, -0.0906, -1.3202,  0.9961,  1.2275],\n",
      "         [-0.2635, -0.3130, -0.3963, -0.4429, -0.7883,  2.2041],\n",
      "         [ 0.8773,  1.2288,  0.1087,  0.4236, -1.4749, -1.1635],\n",
      "         [ 0.1498, -1.1532, -0.9436, -0.3592,  0.4572,  1.8491],\n",
      "         [ 0.8550,  1.0077,  0.7676,  0.0741, -1.3853, -1.3190],\n",
      "         [-0.5318, -0.6230,  0.9102, -0.7098, -0.8535,  1.8078],\n",
      "         [ 0.5117, -1.6991,  0.4789, -0.2073, -0.5812,  1.4970],\n",
      "         [-1.3023,  0.6131, -1.1757, -0.0883,  0.4151,  1.5381],\n",
      "         [ 0.2884,  1.1179, -1.3051,  0.8727, -1.4165,  0.4426]],\n",
      "\n",
      "        [[ 0.6224, -1.8781,  0.0631, -0.5594,  0.5342,  1.2178],\n",
      "         [ 0.0754, -1.2188, -0.8798, -0.3358,  0.5318,  1.8273],\n",
      "         [ 1.1716, -1.2042, -0.6291,  1.5455, -0.4085, -0.4754],\n",
      "         [ 1.1030, -1.7516, -0.2668,  0.5887, -0.6253,  0.9521],\n",
      "         [ 1.7270, -1.3856,  0.5061,  0.1522, -0.8989, -0.1007],\n",
      "         [ 1.2951, -1.7338, -0.5984,  0.2586, -0.1531,  0.9317],\n",
      "         [ 0.3664, -0.4340, -0.0459,  1.9558, -1.1979, -0.6443],\n",
      "         [ 0.9830, -2.0558,  0.5046,  0.0904, -0.2251,  0.7028],\n",
      "         [ 0.4728, -0.7170, -0.8595,  2.0124, -0.2797, -0.6291],\n",
      "         [ 0.5361, -1.6558, -0.6426, -0.3450,  0.7231,  1.3842]],\n",
      "\n",
      "        [[ 0.8343, -1.7325, -0.6701,  0.3055, -0.0625,  1.3252],\n",
      "         [ 0.2021, -1.8091,  0.5980, -0.7961,  0.7215,  1.0837],\n",
      "         [ 0.1598, -1.7972, -0.1352, -0.4417,  0.9083,  1.3061],\n",
      "         [-0.8401, -0.9482, -0.4497,  0.7193, -0.3635,  1.8823],\n",
      "         [ 0.3084, -1.9931,  0.6501,  0.2034, -0.3337,  1.1649],\n",
      "         [ 0.8183, -1.9582, -0.3300,  0.3236,  0.0139,  1.1323],\n",
      "         [-0.2473, -0.7398,  0.4074,  2.0321, -0.8696, -0.5829],\n",
      "         [ 0.1481, -1.9125,  0.7654, -0.6336,  0.8069,  0.8257],\n",
      "         [-0.9226, -1.4322,  1.3245, -0.1581,  0.0411,  1.1474],\n",
      "         [-0.1833, -1.3686, -0.7172, -0.1074,  0.5783,  1.7981]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "query shape =  torch.Size([4, 10, 6])\n",
      "MASK1 =  tensor([[0., -inf, -inf, -inf],\n",
      "        [0., 0., -inf, -inf],\n",
      "        [0., 0., 0., -inf],\n",
      "        [0., 0., 0., 0.]]) False\n",
      "\n",
      " _in_projection_packed \n",
      "\n",
      "Shapes =  torch.Size([4, 10, 6]) torch.Size([4, 10, 6]) torch.Size([4, 10, 6])\n",
      "\n",
      "Q :-  tensor([[[-1.3781e+00,  5.1814e-01,  1.3760e+00,  4.3719e-02, -4.3613e-01,\n",
      "           6.2693e-02],\n",
      "         [-4.3700e-01,  4.4052e-01, -1.9085e-01,  1.2806e+00,  6.9407e-01,\n",
      "          -1.9148e-01],\n",
      "         [ 7.9708e-01,  1.9901e-01, -9.1764e-01,  7.6717e-01,  1.2094e+00,\n",
      "          -2.4325e-01],\n",
      "         [-5.4085e-02,  7.2404e-01,  1.2304e+00, -6.6776e-01, -2.7850e-02,\n",
      "           1.0782e-01],\n",
      "         [ 7.4226e-01,  8.3717e-01,  3.9786e-01,  1.8515e-01,  1.0094e+00,\n",
      "          -7.0448e-02],\n",
      "         [ 1.0946e+00, -1.0728e-01, -1.0887e+00,  5.2300e-01,  1.0331e+00,\n",
      "          -1.8161e-01],\n",
      "         [-5.4566e-01,  8.9095e-01, -9.5743e-03,  1.2001e+00,  1.0701e+00,\n",
      "          -2.9799e-01],\n",
      "         [-5.2678e-01,  1.3848e-01,  8.5531e-01,  3.3431e-01,  9.8681e-01,\n",
      "          -1.6508e-01],\n",
      "         [-1.1109e+00, -1.6423e-01,  5.8392e-01, -9.4642e-02,  4.1346e-01,\n",
      "          -1.9464e-01],\n",
      "         [-6.3251e-01,  5.8951e-01, -7.3835e-01,  8.1996e-01,  9.6314e-01,\n",
      "          -4.2060e-01]],\n",
      "\n",
      "        [[-4.7169e-01,  7.3859e-01, -1.9685e-02,  1.1268e+00,  1.6428e+00,\n",
      "          -4.1897e-01],\n",
      "         [ 1.3393e+00, -4.3945e-02, -9.4057e-01,  1.1753e-01,  1.6679e-01,\n",
      "           5.2950e-02],\n",
      "         [ 7.2358e-02,  8.9759e-01, -4.1059e-01,  8.4002e-01,  1.2383e+00,\n",
      "          -3.2330e-01],\n",
      "         [-8.7326e-01,  6.6265e-01,  1.5799e+00, -4.8867e-01, -8.6227e-01,\n",
      "           2.3445e-01],\n",
      "         [ 9.3411e-01,  2.9618e-01, -6.8036e-01,  6.5001e-01,  1.1852e+00,\n",
      "          -1.8936e-01],\n",
      "         [-7.1064e-01,  4.9673e-01,  1.2664e+00, -7.6067e-01, -1.1733e+00,\n",
      "           2.7189e-01],\n",
      "         [ 1.0984e-01,  5.6887e-01, -9.5788e-01,  3.4974e-01,  8.1802e-01,\n",
      "          -3.2877e-01],\n",
      "         [ 9.3995e-01,  4.3216e-01, -7.5887e-01, -1.0957e-01,  9.1406e-01,\n",
      "          -2.0373e-01],\n",
      "         [-4.5939e-01,  1.9168e-01, -4.6421e-01,  1.3399e+00,  1.1663e+00,\n",
      "          -3.3310e-01],\n",
      "         [-9.4174e-01,  1.0354e+00,  1.3541e+00,  5.9917e-01,  6.3284e-01,\n",
      "          -9.7873e-02]],\n",
      "\n",
      "        [[ 1.4304e+00, -2.1024e-02, -9.5361e-01, -1.1123e-01,  6.1288e-01,\n",
      "          -5.6832e-02],\n",
      "         [ 9.5549e-01,  2.1994e-01, -7.7460e-01,  6.3572e-01,  1.2029e+00,\n",
      "          -2.0233e-01],\n",
      "         [ 5.4724e-01,  2.1520e-02,  5.8433e-01, -5.3836e-01,  5.6695e-01,\n",
      "           1.3830e-02],\n",
      "         [ 9.8243e-01,  4.4146e-01, -9.5812e-02, -2.5390e-01,  9.8174e-01,\n",
      "          -1.1659e-01],\n",
      "         [ 9.2653e-01,  4.3650e-01,  3.8104e-01, -9.4375e-01, -1.5170e-01,\n",
      "           1.6270e-01],\n",
      "         [ 1.2345e+00,  3.6527e-01, -6.4702e-02, -2.0101e-01,  7.9039e-01,\n",
      "          -1.4941e-02],\n",
      "         [-4.0301e-01,  1.3028e-01,  6.5632e-01, -4.2141e-01,  6.5188e-01,\n",
      "          -1.5597e-01],\n",
      "         [ 1.2590e+00,  8.6137e-02, -6.0310e-01, -5.6725e-01,  5.5609e-01,\n",
      "          -5.7595e-02],\n",
      "         [ 3.4275e-02, -1.7647e-01,  5.6213e-01, -2.3477e-01,  8.0617e-01,\n",
      "          -1.0134e-01],\n",
      "         [ 1.3364e+00,  2.7063e-02, -7.7054e-01,  2.1747e-01,  8.8539e-01,\n",
      "          -8.4089e-02]],\n",
      "\n",
      "        [[ 1.1283e+00,  3.2774e-01, -3.6111e-01,  8.3041e-02,  1.1510e+00,\n",
      "          -1.4177e-01],\n",
      "         [ 1.3360e+00, -2.6279e-01, -1.3199e+00, -1.7305e-01,  4.1972e-01,\n",
      "          -8.0006e-02],\n",
      "         [ 1.3356e+00, -2.3668e-01, -1.1932e+00,  1.3155e-01,  8.2879e-01,\n",
      "          -1.3288e-01],\n",
      "         [ 8.0717e-02,  2.6890e-01, -8.7323e-01,  8.0794e-01,  1.8845e+00,\n",
      "          -5.1907e-01],\n",
      "         [ 9.8563e-01,  7.5682e-02, -9.8720e-01, -2.5792e-01,  1.0013e+00,\n",
      "          -2.4689e-01],\n",
      "         [ 1.2298e+00,  1.5752e-01, -5.3798e-01, -1.1301e-01,  1.0469e+00,\n",
      "          -1.3518e-01],\n",
      "         [-3.8267e-01, -2.5233e-01,  1.7326e-02, -4.0409e-01,  8.5074e-01,\n",
      "          -2.8192e-01],\n",
      "         [ 1.3427e+00, -4.3630e-01, -1.3913e+00, -3.0797e-01,  3.5977e-01,\n",
      "          -7.8190e-02],\n",
      "         [ 4.7220e-01, -3.2327e-01, -1.6572e+00,  1.8460e-03,  8.7695e-01,\n",
      "          -3.7521e-01],\n",
      "         [ 9.0991e-01,  6.0667e-02, -9.8918e-01,  6.2611e-01,  1.3640e+00,\n",
      "          -2.7551e-01]]], grad_fn=<SelectBackward0>)\n",
      "\n",
      "K :-  tensor([[[-0.1119,  1.1423, -0.6370,  0.5296,  0.6890, -1.1547],\n",
      "         [ 0.1935,  0.5455,  1.0715, -0.2175, -0.1550, -0.9511],\n",
      "         [-0.1975, -0.4030,  1.1978, -1.1105, -0.4961,  0.1925],\n",
      "         [-0.6974,  0.3815, -0.7662, -0.5353,  0.2679,  0.1170],\n",
      "         [-0.5508, -0.1283,  0.4033, -1.4486, -0.3444,  0.1769],\n",
      "         [ 0.0117, -0.8124,  1.0395, -1.0150, -0.6088,  0.4777],\n",
      "         [-0.5320,  0.9631,  1.1052, -0.7015,  0.0711, -0.8718],\n",
      "         [-0.0181, -0.0063, -0.0452, -0.9781,  0.5698, -1.3623],\n",
      "         [-0.3856,  0.4469, -0.4335, -0.2104,  1.0533, -1.1310],\n",
      "         [-1.1339,  1.1147,  1.0652, -0.4630,  0.3773, -0.2694]],\n",
      "\n",
      "        [[-0.7245,  0.6704,  1.1038, -1.3231,  0.3234, -1.0808],\n",
      "         [ 0.2043, -0.7960,  0.6466, -0.3529, -0.9456,  1.1318],\n",
      "         [-0.9606,  0.6643,  1.1422, -1.0936, -0.0931, -0.0694],\n",
      "         [-0.2016,  0.9528, -1.0243,  0.5849,  0.4081, -0.4155],\n",
      "         [-0.1766, -0.4914,  1.0603, -1.2332, -0.5640,  0.2495],\n",
      "         [-0.2824,  0.8810, -1.1333,  0.8716,  0.3434,  0.0822],\n",
      "         [-1.2860,  0.6829,  0.9186, -0.6398,  0.0439,  0.6273],\n",
      "         [-1.0852, -0.1526,  0.6081, -1.1765, -0.2623,  1.0809],\n",
      "         [ 0.0701,  0.3375,  1.2171, -0.5889,  0.0733, -1.1481],\n",
      "         [-0.3995,  1.0426,  0.0677, -0.6305,  0.4435, -1.3463]],\n",
      "\n",
      "        [[-0.2786, -0.8776,  0.5809, -0.9167, -0.6925,  1.2227],\n",
      "         [-0.1727, -0.5471,  1.0751, -1.2307, -0.5506,  0.2779],\n",
      "         [-0.2120, -0.6701, -0.4535, -1.0707,  0.1159,  0.0551],\n",
      "         [-0.8043, -0.4391,  0.2462, -1.4600, -0.2132,  0.7059],\n",
      "         [-0.6544, -0.2892, -0.5610, -0.5720, -0.2845,  1.2369],\n",
      "         [-0.3888, -0.7114,  0.2495, -1.3241, -0.5058,  0.7683],\n",
      "         [-0.7458,  0.1510, -0.4782, -0.8447,  0.7769, -0.4746],\n",
      "         [-0.7317, -0.6881,  0.1389, -1.0324, -0.3566,  1.3308],\n",
      "         [-0.0963, -0.5330, -0.3324, -1.0310,  0.4519, -0.6228],\n",
      "         [-0.0355, -0.9296,  0.7522, -1.1227, -0.7079,  0.7363]],\n",
      "\n",
      "        [[-0.4944, -0.6482,  0.5848, -1.5077, -0.4165,  0.5709],\n",
      "         [-0.2898, -0.8495,  0.5943, -0.5842, -0.6240,  1.3850],\n",
      "         [-0.1098, -0.9954,  0.7972, -0.9546, -0.6313,  0.9391],\n",
      "         [-0.9807,  0.1138,  1.2303, -1.5889,  0.2942, -0.3802],\n",
      "         [-1.0432, -0.4435,  0.5275, -1.2254, -0.1202,  1.0927],\n",
      "         [-0.5374, -0.7785,  0.4849, -1.4262, -0.3958,  0.8083],\n",
      "         [-0.8528, -0.0223, -0.2709, -0.8535,  0.9126, -0.3553],\n",
      "         [-0.2651, -0.9671,  0.4822, -0.5238, -0.5532,  1.4369],\n",
      "         [-1.0137, -0.1719,  0.7948, -0.6402,  0.1386,  0.9020],\n",
      "         [-0.2741, -0.6066,  1.1325, -1.3156, -0.4128,  0.2509]]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "\n",
      "V :-  tensor([[[ 4.6699e-01,  3.2412e-01,  5.3707e-01,  6.0436e-01, -1.1585e+00,\n",
      "          -6.9683e-01],\n",
      "         [-4.1654e-01,  1.5464e+00,  2.1587e-01,  1.9596e-01, -2.2454e-01,\n",
      "           4.8698e-01],\n",
      "         [-7.7605e-01,  6.3052e-01, -3.0359e-01,  6.9977e-04,  5.8858e-01,\n",
      "           1.3055e+00],\n",
      "         [ 3.3173e-03, -7.8859e-01,  2.5447e-01,  1.0437e+00, -6.1058e-01,\n",
      "           1.8416e-01],\n",
      "         [-9.0910e-01, -1.1748e-01, -4.8496e-02,  9.9915e-01, -2.2404e-01,\n",
      "           1.4218e+00],\n",
      "         [-8.2966e-01,  2.3243e-01, -4.8741e-01, -2.5802e-01,  7.0168e-01,\n",
      "           1.2673e+00],\n",
      "         [-2.5243e-01,  1.5808e+00,  3.5968e-01,  6.2665e-01, -9.8844e-02,\n",
      "           7.0524e-01],\n",
      "         [-6.9050e-01, -2.1413e-01, -4.6960e-01,  1.2521e-01, -1.3477e+00,\n",
      "           1.1438e+00],\n",
      "         [ 3.5586e-01, -2.5437e-01, -2.8441e-01, -3.1621e-01, -8.0169e-01,\n",
      "           6.1818e-02],\n",
      "         [ 5.6481e-01,  1.4797e+00,  3.4368e-01,  1.3008e-01,  8.7192e-01,\n",
      "           1.4827e-01]],\n",
      "\n",
      "        [[-4.9680e-01,  1.1741e+00, -5.8056e-02,  4.2574e-01, -2.9254e-01,\n",
      "           1.3367e+00],\n",
      "         [-5.6456e-01,  3.0220e-02, -1.1766e-01,  2.8743e-03,  9.4248e-01,\n",
      "           5.2787e-01],\n",
      "         [-2.2397e-01,  1.1636e+00,  2.4792e-01,  6.5775e-01,  5.4410e-01,\n",
      "           9.3483e-01],\n",
      "         [ 4.8618e-01, -2.3451e-01,  6.8705e-01,  9.7391e-01, -9.6662e-01,\n",
      "          -8.7724e-01],\n",
      "         [-9.3094e-01,  4.1186e-01, -3.0479e-01,  1.9608e-01,  4.1384e-01,\n",
      "           1.4328e+00],\n",
      "         [ 8.0312e-01, -3.8541e-01,  7.3868e-01,  8.0181e-01, -4.8679e-01,\n",
      "          -1.2661e+00],\n",
      "         [ 4.9800e-01,  9.0673e-01,  2.8999e-01,  2.4698e-01,  1.3640e+00,\n",
      "           2.3081e-01],\n",
      "         [-1.2305e-01, -8.5138e-02, -9.2768e-02,  3.5121e-01,  1.1386e+00,\n",
      "           8.7730e-01],\n",
      "         [-4.9737e-01,  1.4181e+00, -1.3883e-01, -1.8313e-01, -1.9800e-01,\n",
      "           8.8475e-01],\n",
      "         [-3.3058e-01,  6.6681e-01,  3.8623e-01,  1.0915e+00, -1.2909e+00,\n",
      "           5.7202e-01]],\n",
      "\n",
      "        [[-5.6392e-01, -3.6462e-01, -3.4581e-01, -3.8941e-03,  1.0160e+00,\n",
      "           9.4195e-01],\n",
      "         [-9.1086e-01,  3.8300e-01, -3.5564e-01,  9.8382e-02,  4.7698e-01,\n",
      "           1.4343e+00],\n",
      "         [-6.6492e-01, -1.2360e+00, -5.3717e-01,  2.4520e-01, -6.1445e-01,\n",
      "           1.0635e+00],\n",
      "         [-5.9298e-01, -6.2050e-01, -2.9882e-01,  5.3842e-01,  3.3521e-01,\n",
      "           1.2933e+00],\n",
      "         [-7.3867e-02, -1.1109e+00,  1.0264e-01,  7.8004e-01,  3.4227e-01,\n",
      "           2.3482e-01],\n",
      "         [-8.6768e-01, -6.6373e-01, -3.1514e-01,  5.5042e-01,  2.0539e-01,\n",
      "           1.3229e+00],\n",
      "         [ 4.4733e-02, -7.6124e-01, -3.3226e-01,  1.3944e-01, -5.8866e-01,\n",
      "           5.8509e-01],\n",
      "         [-2.6243e-01, -8.3562e-01, -3.1694e-01,  1.6627e-01,  9.4597e-01,\n",
      "           8.1257e-01],\n",
      "         [-6.2201e-01, -9.6149e-01, -6.9570e-01, -1.0855e-01, -9.1750e-01,\n",
      "           1.1144e+00],\n",
      "         [-9.3648e-01, -1.8030e-01, -4.4896e-01,  3.6090e-02,  5.7514e-01,\n",
      "           1.3209e+00]],\n",
      "\n",
      "        [[-8.8239e-01, -3.4651e-01, -4.1199e-01,  3.6679e-01,  3.3864e-01,\n",
      "           1.5334e+00],\n",
      "         [-2.2370e-01, -2.8270e-01, -3.4010e-01, -3.4126e-01,  1.3847e+00,\n",
      "           5.6114e-01],\n",
      "         [-6.6335e-01, -1.9110e-01, -5.3216e-01, -3.3116e-01,  9.7593e-01,\n",
      "           1.1016e+00],\n",
      "         [-3.7268e-01,  7.1614e-01, -4.5919e-01, -1.6170e-01,  5.1801e-01,\n",
      "           1.5253e+00],\n",
      "         [-9.0201e-02, -3.9566e-01, -3.8058e-01, -6.2224e-02,  1.2068e+00,\n",
      "           9.4982e-01],\n",
      "         [-7.4213e-01, -5.5475e-01, -4.8335e-01,  1.8790e-01,  5.5495e-01,\n",
      "           1.4050e+00],\n",
      "         [ 2.5356e-01, -7.3742e-01, -5.7942e-01, -4.2809e-01, -1.1611e-01,\n",
      "           5.7760e-01],\n",
      "         [-1.4855e-01, -4.7247e-01, -4.4090e-01, -5.2353e-01,  1.4247e+00,\n",
      "           4.9217e-01],\n",
      "         [ 4.9470e-01,  1.8672e-01, -3.6355e-01, -7.4557e-01,  1.6914e+00,\n",
      "           3.4810e-01],\n",
      "         [-8.2717e-01,  3.4032e-01, -5.0106e-01, -1.4564e-01,  6.0663e-01,\n",
      "           1.4975e+00]]], grad_fn=<SelectBackward0>)\n",
      "\n",
      "tgt_len =  4\n",
      "bsz =  10\n",
      "num_heads =  2\n",
      "head_dim =  3\n",
      "\n",
      "MASK2 =  tensor([[[0., -inf, -inf, -inf],\n",
      "         [0., 0., -inf, -inf],\n",
      "         [0., 0., 0., -inf],\n",
      "         [0., 0., 0., 0.]]])\n",
      "Attention mask NOT NONE =  tensor([[[[0., -inf, -inf, -inf],\n",
      "          [0., 0., -inf, -inf],\n",
      "          [0., 0., 0., -inf],\n",
      "          [0., 0., 0., 0.]]]])\n",
      "\n",
      "SHAPES RIGHT BEFORE ATTENTION :-  torch.Size([10, 2, 4, 3]) torch.Size([10, 2, 4, 3]) torch.Size([10, 2, 4, 3])\n",
      "\n",
      "Q reshaped =  tensor([[[[-1.3781e+00,  5.1814e-01,  1.3760e+00],\n",
      "          [-4.7169e-01,  7.3859e-01, -1.9685e-02],\n",
      "          [ 1.4304e+00, -2.1024e-02, -9.5361e-01],\n",
      "          [ 1.1283e+00,  3.2774e-01, -3.6111e-01]],\n",
      "\n",
      "         [[ 4.3719e-02, -4.3613e-01,  6.2693e-02],\n",
      "          [ 1.1268e+00,  1.6428e+00, -4.1897e-01],\n",
      "          [-1.1123e-01,  6.1288e-01, -5.6832e-02],\n",
      "          [ 8.3041e-02,  1.1510e+00, -1.4177e-01]]],\n",
      "\n",
      "\n",
      "        [[[-4.3700e-01,  4.4052e-01, -1.9085e-01],\n",
      "          [ 1.3393e+00, -4.3945e-02, -9.4057e-01],\n",
      "          [ 9.5549e-01,  2.1994e-01, -7.7460e-01],\n",
      "          [ 1.3360e+00, -2.6279e-01, -1.3199e+00]],\n",
      "\n",
      "         [[ 1.2806e+00,  6.9407e-01, -1.9148e-01],\n",
      "          [ 1.1753e-01,  1.6679e-01,  5.2950e-02],\n",
      "          [ 6.3572e-01,  1.2029e+00, -2.0233e-01],\n",
      "          [-1.7305e-01,  4.1972e-01, -8.0006e-02]]],\n",
      "\n",
      "\n",
      "        [[[ 7.9708e-01,  1.9901e-01, -9.1764e-01],\n",
      "          [ 7.2358e-02,  8.9759e-01, -4.1059e-01],\n",
      "          [ 5.4724e-01,  2.1520e-02,  5.8433e-01],\n",
      "          [ 1.3356e+00, -2.3668e-01, -1.1932e+00]],\n",
      "\n",
      "         [[ 7.6717e-01,  1.2094e+00, -2.4325e-01],\n",
      "          [ 8.4002e-01,  1.2383e+00, -3.2330e-01],\n",
      "          [-5.3836e-01,  5.6695e-01,  1.3830e-02],\n",
      "          [ 1.3155e-01,  8.2879e-01, -1.3288e-01]]],\n",
      "\n",
      "\n",
      "        [[[-5.4085e-02,  7.2404e-01,  1.2304e+00],\n",
      "          [-8.7326e-01,  6.6265e-01,  1.5799e+00],\n",
      "          [ 9.8243e-01,  4.4146e-01, -9.5812e-02],\n",
      "          [ 8.0717e-02,  2.6890e-01, -8.7323e-01]],\n",
      "\n",
      "         [[-6.6776e-01, -2.7850e-02,  1.0782e-01],\n",
      "          [-4.8867e-01, -8.6227e-01,  2.3445e-01],\n",
      "          [-2.5390e-01,  9.8174e-01, -1.1659e-01],\n",
      "          [ 8.0794e-01,  1.8845e+00, -5.1907e-01]]],\n",
      "\n",
      "\n",
      "        [[[ 7.4226e-01,  8.3717e-01,  3.9786e-01],\n",
      "          [ 9.3411e-01,  2.9618e-01, -6.8036e-01],\n",
      "          [ 9.2653e-01,  4.3650e-01,  3.8104e-01],\n",
      "          [ 9.8563e-01,  7.5682e-02, -9.8720e-01]],\n",
      "\n",
      "         [[ 1.8515e-01,  1.0094e+00, -7.0448e-02],\n",
      "          [ 6.5001e-01,  1.1852e+00, -1.8936e-01],\n",
      "          [-9.4375e-01, -1.5170e-01,  1.6270e-01],\n",
      "          [-2.5792e-01,  1.0013e+00, -2.4689e-01]]],\n",
      "\n",
      "\n",
      "        [[[ 1.0946e+00, -1.0728e-01, -1.0887e+00],\n",
      "          [-7.1064e-01,  4.9673e-01,  1.2664e+00],\n",
      "          [ 1.2345e+00,  3.6527e-01, -6.4702e-02],\n",
      "          [ 1.2298e+00,  1.5752e-01, -5.3798e-01]],\n",
      "\n",
      "         [[ 5.2300e-01,  1.0331e+00, -1.8161e-01],\n",
      "          [-7.6067e-01, -1.1733e+00,  2.7189e-01],\n",
      "          [-2.0101e-01,  7.9039e-01, -1.4941e-02],\n",
      "          [-1.1301e-01,  1.0469e+00, -1.3518e-01]]],\n",
      "\n",
      "\n",
      "        [[[-5.4566e-01,  8.9095e-01, -9.5743e-03],\n",
      "          [ 1.0984e-01,  5.6887e-01, -9.5788e-01],\n",
      "          [-4.0301e-01,  1.3028e-01,  6.5632e-01],\n",
      "          [-3.8267e-01, -2.5233e-01,  1.7326e-02]],\n",
      "\n",
      "         [[ 1.2001e+00,  1.0701e+00, -2.9799e-01],\n",
      "          [ 3.4974e-01,  8.1802e-01, -3.2877e-01],\n",
      "          [-4.2141e-01,  6.5188e-01, -1.5597e-01],\n",
      "          [-4.0409e-01,  8.5074e-01, -2.8192e-01]]],\n",
      "\n",
      "\n",
      "        [[[-5.2678e-01,  1.3848e-01,  8.5531e-01],\n",
      "          [ 9.3995e-01,  4.3216e-01, -7.5887e-01],\n",
      "          [ 1.2590e+00,  8.6137e-02, -6.0310e-01],\n",
      "          [ 1.3427e+00, -4.3630e-01, -1.3913e+00]],\n",
      "\n",
      "         [[ 3.3431e-01,  9.8681e-01, -1.6508e-01],\n",
      "          [-1.0957e-01,  9.1406e-01, -2.0373e-01],\n",
      "          [-5.6725e-01,  5.5609e-01, -5.7595e-02],\n",
      "          [-3.0797e-01,  3.5977e-01, -7.8190e-02]]],\n",
      "\n",
      "\n",
      "        [[[-1.1109e+00, -1.6423e-01,  5.8392e-01],\n",
      "          [-4.5939e-01,  1.9168e-01, -4.6421e-01],\n",
      "          [ 3.4275e-02, -1.7647e-01,  5.6213e-01],\n",
      "          [ 4.7220e-01, -3.2327e-01, -1.6572e+00]],\n",
      "\n",
      "         [[-9.4642e-02,  4.1346e-01, -1.9464e-01],\n",
      "          [ 1.3399e+00,  1.1663e+00, -3.3310e-01],\n",
      "          [-2.3477e-01,  8.0617e-01, -1.0134e-01],\n",
      "          [ 1.8460e-03,  8.7695e-01, -3.7521e-01]]],\n",
      "\n",
      "\n",
      "        [[[-6.3251e-01,  5.8951e-01, -7.3835e-01],\n",
      "          [-9.4174e-01,  1.0354e+00,  1.3541e+00],\n",
      "          [ 1.3364e+00,  2.7063e-02, -7.7054e-01],\n",
      "          [ 9.0991e-01,  6.0667e-02, -9.8918e-01]],\n",
      "\n",
      "         [[ 8.1996e-01,  9.6314e-01, -4.2060e-01],\n",
      "          [ 5.9917e-01,  6.3284e-01, -9.7873e-02],\n",
      "          [ 2.1747e-01,  8.8539e-01, -8.4089e-02],\n",
      "          [ 6.2611e-01,  1.3640e+00, -2.7551e-01]]]], grad_fn=<ViewBackward0>)\n",
      "\n",
      "K reshaped =  tensor([[[[-0.1119,  1.1423, -0.6370],\n",
      "          [-0.7245,  0.6704,  1.1038],\n",
      "          [-0.2786, -0.8776,  0.5809],\n",
      "          [-0.4944, -0.6482,  0.5848]],\n",
      "\n",
      "         [[ 0.5296,  0.6890, -1.1547],\n",
      "          [-1.3231,  0.3234, -1.0808],\n",
      "          [-0.9167, -0.6925,  1.2227],\n",
      "          [-1.5077, -0.4165,  0.5709]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1935,  0.5455,  1.0715],\n",
      "          [ 0.2043, -0.7960,  0.6466],\n",
      "          [-0.1727, -0.5471,  1.0751],\n",
      "          [-0.2898, -0.8495,  0.5943]],\n",
      "\n",
      "         [[-0.2175, -0.1550, -0.9511],\n",
      "          [-0.3529, -0.9456,  1.1318],\n",
      "          [-1.2307, -0.5506,  0.2779],\n",
      "          [-0.5842, -0.6240,  1.3850]]],\n",
      "\n",
      "\n",
      "        [[[-0.1975, -0.4030,  1.1978],\n",
      "          [-0.9606,  0.6643,  1.1422],\n",
      "          [-0.2120, -0.6701, -0.4535],\n",
      "          [-0.1098, -0.9954,  0.7972]],\n",
      "\n",
      "         [[-1.1105, -0.4961,  0.1925],\n",
      "          [-1.0936, -0.0931, -0.0694],\n",
      "          [-1.0707,  0.1159,  0.0551],\n",
      "          [-0.9546, -0.6313,  0.9391]]],\n",
      "\n",
      "\n",
      "        [[[-0.6974,  0.3815, -0.7662],\n",
      "          [-0.2016,  0.9528, -1.0243],\n",
      "          [-0.8043, -0.4391,  0.2462],\n",
      "          [-0.9807,  0.1138,  1.2303]],\n",
      "\n",
      "         [[-0.5353,  0.2679,  0.1170],\n",
      "          [ 0.5849,  0.4081, -0.4155],\n",
      "          [-1.4600, -0.2132,  0.7059],\n",
      "          [-1.5889,  0.2942, -0.3802]]],\n",
      "\n",
      "\n",
      "        [[[-0.5508, -0.1283,  0.4033],\n",
      "          [-0.1766, -0.4914,  1.0603],\n",
      "          [-0.6544, -0.2892, -0.5610],\n",
      "          [-1.0432, -0.4435,  0.5275]],\n",
      "\n",
      "         [[-1.4486, -0.3444,  0.1769],\n",
      "          [-1.2332, -0.5640,  0.2495],\n",
      "          [-0.5720, -0.2845,  1.2369],\n",
      "          [-1.2254, -0.1202,  1.0927]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0117, -0.8124,  1.0395],\n",
      "          [-0.2824,  0.8810, -1.1333],\n",
      "          [-0.3888, -0.7114,  0.2495],\n",
      "          [-0.5374, -0.7785,  0.4849]],\n",
      "\n",
      "         [[-1.0150, -0.6088,  0.4777],\n",
      "          [ 0.8716,  0.3434,  0.0822],\n",
      "          [-1.3241, -0.5058,  0.7683],\n",
      "          [-1.4262, -0.3958,  0.8083]]],\n",
      "\n",
      "\n",
      "        [[[-0.5320,  0.9631,  1.1052],\n",
      "          [-1.2860,  0.6829,  0.9186],\n",
      "          [-0.7458,  0.1510, -0.4782],\n",
      "          [-0.8528, -0.0223, -0.2709]],\n",
      "\n",
      "         [[-0.7015,  0.0711, -0.8718],\n",
      "          [-0.6398,  0.0439,  0.6273],\n",
      "          [-0.8447,  0.7769, -0.4746],\n",
      "          [-0.8535,  0.9126, -0.3553]]],\n",
      "\n",
      "\n",
      "        [[[-0.0181, -0.0063, -0.0452],\n",
      "          [-1.0852, -0.1526,  0.6081],\n",
      "          [-0.7317, -0.6881,  0.1389],\n",
      "          [-0.2651, -0.9671,  0.4822]],\n",
      "\n",
      "         [[-0.9781,  0.5698, -1.3623],\n",
      "          [-1.1765, -0.2623,  1.0809],\n",
      "          [-1.0324, -0.3566,  1.3308],\n",
      "          [-0.5238, -0.5532,  1.4369]]],\n",
      "\n",
      "\n",
      "        [[[-0.3856,  0.4469, -0.4335],\n",
      "          [ 0.0701,  0.3375,  1.2171],\n",
      "          [-0.0963, -0.5330, -0.3324],\n",
      "          [-1.0137, -0.1719,  0.7948]],\n",
      "\n",
      "         [[-0.2104,  1.0533, -1.1310],\n",
      "          [-0.5889,  0.0733, -1.1481],\n",
      "          [-1.0310,  0.4519, -0.6228],\n",
      "          [-0.6402,  0.1386,  0.9020]]],\n",
      "\n",
      "\n",
      "        [[[-1.1339,  1.1147,  1.0652],\n",
      "          [-0.3995,  1.0426,  0.0677],\n",
      "          [-0.0355, -0.9296,  0.7522],\n",
      "          [-0.2741, -0.6066,  1.1325]],\n",
      "\n",
      "         [[-0.4630,  0.3773, -0.2694],\n",
      "          [-0.6305,  0.4435, -1.3463],\n",
      "          [-1.1227, -0.7079,  0.7363],\n",
      "          [-1.3156, -0.4128,  0.2509]]]], grad_fn=<ViewBackward0>)\n",
      "\n",
      "V reshaped =  tensor([[[[ 4.6699e-01,  3.2412e-01,  5.3707e-01],\n",
      "          [-4.9680e-01,  1.1741e+00, -5.8056e-02],\n",
      "          [-5.6392e-01, -3.6462e-01, -3.4581e-01],\n",
      "          [-8.8239e-01, -3.4651e-01, -4.1199e-01]],\n",
      "\n",
      "         [[ 6.0436e-01, -1.1585e+00, -6.9683e-01],\n",
      "          [ 4.2574e-01, -2.9254e-01,  1.3367e+00],\n",
      "          [-3.8941e-03,  1.0160e+00,  9.4195e-01],\n",
      "          [ 3.6679e-01,  3.3864e-01,  1.5334e+00]]],\n",
      "\n",
      "\n",
      "        [[[-4.1654e-01,  1.5464e+00,  2.1587e-01],\n",
      "          [-5.6456e-01,  3.0220e-02, -1.1766e-01],\n",
      "          [-9.1086e-01,  3.8300e-01, -3.5564e-01],\n",
      "          [-2.2370e-01, -2.8270e-01, -3.4010e-01]],\n",
      "\n",
      "         [[ 1.9596e-01, -2.2454e-01,  4.8698e-01],\n",
      "          [ 2.8743e-03,  9.4248e-01,  5.2787e-01],\n",
      "          [ 9.8382e-02,  4.7698e-01,  1.4343e+00],\n",
      "          [-3.4126e-01,  1.3847e+00,  5.6114e-01]]],\n",
      "\n",
      "\n",
      "        [[[-7.7605e-01,  6.3052e-01, -3.0359e-01],\n",
      "          [-2.2397e-01,  1.1636e+00,  2.4792e-01],\n",
      "          [-6.6492e-01, -1.2360e+00, -5.3717e-01],\n",
      "          [-6.6335e-01, -1.9110e-01, -5.3216e-01]],\n",
      "\n",
      "         [[ 6.9977e-04,  5.8858e-01,  1.3055e+00],\n",
      "          [ 6.5775e-01,  5.4410e-01,  9.3483e-01],\n",
      "          [ 2.4520e-01, -6.1445e-01,  1.0635e+00],\n",
      "          [-3.3116e-01,  9.7593e-01,  1.1016e+00]]],\n",
      "\n",
      "\n",
      "        [[[ 3.3173e-03, -7.8859e-01,  2.5447e-01],\n",
      "          [ 4.8618e-01, -2.3451e-01,  6.8705e-01],\n",
      "          [-5.9298e-01, -6.2050e-01, -2.9882e-01],\n",
      "          [-3.7268e-01,  7.1614e-01, -4.5919e-01]],\n",
      "\n",
      "         [[ 1.0437e+00, -6.1058e-01,  1.8416e-01],\n",
      "          [ 9.7391e-01, -9.6662e-01, -8.7724e-01],\n",
      "          [ 5.3842e-01,  3.3521e-01,  1.2933e+00],\n",
      "          [-1.6170e-01,  5.1801e-01,  1.5253e+00]]],\n",
      "\n",
      "\n",
      "        [[[-9.0910e-01, -1.1748e-01, -4.8496e-02],\n",
      "          [-9.3094e-01,  4.1186e-01, -3.0479e-01],\n",
      "          [-7.3867e-02, -1.1109e+00,  1.0264e-01],\n",
      "          [-9.0201e-02, -3.9566e-01, -3.8058e-01]],\n",
      "\n",
      "         [[ 9.9915e-01, -2.2404e-01,  1.4218e+00],\n",
      "          [ 1.9608e-01,  4.1384e-01,  1.4328e+00],\n",
      "          [ 7.8004e-01,  3.4227e-01,  2.3482e-01],\n",
      "          [-6.2224e-02,  1.2068e+00,  9.4982e-01]]],\n",
      "\n",
      "\n",
      "        [[[-8.2966e-01,  2.3243e-01, -4.8741e-01],\n",
      "          [ 8.0312e-01, -3.8541e-01,  7.3868e-01],\n",
      "          [-8.6768e-01, -6.6373e-01, -3.1514e-01],\n",
      "          [-7.4213e-01, -5.5475e-01, -4.8335e-01]],\n",
      "\n",
      "         [[-2.5802e-01,  7.0168e-01,  1.2673e+00],\n",
      "          [ 8.0181e-01, -4.8679e-01, -1.2661e+00],\n",
      "          [ 5.5042e-01,  2.0539e-01,  1.3229e+00],\n",
      "          [ 1.8790e-01,  5.5495e-01,  1.4050e+00]]],\n",
      "\n",
      "\n",
      "        [[[-2.5243e-01,  1.5808e+00,  3.5968e-01],\n",
      "          [ 4.9800e-01,  9.0673e-01,  2.8999e-01],\n",
      "          [ 4.4733e-02, -7.6124e-01, -3.3226e-01],\n",
      "          [ 2.5356e-01, -7.3742e-01, -5.7942e-01]],\n",
      "\n",
      "         [[ 6.2665e-01, -9.8844e-02,  7.0524e-01],\n",
      "          [ 2.4698e-01,  1.3640e+00,  2.3081e-01],\n",
      "          [ 1.3944e-01, -5.8866e-01,  5.8509e-01],\n",
      "          [-4.2809e-01, -1.1611e-01,  5.7760e-01]]],\n",
      "\n",
      "\n",
      "        [[[-6.9050e-01, -2.1413e-01, -4.6960e-01],\n",
      "          [-1.2305e-01, -8.5138e-02, -9.2768e-02],\n",
      "          [-2.6243e-01, -8.3562e-01, -3.1694e-01],\n",
      "          [-1.4855e-01, -4.7247e-01, -4.4090e-01]],\n",
      "\n",
      "         [[ 1.2521e-01, -1.3477e+00,  1.1438e+00],\n",
      "          [ 3.5121e-01,  1.1386e+00,  8.7730e-01],\n",
      "          [ 1.6627e-01,  9.4597e-01,  8.1257e-01],\n",
      "          [-5.2353e-01,  1.4247e+00,  4.9217e-01]]],\n",
      "\n",
      "\n",
      "        [[[ 3.5586e-01, -2.5437e-01, -2.8441e-01],\n",
      "          [-4.9737e-01,  1.4181e+00, -1.3883e-01],\n",
      "          [-6.2201e-01, -9.6149e-01, -6.9570e-01],\n",
      "          [ 4.9470e-01,  1.8672e-01, -3.6355e-01]],\n",
      "\n",
      "         [[-3.1621e-01, -8.0169e-01,  6.1818e-02],\n",
      "          [-1.8313e-01, -1.9800e-01,  8.8475e-01],\n",
      "          [-1.0855e-01, -9.1750e-01,  1.1144e+00],\n",
      "          [-7.4557e-01,  1.6914e+00,  3.4810e-01]]],\n",
      "\n",
      "\n",
      "        [[[ 5.6481e-01,  1.4797e+00,  3.4368e-01],\n",
      "          [-3.3058e-01,  6.6681e-01,  3.8623e-01],\n",
      "          [-9.3648e-01, -1.8030e-01, -4.4896e-01],\n",
      "          [-8.2717e-01,  3.4032e-01, -5.0106e-01]],\n",
      "\n",
      "         [[ 1.3008e-01,  8.7192e-01,  1.4827e-01],\n",
      "          [ 1.0915e+00, -1.2909e+00,  5.7202e-01],\n",
      "          [ 3.6090e-02,  5.7514e-01,  1.3209e+00],\n",
      "          [-1.4564e-01,  6.0663e-01,  1.4975e+00]]]], grad_fn=<ViewBackward0>)\n",
      "\n",
      "Attention mask =  tensor([[[[0., -inf, -inf, -inf],\n",
      "          [0., 0., -inf, -inf],\n",
      "          [0., 0., 0., -inf],\n",
      "          [0., 0., 0., 0.]]]])\n",
      "scaled_dot_product_attention output = \n",
      "tensor([[[[ 4.6699e-01,  3.2412e-01,  5.3707e-01],\n",
      "          [-1.8585e-03,  7.3761e-01,  2.4756e-01],\n",
      "          [ 5.5573e-02,  2.5491e-01,  2.1643e-01],\n",
      "          [-1.6841e-01,  2.1565e-01,  7.3188e-02]],\n",
      "\n",
      "         [[ 6.0436e-01, -1.1585e+00, -6.9683e-01],\n",
      "          [ 5.7360e-01, -1.0093e+00, -3.4662e-01],\n",
      "          [ 3.9170e-01, -3.1221e-01,  4.6333e-01],\n",
      "          [ 4.3600e-01, -3.8688e-01,  4.6908e-01]]],\n",
      "\n",
      "\n",
      "        [[[-4.1654e-01,  1.5464e+00,  2.1587e-01],\n",
      "          [-5.0060e-01,  6.8540e-01,  2.6470e-02],\n",
      "          [-6.0034e-01,  6.7591e-01, -5.7523e-02],\n",
      "          [-5.1102e-01,  3.3020e-01, -1.4878e-01]],\n",
      "\n",
      "         [[ 1.9596e-01, -2.2454e-01,  4.8698e-01],\n",
      "          [ 1.0046e-01,  3.5265e-01,  5.0720e-01],\n",
      "          [ 1.2832e-01,  2.1115e-01,  7.2442e-01],\n",
      "          [ 2.2463e-03,  5.9184e-01,  7.6727e-01]]],\n",
      "\n",
      "\n",
      "        [[[-7.7605e-01,  6.3052e-01, -3.0359e-01],\n",
      "          [-4.2795e-01,  9.6664e-01,  4.4144e-02],\n",
      "          [-5.6557e-01,  3.5633e-01, -1.7668e-01],\n",
      "          [-6.4708e-01, -4.8759e-01, -4.3557e-01]],\n",
      "\n",
      "         [[ 6.9977e-04,  5.8858e-01,  1.3055e+00],\n",
      "          [ 3.8537e-01,  5.6254e-01,  1.0885e+00],\n",
      "          [ 3.1194e-01,  1.3906e-01,  1.0923e+00],\n",
      "          [ 1.8964e-01,  2.8979e-01,  1.0895e+00]]],\n",
      "\n",
      "\n",
      "        [[[ 3.3173e-03, -7.8859e-01,  2.5447e-01],\n",
      "          [ 2.1272e-01, -5.4830e-01,  4.4206e-01],\n",
      "          [ 1.0101e-01, -4.8867e-01,  3.3778e-01],\n",
      "          [ 5.7800e-02, -3.7060e-01,  2.5864e-01]],\n",
      "\n",
      "         [[ 1.0437e+00, -6.1058e-01,  1.8416e-01],\n",
      "          [ 1.0166e+00, -7.4854e-01, -2.2711e-01],\n",
      "          [ 8.6812e-01, -4.4805e-01,  1.5502e-01],\n",
      "          [ 7.6725e-01, -5.3957e-01, -7.0080e-02]]],\n",
      "\n",
      "\n",
      "        [[[-9.0910e-01, -1.1748e-01, -4.8496e-02],\n",
      "          [-9.1937e-01,  1.3154e-01, -1.6906e-01],\n",
      "          [-7.1537e-01, -1.3341e-01, -1.2097e-01],\n",
      "          [-4.5047e-01, -4.4518e-01, -9.6659e-02]],\n",
      "\n",
      "         [[ 9.9915e-01, -2.2404e-01,  1.4218e+00],\n",
      "          [ 6.1313e-01,  8.2576e-02,  1.4271e+00],\n",
      "          [ 6.5905e-01,  1.4917e-01,  1.1138e+00],\n",
      "          [ 4.7717e-01,  4.3404e-01,  1.0393e+00]]],\n",
      "\n",
      "\n",
      "        [[[-8.2966e-01,  2.3243e-01, -4.8741e-01],\n",
      "          [-3.8486e-01,  6.4117e-02, -1.5340e-01],\n",
      "          [-1.6616e-01, -2.5492e-01,  6.2938e-02],\n",
      "          [-1.6772e-01, -3.3990e-01,  4.0961e-02]],\n",
      "\n",
      "         [[-2.5802e-01,  7.0168e-01,  1.2673e+00],\n",
      "          [-7.0255e-02,  4.9112e-01,  8.1846e-01],\n",
      "          [ 4.0155e-01,  9.5620e-02,  3.3794e-01],\n",
      "          [ 3.8169e-01,  1.5808e-01,  4.6009e-01]]],\n",
      "\n",
      "\n",
      "        [[[-2.5243e-01,  1.5808e+00,  3.5968e-01],\n",
      "          [ 1.1591e-01,  1.2500e+00,  3.2547e-01],\n",
      "          [ 1.1912e-01,  8.1891e-01,  1.8810e-01],\n",
      "          [ 1.5635e-01,  1.9711e-01, -8.2013e-02]],\n",
      "\n",
      "         [[ 6.2665e-01, -9.8844e-02,  7.0524e-01],\n",
      "          [ 4.6368e-01,  5.2907e-01,  5.0160e-01],\n",
      "          [ 3.2291e-01,  9.3872e-02,  5.2751e-01],\n",
      "          [ 8.5176e-02, -7.3407e-03,  5.4997e-01]]],\n",
      "\n",
      "\n",
      "        [[[-6.9050e-01, -2.1413e-01, -4.6960e-01],\n",
      "          [-5.2670e-01, -1.7690e-01, -3.6082e-01],\n",
      "          [-4.6063e-01, -3.6565e-01, -3.5426e-01],\n",
      "          [-3.8507e-01, -4.1418e-01, -3.8679e-01]],\n",
      "\n",
      "         [[ 1.2521e-01, -1.3477e+00,  1.1438e+00],\n",
      "          [ 1.9950e-01, -5.3035e-01,  1.0562e+00],\n",
      "          [ 2.0646e-01,  6.7044e-02,  9.6803e-01],\n",
      "          [ 5.8485e-02,  3.6560e-01,  8.6808e-01]]],\n",
      "\n",
      "\n",
      "        [[[ 3.5586e-01, -2.5437e-01, -2.8441e-01],\n",
      "          [ 4.8699e-02,  3.4773e-01, -2.3200e-01],\n",
      "          [-3.1491e-01,  2.8622e-01, -3.4056e-01],\n",
      "          [-1.2981e-01, -3.6759e-01, -4.5916e-01]],\n",
      "\n",
      "         [[-3.1621e-01, -8.0169e-01,  6.1818e-02],\n",
      "          [-2.7908e-01, -6.3326e-01,  2.9142e-01],\n",
      "          [-2.1204e-01, -6.7782e-01,  6.2966e-01],\n",
      "          [-2.9958e-01, -3.1515e-01,  5.5264e-01]]],\n",
      "\n",
      "\n",
      "        [[[ 5.6481e-01,  1.4797e+00,  3.4368e-01],\n",
      "          [ 3.6107e-01,  1.2947e+00,  3.5336e-01],\n",
      "          [-4.3761e-01,  4.4795e-01,  3.7366e-02],\n",
      "          [-4.7523e-01,  4.7666e-01, -3.6820e-02]],\n",
      "\n",
      "         [[ 1.3008e-01,  8.7192e-01,  1.4827e-01],\n",
      "          [ 6.1733e-01, -2.2413e-01,  3.6302e-01],\n",
      "          [ 5.1085e-01, -8.4123e-02,  5.5382e-01],\n",
      "          [ 4.8562e-01, -9.3006e-02,  6.1667e-01]]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n",
      "\n",
      "x + self_atten(x) = \n",
      "tensor([[[-0.4661,  1.2766, -0.9028,  0.8386, -1.6538, -1.4766],\n",
      "         [-0.9018,  0.5763, -1.5451, -1.4787,  0.0759,  1.5893],\n",
      "         [ 0.2700, -1.0645, -0.4454, -1.2488,  0.6556,  2.2879],\n",
      "         [ 1.5903, -0.7169, -0.1695,  1.0480, -1.9582, -1.0097],\n",
      "         [ 1.7363, -1.6465, -0.8406, -0.3596, -0.9397,  1.1131],\n",
      "         [ 0.3647, -1.2832, -0.0178, -1.0537,  1.2389,  2.0224],\n",
      "         [-0.4154,  0.2935, -1.5433, -1.2605, -1.0984,  1.8880],\n",
      "         [-0.1833, -0.3154, -0.7202,  1.6092, -0.1696, -0.6510],\n",
      "         [-0.9778,  0.6604,  0.3010,  2.1552, -0.5902, -1.3061],\n",
      "         [-0.8547,  0.5892, -0.2878, -0.9491, -1.2898,  2.0375]],\n",
      "\n",
      "        [[-1.0033, -0.3440, -1.6801,  0.3452, -1.0333,  1.5047],\n",
      "         [ 0.7024, -1.3956, -0.1638, -1.8403,  0.9156,  1.4554],\n",
      "         [ 0.3010, -0.4447, -0.5644, -1.2586, -1.0143,  2.4260],\n",
      "         [ 0.8511,  0.7611, -0.2710,  0.5765, -1.9214, -1.7086],\n",
      "         [ 0.6818, -1.5509, -0.6322, -1.0962,  0.3670,  1.8771],\n",
      "         [ 1.2195,  1.0609,  1.0510, -0.2306, -1.3063, -1.1394],\n",
      "         [-0.0926, -0.6784,  0.2800, -1.3703, -1.2766,  1.9526],\n",
      "         [ 0.6027, -1.9546,  0.9906, -0.5386, -0.4243,  1.3283],\n",
      "         [-1.5750,  0.5840, -0.9919, -0.1780,  0.6088,  1.3996],\n",
      "         [ 0.4438,  0.9073, -2.0313,  0.2947, -1.8795,  0.2869]],\n",
      "\n",
      "        [[ 0.7331, -2.1073, -0.1160, -0.7673,  0.3144,  0.9671],\n",
      "         [ 0.3428, -1.4184, -0.8470, -0.9282,  0.4910,  2.0154],\n",
      "         [ 1.5560, -1.3937, -0.4327,  0.9429, -0.4410, -0.3913],\n",
      "         [ 1.2415, -2.1149, -0.4908,  0.6251, -0.9944,  0.5235],\n",
      "         [ 2.2311, -1.7036,  0.7491, -0.3918, -1.0236, -0.0942],\n",
      "         [ 1.5362, -1.8658, -0.5796,  0.1458, -0.2886,  0.8672],\n",
      "         [ 0.6113, -0.5188, -0.3991,  1.5090, -1.4589, -0.6726],\n",
      "         [ 1.2919, -2.1175,  0.9879, -0.1895, -0.1017,  0.7193],\n",
      "         [ 0.2771, -0.8716, -0.4848,  1.7655, -0.0317, -0.7439],\n",
      "         [ 0.7621, -1.9761, -0.7533, -0.8561,  0.5361,  1.3841]],\n",
      "\n",
      "        [[ 0.9160, -2.0241, -0.7445,  0.0185, -0.2198,  1.1337],\n",
      "         [ 0.6007, -1.8003,  0.7773, -1.2556,  0.7552,  1.3685],\n",
      "         [ 0.5773, -1.8329,  0.4696, -0.7694,  1.0783,  1.4286],\n",
      "         [-0.8327, -1.3081, -0.6797,  0.7569, -0.6695,  1.5164],\n",
      "         [ 0.8721, -2.0936,  0.9501, -0.1073, -0.4176,  1.1851],\n",
      "         [ 1.1113, -2.0593, -0.2468,  0.2192, -0.1029,  1.0657],\n",
      "         [-0.0973, -0.6749,  0.4397,  1.8505, -0.8943, -0.6480],\n",
      "         [ 0.5080, -1.8197,  1.2648, -0.8480,  0.9727,  0.9607],\n",
      "         [-1.0055, -1.3343,  1.8943, -0.1276,  0.3776,  1.0867],\n",
      "         [ 0.0415, -1.6748, -0.7773, -0.6586,  0.4290,  1.8236]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "\n",
      "norm1(x + self_atten(x)) = \n",
      "tensor([[[-0.0622,  1.5142, -0.4572,  1.1180, -1.1365, -0.9762],\n",
      "         [-0.5455,  0.7527, -1.1105, -1.0522,  0.3132,  1.6423],\n",
      "         [ 0.1622, -0.9528, -0.4355, -1.1068,  0.4845,  1.8484],\n",
      "         [ 1.4824, -0.4252,  0.0274,  1.0340, -1.4514, -0.6672],\n",
      "         [ 1.5865, -1.2494, -0.5738, -0.1705, -0.6569,  1.0641],\n",
      "         [ 0.1304, -1.2753, -0.1959, -1.0796,  0.8760,  1.5444],\n",
      "         [-0.0507,  0.5542, -1.0132, -0.7718, -0.6335,  1.9149],\n",
      "         [-0.1428, -0.3119, -0.8300,  2.1513, -0.1252, -0.7414],\n",
      "         [-0.8725,  0.5312,  0.2233,  1.8120, -0.5403, -1.1538],\n",
      "         [-0.6401,  0.6278, -0.1422, -0.7230, -1.0221,  1.8996]],\n",
      "\n",
      "        [[-0.6060,  0.0233, -1.2521,  0.6813, -0.6347,  1.7881],\n",
      "         [ 0.6251, -1.1079, -0.0904, -1.4752,  0.8013,  1.2471],\n",
      "         [ 0.3203, -0.2867, -0.3842, -0.9493, -0.7504,  2.0502],\n",
      "         [ 0.9947,  0.9159,  0.0126,  0.7544, -1.4319, -1.2457],\n",
      "         [ 0.6372, -1.2834, -0.4932, -0.8923,  0.3663,  1.6653],\n",
      "         [ 1.0505,  0.9004,  0.8910, -0.3215, -1.3391, -1.1813],\n",
      "         [ 0.0930, -0.4263,  0.4234, -1.0397, -0.9567,  1.9063],\n",
      "         [ 0.5421, -1.7607,  0.8914, -0.4856, -0.3827,  1.1955],\n",
      "         [-1.5290,  0.6014, -0.9536, -0.1505,  0.6258,  1.4060],\n",
      "         [ 0.6618,  1.0584, -1.4559,  0.5342, -1.3260,  0.5276]],\n",
      "\n",
      "        [[ 0.8635, -1.8747,  0.0450, -0.5829,  0.4599,  1.0892],\n",
      "         [ 0.3468, -1.1793, -0.6842, -0.7546,  0.4752,  1.7960],\n",
      "         [ 1.6120, -1.3925, -0.4136,  0.9875, -0.4220, -0.3715],\n",
      "         [ 1.2769, -1.6928, -0.2558,  0.7315, -0.7014,  0.6416],\n",
      "         [ 1.7906, -1.3132,  0.6216, -0.2784, -0.7768, -0.0437],\n",
      "         [ 1.4478, -1.6955, -0.5071,  0.1632, -0.2382,  0.8297],\n",
      "         [ 0.7990, -0.3796, -0.2547,  1.7353, -1.3600, -0.5400],\n",
      "         [ 1.0581, -1.9644,  0.7886, -0.2552, -0.1774,  0.5505],\n",
      "         [ 0.3285, -0.9637, -0.5286,  2.0028, -0.0189, -0.8200],\n",
      "         [ 0.7977, -1.5957, -0.5269, -0.6167,  0.6002,  1.3414]],\n",
      "\n",
      "        [[ 1.0132, -1.7725, -0.5601,  0.1629, -0.0630,  1.2195],\n",
      "         [ 0.4505, -1.6042,  0.6016, -1.1381,  0.5827,  1.1076],\n",
      "         [ 0.3731, -1.7744,  0.2771, -0.8268,  0.8194,  1.1315],\n",
      "         [-0.6331, -1.1108, -0.4793,  0.9645, -0.4690,  1.7278],\n",
      "         [ 0.7168, -1.9163,  0.7860, -0.1528, -0.4283,  0.9947],\n",
      "         [ 1.0513, -1.9422, -0.2310,  0.2089, -0.0951,  1.0082],\n",
      "         [-0.0992, -0.7136,  0.4721,  1.9728, -0.9470, -0.6850],\n",
      "         [ 0.2980, -1.7733,  0.9715, -0.9086,  0.7115,  0.7008],\n",
      "         [-1.0262, -1.3187,  1.5525, -0.2455,  0.2037,  0.8343],\n",
      "         [ 0.1616, -1.4004, -0.5836, -0.4755,  0.5143,  1.7836]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "\n",
      "feed_fwd_op =  tensor([[[ 0.3782,  0.4030, -0.0401, -0.1106,  0.5218,  0.3636],\n",
      "         [ 0.4104,  0.4159, -0.0936, -0.1042,  0.4564,  0.3117],\n",
      "         [ 0.3392,  0.4031,  0.0154, -0.0878,  0.5978,  0.4220],\n",
      "         [ 0.2382,  0.4243,  0.1859, -0.0167,  0.8168,  0.6086],\n",
      "         [ 0.1980,  0.4337,  0.2608,  0.0080,  0.9091,  0.6907],\n",
      "         [ 0.3812,  0.4062,  0.0250, -0.1397,  0.5598,  0.4384],\n",
      "         [ 0.3540,  0.4646, -0.0423,  0.0239,  0.5556,  0.3814],\n",
      "         [ 0.3240,  0.4225,  0.1588, -0.1386,  0.7150,  0.5806],\n",
      "         [ 0.4325,  0.3881, -0.1084, -0.1713,  0.4195,  0.2870],\n",
      "         [ 0.4244,  0.4775, -0.1501,  0.0081,  0.4158,  0.2723]],\n",
      "\n",
      "        [[ 0.3204,  0.4678,  0.0035,  0.0497,  0.6202,  0.4306],\n",
      "         [ 0.4552,  0.4105,  0.0017, -0.1085,  0.4414,  0.4461],\n",
      "         [ 0.3033,  0.4348,  0.0475, -0.0048,  0.6607,  0.4661],\n",
      "         [ 0.3271,  0.4050,  0.0314, -0.0770,  0.6209,  0.4394],\n",
      "         [ 0.2978,  0.4145,  0.1050, -0.0690,  0.7016,  0.5203],\n",
      "         [ 0.4084,  0.3961, -0.0548, -0.1333,  0.4748,  0.3523],\n",
      "         [ 0.3383,  0.4226,  0.0050, -0.0491,  0.5953,  0.4175],\n",
      "         [ 0.3194,  0.4240,  0.1701, -0.1372,  0.7275,  0.5931],\n",
      "         [ 0.4518,  0.4033, -0.1450, -0.1528,  0.3785,  0.2535],\n",
      "         [ 0.2815,  0.4670,  0.0592,  0.0707,  0.6960,  0.4889]],\n",
      "\n",
      "        [[ 0.3114,  0.4339,  0.2417, -0.1585,  0.7856,  0.6731],\n",
      "         [ 0.3045,  0.4113,  0.0811, -0.0670,  0.6782,  0.4940],\n",
      "         [ 0.2279,  0.4523,  0.3900, -0.1070,  0.9727,  0.8347],\n",
      "         [ 0.2041,  0.4518,  0.3910, -0.0697,  0.9963,  0.8353],\n",
      "         [ 0.2716,  0.4305,  0.2253, -0.0902,  0.8130,  0.6531],\n",
      "         [ 0.2154,  0.4478,  0.3596, -0.0711,  0.9630,  0.8007],\n",
      "         [ 0.2439,  0.4260,  0.1977, -0.0319,  0.8198,  0.6218],\n",
      "         [ 0.3081,  0.4386,  0.2777, -0.1755,  0.8153,  0.7121],\n",
      "         [ 0.3074,  0.4411,  0.2956, -0.1838,  0.8287,  0.7319],\n",
      "         [ 0.3008,  0.4289,  0.2049, -0.1144,  0.7677,  0.6340]],\n",
      "\n",
      "        [[ 0.2290,  0.4452,  0.3384, -0.0816,  0.9348,  0.7775],\n",
      "         [ 0.4046,  0.4149,  0.0771, -0.1822,  0.5696,  0.5018],\n",
      "         [ 0.3932,  0.4242,  0.1458, -0.1984,  0.6295,  0.5781],\n",
      "         [ 0.2772,  0.4200,  0.1466, -0.0577,  0.7514,  0.5661],\n",
      "         [ 0.2974,  0.4348,  0.2518, -0.1450,  0.8071,  0.6831],\n",
      "         [ 0.2411,  0.4489,  0.3637, -0.1141,  0.9412,  0.8058],\n",
      "         [ 0.3314,  0.4243,  0.1705, -0.1565,  0.7162,  0.5938],\n",
      "         [ 0.4544,  0.4208,  0.1019, -0.2432,  0.5326,  0.5377],\n",
      "         [ 0.4383,  0.4012, -0.0149, -0.2293,  0.4808,  0.3908],\n",
      "         [ 0.3075,  0.4181,  0.1294, -0.0971,  0.7099,  0.5477]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "\n",
      "norm2(norm1(x + self_atten(x)) + feed_fwd_op) = \n",
      "tensor([[[ 6.6782e-02,  1.7533e+00, -7.9001e-01,  7.9493e-01, -9.1361e-01,\n",
      "          -9.1140e-01],\n",
      "         [-3.1363e-01,  7.9776e-01, -1.2249e+00, -1.1842e+00,  4.5765e-01,\n",
      "           1.4673e+00],\n",
      "         [ 1.9029e-01, -7.1964e-01, -6.0745e-01, -1.2778e+00,  6.9312e-01,\n",
      "           1.7215e+00],\n",
      "         [ 1.7361e+00, -4.8694e-01, -2.1036e-01,  8.2795e-01, -1.3052e+00,\n",
      "          -5.6152e-01],\n",
      "         [ 1.3593e+00, -1.2247e+00, -7.2520e-01, -5.7567e-01, -1.6343e-01,\n",
      "           1.3298e+00],\n",
      "         [ 2.0113e-01, -9.9010e-01, -3.8770e-01, -1.2922e+00,  9.9852e-01,\n",
      "           1.4704e+00],\n",
      "         [ 1.2275e-02,  6.4883e-01, -1.1967e+00, -9.2305e-01, -3.2685e-01,\n",
      "           1.7855e+00],\n",
      "         [-1.9401e-01, -2.7816e-01, -1.2114e+00,  1.9921e+00,  2.9365e-01,\n",
      "          -6.0219e-01],\n",
      "         [-7.6970e-01,  8.4527e-01, -1.1052e-01,  1.7023e+00, -3.9058e-01,\n",
      "          -1.2768e+00],\n",
      "         [-4.3576e-01,  8.2373e-01, -5.0884e-01, -9.1167e-01, -8.0819e-01,\n",
      "           1.8407e+00]],\n",
      "\n",
      "        [[-5.6691e-01,  1.6578e-01, -1.4753e+00,  3.9207e-01, -3.1113e-01,\n",
      "           1.7955e+00],\n",
      "         [ 6.9242e-01, -8.3491e-01, -3.1198e-01, -1.5963e+00,  8.3184e-01,\n",
      "           1.2190e+00],\n",
      "         [ 2.7969e-01, -1.5534e-01, -5.9893e-01, -1.1638e+00, -3.7296e-01,\n",
      "           2.0113e+00],\n",
      "         [ 1.1581e+00,  1.1571e+00, -2.7770e-01,  4.3400e-01, -1.2385e+00,\n",
      "          -1.2331e+00],\n",
      "         [ 5.2621e-01, -1.0385e+00, -6.2155e-01, -1.1186e+00,  6.4153e-01,\n",
      "           1.6110e+00],\n",
      "         [ 1.2389e+00,  1.0738e+00,  6.0566e-01, -7.0712e-01, -1.1236e+00,\n",
      "          -1.0876e+00],\n",
      "         [ 1.3651e-01, -2.7867e-01,  1.3378e-01, -1.3143e+00, -6.2001e-01,\n",
      "           1.9427e+00],\n",
      "         [ 4.8712e-01, -1.6042e+00,  6.7742e-01, -9.2505e-01, -4.4290e-03,\n",
      "           1.3691e+00],\n",
      "         [-1.1811e+00,  7.4686e-01, -1.2010e+00, -4.6450e-01,  7.4649e-01,\n",
      "           1.3533e+00],\n",
      "         [ 5.8714e-01,  1.1572e+00, -1.7049e+00,  2.5571e-01, -9.5395e-01,\n",
      "           6.5878e-01]],\n",
      "\n",
      "        [[ 6.9288e-01, -1.5906e+00, -8.2471e-02, -9.7995e-01,  7.5449e-01,\n",
      "           1.2056e+00],\n",
      "         [ 2.8903e-01, -9.3820e-01, -7.9560e-01, -9.8457e-01,  7.2324e-01,\n",
      "           1.7061e+00],\n",
      "         [ 1.6310e+00, -1.6592e+00, -5.7445e-01,  4.9562e-01,  1.0527e-01,\n",
      "           1.7444e-03],\n",
      "         [ 1.0941e+00, -1.8460e+00, -3.5958e-01,  2.0910e-01, -1.8706e-01,\n",
      "           1.0895e+00],\n",
      "         [ 1.7734e+00, -1.3384e+00,  4.8925e-01, -7.9516e-01, -3.6749e-01,\n",
      "           2.3835e-01],\n",
      "         [ 1.1807e+00, -1.6582e+00, -5.8523e-01, -3.5156e-01,  2.6560e-01,\n",
      "           1.1487e+00],\n",
      "         [ 8.7755e-01, -4.4067e-01, -5.7755e-01,  1.7512e+00, -1.2167e+00,\n",
      "          -3.9384e-01],\n",
      "         [ 9.2657e-01, -1.8357e+00,  6.4013e-01, -7.8965e-01,  2.3097e-01,\n",
      "           8.2766e-01],\n",
      "         [ 2.9487e-01, -1.1750e+00, -8.0758e-01,  1.7959e+00,  5.1549e-01,\n",
      "          -6.2366e-01],\n",
      "         [ 6.2461e-01, -1.3183e+00, -5.9372e-01, -9.4465e-01,  8.5553e-01,\n",
      "           1.3766e+00]],\n",
      "\n",
      "        [[ 7.4514e-01, -1.6433e+00, -6.1553e-01, -3.3400e-01,  4.0091e-01,\n",
      "           1.4468e+00],\n",
      "         [ 4.9096e-01, -1.3096e+00,  3.3559e-01, -1.4249e+00,  7.5267e-01,\n",
      "           1.1552e+00],\n",
      "         [ 3.7861e-01, -1.4527e+00,  8.1536e-02, -1.1715e+00,  9.6924e-01,\n",
      "           1.1949e+00],\n",
      "         [-6.9834e-01, -1.0294e+00, -6.7541e-01,  5.4984e-01, -6.7488e-02,\n",
      "           1.9208e+00],\n",
      "         [ 6.0403e-01, -1.8043e+00,  6.2683e-01, -6.6191e-01, -9.1127e-03,\n",
      "           1.2444e+00],\n",
      "         [ 7.9746e-01, -1.8327e+00, -2.9753e-01, -3.3326e-01,  3.7607e-01,\n",
      "           1.2900e+00],\n",
      "         [-1.5695e-01, -8.7207e-01,  4.0579e-01,  2.0156e+00, -7.9184e-01,\n",
      "          -6.0049e-01],\n",
      "         [ 4.0635e-01, -1.4872e+00,  6.9511e-01, -1.3067e+00,  8.4877e-01,\n",
      "           8.4371e-01],\n",
      "         [-8.7843e-01, -1.2262e+00,  1.3646e+00, -7.5910e-01,  4.6432e-01,\n",
      "           1.0348e+00],\n",
      "         [ 1.1551e-01, -1.1434e+00, -6.8528e-01, -7.8798e-01,  7.7045e-01,\n",
      "           1.7307e+00]]], grad_fn=<NativeLayerNormBackward0>)\n",
      "\n",
      "##################################\n",
      "FWD PASS END\n",
      "\n",
      "output shape =  torch.Size([4, 10, 20])\n",
      "Epoch: 1, Loss: 3.151482105255127\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)\n",
    "\n",
    "model.train()\n",
    "\n",
    "for epoch in range(1):\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    print(\"FWD PASS START\\n\")\n",
    "    print(\"src_data shape = \", src_data.shape)\n",
    "    \n",
    "    output = model(src_data[:-1, :])\n",
    "    print(\"FWD PASS END\\n\")\n",
    "\n",
    "    print(\"output shape = \",output.shape)\n",
    "\n",
    "    loss = criterion(output.contiguous().view(-1, vocab_size), src_data[1:, :].contiguous().view(-1))\n",
    "\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f\"Epoch: {epoch+1}, Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions to get the intermediate outputs "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function to fetch word embeddings with help of token indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def look_up_table(sentence, vocab_embeds, embedding):\n",
    "\n",
    "    for i in range(sentence.size(0)):\n",
    "        for j in range(sentence.size(1)):\n",
    "            \n",
    "            # Get the index for the current word token index in the sequence\n",
    "            word_index = sentence[i, j].item()\n",
    "\n",
    "            if word_index < 0 or word_index >= vocab_embeds.size(0):\n",
    "                raise ValueError(f\"Invalid word index: {word_index}\")\n",
    "\n",
    "            # Lookup the corresponding embedding vector for the word\n",
    "            embedding[i, j, :] = vocab_embeds[word_index, :]\n",
    "\n",
    "            print(f\"Word index: {word_index}, Embedding: {vocab_embeds[word_index, :]}\")\n",
    "    print()\n",
    "\n",
    "    return embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embeddings and Positional encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding_outputs(src_data, max_seq_len, state_dict, d_model):\n",
    "\n",
    "    src_vocab_embeds = state_dict[\"src_embedding.weight\"]\n",
    "\n",
    "    src_embedding = torch.zeros(src_data.size(0), src_data.size(1), d_model)\n",
    "    print(\"Source sentence embedding\")\n",
    "    src_embedding =  look_up_table(src_data, src_vocab_embeds, src_embedding)\n",
    "    print(src_embedding.shape)\n",
    "\n",
    "    pe = PositionalEncoding(d_model = d_model, dropout=0, max_len=max_seq_len)\n",
    "\n",
    "    print(\"PE of src data:\")\n",
    "    print(pe(src_data).transpose(0,1))\n",
    "    print()\n",
    "\n",
    "    pe_src_embeds = src_embedding + pe(src_data).transpose(0,1)\n",
    "\n",
    "    print(\"PE source embeddings : \\n\")\n",
    "    print(pe_src_embeds)\n",
    "    print()\n",
    "\n",
    "    return pe_src_embeds\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder function to display the intermediate outputs and get the final outputs from the decoder\n",
    "\n",
    "### Masked self attention \n",
    "\n",
    "#### Functions to perform the attention calculation with Q,K and V matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def atten_product_needs_wts_false(Q, V, K, bsz, head_dim, src_len, tgt_len, embed_dim, attn_mask):\n",
    "\n",
    "\n",
    "    # *** For multi-head attention ***\n",
    "    #  (bsz*num_heads, src_len , head_dim) -> (bsz, num_heads, tgt_len, head_dim)\n",
    "    Q1 = Q.view(bsz, num_heads, tgt_len, head_dim)\n",
    "    K1 = K.view(bsz, num_heads, src_len, head_dim)\n",
    "    V1 = V.view(bsz, num_heads, src_len, head_dim)\n",
    "\n",
    "\n",
    "    L, S = Q1.size(-2), K1.size(-2)\n",
    "\n",
    "    scale_factor = 1 / math.sqrt(Q1.size(-1)) \n",
    "    # scale_factor = 1\n",
    "    attn_bias = torch.zeros(L, S, dtype=Q1.dtype)\n",
    "\n",
    "    if attn_mask is not None:\n",
    "        if attn_mask.dtype == torch.bool:\n",
    "            # attn_mask.masked_fill_(attn_mask.logical_not(), float(\"-inf\"))\n",
    "\n",
    "            masked_tensor = attn_mask.float().masked_fill(attn_mask, float('-inf'))\n",
    "            masked_tensor = masked_tensor.masked_fill(~attn_mask, 0)\n",
    "            attn_mask = masked_tensor\n",
    "\n",
    "            print(\"Attnetion mask infunction = \")\n",
    "            print(attn_mask)\n",
    "            print()\n",
    "            attn_bias = attn_bias.unsqueeze(0).unsqueeze(0)\n",
    "            attn_bias += attn_mask\n",
    "\n",
    "        else:\n",
    "            attn_bias += attn_mask\n",
    "            attn_bias = attn_bias.unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "\n",
    "    # print(\"Attnetion bias = \", attn_bias.shape)\n",
    "    # print(attn_bias)\n",
    "    # print()\n",
    "            \n",
    "\n",
    "    # (bsz, num_heads, tgt_len, head_dim) @ (bsz, num_heads, head_dim, tgt_len) -> (bsz, num_heads, tgt_len, tgt_len) \n",
    "    attn_weight = Q1 @ K1.transpose(-2, -1) * scale_factor\n",
    "    attn_weight += attn_bias\n",
    "\n",
    "\n",
    "    # print(\"INTERMEDIATE PDT = \", attn_weight)\n",
    "\n",
    "\n",
    "    # (bsz, num_heads, tgt_len, tgt_len) \n",
    "    attn_weight = torch.softmax(attn_weight, dim=-1)\n",
    "\n",
    "    # print(\"ATTN PDT = \", attn_weight)\n",
    "\n",
    "    sum_last_dim = attn_weight.sum(dim=-1)\n",
    "    tolerance = 1e-6  \n",
    "    assert torch.allclose(sum_last_dim, torch.ones_like(sum_last_dim), atol=tolerance), \"Attention weights sum is not approximately equal to 1\"\n",
    "\n",
    "    # (bsz, num_heads, tgt_len, tgt_len) @ (bsz, num_heads, tgt_len, head_dim) -> (bsz, num_heads, tgt_len, head_dim) \n",
    "    attn_output = attn_weight @ V1\n",
    "\n",
    "    # print(\"Dot product attention  = \")\n",
    "    # print(attn_weight.shape, attn_weight)\n",
    "\n",
    "    # print(attn_output.shape)\n",
    "    # print(bsz, tgt_len, embed_dim)\n",
    "    \n",
    "    # (bsz*tgt_len, embed_dim)\n",
    "    attn_output = attn_output.permute(2, 0, 1, 3).contiguous().view(bsz * tgt_len, embed_dim)\n",
    "\n",
    "    print(\"Attention output = \")\n",
    "    print(attn_weight.shape, attn_weight)\n",
    "\n",
    "    return attn_output\n",
    "\n",
    "\n",
    "\n",
    "def atten_product_needs_wts_true(Q, K, V, bsz, tgt_len, embed_dim, attn_mask):\n",
    "\n",
    "    # *** For multi-head attention ***\n",
    "    #  (bsz*num_heads, src_len , head_dim)\n",
    "    \n",
    "    B, Nt, E = Q.shape\n",
    "\n",
    "    Q_scaled = Q / math.sqrt(E)\n",
    "\n",
    "    if attn_mask is not None:\n",
    "        temp_pdt_matrix = torch.baddbmm(attn_mask, Q_scaled, K.transpose(-2, -1))\n",
    "    else:\n",
    "        temp_pdt_matrix = torch.bmm(Q_scaled, K.transpose(-2, -1))\n",
    "\n",
    "    attn_wt_matrix = torch.nn.functional.softmax(temp_pdt_matrix, dim=-1)\n",
    "\n",
    "    attn_output = torch.bmm(attn_wt_matrix, V)\n",
    "\n",
    "    attn_output = attn_output.transpose(0, 1).contiguous().view(tgt_len * bsz, embed_dim)\n",
    "\n",
    "\n",
    "    sum_last_dim = attn_wt_matrix.sum(dim=-1)\n",
    "\n",
    "    tolerance = 1e-6  \n",
    "    assert torch.allclose(sum_last_dim, torch.ones_like(sum_last_dim), atol=tolerance), \"Attention weights sum is not approximately equal to 1\"\n",
    "\n",
    "\n",
    "    print(\"Encoder Attention output = \")\n",
    "    print(attn_output)\n",
    "    print()\n",
    "\n",
    "    return attn_output, attn_wt_matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function to get the Q,K,V matrices from the model's intialised weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_qkv(query, key, value ,W, b):\n",
    "\n",
    "    # embed_dim\n",
    "    E = query.size(-1)\n",
    "\n",
    "    if key is value:\n",
    "        if query is key:\n",
    "            \n",
    "            # (src_len, bsz, embed_dim) @ (embed_dim*num_heads, embed_dim).T -> (src_len, bsz, embed_dim*num_heads)\n",
    "            tempop1 = query@W.T\n",
    "\n",
    "            # reshape to 3, E and not E, 3 is deliberate for better memory coalescing and keeping same order as chunk()\n",
    "            tempop1 = tempop1.unflatten(-1, (3, E)).unsqueeze(0).transpose(0, -2).squeeze(-2).contiguous()\n",
    "\n",
    "            # (src_len, bsz, embed_dim)\n",
    "            return tempop1[0], tempop1[1], tempop1[2]\n",
    "        \n",
    "\n",
    "        else:\n",
    "\n",
    "            # (embed_dim*1, embed_dim)\n",
    "            # (embed_dim*2, embed_dim)\n",
    "            W_q, W_kv = W.split([E, E * 2])\n",
    "\n",
    "            if b is None:\n",
    "                b_q = b_kv = None\n",
    "            else:\n",
    "                b_q, b_kv = b.split([E, E * 2])\n",
    "\n",
    "            # (src_len, bsz, embed_dim) @ (embed_dim*1, embed_dim).T -> (src_len, bsz, embed_dim)\n",
    "            q_matmul = query@W_q.T\n",
    "\n",
    "            # (src_len, bsz, embed_dim) @ (embed_dim*2, embed_dim).T -> (src_len, bsz, embed_dim*2)\n",
    "            kv_matmul = key@W_kv.T\n",
    "\n",
    "            kv_matmul = kv_matmul.unflatten(-1, (2, E)).unsqueeze(0).transpose(0, -2).squeeze(-2).contiguous()\n",
    "\n",
    "            # (src_len, bsz, embed_dim)\n",
    "            return q_matmul, kv_matmul[0], kv_matmul[1]\n",
    "\n",
    "    else:\n",
    "\n",
    "        W_q, W_k, W_v = W.chunk(3)\n",
    "        if b is None:\n",
    "            b_q = b_k = b_v = None\n",
    "        else:\n",
    "            b_q, b_k, b_v = b.chunk(3)\n",
    "\n",
    "\n",
    "        q_matmul = query@W_q.T\n",
    "        k_matmul = key@W_k.T\n",
    "        v_matmul = value@W_v.T\n",
    "\n",
    "        return q_matmul, k_matmul, v_matmul\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decoder block's self attention output function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoder_block_self_attn_output(x, state_dict, layer_num, num_heads, tgt_mask = None,need_weights = False):\n",
    "\n",
    "    # (tgt_len, bsz, embed_dim)\n",
    "    query_dec = key_dec = value_dec = x\n",
    "\n",
    "    tgt_len, bsz, embed_dim = x.shape\n",
    " \n",
    "    # (embed_dim*num_heads, embed_dim)\n",
    "    W_dec = state_dict[\"layers.{}.self_attn.in_proj_weight\".format(layer_num)]\n",
    "    b_dec = state_dict[\"layers.{}.self_attn.in_proj_bias\".format(layer_num)]\n",
    "\n",
    "\n",
    "    head_dim = embed_dim//num_heads\n",
    "\n",
    "    # (tgt_len, bsz, embed_dim)\n",
    "    Q_dec,K_dec,V_dec = get_qkv(query_dec, key_dec, value_dec ,W_dec, b_dec)\n",
    "    \n",
    "    # Q_dec = Q_dec.unsqueeze(0)\n",
    "    # K_dec = K_dec.unsqueeze(0)\n",
    "    # V_dec = V_dec.unsqueeze(0)\n",
    "\n",
    "    # (1, tgt_len, bsz, embed_dim)\n",
    "    # print(Q_dec.shape, K_dec.shape , V_dec.shape)\n",
    "    # print(tgt_len, bsz * num_heads, head_dim)\n",
    "\n",
    "    # (1, tgt_len, bsz, embed_dim) -> ( bsz*num_heads, tgt_len , head_dim )\n",
    "    Q_dec = Q_dec.reshape(tgt_len, bsz * num_heads, head_dim).transpose(0, 1)\n",
    "    K_dec = K_dec.reshape(K_dec.shape[0], bsz * num_heads, head_dim).transpose(0, 1)\n",
    "    V_dec = V_dec.reshape(V_dec.shape[0], bsz * num_heads, head_dim).transpose(0, 1)\n",
    "\n",
    "    print(\"Q_dec_{} = \".format(layer_num))\n",
    "    print(Q_dec)\n",
    "    print()\n",
    "\n",
    "    print(\"K_dec_{} = \".format(layer_num))\n",
    "    print(K_dec)\n",
    "    print()\n",
    "\n",
    "    print(\"V_dec_{} = \".format(layer_num))\n",
    "    print(V_dec)\n",
    "    print()\n",
    "\n",
    "    src_len = K_dec.size(1)\n",
    "\n",
    "\n",
    "    attn_mask = tgt_mask\n",
    "    if attn_mask is not None:\n",
    "\n",
    "        # Ensuring attn_mask's dim is 3\n",
    "        if attn_mask.dim() == 2:\n",
    "            correct_2d_size = (tgt_len, src_len)\n",
    "            if attn_mask.shape != correct_2d_size:\n",
    "                raise RuntimeError(f\"The shape of the 2D attn_mask is {attn_mask.shape}, but should be {correct_2d_size}.\")\n",
    "            attn_mask = attn_mask.unsqueeze(0)\n",
    "        elif attn_mask.dim() == 3:\n",
    "            correct_3d_size = (bsz * num_heads, tgt_len, src_len)\n",
    "            if attn_mask.shape != correct_3d_size:\n",
    "                raise RuntimeError(f\"The shape of the 3D attn_mask is {attn_mask.shape}, but should be {correct_3d_size}.\")\n",
    "        else:\n",
    "            raise RuntimeError(f\"attn_mask's dimension {attn_mask.dim()} is not supported\")\n",
    "\n",
    "    # attn_mask can be either (L,S) or (N*num_heads, L, S)\n",
    "    # if attn_mask's shape is (1, L, S) we need to unsqueeze to (1, 1, L, S)\n",
    "    # in order to match the input for SDPA of (N, num_heads, L, S)\n",
    "    if attn_mask is not None:\n",
    "        if attn_mask.size(0) == 1 and attn_mask.dim() == 3:\n",
    "            attn_mask = attn_mask.unsqueeze(0)\n",
    "        else:\n",
    "            attn_mask = attn_mask.view(bsz, num_heads, -1, src_len)\n",
    "\n",
    "    \n",
    "    \n",
    "    if need_weights is False:\n",
    "        attn_output = atten_product_needs_wts_false(Q = Q_dec, V = V_dec, K = K_dec, bsz = bsz, head_dim=head_dim, src_len=src_len, tgt_len=tgt_len, attn_mask = attn_mask, embed_dim=embed_dim)\n",
    "\n",
    "        print(\"Decoder Self Attention = \")\n",
    "        print(attn_output)\n",
    "        print()\n",
    "\n",
    "        op_dec_1 = torch.matmul(attn_output, state_dict[\"layers.{}.self_attn.out_proj.weight\".format(layer_num)].t()) + state_dict[\"layers.{}.self_attn.out_proj.bias\".format(layer_num)]\n",
    "        attn_dec_output = op_dec_1.view(tgt_len, bsz, attn_output.size(1))\n",
    "\n",
    "        return attn_dec_output, None\n",
    "    \n",
    "    else:\n",
    "\n",
    "        attn_dec_output,attn_wt_matrix_dec = atten_product_needs_wts_true(Q=Q_dec, K=K_dec, V=V_dec, bsz=bsz, tgt_len=tgt_len, attn_mask = attn_mask, embed_dim=embed_dim)\n",
    "\n",
    "        print(\"Decoder Attention output = \")\n",
    "        print(attn_wt_matrix_dec)\n",
    "        print()\n",
    "\n",
    "        op_dec_1 = torch.matmul(attn_dec_output, state_dict[\"layers.{}.self_attn.out_proj.weight\".format(layer_num)].t()) + state_dict[\"layers.{}.self_attn.out_proj.bias\".format(layer_num)]\n",
    "        attn_dec_output = op_dec_1.view(tgt_len, bsz, attn_dec_output.size(1))\n",
    "    \n",
    "\n",
    "        return attn_dec_output, attn_wt_matrix_dec\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Post self attention in decoder\n",
    "\n",
    "#### Function to perform the linear layer calculations after deccoder's self attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dec_post_self_attn(x, attn_dec_output, state_dict, layer_num, bsz, tgt_len):\n",
    "\n",
    "    # (bsz*src_len , embed_dim) @ (embed_dim , embed_dim).T -> (bsz*src_len , embed_dim)\n",
    "    # op_dec_1 = torch.matmul(attn_dec_output, state_dict[\"layers.{}.self_attn.out_proj.weight\".format(layer_num)].t()) + state_dict[\"layers.{}.self_attn.out_proj.bias\".format(layer_num)]\n",
    "\n",
    "    # print(op_dec_1.shape)\n",
    "\n",
    "    # # (bsz*src_len , embed_dim) -> (src_len, bsz, embed_dim)\n",
    "\n",
    "    # print()\n",
    "\n",
    "    # attn_dec_output = op_dec_1.view(tgt_len, bsz, attn_dec_output.size(1))\n",
    "\n",
    "\n",
    "    # Here src is the original passed inputs to the 1st transformer encoder layer which \n",
    "    # are pe_src_embeds \n",
    "\n",
    "    # (src_len, bsz, embed_dim)\n",
    "    output_dec_1 = attn_dec_output + x\n",
    "\n",
    "    #  (src_len, bsz, embed_dim) @ (embed_dim) -> (src_len, bsz, embed_dim) \n",
    "    linear_result_dec_1 = output_dec_1*state_dict[\"layers.{}.norm1.weight\".format(layer_num)] + state_dict[\"layers.{}.norm1.bias\".format(layer_num)]\n",
    "\n",
    "    # Layer normalization from Torch's implementation\n",
    "    layernorm_dec_1 = torch.nn.LayerNorm(normalized_shape=linear_result_dec_1.shape[2:])\n",
    "    linear_op_dec_1 = layernorm_dec_1(linear_result_dec_1)\n",
    "\n",
    "\n",
    "    # Manual Layer Normalization\n",
    "    x = linear_result_dec_1\n",
    "\n",
    "    # Obtained layer norm weights and biases (learnable)\n",
    "    w = layernorm_dec_1.weight\n",
    "    b = layernorm_dec_1.bias\n",
    "\n",
    "    linear_result_dec_1f = w*x + b\n",
    "\n",
    "    epsilon = 1e-5  \n",
    "    mean = linear_result_dec_1f.mean(dim=-1, keepdim=True)\n",
    "    std = linear_result_dec_1f.std(dim=-1, unbiased=False, keepdim=True)\n",
    "    normalized_result_dec_1 = (linear_result_dec_1f - mean) / (std + epsilon) * w + b\n",
    "\n",
    "\n",
    "    op_dec_1 = torch.matmul(normalized_result_dec_1, state_dict[\"layers.{}.linear1.weight\".format(layer_num)].t()) + state_dict[\"layers.{}.linear1.bias\".format(layer_num)]\n",
    "    op_dec_1_relu = torch.nn.functional.relu(op_dec_1)\n",
    "    op_dec_2 = torch.matmul(op_dec_1_relu, state_dict[\"layers.{}.linear2.weight\".format(layer_num)].t()) + state_dict[\"layers.{}.linear2.bias\".format(layer_num)]\n",
    "\n",
    "\n",
    "    output_dec_2 = op_dec_2 + linear_op_dec_1\n",
    "    output_dec_2_norm = output_dec_2*state_dict[\"layers.{}.norm2.weight\".format(layer_num)] + state_dict[\"layers.{}.norm2.bias\".format(layer_num)]\n",
    "\n",
    "    # Layer normalization from Torch's implementation\n",
    "    layernorm_dec_final = torch.nn.LayerNorm(normalized_shape=output_dec_2_norm.shape[2:])\n",
    "    output_dec_final = layernorm_dec_final(output_dec_2_norm)\n",
    "\n",
    "\n",
    "    # Manual Layer Normalization \n",
    "    x = output_dec_2_norm\n",
    "\n",
    "    # Obtained layer norm weights and biases (learnable)\n",
    "    w = layernorm_dec_final.weight\n",
    "    b = layernorm_dec_final.bias\n",
    "\n",
    "    linear_result_dec_2 = w*x + b\n",
    "\n",
    "    epsilon = 1e-5  \n",
    "    mean = linear_result_dec_2.mean(dim=-1, keepdim=True)\n",
    "    std = linear_result_dec_2.std(dim=-1, unbiased=False, keepdim=True)\n",
    "    output_dec_final = (linear_result_dec_2 - mean) / (std + epsilon) * w + b\n",
    "\n",
    "    print(\"Final Encoder {} Output :\".format(layer_num))\n",
    "    print(\"norm2(norm1(x + self_atten(x)) + feed_fwd_op)\\n\")\n",
    "    print(output_dec_final)\n",
    "    print()\n",
    "\n",
    "    # (src_len, bsz, embed_dim) \n",
    "    return output_dec_final\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function to perform the linear layer calculations after transformer blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feef_fwd_transformer(dec_output_final, state_dict):\n",
    "\n",
    "    # (tgt_len, bsz, embed_dim) @ (vocab_size, embed_dim).T -> (tgt_len, bsz, vocab_size)\n",
    "    final_op = dec_output_final@state_dict[\"fc.weight\"].T + state_dict[\"fc.bias\"]\n",
    "\n",
    "    return final_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_intermediate_outputs_mask(src_data ,d_model, state_dict , num_decoder_layers, tgt_mask, max_seq_len, d_ff):\n",
    "\n",
    "    pe_src_embeds = get_embedding_outputs(src_data=src_data,  state_dict=state_dict, max_seq_len=max_seq_len, d_model = d_model)\n",
    "    print(\"###\"*25)\n",
    "    print(\"### Decoder Start ###\")\n",
    "    print()\n",
    "\n",
    "    x_dec = pe_src_embeds\n",
    "    for lno in range(num_decoder_layers):\n",
    "        attn_dec_output, attn_wt_matrix = decoder_block_self_attn_output(x_dec, state_dict, layer_num = lno, num_heads=num_heads, need_weights = False, tgt_mask=tgt_mask)\n",
    "\n",
    "\n",
    "        if lno == 0:\n",
    "            tgt_len, bsz, embed_dim = x_dec.shape\n",
    "\n",
    "        output_dec_final = dec_post_self_attn(x_dec, attn_dec_output, state_dict, layer_num = lno , bsz = bsz, tgt_len = tgt_len)\n",
    "\n",
    "        x_dec = output_dec_final\n",
    "\n",
    "    \n",
    "    print(\"### Decoder End ###\")\n",
    "    \n",
    "    final_op = feef_fwd_transformer(x_dec, state_dict)\n",
    "\n",
    "    return final_op\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import functional as F\n",
    "\n",
    "need_weights = False\n",
    "\n",
    "src_mask = None\n",
    "\n",
    "tgt_mask = None\n",
    "\n",
    "memory_mask = None\n",
    "\n",
    "embed_dim = 6\n",
    "\n",
    "num_heads = 2\n",
    "\n",
    "max_seq_len = 5\n",
    "\n",
    "num_decoder_layers = 2\n",
    " \n",
    "\n",
    "def generate_square_subsequent_mask(self, tgt):\n",
    "        seq_length = tgt.size(0)\n",
    "        nopeak_mask = (torch.triu(torch.ones(seq_length, seq_length), diagonal=1)).bool()\n",
    "\n",
    "        return nopeak_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_mask(src):\n",
    "    seq_length = src.size(0)\n",
    "    nopeak_mask = (torch.triu(torch.ones(seq_length, seq_length), diagonal=1)).bool()\n",
    "    # tgt_mask = tgt_mask & nopeak_mask\n",
    "    return nopeak_mask\n",
    "\n",
    "tgt_mask = generate_mask(src_data[:-1, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source sentence embedding\n",
      "Word index: 2, Embedding: tensor([ 0.1198,  1.2377,  1.1168, -0.2473, -1.3527, -1.6959])\n",
      "Word index: 15, Embedding: tensor([-1.8817, -0.0497, -1.0450, -0.9565,  0.0335,  0.7101])\n",
      "Word index: 9, Embedding: tensor([ 0.5433, -0.3952, -0.4462,  0.7440,  1.5210,  3.4105])\n",
      "Word index: 16, Embedding: tensor([ 1.6459, -1.3602,  0.3446,  0.5199, -2.6133, -1.6965])\n",
      "Word index: 5, Embedding: tensor([ 0.9463, -0.8437, -0.6136,  0.0316, -0.4927,  0.2484])\n",
      "Word index: 4, Embedding: tensor([ 0.7502, -0.5855, -0.1734,  0.1835,  1.3894,  1.5863])\n",
      "Word index: 12, Embedding: tensor([-1.6293, -0.5497, -0.4798, -0.4997, -1.0670,  1.1149])\n",
      "Word index: 11, Embedding: tensor([ 1.1108,  1.2899, -1.4782,  2.5672, -0.4731,  0.3356])\n",
      "Word index: 7, Embedding: tensor([-0.2897,  0.0525,  0.5229,  2.3022, -1.4689, -1.5867])\n",
      "Word index: 10, Embedding: tensor([-1.5312, -1.2341,  1.8197, -0.5515, -0.5692,  0.9200])\n",
      "Word index: 12, Embedding: tensor([-1.6293, -0.5497, -0.4798, -0.4997, -1.0670,  1.1149])\n",
      "Word index: 18, Embedding: tensor([ 0.0930, -0.6661,  0.6080, -0.7300,  1.3750,  0.6596])\n",
      "Word index: 12, Embedding: tensor([-1.6293, -0.5497, -0.4798, -0.4997, -1.0670,  1.1149])\n",
      "Word index: 2, Embedding: tensor([ 0.1198,  1.2377,  1.1168, -0.2473, -1.3527, -1.6959])\n",
      "Word index: 9, Embedding: tensor([ 0.5433, -0.3952, -0.4462,  0.7440,  1.5210,  3.4105])\n",
      "Word index: 2, Embedding: tensor([ 0.1198,  1.2377,  1.1168, -0.2473, -1.3527, -1.6959])\n",
      "Word index: 10, Embedding: tensor([-1.5312, -1.2341,  1.8197, -0.5515, -0.5692,  0.9200])\n",
      "Word index: 14, Embedding: tensor([ 0.8419, -0.4000,  1.0395,  0.3582, -0.2460,  2.3025])\n",
      "Word index: 15, Embedding: tensor([-1.8817, -0.0497, -1.0450, -0.9565,  0.0335,  0.7101])\n",
      "Word index: 13, Embedding: tensor([-0.1407,  0.8058, -0.0933,  0.6871, -0.8383,  0.0009])\n",
      "Word index: 1, Embedding: tensor([-0.3160, -2.1152,  0.3223, -1.2633,  0.3500,  0.3081])\n",
      "Word index: 9, Embedding: tensor([ 0.5433, -0.3952, -0.4462,  0.7440,  1.5210,  3.4105])\n",
      "Word index: 17, Embedding: tensor([-0.2282,  0.2800, -0.7015,  1.0367, -0.6037, -1.2788])\n",
      "Word index: 19, Embedding: tensor([ 0.4766, -1.0163,  0.1804,  0.1083, -0.7548,  0.2443])\n",
      "Word index: 16, Embedding: tensor([ 1.6459, -1.3602,  0.3446,  0.5199, -2.6133, -1.6965])\n",
      "Word index: 5, Embedding: tensor([ 0.9463, -0.8437, -0.6136,  0.0316, -0.4927,  0.2484])\n",
      "Word index: 7, Embedding: tensor([-0.2897,  0.0525,  0.5229,  2.3022, -1.4689, -1.5867])\n",
      "Word index: 19, Embedding: tensor([ 0.4766, -1.0163,  0.1804,  0.1083, -0.7548,  0.2443])\n",
      "Word index: 17, Embedding: tensor([-0.2282,  0.2800, -0.7015,  1.0367, -0.6037, -1.2788])\n",
      "Word index: 4, Embedding: tensor([ 0.7502, -0.5855, -0.1734,  0.1835,  1.3894,  1.5863])\n",
      "Word index: 5, Embedding: tensor([ 0.9463, -0.8437, -0.6136,  0.0316, -0.4927,  0.2484])\n",
      "Word index: 1, Embedding: tensor([-0.3160, -2.1152,  0.3223, -1.2633,  0.3500,  0.3081])\n",
      "Word index: 4, Embedding: tensor([ 0.7502, -0.5855, -0.1734,  0.1835,  1.3894,  1.5863])\n",
      "Word index: 12, Embedding: tensor([-1.6293, -0.5497, -0.4798, -0.4997, -1.0670,  1.1149])\n",
      "Word index: 6, Embedding: tensor([ 0.4397,  0.1124,  0.6408,  0.4412, -0.1023,  0.7924])\n",
      "Word index: 5, Embedding: tensor([ 0.9463, -0.8437, -0.6136,  0.0316, -0.4927,  0.2484])\n",
      "Word index: 7, Embedding: tensor([-0.2897,  0.0525,  0.5229,  2.3022, -1.4689, -1.5867])\n",
      "Word index: 1, Embedding: tensor([-0.3160, -2.1152,  0.3223, -1.2633,  0.3500,  0.3081])\n",
      "Word index: 10, Embedding: tensor([-1.5312, -1.2341,  1.8197, -0.5515, -0.5692,  0.9200])\n",
      "Word index: 9, Embedding: tensor([ 0.5433, -0.3952, -0.4462,  0.7440,  1.5210,  3.4105])\n",
      "\n",
      "torch.Size([4, 10, 6])\n",
      "PE of src data:\n",
      "tensor([[[ 0.0000,  1.0000,  0.0000,  1.0000,  0.0000,  1.0000]],\n",
      "\n",
      "        [[ 0.8415,  0.5403,  0.0464,  0.9989,  0.0022,  1.0000]],\n",
      "\n",
      "        [[ 0.9093, -0.4161,  0.0927,  0.9957,  0.0043,  1.0000]],\n",
      "\n",
      "        [[ 0.1411, -0.9900,  0.1388,  0.9903,  0.0065,  1.0000]]])\n",
      "\n",
      "PE source embeddings : \n",
      "\n",
      "tensor([[[ 0.1198,  2.2377,  1.1168,  0.7527, -1.3527, -0.6959],\n",
      "         [-1.8817,  0.9503, -1.0450,  0.0435,  0.0335,  1.7101],\n",
      "         [ 0.5433,  0.6048, -0.4462,  1.7440,  1.5210,  4.4105],\n",
      "         [ 1.6459, -0.3602,  0.3446,  1.5199, -2.6133, -0.6965],\n",
      "         [ 0.9463,  0.1563, -0.6136,  1.0316, -0.4927,  1.2484],\n",
      "         [ 0.7502,  0.4145, -0.1734,  1.1835,  1.3894,  2.5863],\n",
      "         [-1.6293,  0.4503, -0.4798,  0.5003, -1.0670,  2.1149],\n",
      "         [ 1.1108,  2.2899, -1.4782,  3.5672, -0.4731,  1.3356],\n",
      "         [-0.2897,  1.0525,  0.5229,  3.3022, -1.4689, -0.5867],\n",
      "         [-1.5312, -0.2341,  1.8197,  0.4485, -0.5692,  1.9200]],\n",
      "\n",
      "        [[-0.7879, -0.0094, -0.4334,  0.4992, -1.0648,  2.1149],\n",
      "         [ 0.9344, -0.1258,  0.6544,  0.2689,  1.3772,  1.6596],\n",
      "         [-0.7879, -0.0094, -0.4334,  0.4992, -1.0648,  2.1149],\n",
      "         [ 0.9613,  1.7780,  1.1632,  0.7516, -1.3505, -0.6959],\n",
      "         [ 1.3848,  0.1451, -0.3998,  1.7429,  1.5231,  4.4105],\n",
      "         [ 0.9613,  1.7780,  1.1632,  0.7516, -1.3505, -0.6959],\n",
      "         [-0.6897, -0.6938,  1.8661,  0.4474, -0.5671,  1.9200],\n",
      "         [ 1.6834,  0.1403,  1.0859,  1.3571, -0.2438,  3.3025],\n",
      "         [-1.0402,  0.4906, -0.9986,  0.0424,  0.0357,  1.7101],\n",
      "         [ 0.7008,  1.3461, -0.0469,  1.6860, -0.8362,  1.0009]],\n",
      "\n",
      "        [[ 0.5933, -2.5314,  0.4150, -0.2676,  0.3543,  1.3081],\n",
      "         [ 1.4526, -0.8113, -0.3535,  1.7397,  1.5253,  4.4105],\n",
      "         [ 0.6811, -0.1362, -0.6088,  2.0324, -0.5994, -0.2788],\n",
      "         [ 1.3859, -1.4325,  0.2731,  1.1040, -0.7505,  1.2443],\n",
      "         [ 2.5552, -1.7763,  0.4373,  1.5156, -2.6090, -0.6965],\n",
      "         [ 1.8556, -1.2598, -0.5209,  1.0273, -0.4884,  1.2484],\n",
      "         [ 0.6196, -0.3636,  0.6156,  3.2979, -1.4646, -0.5867],\n",
      "         [ 1.3859, -1.4325,  0.2731,  1.1040, -0.7505,  1.2443],\n",
      "         [ 0.6811, -0.1362, -0.6088,  2.0324, -0.5994, -0.2788],\n",
      "         [ 1.6595, -1.0016, -0.0807,  1.1792,  1.3937,  2.5863]],\n",
      "\n",
      "        [[ 1.0874, -1.8337, -0.4748,  1.0219, -0.4862,  1.2484],\n",
      "         [-0.1749, -3.1052,  0.4611, -0.2730,  0.3564,  1.3081],\n",
      "         [ 0.8913, -1.5755, -0.0346,  1.1738,  1.3958,  2.5863],\n",
      "         [-1.4882, -1.5397, -0.3410,  0.4906, -1.0605,  2.1149],\n",
      "         [ 0.5808, -0.8776,  0.7796,  1.4315, -0.0958,  1.7924],\n",
      "         [ 1.0874, -1.8337, -0.4748,  1.0219, -0.4862,  1.2484],\n",
      "         [-0.1485, -0.9375,  0.6617,  3.2925, -1.4624, -0.5867],\n",
      "         [-0.1749, -3.1052,  0.4611, -0.2730,  0.3564,  1.3081],\n",
      "         [-1.3901, -2.2241,  1.9585,  0.4388, -0.5628,  1.9200],\n",
      "         [ 0.6844, -1.3851, -0.3074,  1.7343,  1.5274,  4.4105]]])\n",
      "\n",
      "###########################################################################\n",
      "### Decoder Start ###\n",
      "\n",
      "Q_dec_0 = \n",
      "tensor([[[-1.4552,  0.5793,  1.1043],\n",
      "         [-0.4717,  0.8204, -0.3588],\n",
      "         [ 1.6194, -0.1178, -1.2677],\n",
      "         [ 1.0439,  0.4257, -0.1776]],\n",
      "\n",
      "        [[-0.2463, -0.9246,  0.0794],\n",
      "         [ 1.0109,  1.6849, -0.4990],\n",
      "         [-0.2970,  0.9185, -0.1552],\n",
      "         [-0.1077,  1.3278, -0.1918]],\n",
      "\n",
      "        [[-0.9463,  0.2863, -0.5349],\n",
      "         [ 1.1548, -0.0225, -0.8391],\n",
      "         [ 1.8677,  0.6317, -1.2890],\n",
      "         [ 1.5479, -0.4095, -1.7861]],\n",
      "\n",
      "        [[ 1.5734,  1.4188, -0.4736],\n",
      "         [ 0.0722,  0.0465,  0.0289],\n",
      "         [ 0.9781,  2.0269, -0.3672],\n",
      "         [-0.1849,  1.3902, -0.3299]],\n",
      "\n",
      "        [[ 0.9250,  0.6758, -1.1312],\n",
      "         [-0.4717,  0.8204, -0.3588],\n",
      "         [-0.1710,  0.1705,  0.7718],\n",
      "         [ 1.7276, -0.0578, -1.3506]],\n",
      "\n",
      "        [[ 1.5125,  1.9137, -0.4423],\n",
      "         [ 1.0109,  1.6849, -0.4990],\n",
      "         [-0.1649,  0.6675, -0.0899],\n",
      "         [ 0.3316,  1.4646, -0.2622]],\n",
      "\n",
      "        [[-0.3673,  1.0187,  1.5554],\n",
      "         [-0.9296,  0.6939,  1.2640],\n",
      "         [ 0.9377,  0.5774, -0.0895],\n",
      "         [-0.1260,  0.3699, -1.1947]],\n",
      "\n",
      "        [[-0.9398,  0.1094,  0.0388],\n",
      "         [-0.5838, -1.0991,  0.2025],\n",
      "         [-0.3567,  0.9347, -0.1394],\n",
      "         [ 0.9261,  2.4443, -0.7218]],\n",
      "\n",
      "        [[ 0.1726,  0.7615,  0.4986],\n",
      "         [ 1.4505,  0.7904, -0.9715],\n",
      "         [ 0.5754,  0.9746,  1.3975],\n",
      "         [ 0.6947,  0.2654, -0.7723]],\n",
      "\n",
      "        [[ 0.3146,  0.7429, -0.0921],\n",
      "         [ 1.1751,  1.7392, -0.3192],\n",
      "         [-1.4742,  0.2226,  0.1139],\n",
      "         [ 0.0086,  1.1295, -0.3056]],\n",
      "\n",
      "        [[ 0.8563,  0.2781, -0.6744],\n",
      "         [-0.9296,  0.6939,  1.2640],\n",
      "         [ 1.1153,  0.7174,  0.3408],\n",
      "         [ 1.0439,  0.4257, -0.1776]],\n",
      "\n",
      "        [[ 0.7538,  0.8798, -0.1626],\n",
      "         [-0.5838, -1.0991,  0.2025],\n",
      "         [-0.2198,  0.8561, -0.0170],\n",
      "         [-0.1077,  1.3278, -0.1918]],\n",
      "\n",
      "        [[-0.9973,  0.7058, -0.5184],\n",
      "         [ 0.1234,  0.1942, -1.4879],\n",
      "         [-0.5224,  0.1084,  0.6557],\n",
      "         [-0.5938, -0.1834,  0.1373]],\n",
      "\n",
      "        [[ 1.3484,  1.8594, -0.6221],\n",
      "         [ 0.1284,  1.0253, -0.4746],\n",
      "         [-0.6820,  0.8819, -0.2641],\n",
      "         [-0.5698,  1.3535, -0.4389]],\n",
      "\n",
      "        [[-0.9163,  1.0306,  1.6244],\n",
      "         [ 0.9064,  1.1511, -0.4595],\n",
      "         [ 0.9377,  0.5774, -0.0895],\n",
      "         [ 1.5479, -0.4095, -1.7861]],\n",
      "\n",
      "        [[ 0.8897,  1.1200, -0.1834],\n",
      "         [ 0.2545,  0.9119, -0.2173],\n",
      "         [-0.3567,  0.9347, -0.1394],\n",
      "         [-0.1849,  1.3902, -0.3299]],\n",
      "\n",
      "        [[-1.4651,  0.1525,  0.8135],\n",
      "         [-0.4207,  0.4009, -0.3753],\n",
      "         [-0.1710,  0.1705,  0.7718],\n",
      "         [ 0.4690, -0.2563, -2.3238]],\n",
      "\n",
      "        [[-0.1476,  0.7686, -0.3392],\n",
      "         [ 1.2359,  1.2443, -0.3505],\n",
      "         [-0.1649,  0.6675, -0.0899],\n",
      "         [ 0.0436,  1.7847, -0.6973]],\n",
      "\n",
      "        [[-0.4022,  0.0795, -1.6476],\n",
      "         [-0.6015,  0.8501,  0.8121],\n",
      "         [ 1.7990,  0.2340, -0.8322],\n",
      "         [ 1.7962,  0.3400, -1.8074]],\n",
      "\n",
      "        [[ 0.4659,  1.1998, -0.5977],\n",
      "         [ 0.3264,  0.4492, -0.1213],\n",
      "         [ 0.2194,  0.9930, -0.0875],\n",
      "         [ 1.0903,  2.4986, -0.5420]]])\n",
      "\n",
      "K_dec_0 = \n",
      "tensor([[[-0.7554,  1.7286, -0.8978],\n",
      "         [-1.2473,  0.9376,  1.1841],\n",
      "         [-0.6435, -1.0317,  0.6396],\n",
      "         [-0.9058, -0.4974,  0.3891]],\n",
      "\n",
      "        [[ 1.0559,  0.9876, -0.9088],\n",
      "         [-1.3301,  0.4544, -0.7485],\n",
      "         [-1.2680, -0.5859,  1.5287],\n",
      "         [-1.7719, -0.1159,  0.4630]],\n",
      "\n",
      "        [[-0.2158,  0.7585,  1.4128],\n",
      "         [-0.4551, -0.1594,  0.2473],\n",
      "         [-1.5484, -0.0611,  1.3556],\n",
      "         [-0.6931, -1.2807,  0.9873]],\n",
      "\n",
      "        [[-0.6221,  0.3783, -1.4847],\n",
      "         [-0.1606, -0.2279,  0.2952],\n",
      "         [-2.1476, -0.0749, -0.4453],\n",
      "         [-1.5567, -0.5022,  1.6273]],\n",
      "\n",
      "        [[-1.4214,  0.6571,  1.4936],\n",
      "         [-1.2473,  0.9376,  1.1841],\n",
      "         [-0.4345,  0.0176, -0.3898],\n",
      "         [-0.8836, -0.7651,  0.8993]],\n",
      "\n",
      "        [[-1.6008,  0.3131, -1.3995],\n",
      "         [-1.3301,  0.4544, -0.7485],\n",
      "         [-0.8830,  0.5938, -0.7997],\n",
      "         [-1.6327, -0.2086,  0.2832]],\n",
      "\n",
      "        [[-1.3343,  0.8064, -1.0069],\n",
      "         [-0.8047,  1.4784, -1.1399],\n",
      "         [-1.3003, -0.0599,  0.0913],\n",
      "         [-1.3746,  0.2206,  1.6358]],\n",
      "\n",
      "        [[-0.7735,  0.6453,  0.1352],\n",
      "         [ 0.9175,  0.7486, -0.4527],\n",
      "         [-1.4286,  0.0767,  0.5807],\n",
      "         [-2.0272,  0.3891, -0.1518]],\n",
      "\n",
      "        [[-0.7292,  0.4698,  0.1795],\n",
      "         [-1.4707,  0.4070,  1.2515],\n",
      "         [-1.4613,  0.0881, -1.1450],\n",
      "         [-1.5041,  0.1916,  0.4196]],\n",
      "\n",
      "        [[-0.9364,  0.1884, -0.5898],\n",
      "         [-1.7391,  0.0741, -0.9433],\n",
      "         [-1.3203,  0.2574,  1.0893],\n",
      "         [-1.2258,  0.4304,  0.1108]],\n",
      "\n",
      "        [[-0.7070,  0.2021,  0.6897],\n",
      "         [-0.8047,  1.4784, -1.1399],\n",
      "         [-0.8561, -0.2485,  0.0415],\n",
      "         [-0.9058, -0.4974,  0.3891]],\n",
      "\n",
      "        [[-0.7972,  0.0957, -0.7696],\n",
      "         [ 0.9175,  0.7486, -0.4527],\n",
      "         [-1.4832, -0.1995,  0.3643],\n",
      "         [-1.7719, -0.1159,  0.4630]],\n",
      "\n",
      "        [[-1.1980,  1.1878,  1.4262],\n",
      "         [-1.8970,  0.7462,  0.7866],\n",
      "         [-1.4836,  0.4451, -0.7665],\n",
      "         [-1.5333,  0.1961, -0.4188]],\n",
      "\n",
      "        [[-1.1918,  0.6934, -1.2046],\n",
      "         [-0.7302,  0.6129,  0.4804],\n",
      "         [-1.0995,  1.3892, -0.7659],\n",
      "         [-1.3881,  1.4728, -0.6673]],\n",
      "\n",
      "        [[-0.9475,  1.4121, -0.1886],\n",
      "         [-2.2158,  1.1479,  0.4586],\n",
      "         [-1.3003, -0.0599,  0.0913],\n",
      "         [-0.6931, -1.2807,  0.9873]],\n",
      "\n",
      "        [[-1.0630,  1.2727, -2.9474],\n",
      "         [-1.1509,  0.3941, -0.0855],\n",
      "         [-1.4286,  0.0767,  0.5807],\n",
      "         [-1.5567, -0.5022,  1.6273]],\n",
      "\n",
      "        [[-1.3567,  1.1633, -0.6284],\n",
      "         [-0.2651,  0.5083,  1.1707],\n",
      "         [-0.4345,  0.0176, -0.3898],\n",
      "         [-2.0243,  0.0292,  1.2384]],\n",
      "\n",
      "        [[-0.5527,  1.7772, -1.7201],\n",
      "         [-0.7605,  0.1393, -1.0286],\n",
      "         [-0.8830,  0.5938, -0.7997],\n",
      "         [-1.4273,  0.5476,  1.0771]],\n",
      "\n",
      "        [[-1.8477,  0.9964,  1.0288],\n",
      "         [-1.0838,  1.2662, -0.1494],\n",
      "         [-0.8340, -0.5162,  0.5516],\n",
      "         [-1.5980, -0.3101,  1.7033]],\n",
      "\n",
      "        [[-0.5919,  0.8519,  0.0243],\n",
      "         [-0.4420,  0.8095, -1.2754],\n",
      "         [-1.3440, -0.2922,  0.1845],\n",
      "         [-2.4362,  0.0088, -0.3467]]])\n",
      "\n",
      "V_dec_0 = \n",
      "tensor([[[ 9.9826e-01,  1.1588e-01,  1.0535e+00],\n",
      "         [-1.2033e-01,  1.2405e+00,  8.6981e-02],\n",
      "         [-4.5990e-01, -6.0168e-01, -5.5067e-01],\n",
      "         [-8.3972e-01, -6.8078e-01, -4.2016e-01]],\n",
      "\n",
      "        [[ 4.7661e-01, -8.0867e-01, -1.3011e+00],\n",
      "         [ 3.9194e-01,  2.3666e-01,  1.1554e+00],\n",
      "         [-1.6172e-01,  1.3981e+00,  1.1582e+00],\n",
      "         [ 4.3451e-01,  1.7881e-01,  1.7117e+00]],\n",
      "\n",
      "        [[-2.0628e-01,  1.8654e+00, -8.2431e-02],\n",
      "         [-7.3881e-01, -6.5794e-01,  3.2566e-01],\n",
      "         [-2.1049e+00, -5.7002e-01,  1.5704e-01],\n",
      "         [-2.7632e-01, -3.6041e-01, -9.6534e-01]],\n",
      "\n",
      "        [[-2.3835e-01, -1.2486e-01,  8.0209e-01],\n",
      "         [-3.2283e-01,  1.5291e-01,  7.0044e-01],\n",
      "         [-8.3138e-02, -2.2078e-01,  3.0860e+00],\n",
      "         [-5.6613e-01,  1.8912e+00,  1.3184e+00]],\n",
      "\n",
      "        [[-1.8096e+00,  2.2229e-01,  4.4281e-01],\n",
      "         [-1.2033e-01,  1.2405e+00,  8.6981e-02],\n",
      "         [-5.0765e-01, -8.0769e-01, -2.7915e-01],\n",
      "         [-1.4173e+00, -8.3921e-01, -3.8387e-01]],\n",
      "\n",
      "        [[-2.4140e-01, -6.3873e-01,  2.6339e+00],\n",
      "         [ 3.9194e-01,  2.3666e-01,  1.1554e+00],\n",
      "         [ 1.8098e-01, -1.0631e+00,  9.4828e-01],\n",
      "         [-4.9601e-01,  3.4702e-01,  2.2125e+00]],\n",
      "\n",
      "        [[ 3.6765e-01, -9.3427e-01,  3.6855e-01],\n",
      "         [ 7.6500e-01, -4.1629e-01,  1.1082e+00],\n",
      "         [-5.1538e-01, -8.7015e-01, -2.8922e-02],\n",
      "         [ 1.1502e-03,  1.2217e+00, -6.6814e-01]],\n",
      "\n",
      "        [[ 1.3489e+00, -5.9482e-01,  1.1218e-01],\n",
      "         [ 7.4823e-01, -8.1928e-01, -1.1521e+00],\n",
      "         [ 5.3999e-01,  2.3188e-01,  1.2887e+00],\n",
      "         [-1.2583e-01,  1.1584e+00,  1.6186e+00]],\n",
      "\n",
      "        [[-7.2794e-01, -1.2974e-01,  2.8027e-01],\n",
      "         [-2.0428e+00, -3.0987e-01,  4.9750e-01],\n",
      "         [ 7.2291e-02, -1.7266e+00,  8.2789e-02],\n",
      "         [-4.0084e-01, -6.0786e-01, -2.4693e-03]],\n",
      "\n",
      "        [[ 6.8065e-01, -7.3234e-01,  1.0993e+00],\n",
      "         [ 3.0223e-02, -6.4934e-01,  2.7830e+00],\n",
      "         [ 1.5072e+00, -1.7686e-01,  5.6428e-01],\n",
      "         [-1.6253e-01,  3.6745e-01,  1.2956e+00]],\n",
      "\n",
      "        [[-1.3055e+00, -2.8817e-01,  3.1656e-01],\n",
      "         [ 7.6500e-01, -4.1629e-01,  1.1082e+00],\n",
      "         [-1.0233e+00, -9.2205e-01, -5.4934e-03],\n",
      "         [-8.3972e-01, -6.8078e-01, -4.2016e-01]],\n",
      "\n",
      "        [[-2.4987e-01, -5.6413e-01,  1.6002e+00],\n",
      "         [ 7.4823e-01, -8.1928e-01, -1.1521e+00],\n",
      "         [ 8.3892e-01, -3.1438e-01,  1.5514e+00],\n",
      "         [ 4.3451e-01,  1.7881e-01,  1.7117e+00]],\n",
      "\n",
      "        [[ 1.1293e-01,  1.7727e+00,  3.2291e-02],\n",
      "         [ 6.5525e-01,  3.0311e-01,  1.7769e-01],\n",
      "         [ 1.2854e-01, -1.3842e+00, -2.9655e-01],\n",
      "         [ 3.1211e-01, -1.1429e+00, -7.1122e-01]],\n",
      "\n",
      "        [[ 1.2031e-01,  2.4727e-01,  1.0063e+00],\n",
      "         [-4.4895e-01,  1.4830e+00,  3.9679e-01],\n",
      "         [-8.6533e-02, -8.1345e-01,  8.3516e-01],\n",
      "         [-4.9094e-01, -3.2026e-01,  9.9539e-01]],\n",
      "\n",
      "        [[-1.3803e+00, -3.4877e-01,  5.7254e-01],\n",
      "         [-7.2173e-01, -4.9650e-01,  9.9179e-01],\n",
      "         [-5.1538e-01, -8.7015e-01, -2.8922e-02],\n",
      "         [-2.7632e-01, -3.6041e-01, -9.6534e-01]],\n",
      "\n",
      "        [[ 6.4705e-01, -2.9206e+00,  1.7819e+00],\n",
      "         [ 6.0714e-01, -1.2490e-02,  1.4317e+00],\n",
      "         [ 5.3999e-01,  2.3188e-01,  1.2887e+00],\n",
      "         [-5.6613e-01,  1.8912e+00,  1.3184e+00]],\n",
      "\n",
      "        [[ 4.2389e-01, -5.9185e-01, -1.0787e-02],\n",
      "         [-4.3955e-01,  1.3332e+00, -2.7741e-02],\n",
      "         [-5.0765e-01, -8.0769e-01, -2.7915e-01],\n",
      "         [ 7.7673e-01,  2.8424e-01, -5.7743e-01]],\n",
      "\n",
      "        [[-2.4479e-01, -1.2314e+00,  3.8306e-01],\n",
      "         [ 3.3270e-02, -1.3547e-01,  9.5114e-01],\n",
      "         [ 1.8098e-01, -1.0631e+00,  9.4828e-01],\n",
      "         [-9.6672e-01,  2.4048e+00,  8.6006e-01]],\n",
      "\n",
      "        [[ 8.8852e-01,  8.3528e-01,  1.2300e-01],\n",
      "         [-2.9830e-01, -6.6192e-02,  6.7677e-01],\n",
      "         [-1.6009e+00, -1.0805e+00,  3.0794e-02],\n",
      "         [-1.9214e+00, -3.2874e-01, -2.5762e-01]],\n",
      "\n",
      "        [[-7.2057e-01,  1.4936e+00,  2.4773e-01],\n",
      "         [ 5.8927e-01, -1.1738e+00,  5.9630e-01],\n",
      "         [-9.1605e-02, -1.4617e-01,  2.0523e+00],\n",
      "         [-4.8755e-01,  2.7241e-01,  3.2463e+00]]])\n",
      "\n",
      "Attnetion mask infunction = \n",
      "tensor([[[[0., -inf, -inf, -inf],\n",
      "          [0., 0., -inf, -inf],\n",
      "          [0., 0., 0., -inf],\n",
      "          [0., 0., 0., 0.]]]])\n",
      "\n",
      "Attention output = \n",
      "torch.Size([10, 2, 4, 4]) tensor([[[[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.6619, 0.3381, 0.0000, 0.0000],\n",
      "          [0.6329, 0.0919, 0.2752, 0.0000],\n",
      "          [0.4130, 0.2042, 0.1915, 0.1913]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.8763, 0.1237, 0.0000, 0.0000],\n",
      "          [0.3790, 0.4240, 0.1970, 0.0000],\n",
      "          [0.4064, 0.3077, 0.1073, 0.1786]]],\n",
      "\n",
      "\n",
      "        [[[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.3972, 0.6028, 0.0000, 0.0000],\n",
      "          [0.4000, 0.5264, 0.0736, 0.0000],\n",
      "          [0.1570, 0.5240, 0.0615, 0.2575]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.4918, 0.5082, 0.0000, 0.0000],\n",
      "          [0.6108, 0.2674, 0.1218, 0.0000],\n",
      "          [0.4185, 0.1745, 0.2809, 0.1261]]],\n",
      "\n",
      "\n",
      "        [[[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.4627, 0.5373, 0.0000, 0.0000],\n",
      "          [0.4448, 0.3915, 0.1637, 0.0000],\n",
      "          [0.0581, 0.0871, 0.6894, 0.1655]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.4731, 0.5269, 0.0000, 0.0000],\n",
      "          [0.3329, 0.3312, 0.3358, 0.0000],\n",
      "          [0.2564, 0.2758, 0.3407, 0.1271]]],\n",
      "\n",
      "\n",
      "        [[[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.5280, 0.4720, 0.0000, 0.0000],\n",
      "          [0.2942, 0.4937, 0.2121, 0.0000],\n",
      "          [0.3638, 0.4428, 0.1414, 0.0520]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.6691, 0.3309, 0.0000, 0.0000],\n",
      "          [0.3854, 0.3016, 0.3131, 0.0000],\n",
      "          [0.1882, 0.6869, 0.0494, 0.0756]]],\n",
      "\n",
      "\n",
      "        [[[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.7775, 0.2225, 0.0000, 0.0000],\n",
      "          [0.3323, 0.5955, 0.0722, 0.0000],\n",
      "          [0.2979, 0.1359, 0.3782, 0.1880]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.6443, 0.3557, 0.0000, 0.0000],\n",
      "          [0.2238, 0.4266, 0.3496, 0.0000],\n",
      "          [0.2636, 0.2594, 0.2046, 0.2724]]],\n",
      "\n",
      "\n",
      "        [[[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.6838, 0.3162, 0.0000, 0.0000],\n",
      "          [0.3603, 0.4006, 0.2391, 0.0000],\n",
      "          [0.2380, 0.3705, 0.2081, 0.1834]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.7221, 0.2779, 0.0000, 0.0000],\n",
      "          [0.3290, 0.3643, 0.3067, 0.0000],\n",
      "          [0.2529, 0.3621, 0.1856, 0.1993]]],\n",
      "\n",
      "\n",
      "        [[[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.3893, 0.6107, 0.0000, 0.0000],\n",
      "          [0.4173, 0.3934, 0.1893, 0.0000],\n",
      "          [0.2301, 0.2913, 0.2308, 0.2477]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.6166, 0.3834, 0.0000, 0.0000],\n",
      "          [0.3443, 0.2131, 0.4426, 0.0000],\n",
      "          [0.2113, 0.1112, 0.3159, 0.3617]]],\n",
      "\n",
      "\n",
      "        [[[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.7332, 0.2668, 0.0000, 0.0000],\n",
      "          [0.5144, 0.2292, 0.2564, 0.0000],\n",
      "          [0.3765, 0.0662, 0.2916, 0.2657]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.6973, 0.3027, 0.0000, 0.0000],\n",
      "          [0.5184, 0.2610, 0.2207, 0.0000],\n",
      "          [0.6256, 0.1809, 0.1272, 0.0664]]],\n",
      "\n",
      "\n",
      "        [[[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.6914, 0.3086, 0.0000, 0.0000],\n",
      "          [0.2643, 0.4960, 0.2397, 0.0000],\n",
      "          [0.4315, 0.0572, 0.4765, 0.0348]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.8123, 0.1877, 0.0000, 0.0000],\n",
      "          [0.4658, 0.2438, 0.2904, 0.0000],\n",
      "          [0.6989, 0.0973, 0.1414, 0.0624]]],\n",
      "\n",
      "\n",
      "        [[[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.6649, 0.3351, 0.0000, 0.0000],\n",
      "          [0.1254, 0.5063, 0.3683, 0.0000],\n",
      "          [0.0772, 0.6145, 0.2700, 0.0383]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.4730, 0.5270, 0.0000, 0.0000],\n",
      "          [0.3952, 0.4198, 0.1850, 0.0000],\n",
      "          [0.3609, 0.5604, 0.0410, 0.0376]]]])\n",
      "Decoder Self Attention = \n",
      "tensor([[ 9.9826e-01,  1.1588e-01,  1.0535e+00,  4.7661e-01, -8.0867e-01,\n",
      "         -1.3011e+00],\n",
      "        [-2.0628e-01,  1.8654e+00, -8.2431e-02, -2.3835e-01, -1.2486e-01,\n",
      "          8.0209e-01],\n",
      "        [-1.8096e+00,  2.2229e-01,  4.4281e-01, -2.4140e-01, -6.3873e-01,\n",
      "          2.6339e+00],\n",
      "        [ 3.6765e-01, -9.3427e-01,  3.6855e-01,  1.3489e+00, -5.9482e-01,\n",
      "          1.1218e-01],\n",
      "        [-7.2794e-01, -1.2974e-01,  2.8027e-01,  6.8065e-01, -7.3234e-01,\n",
      "          1.0993e+00],\n",
      "        [-1.3055e+00, -2.8817e-01,  3.1656e-01, -2.4987e-01, -5.6413e-01,\n",
      "          1.6002e+00],\n",
      "        [ 1.1293e-01,  1.7727e+00,  3.2291e-02,  1.2031e-01,  2.4727e-01,\n",
      "          1.0063e+00],\n",
      "        [-1.3803e+00, -3.4877e-01,  5.7254e-01,  6.4705e-01, -2.9206e+00,\n",
      "          1.7819e+00],\n",
      "        [ 4.2389e-01, -5.9185e-01, -1.0787e-02, -2.4479e-01, -1.2314e+00,\n",
      "          3.8306e-01],\n",
      "        [ 8.8852e-01,  8.3528e-01,  1.2300e-01, -7.2057e-01,  1.4936e+00,\n",
      "          2.4773e-01],\n",
      "        [ 6.2011e-01,  4.9608e-01,  7.2676e-01,  4.6613e-01, -6.7932e-01,\n",
      "         -9.9714e-01],\n",
      "        [-5.2727e-01,  3.4440e-01,  1.6355e-01, -2.8128e-01,  1.6291e-02,\n",
      "          7.5043e-01],\n",
      "        [-9.0192e-01,  7.6941e-01,  2.5162e-01,  9.2331e-02, -1.7745e-01,\n",
      "          1.8548e+00],\n",
      "        [ 5.5521e-01, -6.8977e-01,  7.1768e-01,  1.1502e+00, -6.6909e-01,\n",
      "         -3.0614e-01],\n",
      "        [-1.0206e+00, -1.6983e-01,  3.2861e-01,  4.4932e-01, -7.0282e-01,\n",
      "          1.6982e+00],\n",
      "        [-6.5085e-01, -3.2868e-01,  5.6686e-01,  2.7456e-02, -6.3502e-01,\n",
      "          8.3549e-01],\n",
      "        [ 4.4411e-01,  8.7526e-01,  1.2108e-01, -9.7940e-02,  7.2106e-01,\n",
      "          7.7263e-01],\n",
      "        [-1.2046e+00, -3.8818e-01,  6.8439e-01,  6.3497e-01, -2.0404e+00,\n",
      "          1.6759e+00],\n",
      "        [ 1.5741e-01,  2.2750e-03, -1.6019e-02, -1.9260e-01, -1.0257e+00,\n",
      "          4.8970e-01],\n",
      "        [ 4.9083e-01,  5.3321e-01,  3.0856e-01, -3.0233e-02,  8.7800e-02,\n",
      "          4.3144e-01],\n",
      "        [ 4.9420e-01,  2.1751e-02,  5.2324e-01,  3.1495e-01,  6.9270e-02,\n",
      "          2.2487e-01],\n",
      "        [-6.2629e-01,  3.5793e-01,  1.5000e-01, -2.4203e-01, -6.2279e-02,\n",
      "          1.0532e+00],\n",
      "        [-9.3508e-01,  4.5238e-01,  1.8532e-01,  1.1023e-01, -4.9129e-01,\n",
      "          1.5781e+00],\n",
      "        [ 3.7652e-01, -6.6495e-01,  6.4940e-01,  9.1454e-01, -4.0370e-01,\n",
      "          9.9244e-02],\n",
      "        [-1.4532e+00, -3.5230e-01,  3.9537e-01,  6.9210e-01, -5.0275e-01,\n",
      "          1.6306e+00],\n",
      "        [-4.0869e-01, -4.9105e-01,  5.5665e-01,  4.4769e-01, -5.8048e-01,\n",
      "          5.8262e-01],\n",
      "        [ 3.2924e-01,  5.9695e-01,  2.7240e-02, -9.2551e-02,  4.1197e-02,\n",
      "          8.0067e-01],\n",
      "        [-1.0075e+00, -5.1632e-01,  5.1444e-01,  6.1301e-01, -1.4661e+00,\n",
      "          1.5817e+00],\n",
      "        [-2.2767e-01,  3.1115e-01, -8.3533e-02, -5.3348e-02, -9.1533e-01,\n",
      "          6.8572e-01],\n",
      "        [-6.2932e-01, -3.2680e-01,  3.6941e-01, -5.4347e-02,  7.0513e-02,\n",
      "          7.2785e-01],\n",
      "        [ 1.3900e-01,  5.5737e-02,  2.6703e-01,  3.7454e-01, -7.3843e-02,\n",
      "          2.5679e-01],\n",
      "        [-6.2008e-01, -1.7967e-01, -8.1173e-02, -2.5083e-01,  1.5089e-01,\n",
      "          1.4910e+00],\n",
      "        [-7.0000e-01, -5.7470e-01, -2.2267e-01,  4.4797e-02, -4.1657e-01,\n",
      "          1.5984e+00],\n",
      "        [ 3.9969e-01, -5.8368e-01,  5.8598e-01,  7.8495e-01, -5.7573e-01,\n",
      "         -5.8434e-01],\n",
      "        [-5.4253e-01, -8.4796e-01,  1.8196e-01,  4.5143e-01, -2.9758e-01,\n",
      "          1.4800e+00],\n",
      "        [-3.9431e-01, -5.3957e-01,  4.0771e-01,  4.5008e-01, -4.6205e-01,\n",
      "          6.1683e-01],\n",
      "        [ 3.2387e-01, -1.0630e-01, -1.8545e-01, -2.2939e-01, -1.5561e-01,\n",
      "          8.8052e-01],\n",
      "        [-7.9118e-01, -5.1365e-01,  1.6306e-02,  5.4569e-01, -1.6743e+00,\n",
      "          1.6250e+00],\n",
      "        [-5.7072e-02, -5.5414e-01, -1.5935e-01, -2.0263e-01, -8.7386e-01,\n",
      "          5.4804e-01],\n",
      "        [-6.2056e-01, -2.8055e-01,  4.2382e-01,  4.8039e-02, -1.1442e-01,\n",
      "          6.2996e-01]])\n",
      "\n",
      "Final Encoder 0 Output :\n",
      "norm2(norm1(x + self_atten(x)) + feed_fwd_op)\n",
      "\n",
      "tensor([[[-0.0326,  1.7392, -0.2331,  0.7041, -1.2516, -0.9259],\n",
      "         [-0.9507,  0.9642, -1.1234, -0.7091,  0.2984,  1.5206],\n",
      "         [-0.2402, -1.0165, -0.7649, -0.4848,  0.5649,  1.9416],\n",
      "         [ 1.4589, -0.2760, -0.0505,  0.9832, -1.6000, -0.5157],\n",
      "         [ 1.2051, -1.0559, -1.0730,  0.3423, -0.7080,  1.2895],\n",
      "         [-0.1103, -1.3751, -0.6142, -0.4670,  0.9609,  1.6057],\n",
      "         [-0.6938,  0.7205, -0.9127, -0.3655, -0.6489,  1.9004],\n",
      "         [ 0.0479,  0.1756, -1.4313,  1.9098, -0.4611, -0.2410],\n",
      "         [-0.5913,  0.5724, -0.0042,  1.8889, -0.8498, -1.0161],\n",
      "         [-1.2468,  0.3232,  0.5049, -0.4374, -0.9001,  1.7562]],\n",
      "\n",
      "        [[-0.6665,  0.1599, -1.1635,  0.6118, -0.7441,  1.8024],\n",
      "         [ 0.4361, -1.2489, -0.0906, -1.3202,  0.9961,  1.2275],\n",
      "         [-0.2635, -0.3130, -0.3963, -0.4429, -0.7883,  2.2040],\n",
      "         [ 0.8773,  1.2288,  0.1087,  0.4236, -1.4749, -1.1635],\n",
      "         [ 0.1498, -1.1532, -0.9436, -0.3592,  0.4572,  1.8491],\n",
      "         [ 0.8549,  1.0077,  0.7676,  0.0741, -1.3853, -1.3190],\n",
      "         [-0.5318, -0.6230,  0.9102, -0.7098, -0.8535,  1.8078],\n",
      "         [ 0.5117, -1.6991,  0.4789, -0.2073, -0.5812,  1.4970],\n",
      "         [-1.3023,  0.6131, -1.1757, -0.0883,  0.4151,  1.5380],\n",
      "         [ 0.2884,  1.1179, -1.3051,  0.8727, -1.4165,  0.4426]],\n",
      "\n",
      "        [[ 0.6224, -1.8781,  0.0631, -0.5594,  0.5342,  1.2178],\n",
      "         [ 0.0754, -1.2188, -0.8798, -0.3358,  0.5318,  1.8273],\n",
      "         [ 1.1716, -1.2042, -0.6291,  1.5455, -0.4085, -0.4754],\n",
      "         [ 1.1030, -1.7516, -0.2668,  0.5887, -0.6253,  0.9520],\n",
      "         [ 1.7270, -1.3856,  0.5061,  0.1522, -0.8989, -0.1007],\n",
      "         [ 1.2951, -1.7338, -0.5984,  0.2586, -0.1531,  0.9317],\n",
      "         [ 0.3664, -0.4340, -0.0459,  1.9558, -1.1979, -0.6443],\n",
      "         [ 0.9830, -2.0558,  0.5046,  0.0904, -0.2251,  0.7028],\n",
      "         [ 0.4728, -0.7170, -0.8595,  2.0124, -0.2797, -0.6291],\n",
      "         [ 0.5361, -1.6558, -0.6426, -0.3450,  0.7231,  1.3842]],\n",
      "\n",
      "        [[ 0.8343, -1.7325, -0.6701,  0.3055, -0.0625,  1.3252],\n",
      "         [ 0.2021, -1.8091,  0.5980, -0.7961,  0.7215,  1.0836],\n",
      "         [ 0.1598, -1.7972, -0.1352, -0.4417,  0.9083,  1.3061],\n",
      "         [-0.8401, -0.9482, -0.4497,  0.7193, -0.3635,  1.8823],\n",
      "         [ 0.3084, -1.9931,  0.6501,  0.2034, -0.3337,  1.1649],\n",
      "         [ 0.8183, -1.9582, -0.3300,  0.3236,  0.0139,  1.1323],\n",
      "         [-0.2472, -0.7398,  0.4074,  2.0321, -0.8696, -0.5829],\n",
      "         [ 0.1481, -1.9125,  0.7654, -0.6336,  0.8069,  0.8257],\n",
      "         [-0.9226, -1.4322,  1.3245, -0.1581,  0.0411,  1.1474],\n",
      "         [-0.1833, -1.3685, -0.7172, -0.1074,  0.5783,  1.7981]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "\n",
      "Q_dec_1 = \n",
      "tensor([[[-1.3781e+00,  5.1813e-01,  1.3760e+00],\n",
      "         [-4.7169e-01,  7.3859e-01, -1.9685e-02],\n",
      "         [ 1.4304e+00, -2.1024e-02, -9.5361e-01],\n",
      "         [ 1.1283e+00,  3.2774e-01, -3.6110e-01]],\n",
      "\n",
      "        [[ 4.3719e-02, -4.3613e-01,  6.2692e-02],\n",
      "         [ 1.1268e+00,  1.6428e+00, -4.1897e-01],\n",
      "         [-1.1123e-01,  6.1287e-01, -5.6832e-02],\n",
      "         [ 8.3040e-02,  1.1509e+00, -1.4177e-01]],\n",
      "\n",
      "        [[-4.3700e-01,  4.4051e-01, -1.9086e-01],\n",
      "         [ 1.3393e+00, -4.3944e-02, -9.4057e-01],\n",
      "         [ 9.5549e-01,  2.1994e-01, -7.7460e-01],\n",
      "         [ 1.3360e+00, -2.6279e-01, -1.3199e+00]],\n",
      "\n",
      "        [[ 1.2806e+00,  6.9407e-01, -1.9148e-01],\n",
      "         [ 1.1753e-01,  1.6679e-01,  5.2950e-02],\n",
      "         [ 6.3572e-01,  1.2028e+00, -2.0233e-01],\n",
      "         [-1.7304e-01,  4.1972e-01, -8.0005e-02]],\n",
      "\n",
      "        [[ 7.9707e-01,  1.9901e-01, -9.1764e-01],\n",
      "         [ 7.2358e-02,  8.9758e-01, -4.1059e-01],\n",
      "         [ 5.4724e-01,  2.1521e-02,  5.8433e-01],\n",
      "         [ 1.3356e+00, -2.3668e-01, -1.1932e+00]],\n",
      "\n",
      "        [[ 7.6717e-01,  1.2094e+00, -2.4325e-01],\n",
      "         [ 8.4002e-01,  1.2383e+00, -3.2330e-01],\n",
      "         [-5.3835e-01,  5.6694e-01,  1.3831e-02],\n",
      "         [ 1.3155e-01,  8.2879e-01, -1.3288e-01]],\n",
      "\n",
      "        [[-5.4085e-02,  7.2404e-01,  1.2304e+00],\n",
      "         [-8.7326e-01,  6.6265e-01,  1.5799e+00],\n",
      "         [ 9.8242e-01,  4.4145e-01, -9.5808e-02],\n",
      "         [ 8.0717e-02,  2.6890e-01, -8.7322e-01]],\n",
      "\n",
      "        [[-6.6776e-01, -2.7851e-02,  1.0782e-01],\n",
      "         [-4.8866e-01, -8.6226e-01,  2.3445e-01],\n",
      "         [-2.5390e-01,  9.8174e-01, -1.1659e-01],\n",
      "         [ 8.0794e-01,  1.8845e+00, -5.1907e-01]],\n",
      "\n",
      "        [[ 7.4226e-01,  8.3716e-01,  3.9787e-01],\n",
      "         [ 9.3411e-01,  2.9618e-01, -6.8035e-01],\n",
      "         [ 9.2652e-01,  4.3650e-01,  3.8104e-01],\n",
      "         [ 9.8563e-01,  7.5682e-02, -9.8719e-01]],\n",
      "\n",
      "        [[ 1.8515e-01,  1.0094e+00, -7.0447e-02],\n",
      "         [ 6.5000e-01,  1.1852e+00, -1.8936e-01],\n",
      "         [-9.4375e-01, -1.5170e-01,  1.6270e-01],\n",
      "         [-2.5792e-01,  1.0013e+00, -2.4689e-01]],\n",
      "\n",
      "        [[ 1.0946e+00, -1.0728e-01, -1.0887e+00],\n",
      "         [-7.1064e-01,  4.9673e-01,  1.2664e+00],\n",
      "         [ 1.2345e+00,  3.6527e-01, -6.4699e-02],\n",
      "         [ 1.2298e+00,  1.5752e-01, -5.3797e-01]],\n",
      "\n",
      "        [[ 5.2300e-01,  1.0331e+00, -1.8161e-01],\n",
      "         [-7.6066e-01, -1.1733e+00,  2.7189e-01],\n",
      "         [-2.0101e-01,  7.9038e-01, -1.4940e-02],\n",
      "         [-1.1301e-01,  1.0469e+00, -1.3518e-01]],\n",
      "\n",
      "        [[-5.4566e-01,  8.9094e-01, -9.5757e-03],\n",
      "         [ 1.0984e-01,  5.6886e-01, -9.5788e-01],\n",
      "         [-4.0300e-01,  1.3028e-01,  6.5632e-01],\n",
      "         [-3.8267e-01, -2.5232e-01,  1.7327e-02]],\n",
      "\n",
      "        [[ 1.2001e+00,  1.0701e+00, -2.9798e-01],\n",
      "         [ 3.4974e-01,  8.1802e-01, -3.2877e-01],\n",
      "         [-4.2141e-01,  6.5188e-01, -1.5597e-01],\n",
      "         [-4.0409e-01,  8.5074e-01, -2.8191e-01]],\n",
      "\n",
      "        [[-5.2678e-01,  1.3848e-01,  8.5531e-01],\n",
      "         [ 9.3994e-01,  4.3216e-01, -7.5886e-01],\n",
      "         [ 1.2590e+00,  8.6138e-02, -6.0310e-01],\n",
      "         [ 1.3427e+00, -4.3630e-01, -1.3913e+00]],\n",
      "\n",
      "        [[ 3.3431e-01,  9.8680e-01, -1.6508e-01],\n",
      "         [-1.0957e-01,  9.1405e-01, -2.0373e-01],\n",
      "         [-5.6725e-01,  5.5608e-01, -5.7594e-02],\n",
      "         [-3.0797e-01,  3.5977e-01, -7.8190e-02]],\n",
      "\n",
      "        [[-1.1109e+00, -1.6422e-01,  5.8392e-01],\n",
      "         [-4.5939e-01,  1.9167e-01, -4.6421e-01],\n",
      "         [ 3.4275e-02, -1.7647e-01,  5.6213e-01],\n",
      "         [ 4.7220e-01, -3.2326e-01, -1.6572e+00]],\n",
      "\n",
      "        [[-9.4642e-02,  4.1346e-01, -1.9464e-01],\n",
      "         [ 1.3399e+00,  1.1663e+00, -3.3310e-01],\n",
      "         [-2.3476e-01,  8.0617e-01, -1.0134e-01],\n",
      "         [ 1.8459e-03,  8.7695e-01, -3.7521e-01]],\n",
      "\n",
      "        [[-6.3250e-01,  5.8950e-01, -7.3835e-01],\n",
      "         [-9.4174e-01,  1.0354e+00,  1.3541e+00],\n",
      "         [ 1.3364e+00,  2.7064e-02, -7.7053e-01],\n",
      "         [ 9.0991e-01,  6.0667e-02, -9.8917e-01]],\n",
      "\n",
      "        [[ 8.1995e-01,  9.6314e-01, -4.2060e-01],\n",
      "         [ 5.9917e-01,  6.3284e-01, -9.7872e-02],\n",
      "         [ 2.1747e-01,  8.8538e-01, -8.4088e-02],\n",
      "         [ 6.2611e-01,  1.3640e+00, -2.7551e-01]]],\n",
      "       grad_fn=<TransposeBackward0>)\n",
      "\n",
      "K_dec_1 = \n",
      "tensor([[[-0.1119,  1.1423, -0.6369],\n",
      "         [-0.7245,  0.6704,  1.1038],\n",
      "         [-0.2786, -0.8776,  0.5809],\n",
      "         [-0.4944, -0.6482,  0.5847]],\n",
      "\n",
      "        [[ 0.5296,  0.6890, -1.1547],\n",
      "         [-1.3231,  0.3234, -1.0808],\n",
      "         [-0.9167, -0.6925,  1.2226],\n",
      "         [-1.5077, -0.4165,  0.5709]],\n",
      "\n",
      "        [[ 0.1935,  0.5455,  1.0715],\n",
      "         [ 0.2043, -0.7960,  0.6466],\n",
      "         [-0.1727, -0.5471,  1.0751],\n",
      "         [-0.2898, -0.8495,  0.5943]],\n",
      "\n",
      "        [[-0.2175, -0.1550, -0.9511],\n",
      "         [-0.3529, -0.9456,  1.1317],\n",
      "         [-1.2307, -0.5506,  0.2779],\n",
      "         [-0.5842, -0.6240,  1.3850]],\n",
      "\n",
      "        [[-0.1975, -0.4030,  1.1978],\n",
      "         [-0.9606,  0.6643,  1.1422],\n",
      "         [-0.2120, -0.6701, -0.4535],\n",
      "         [-0.1098, -0.9954,  0.7972]],\n",
      "\n",
      "        [[-1.1105, -0.4961,  0.1925],\n",
      "         [-1.0936, -0.0931, -0.0694],\n",
      "         [-1.0707,  0.1159,  0.0551],\n",
      "         [-0.9546, -0.6313,  0.9391]],\n",
      "\n",
      "        [[-0.6974,  0.3815, -0.7662],\n",
      "         [-0.2016,  0.9528, -1.0243],\n",
      "         [-0.8043, -0.4391,  0.2462],\n",
      "         [-0.9807,  0.1138,  1.2303]],\n",
      "\n",
      "        [[-0.5353,  0.2679,  0.1170],\n",
      "         [ 0.5849,  0.4081, -0.4155],\n",
      "         [-1.4600, -0.2132,  0.7059],\n",
      "         [-1.5889,  0.2942, -0.3802]],\n",
      "\n",
      "        [[-0.5508, -0.1283,  0.4033],\n",
      "         [-0.1766, -0.4914,  1.0603],\n",
      "         [-0.6543, -0.2892, -0.5610],\n",
      "         [-1.0432, -0.4435,  0.5275]],\n",
      "\n",
      "        [[-1.4486, -0.3444,  0.1769],\n",
      "         [-1.2332, -0.5640,  0.2495],\n",
      "         [-0.5720, -0.2845,  1.2369],\n",
      "         [-1.2254, -0.1202,  1.0927]],\n",
      "\n",
      "        [[ 0.0117, -0.8124,  1.0395],\n",
      "         [-0.2824,  0.8810, -1.1333],\n",
      "         [-0.3888, -0.7114,  0.2495],\n",
      "         [-0.5374, -0.7785,  0.4849]],\n",
      "\n",
      "        [[-1.0150, -0.6088,  0.4777],\n",
      "         [ 0.8716,  0.3434,  0.0822],\n",
      "         [-1.3241, -0.5058,  0.7682],\n",
      "         [-1.4262, -0.3958,  0.8083]],\n",
      "\n",
      "        [[-0.5320,  0.9631,  1.1052],\n",
      "         [-1.2860,  0.6829,  0.9186],\n",
      "         [-0.7458,  0.1510, -0.4782],\n",
      "         [-0.8528, -0.0223, -0.2709]],\n",
      "\n",
      "        [[-0.7015,  0.0711, -0.8718],\n",
      "         [-0.6398,  0.0439,  0.6273],\n",
      "         [-0.8447,  0.7769, -0.4746],\n",
      "         [-0.8535,  0.9126, -0.3553]],\n",
      "\n",
      "        [[-0.0181, -0.0063, -0.0452],\n",
      "         [-1.0852, -0.1526,  0.6081],\n",
      "         [-0.7317, -0.6881,  0.1389],\n",
      "         [-0.2651, -0.9671,  0.4822]],\n",
      "\n",
      "        [[-0.9781,  0.5698, -1.3623],\n",
      "         [-1.1764, -0.2623,  1.0809],\n",
      "         [-1.0324, -0.3566,  1.3308],\n",
      "         [-0.5238, -0.5532,  1.4369]],\n",
      "\n",
      "        [[-0.3856,  0.4469, -0.4335],\n",
      "         [ 0.0701,  0.3375,  1.2171],\n",
      "         [-0.0963, -0.5330, -0.3324],\n",
      "         [-1.0137, -0.1719,  0.7948]],\n",
      "\n",
      "        [[-0.2103,  1.0533, -1.1310],\n",
      "         [-0.5889,  0.0733, -1.1481],\n",
      "         [-1.0310,  0.4519, -0.6228],\n",
      "         [-0.6402,  0.1386,  0.9020]],\n",
      "\n",
      "        [[-1.1339,  1.1147,  1.0652],\n",
      "         [-0.3995,  1.0426,  0.0677],\n",
      "         [-0.0355, -0.9296,  0.7522],\n",
      "         [-0.2741, -0.6066,  1.1325]],\n",
      "\n",
      "        [[-0.4630,  0.3773, -0.2694],\n",
      "         [-0.6305,  0.4435, -1.3463],\n",
      "         [-1.1227, -0.7079,  0.7363],\n",
      "         [-1.3156, -0.4128,  0.2509]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "V_dec_1 = \n",
      "tensor([[[ 4.6698e-01,  3.2411e-01,  5.3707e-01],\n",
      "         [-4.9679e-01,  1.1741e+00, -5.8056e-02],\n",
      "         [-5.6392e-01, -3.6462e-01, -3.4581e-01],\n",
      "         [-8.8239e-01, -3.4651e-01, -4.1199e-01]],\n",
      "\n",
      "        [[ 6.0436e-01, -1.1584e+00, -6.9683e-01],\n",
      "         [ 4.2574e-01, -2.9254e-01,  1.3367e+00],\n",
      "         [-3.8931e-03,  1.0160e+00,  9.4195e-01],\n",
      "         [ 3.6679e-01,  3.3864e-01,  1.5334e+00]],\n",
      "\n",
      "        [[-4.1654e-01,  1.5464e+00,  2.1587e-01],\n",
      "         [-5.6456e-01,  3.0220e-02, -1.1766e-01],\n",
      "         [-9.1086e-01,  3.8300e-01, -3.5564e-01],\n",
      "         [-2.2370e-01, -2.8270e-01, -3.4010e-01]],\n",
      "\n",
      "        [[ 1.9595e-01, -2.2454e-01,  4.8697e-01],\n",
      "         [ 2.8743e-03,  9.4248e-01,  5.2786e-01],\n",
      "         [ 9.8383e-02,  4.7698e-01,  1.4343e+00],\n",
      "         [-3.4126e-01,  1.3847e+00,  5.6114e-01]],\n",
      "\n",
      "        [[-7.7605e-01,  6.3052e-01, -3.0359e-01],\n",
      "         [-2.2397e-01,  1.1636e+00,  2.4792e-01],\n",
      "         [-6.6492e-01, -1.2360e+00, -5.3717e-01],\n",
      "         [-6.6334e-01, -1.9110e-01, -5.3216e-01]],\n",
      "\n",
      "        [[ 7.0039e-04,  5.8858e-01,  1.3055e+00],\n",
      "         [ 6.5775e-01,  5.4410e-01,  9.3482e-01],\n",
      "         [ 2.4520e-01, -6.1445e-01,  1.0635e+00],\n",
      "         [-3.3116e-01,  9.7593e-01,  1.1016e+00]],\n",
      "\n",
      "        [[ 3.3171e-03, -7.8858e-01,  2.5447e-01],\n",
      "         [ 4.8617e-01, -2.3450e-01,  6.8704e-01],\n",
      "         [-5.9298e-01, -6.2050e-01, -2.9882e-01],\n",
      "         [-3.7268e-01,  7.1614e-01, -4.5919e-01]],\n",
      "\n",
      "        [[ 1.0437e+00, -6.1058e-01,  1.8415e-01],\n",
      "         [ 9.7390e-01, -9.6661e-01, -8.7724e-01],\n",
      "         [ 5.3842e-01,  3.3521e-01,  1.2933e+00],\n",
      "         [-1.6170e-01,  5.1801e-01,  1.5253e+00]],\n",
      "\n",
      "        [[-9.0910e-01, -1.1748e-01, -4.8495e-02],\n",
      "         [-9.3094e-01,  4.1186e-01, -3.0478e-01],\n",
      "         [-7.3867e-02, -1.1109e+00,  1.0264e-01],\n",
      "         [-9.0201e-02, -3.9566e-01, -3.8058e-01]],\n",
      "\n",
      "        [[ 9.9914e-01, -2.2404e-01,  1.4218e+00],\n",
      "         [ 1.9608e-01,  4.1384e-01,  1.4328e+00],\n",
      "         [ 7.8003e-01,  3.4227e-01,  2.3482e-01],\n",
      "         [-6.2222e-02,  1.2068e+00,  9.4981e-01]],\n",
      "\n",
      "        [[-8.2966e-01,  2.3242e-01, -4.8740e-01],\n",
      "         [ 8.0312e-01, -3.8541e-01,  7.3867e-01],\n",
      "         [-8.6767e-01, -6.6373e-01, -3.1514e-01],\n",
      "         [-7.4213e-01, -5.5475e-01, -4.8335e-01]],\n",
      "\n",
      "        [[-2.5802e-01,  7.0168e-01,  1.2673e+00],\n",
      "         [ 8.0181e-01, -4.8679e-01, -1.2660e+00],\n",
      "         [ 5.5042e-01,  2.0538e-01,  1.3229e+00],\n",
      "         [ 1.8790e-01,  5.5495e-01,  1.4050e+00]],\n",
      "\n",
      "        [[-2.5243e-01,  1.5808e+00,  3.5968e-01],\n",
      "         [ 4.9800e-01,  9.0673e-01,  2.8999e-01],\n",
      "         [ 4.4732e-02, -7.6123e-01, -3.3226e-01],\n",
      "         [ 2.5356e-01, -7.3742e-01, -5.7941e-01]],\n",
      "\n",
      "        [[ 6.2665e-01, -9.8842e-02,  7.0524e-01],\n",
      "         [ 2.4698e-01,  1.3640e+00,  2.3080e-01],\n",
      "         [ 1.3944e-01, -5.8866e-01,  5.8509e-01],\n",
      "         [-4.2809e-01, -1.1611e-01,  5.7760e-01]],\n",
      "\n",
      "        [[-6.9050e-01, -2.1413e-01, -4.6959e-01],\n",
      "         [-1.2305e-01, -8.5139e-02, -9.2767e-02],\n",
      "         [-2.6243e-01, -8.3562e-01, -3.1694e-01],\n",
      "         [-1.4855e-01, -4.7247e-01, -4.4089e-01]],\n",
      "\n",
      "        [[ 1.2521e-01, -1.3477e+00,  1.1438e+00],\n",
      "         [ 3.5121e-01,  1.1386e+00,  8.7730e-01],\n",
      "         [ 1.6627e-01,  9.4596e-01,  8.1257e-01],\n",
      "         [-5.2353e-01,  1.4247e+00,  4.9217e-01]],\n",
      "\n",
      "        [[ 3.5586e-01, -2.5437e-01, -2.8441e-01],\n",
      "         [-4.9736e-01,  1.4181e+00, -1.3883e-01],\n",
      "         [-6.2201e-01, -9.6148e-01, -6.9570e-01],\n",
      "         [ 4.9470e-01,  1.8672e-01, -3.6355e-01]],\n",
      "\n",
      "        [[-3.1621e-01, -8.0169e-01,  6.1818e-02],\n",
      "         [-1.8313e-01, -1.9800e-01,  8.8475e-01],\n",
      "         [-1.0855e-01, -9.1750e-01,  1.1144e+00],\n",
      "         [-7.4557e-01,  1.6914e+00,  3.4810e-01]],\n",
      "\n",
      "        [[ 5.6480e-01,  1.4797e+00,  3.4368e-01],\n",
      "         [-3.3058e-01,  6.6681e-01,  3.8623e-01],\n",
      "         [-9.3647e-01, -1.8030e-01, -4.4896e-01],\n",
      "         [-8.2717e-01,  3.4032e-01, -5.0106e-01]],\n",
      "\n",
      "        [[ 1.3007e-01,  8.7192e-01,  1.4827e-01],\n",
      "         [ 1.0915e+00, -1.2909e+00,  5.7202e-01],\n",
      "         [ 3.6091e-02,  5.7514e-01,  1.3209e+00],\n",
      "         [-1.4564e-01,  6.0663e-01,  1.4975e+00]]],\n",
      "       grad_fn=<TransposeBackward0>)\n",
      "\n",
      "Attnetion mask infunction = \n",
      "tensor([[[[0., -inf, -inf, -inf],\n",
      "          [0., 0., -inf, -inf],\n",
      "          [0., 0., 0., -inf],\n",
      "          [0., 0., 0., 0.]]]])\n",
      "\n",
      "Attention output = \n",
      "torch.Size([10, 2, 4, 4]) tensor([[[[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.5135, 0.4865, 0.0000, 0.0000],\n",
      "          [0.5920, 0.1377, 0.2704, 0.0000],\n",
      "          [0.4288, 0.1830, 0.2036, 0.1846]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.8278, 0.1722, 0.0000, 0.0000],\n",
      "          [0.3832, 0.3783, 0.2385, 0.0000],\n",
      "          [0.4171, 0.2975, 0.1279, 0.1575]]],\n",
      "\n",
      "\n",
      "        [[[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.4321, 0.5679, 0.0000, 0.0000],\n",
      "          [0.3655, 0.3750, 0.2595, 0.0000],\n",
      "          [0.2074, 0.3545, 0.1841, 0.2540]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.5054, 0.4946, 0.0000, 0.0000],\n",
      "          [0.5306, 0.2286, 0.2408, 0.0000],\n",
      "          [0.2824, 0.2147, 0.2682, 0.2347]]],\n",
      "\n",
      "\n",
      "        [[[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.3695, 0.6305, 0.0000, 0.0000],\n",
      "          [0.4256, 0.3326, 0.2419, 0.0000],\n",
      "          [0.1606, 0.0801, 0.5138, 0.2455]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.4146, 0.5854, 0.0000, 0.0000],\n",
      "          [0.2996, 0.3393, 0.3611, 0.0000],\n",
      "          [0.2226, 0.2757, 0.3024, 0.1994]]],\n",
      "\n",
      "\n",
      "        [[[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.5663, 0.4337, 0.0000, 0.0000],\n",
      "          [0.3052, 0.4744, 0.2204, 0.0000],\n",
      "          [0.3179, 0.4049, 0.1672, 0.1100]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.6125, 0.3875, 0.0000, 0.0000],\n",
      "          [0.3584, 0.3413, 0.3003, 0.0000],\n",
      "          [0.2295, 0.5288, 0.0740, 0.1677]]],\n",
      "\n",
      "\n",
      "        [[[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.5296, 0.4704, 0.0000, 0.0000],\n",
      "          [0.3308, 0.4261, 0.2431, 0.0000],\n",
      "          [0.2408, 0.2016, 0.3905, 0.1672]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.5193, 0.4807, 0.0000, 0.0000],\n",
      "          [0.3855, 0.3518, 0.2627, 0.0000],\n",
      "          [0.2784, 0.2350, 0.2175, 0.2691]]],\n",
      "\n",
      "\n",
      "        [[[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.7276, 0.2724, 0.0000, 0.0000],\n",
      "          [0.3281, 0.4124, 0.2595, 0.0000],\n",
      "          [0.2157, 0.4009, 0.2093, 0.1740]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.8228, 0.1772, 0.0000, 0.0000],\n",
      "          [0.3004, 0.3740, 0.3256, 0.0000],\n",
      "          [0.2074, 0.3363, 0.2202, 0.2361]]],\n",
      "\n",
      "\n",
      "        [[[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.5092, 0.4908, 0.0000, 0.0000],\n",
      "          [0.3802, 0.4134, 0.2063, 0.0000],\n",
      "          [0.2168, 0.2663, 0.2518, 0.2650]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.5708, 0.4292, 0.0000, 0.0000],\n",
      "          [0.3170, 0.2700, 0.4130, 0.0000],\n",
      "          [0.2187, 0.1666, 0.2998, 0.3149]]],\n",
      "\n",
      "\n",
      "        [[[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.7113, 0.2887, 0.0000, 0.0000],\n",
      "          [0.5253, 0.1912, 0.2835, 0.0000],\n",
      "          [0.3928, 0.1054, 0.2313, 0.2705]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.6713, 0.3287, 0.0000, 0.0000],\n",
      "          [0.4091, 0.3081, 0.2828, 0.0000],\n",
      "          [0.3163, 0.2468, 0.2333, 0.2036]]],\n",
      "\n",
      "\n",
      "        [[[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.6400, 0.3600, 0.0000, 0.0000],\n",
      "          [0.2569, 0.4480, 0.2951, 0.0000],\n",
      "          [0.3691, 0.0879, 0.4353, 0.1078]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.7210, 0.2790, 0.0000, 0.0000],\n",
      "          [0.4020, 0.2684, 0.3296, 0.0000],\n",
      "          [0.3737, 0.2283, 0.2467, 0.1513]]],\n",
      "\n",
      "\n",
      "        [[[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.7725, 0.2275, 0.0000, 0.0000],\n",
      "          [0.1577, 0.4326, 0.4097, 0.0000],\n",
      "          [0.1429, 0.3706, 0.2832, 0.2034]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.4932, 0.5068, 0.0000, 0.0000],\n",
      "          [0.3890, 0.4152, 0.1958, 0.0000],\n",
      "          [0.3539, 0.4165, 0.1011, 0.1285]]]], grad_fn=<SoftmaxBackward0>)\n",
      "Decoder Self Attention = \n",
      "tensor([[ 4.6698e-01,  3.2411e-01,  5.3707e-01,  6.0436e-01, -1.1584e+00,\n",
      "         -6.9683e-01],\n",
      "        [-4.1654e-01,  1.5464e+00,  2.1587e-01,  1.9595e-01, -2.2454e-01,\n",
      "          4.8697e-01],\n",
      "        [-7.7605e-01,  6.3052e-01, -3.0359e-01,  7.0039e-04,  5.8858e-01,\n",
      "          1.3055e+00],\n",
      "        [ 3.3171e-03, -7.8858e-01,  2.5447e-01,  1.0437e+00, -6.1058e-01,\n",
      "          1.8415e-01],\n",
      "        [-9.0910e-01, -1.1748e-01, -4.8495e-02,  9.9914e-01, -2.2404e-01,\n",
      "          1.4218e+00],\n",
      "        [-8.2966e-01,  2.3242e-01, -4.8740e-01, -2.5802e-01,  7.0168e-01,\n",
      "          1.2673e+00],\n",
      "        [-2.5243e-01,  1.5808e+00,  3.5968e-01,  6.2665e-01, -9.8842e-02,\n",
      "          7.0524e-01],\n",
      "        [-6.9050e-01, -2.1413e-01, -4.6959e-01,  1.2521e-01, -1.3477e+00,\n",
      "          1.1438e+00],\n",
      "        [ 3.5586e-01, -2.5437e-01, -2.8441e-01, -3.1621e-01, -8.0169e-01,\n",
      "          6.1818e-02],\n",
      "        [ 5.6480e-01,  1.4797e+00,  3.4368e-01,  1.3007e-01,  8.7192e-01,\n",
      "          1.4827e-01],\n",
      "        [-1.8581e-03,  7.3761e-01,  2.4756e-01,  5.7360e-01, -1.0093e+00,\n",
      "         -3.4661e-01],\n",
      "        [-5.0060e-01,  6.8539e-01,  2.6470e-02,  1.0046e-01,  3.5265e-01,\n",
      "          5.0720e-01],\n",
      "        [-4.2795e-01,  9.6663e-01,  4.4143e-02,  3.8537e-01,  5.6254e-01,\n",
      "          1.0885e+00],\n",
      "        [ 2.1272e-01, -5.4830e-01,  4.4206e-01,  1.0166e+00, -7.4853e-01,\n",
      "         -2.2711e-01],\n",
      "        [-9.1937e-01,  1.3154e-01, -1.6906e-01,  6.1313e-01,  8.2574e-02,\n",
      "          1.4271e+00],\n",
      "        [-3.8486e-01,  6.4115e-02, -1.5340e-01, -7.0252e-02,  4.9112e-01,\n",
      "          8.1845e-01],\n",
      "        [ 1.1591e-01,  1.2500e+00,  3.2547e-01,  4.6368e-01,  5.2906e-01,\n",
      "          5.0159e-01],\n",
      "        [-5.2670e-01, -1.7690e-01, -3.6082e-01,  1.9950e-01, -5.3034e-01,\n",
      "          1.0562e+00],\n",
      "        [ 4.8698e-02,  3.4773e-01, -2.3200e-01, -2.7908e-01, -6.3326e-01,\n",
      "          2.9142e-01],\n",
      "        [ 3.6107e-01,  1.2947e+00,  3.5336e-01,  6.1733e-01, -2.2413e-01,\n",
      "          3.6301e-01],\n",
      "        [ 5.5571e-02,  2.5491e-01,  2.1643e-01,  3.9170e-01, -3.1221e-01,\n",
      "          4.6333e-01],\n",
      "        [-6.0034e-01,  6.7590e-01, -5.7523e-02,  1.2832e-01,  2.1115e-01,\n",
      "          7.2442e-01],\n",
      "        [-5.6557e-01,  3.5633e-01, -1.7668e-01,  3.1194e-01,  1.3905e-01,\n",
      "          1.0923e+00],\n",
      "        [ 1.0101e-01, -4.8867e-01,  3.3778e-01,  8.6812e-01, -4.4805e-01,\n",
      "          1.5502e-01],\n",
      "        [-7.1537e-01, -1.3341e-01, -1.2096e-01,  6.5905e-01,  1.4916e-01,\n",
      "          1.1138e+00],\n",
      "        [-1.6616e-01, -2.5492e-01,  6.2936e-02,  4.0155e-01,  9.5619e-02,\n",
      "          3.3794e-01],\n",
      "        [ 1.1912e-01,  8.1891e-01,  1.8810e-01,  3.2291e-01,  9.3872e-02,\n",
      "          5.2750e-01],\n",
      "        [-4.6063e-01, -3.6565e-01, -3.5426e-01,  2.0646e-01,  6.7045e-02,\n",
      "          9.6802e-01],\n",
      "        [-3.1491e-01,  2.8622e-01, -3.4055e-01, -2.1204e-01, -6.7782e-01,\n",
      "          6.2966e-01],\n",
      "        [-4.3761e-01,  4.4795e-01,  3.7366e-02,  5.1085e-01, -8.4121e-02,\n",
      "          5.5382e-01],\n",
      "        [-1.6841e-01,  2.1565e-01,  7.3186e-02,  4.3600e-01, -3.8688e-01,\n",
      "          4.6908e-01],\n",
      "        [-5.1102e-01,  3.3020e-01, -1.4878e-01,  2.2461e-03,  5.9184e-01,\n",
      "          7.6727e-01],\n",
      "        [-6.4707e-01, -4.8758e-01, -4.3557e-01,  1.8964e-01,  2.8979e-01,\n",
      "          1.0895e+00],\n",
      "        [ 5.7797e-02, -3.7060e-01,  2.5863e-01,  7.6725e-01, -5.3957e-01,\n",
      "         -7.0074e-02],\n",
      "        [-4.5047e-01, -4.4518e-01, -9.6659e-02,  4.7716e-01,  4.3404e-01,\n",
      "          1.0393e+00],\n",
      "        [-1.6772e-01, -3.3990e-01,  4.0958e-02,  3.8169e-01,  1.5808e-01,\n",
      "          4.6009e-01],\n",
      "        [ 1.5635e-01,  1.9711e-01, -8.2012e-02,  8.5177e-02, -7.3400e-03,\n",
      "          5.4996e-01],\n",
      "        [-3.8506e-01, -4.1418e-01, -3.8679e-01,  5.8485e-02,  3.6559e-01,\n",
      "          8.6808e-01],\n",
      "        [-1.2981e-01, -3.6758e-01, -4.5916e-01, -2.9958e-01, -3.1514e-01,\n",
      "          5.5264e-01],\n",
      "        [-4.7523e-01,  4.7666e-01, -3.6820e-02,  4.8562e-01, -9.3003e-02,\n",
      "          6.1667e-01]], grad_fn=<ViewBackward0>)\n",
      "\n",
      "Final Encoder 1 Output :\n",
      "norm2(norm1(x + self_atten(x)) + feed_fwd_op)\n",
      "\n",
      "tensor([[[ 6.6781e-02,  1.7533e+00, -7.9001e-01,  7.9493e-01, -9.1361e-01,\n",
      "          -9.1140e-01],\n",
      "         [-3.1363e-01,  7.9776e-01, -1.2249e+00, -1.1842e+00,  4.5765e-01,\n",
      "           1.4673e+00],\n",
      "         [ 1.9029e-01, -7.1964e-01, -6.0745e-01, -1.2778e+00,  6.9312e-01,\n",
      "           1.7215e+00],\n",
      "         [ 1.7361e+00, -4.8693e-01, -2.1036e-01,  8.2795e-01, -1.3052e+00,\n",
      "          -5.6151e-01],\n",
      "         [ 1.3593e+00, -1.2247e+00, -7.2519e-01, -5.7567e-01, -1.6343e-01,\n",
      "           1.3297e+00],\n",
      "         [ 2.0113e-01, -9.9010e-01, -3.8770e-01, -1.2922e+00,  9.9851e-01,\n",
      "           1.4704e+00],\n",
      "         [ 1.2275e-02,  6.4883e-01, -1.1967e+00, -9.2305e-01, -3.2685e-01,\n",
      "           1.7855e+00],\n",
      "         [-1.9400e-01, -2.7816e-01, -1.2114e+00,  1.9921e+00,  2.9365e-01,\n",
      "          -6.0219e-01],\n",
      "         [-7.6969e-01,  8.4527e-01, -1.1051e-01,  1.7023e+00, -3.9057e-01,\n",
      "          -1.2768e+00],\n",
      "         [-4.3576e-01,  8.2373e-01, -5.0884e-01, -9.1167e-01, -8.0819e-01,\n",
      "           1.8407e+00]],\n",
      "\n",
      "        [[-5.6691e-01,  1.6578e-01, -1.4753e+00,  3.9207e-01, -3.1113e-01,\n",
      "           1.7955e+00],\n",
      "         [ 6.9242e-01, -8.3491e-01, -3.1198e-01, -1.5963e+00,  8.3184e-01,\n",
      "           1.2190e+00],\n",
      "         [ 2.7969e-01, -1.5534e-01, -5.9893e-01, -1.1638e+00, -3.7296e-01,\n",
      "           2.0113e+00],\n",
      "         [ 1.1581e+00,  1.1571e+00, -2.7770e-01,  4.3400e-01, -1.2384e+00,\n",
      "          -1.2331e+00],\n",
      "         [ 5.2621e-01, -1.0385e+00, -6.2155e-01, -1.1186e+00,  6.4153e-01,\n",
      "           1.6110e+00],\n",
      "         [ 1.2389e+00,  1.0738e+00,  6.0566e-01, -7.0711e-01, -1.1236e+00,\n",
      "          -1.0876e+00],\n",
      "         [ 1.3651e-01, -2.7867e-01,  1.3378e-01, -1.3143e+00, -6.2001e-01,\n",
      "           1.9427e+00],\n",
      "         [ 4.8712e-01, -1.6042e+00,  6.7742e-01, -9.2504e-01, -4.4316e-03,\n",
      "           1.3691e+00],\n",
      "         [-1.1811e+00,  7.4686e-01, -1.2010e+00, -4.6451e-01,  7.4649e-01,\n",
      "           1.3533e+00],\n",
      "         [ 5.8713e-01,  1.1572e+00, -1.7049e+00,  2.5571e-01, -9.5394e-01,\n",
      "           6.5878e-01]],\n",
      "\n",
      "        [[ 6.9288e-01, -1.5906e+00, -8.2472e-02, -9.7995e-01,  7.5449e-01,\n",
      "           1.2056e+00],\n",
      "         [ 2.8903e-01, -9.3819e-01, -7.9559e-01, -9.8457e-01,  7.2323e-01,\n",
      "           1.7061e+00],\n",
      "         [ 1.6310e+00, -1.6592e+00, -5.7445e-01,  4.9561e-01,  1.0527e-01,\n",
      "           1.7405e-03],\n",
      "         [ 1.0941e+00, -1.8460e+00, -3.5958e-01,  2.0911e-01, -1.8706e-01,\n",
      "           1.0895e+00],\n",
      "         [ 1.7734e+00, -1.3384e+00,  4.8925e-01, -7.9515e-01, -3.6749e-01,\n",
      "           2.3835e-01],\n",
      "         [ 1.1807e+00, -1.6582e+00, -5.8523e-01, -3.5156e-01,  2.6559e-01,\n",
      "           1.1487e+00],\n",
      "         [ 8.7755e-01, -4.4066e-01, -5.7755e-01,  1.7512e+00, -1.2167e+00,\n",
      "          -3.9384e-01],\n",
      "         [ 9.2657e-01, -1.8357e+00,  6.4012e-01, -7.8964e-01,  2.3097e-01,\n",
      "           8.2766e-01],\n",
      "         [ 2.9487e-01, -1.1750e+00, -8.0758e-01,  1.7959e+00,  5.1549e-01,\n",
      "          -6.2366e-01],\n",
      "         [ 6.2461e-01, -1.3183e+00, -5.9372e-01, -9.4465e-01,  8.5553e-01,\n",
      "           1.3766e+00]],\n",
      "\n",
      "        [[ 7.4514e-01, -1.6433e+00, -6.1553e-01, -3.3400e-01,  4.0091e-01,\n",
      "           1.4468e+00],\n",
      "         [ 4.9096e-01, -1.3095e+00,  3.3559e-01, -1.4249e+00,  7.5267e-01,\n",
      "           1.1552e+00],\n",
      "         [ 3.7861e-01, -1.4527e+00,  8.1534e-02, -1.1715e+00,  9.6923e-01,\n",
      "           1.1948e+00],\n",
      "         [-6.9834e-01, -1.0294e+00, -6.7540e-01,  5.4984e-01, -6.7488e-02,\n",
      "           1.9208e+00],\n",
      "         [ 6.0403e-01, -1.8043e+00,  6.2682e-01, -6.6191e-01, -9.1142e-03,\n",
      "           1.2444e+00],\n",
      "         [ 7.9746e-01, -1.8327e+00, -2.9753e-01, -3.3326e-01,  3.7606e-01,\n",
      "           1.2900e+00],\n",
      "         [-1.5695e-01, -8.7207e-01,  4.0578e-01,  2.0156e+00, -7.9184e-01,\n",
      "          -6.0048e-01],\n",
      "         [ 4.0635e-01, -1.4872e+00,  6.9511e-01, -1.3067e+00,  8.4877e-01,\n",
      "           8.4370e-01],\n",
      "         [-8.7842e-01, -1.2262e+00,  1.3646e+00, -7.5909e-01,  4.6432e-01,\n",
      "           1.0348e+00],\n",
      "         [ 1.1551e-01, -1.1434e+00, -6.8527e-01, -7.8797e-01,  7.7045e-01,\n",
      "           1.7307e+00]]], grad_fn=<AddBackward0>)\n",
      "\n",
      "### Decoder End ###\n"
     ]
    }
   ],
   "source": [
    "final_op = get_all_intermediate_outputs_mask(src_data[:-1, :], state_dict = state_dict1 , num_decoder_layers = num_decoder_layers, d_model=d_model,  d_ff = d_ff, tgt_mask = tgt_mask, max_seq_len = src_data[:-1, :].shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "my_project_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
