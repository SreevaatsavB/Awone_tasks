{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spans computation for a given text\n",
    "\n",
    "    def preprocess_spans(self, tokens, ner, classes_to_id):\n",
    "\n",
    "        max_len = self.base_config.max_len\n",
    "\n",
    "        if len(tokens) > max_len:\n",
    "            length = max_len\n",
    "            tokens = tokens[:max_len]\n",
    "        else:\n",
    "            length = len(tokens)\n",
    "\n",
    "        spans_idx = []\n",
    "        for i in range(length):\n",
    "            spans_idx.extend([(i, i + j) for j in range(self.max_width)])\n",
    "\n",
    "        dict_lab = self.get_dict(ner, classes_to_id) if ner else defaultdict(int)\n",
    "\n",
    "        # 0 for null labels\n",
    "        span_label = torch.LongTensor([dict_lab[i] for i in spans_idx])\n",
    "        spans_idx = torch.LongTensor(spans_idx)\n",
    "\n",
    "        # mask for valid spans\n",
    "        valid_span_mask = spans_idx[:, 1] > length - 1\n",
    "\n",
    "        # mask invalid positions\n",
    "        span_label = span_label.masked_fill(valid_span_mask, -1)\n",
    "\n",
    "        return {\n",
    "        'tokens': tokens,\n",
    "        'span_idx': spans_idx,\n",
    "        'span_label': span_label,\n",
    "        'seq_length': length,\n",
    "        'entities': ner,\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    \n",
    "## Span Representation layer \n",
    " \n",
    "### Functions for span representation\n",
    "\n",
    "    def extract_elements(sequence, indices):\n",
    "        B, L, D = sequence.shape\n",
    "        K = indices.shape[1]\n",
    "\n",
    "        # Expand indices to [B, K, D]\n",
    "        expanded_indices = indices.unsqueeze(2).expand(-1, -1, D)\n",
    "\n",
    "        # Gather the elements\n",
    "        extracted_elements = torch.gather(sequence, 1, expanded_indices)\n",
    "\n",
    "        return extracted_elements\n",
    "    \n",
    "def create_projection_layer(hidden_size: int, dropout: float, out_dim: int = None) -> nn.Sequential:\n",
    "            \"\"\"\n",
    "            Creates a projection layer with specified configurations.\n",
    "            \"\"\"\n",
    "            if out_dim is None:\n",
    "                out_dim = hidden_size\n",
    "\n",
    "            return nn.Sequential(\n",
    "                nn.Linear(hidden_size, out_dim * 4),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout),\n",
    "                nn.Linear(out_dim * 4, out_dim)\n",
    "                )\n",
    "\n",
    "### Span Representation layer (default)\n",
    "    class SpanMarkerV0(nn.Module):\n",
    "    \"\"\"\n",
    "    Marks and projects span endpoints using an MLP.\n",
    "    \"\"\"\n",
    "\n",
    "        def __init__(self, hidden_size: int, max_width: int, dropout: float = 0.4):\n",
    "            super().__init__()\n",
    "            self.max_width = max_width\n",
    "            self.project_start = create_projection_layer(hidden_size, dropout)\n",
    "            self.project_end = create_projection_layer(hidden_size, dropout)\n",
    "\n",
    "            self.out_project = create_projection_layer(hidden_size * 2, dropout, hidden_size)\n",
    "\n",
    "        def forward(self, h: torch.Tensor, span_idx: torch.Tensor) -> torch.Tensor:\n",
    "            B, L, D = h.size()\n",
    "\n",
    "            start_rep = self.project_start(h)\n",
    "            end_rep = self.project_end(h)\n",
    "\n",
    "            start_span_rep = extract_elements(start_rep, span_idx[:, :, 0])\n",
    "            end_span_rep = extract_elements(end_rep, span_idx[:, :, 1])\n",
    "\n",
    "            cat = torch.cat([start_span_rep, end_span_rep], dim=-1).relu()\n",
    "\n",
    "            return self.out_project(cat).view(B, L, self.max_width, D)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from typing import List, Tuple, Dict\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader\n",
    "import random\n",
    "\n",
    "\n",
    "class InstructBase(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.max_width = config['max_width']\n",
    "        self.base_config = config\n",
    "\n",
    "    def get_dict(self, spans, classes_to_id):\n",
    "        dict_tag = defaultdict(int)\n",
    "        for span in spans:\n",
    "            if span[2] in classes_to_id:\n",
    "                dict_tag[(span[0], span[1])] = classes_to_id[span[2]]\n",
    "        return dict_tag\n",
    "\n",
    "    def preprocess_spans(self, tokens, ner, classes_to_id):\n",
    "\n",
    "        max_len = self.base_config['max_len']\n",
    "\n",
    "        if len(tokens) > max_len:\n",
    "            length = max_len\n",
    "            tokens = tokens[:max_len]\n",
    "        else:\n",
    "            length = len(tokens)\n",
    "\n",
    "        spans_idx = []\n",
    "        for i in range(length):\n",
    "            spans_idx.extend([(i, i + j) for j in range(self.max_width)])\n",
    "\n",
    "        dict_lab = self.get_dict(ner, classes_to_id) if ner else defaultdict(int)\n",
    "\n",
    "        # 0 for null labels\n",
    "        span_label = torch.LongTensor([dict_lab[i] for i in spans_idx])\n",
    "        spans_idx = torch.LongTensor(spans_idx)\n",
    "\n",
    "        # mask for valid spans\n",
    "        valid_span_mask = spans_idx[:, 1] > length - 1\n",
    "\n",
    "        # mask invalid positions\n",
    "        span_label = span_label.masked_fill(valid_span_mask, -1)\n",
    "\n",
    "        return {\n",
    "            'tokens': tokens,\n",
    "            'span_idx': spans_idx,\n",
    "            'span_label': span_label,\n",
    "            'seq_length': length,\n",
    "            'entities': ner,\n",
    "        }\n",
    "\n",
    "    def collate_fn(self, batch_list, entity_types=None):\n",
    "        # batch_list: list of dict containing tokens, ner\n",
    "        if entity_types is None:\n",
    "            negs = self.get_negatives(batch_list, 100)\n",
    "            class_to_ids = []\n",
    "            id_to_classes = []\n",
    "            for b in batch_list:\n",
    "                # negs = b[\"negative\"]\n",
    "                random.shuffle(negs)\n",
    "\n",
    "                # negs = negs[:sampled_neg]\n",
    "                max_neg_type_ratio = int(self.base_config.max_neg_type_ratio)\n",
    "\n",
    "                if max_neg_type_ratio == 0:\n",
    "                    # no negatives\n",
    "                    neg_type_ratio = 0\n",
    "                else:\n",
    "                    neg_type_ratio = random.randint(0, max_neg_type_ratio)\n",
    "\n",
    "                if neg_type_ratio == 0:\n",
    "                    # no negatives\n",
    "                    negs_i = []\n",
    "                else:\n",
    "                    negs_i = negs[:len(b['ner']) * neg_type_ratio]\n",
    "\n",
    "                # this is the list of all possible entity types (positive and negative)\n",
    "                types = list(set([el[-1] for el in b['ner']] + negs_i))\n",
    "\n",
    "                # shuffle (every epoch)\n",
    "                random.shuffle(types)\n",
    "\n",
    "                if len(types) != 0:\n",
    "                    # prob of higher number shoul\n",
    "                    # random drop\n",
    "                    if self.base_config.random_drop:\n",
    "                        num_ents = random.randint(1, len(types))\n",
    "                        types = types[:num_ents]\n",
    "\n",
    "                # maximum number of entities types\n",
    "                types = types[:int(self.base_config.max_types)]\n",
    "\n",
    "                # supervised training\n",
    "                if \"label\" in b:\n",
    "                    types = sorted(b[\"label\"])\n",
    "\n",
    "                class_to_id = {k: v for v, k in enumerate(types, start=1)}\n",
    "                id_to_class = {k: v for v, k in class_to_id.items()}\n",
    "                class_to_ids.append(class_to_id)\n",
    "                id_to_classes.append(id_to_class)\n",
    "\n",
    "            batch = [\n",
    "                self.preprocess_spans(b[\"tokenized_text\"], b[\"ner\"], class_to_ids[i]) for i, b in enumerate(batch_list)\n",
    "            ]\n",
    "\n",
    "        else:\n",
    "            class_to_ids = {k: v for v, k in enumerate(entity_types, start=1)}\n",
    "            id_to_classes = {k: v for v, k in class_to_ids.items()}\n",
    "            batch = [\n",
    "                self.preprocess_spans(b[\"tokenized_text\"], b[\"ner\"], class_to_ids) for b in batch_list\n",
    "            ]\n",
    "\n",
    "        span_idx = pad_sequence(\n",
    "            [b['span_idx'] for b in batch], batch_first=True, padding_value=0\n",
    "        )\n",
    "\n",
    "        span_label = pad_sequence(\n",
    "            [el['span_label'] for el in batch], batch_first=True, padding_value=-1\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'seq_length': torch.LongTensor([el['seq_length'] for el in batch]),\n",
    "            'span_idx': span_idx,\n",
    "            'tokens': [el['tokens'] for el in batch],\n",
    "            'span_mask': span_label != -1,\n",
    "            'span_label': span_label,\n",
    "            'entities': [el['entities'] for el in batch],\n",
    "            'classes_to_id': class_to_ids,\n",
    "            'id_to_classes': id_to_classes,\n",
    "        }\n",
    "\n",
    "    @staticmethod\n",
    "    def get_negatives(batch_list, sampled_neg=5):\n",
    "        ent_types = []\n",
    "        for b in batch_list:\n",
    "            types = set([el[-1] for el in b['ner']])\n",
    "            ent_types.extend(list(types))\n",
    "        ent_types = list(set(ent_types))\n",
    "        # sample negatives\n",
    "        random.shuffle(ent_types)\n",
    "        return ent_types[:sampled_neg]\n",
    "\n",
    "    def create_dataloader(self, data, entity_types=None, **kwargs):\n",
    "        return DataLoader(data, collate_fn=lambda x: self.collate_fn(x, entity_types), **kwargs)\n",
    "\n",
    "    def set_sampling_params(self, max_types, shuffle_types, random_drop, max_neg_type_ratio, max_len):\n",
    "        \"\"\"\n",
    "        Sets sampling parameters on the given model.\n",
    "\n",
    "        Parameters:\n",
    "        - model: The model object to update.\n",
    "        - max_types: Maximum types parameter.\n",
    "        - shuffle_types: Boolean indicating whether to shuffle types.\n",
    "        - random_drop: Boolean indicating whether to randomly drop elements.\n",
    "        - max_neg_type_ratio: Maximum negative type ratio.\n",
    "        - max_len: Maximum length parameter.\n",
    "        \"\"\"\n",
    "        self.base_config.max_types = max_types\n",
    "        self.base_config.shuffle_types = shuffle_types\n",
    "        self.base_config.random_drop = random_drop\n",
    "        self.base_config.max_neg_type_ratio = max_neg_type_ratio\n",
    "        self.base_config.max_len = max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "  \"lr_encoder\": \"1e-5\",\n",
    "  \"lr_others\": \"5e-5\",\n",
    "  \"num_steps\": 30000,\n",
    "  \"warmup_ratio\": 0.1,\n",
    "  \"train_batch_size\": 8,\n",
    "  \"eval_every\": 5000,\n",
    "  \"max_width\": 12,\n",
    "  \"model_name\": \"microsoft/deberta-v3-large\",\n",
    "  \"fine_tune\": True,\n",
    "  \"subtoken_pooling\": \"first\",\n",
    "  \"hidden_size\": 768,\n",
    "  \"span_mode\": \"markerV0\",\n",
    "  \"dropout\": 0.4,\n",
    "  \"max_neg_type_ratio\": 3,\n",
    "  \"name\": \"abl\",\n",
    "  \"size_sup\": -1,\n",
    "  \"max_types\": 25,\n",
    "  \"shuffle_types\": True,\n",
    "  \"random_drop\": True,\n",
    "  \"max_len\": 384,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "ib = InstructBase(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "InstructBase()"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "\n",
    "def create_projection_layer(hidden_size: int, dropout: float, out_dim: int = None) -> nn.Sequential:\n",
    "    \"\"\"\n",
    "    Creates a projection layer with specified configurations.\n",
    "    \"\"\"\n",
    "    if out_dim is None:\n",
    "        out_dim = hidden_size\n",
    "\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(hidden_size, out_dim * 4),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(dropout),\n",
    "        nn.Linear(out_dim * 4, out_dim)\n",
    "    )\n",
    "\n",
    "def extract_elements(sequence, indices):\n",
    "    B, L, D = sequence.shape\n",
    "    print(\"B,L,D = \", B, L, D)\n",
    "    K = indices.shape[1]\n",
    "\n",
    "    # Expand indices to [B, K, D]\n",
    "    expanded_indices = indices.unsqueeze(2).expand(-1, -1, D)\n",
    "\n",
    "    # Gather the elements\n",
    "    extracted_elements = torch.gather(sequence, 1, expanded_indices)\n",
    "\n",
    "    return extracted_elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpanMarkerV0(nn.Module):\n",
    "    \"\"\"\n",
    "    Marks and projects span endpoints using an MLP.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, hidden_size: int, max_width: int, dropout: float = 0.4):\n",
    "        super().__init__()\n",
    "        self.max_width = max_width\n",
    "        self.project_start = create_projection_layer(hidden_size, dropout)\n",
    "        self.project_end = create_projection_layer(hidden_size, dropout)\n",
    "\n",
    "        self.out_project = create_projection_layer(hidden_size * 2, dropout, hidden_size)\n",
    "\n",
    "    def forward(self, h: torch.Tensor, span_idx: torch.Tensor) -> torch.Tensor:\n",
    "        B, L, D = h.size()\n",
    "\n",
    "        start_rep = self.project_start(h)\n",
    "        end_rep = self.project_end(h)\n",
    "\n",
    "        start_span_rep = extract_elements(start_rep, span_idx[:, :, 0])\n",
    "        end_span_rep = extract_elements(end_rep, span_idx[:, :, 1])\n",
    "\n",
    "        print(start_rep.shape)\n",
    "        print(end_rep.shape)\n",
    "\n",
    "        print()\n",
    "\n",
    "        print(span_idx[:, :, 0])\n",
    "        print(span_idx[:, :, 1])\n",
    "\n",
    "        print()\n",
    "\n",
    "        print(start_span_rep.shape)\n",
    "        print(end_span_rep.shape)\n",
    "\n",
    "\n",
    "\n",
    "        cat = torch.cat([start_span_rep, end_span_rep], dim=-1).relu()\n",
    "\n",
    "        print(cat.shape)\n",
    "\n",
    "        return self.out_project(cat)\n",
    "\n",
    "        return self.out_project(cat).view(B, L, self.max_width, D)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B,L,D =  1 6 10\n",
      "B,L,D =  1 6 10\n",
      "torch.Size([1, 6, 10])\n",
      "torch.Size([1, 6, 10])\n",
      "\n",
      "tensor([[0, 0, 0, 1, 1, 1, 2, 2, 2, 3, 3, 3, 4, 4, 5]])\n",
      "tensor([[0, 1, 2, 1, 2, 3, 2, 3, 4, 3, 4, 5, 4, 5, 5]])\n",
      "\n",
      "torch.Size([1, 15, 10])\n",
      "torch.Size([1, 15, 10])\n",
      "torch.Size([1, 15, 20])\n",
      "###############\n",
      "torch.Size([1, 15, 10])\n",
      "tensor([[[-3.0157e-01, -4.7236e-02, -2.1270e-01,  1.3546e-01,  8.2573e-03,\n",
      "          -4.8060e-02,  1.6734e-01,  1.5996e-01, -2.6834e-02,  3.7026e-01],\n",
      "         [-2.6844e-01, -4.5104e-02, -3.4430e-01, -5.4507e-02,  4.8877e-02,\n",
      "          -8.3942e-02,  2.0335e-01,  1.9588e-01,  1.1190e-03,  3.3777e-01],\n",
      "         [-2.3895e-01,  4.7283e-03, -1.2314e-01,  1.1788e-01,  1.6451e-02,\n",
      "           3.1235e-02,  2.7326e-01,  1.2244e-01, -4.0133e-02,  2.9034e-01],\n",
      "         [-1.2960e-01, -1.5413e-01, -2.6885e-01, -9.8466e-02,  2.3240e-02,\n",
      "          -2.3104e-02,  1.5348e-01,  1.4566e-01,  5.1083e-02,  2.8492e-01],\n",
      "         [-1.7766e-01, -1.7208e-02, -1.9379e-01, -2.3842e-02,  2.7209e-02,\n",
      "          -1.0550e-01,  2.2670e-01,  1.1091e-01,  1.3055e-04,  2.7870e-01],\n",
      "         [-1.0628e-01,  1.1027e-01, -1.6684e-01,  7.7742e-03,  8.4559e-02,\n",
      "          -1.2864e-01,  3.5703e-01,  7.1769e-02, -3.6264e-02,  3.2403e-01],\n",
      "         [-4.9117e-02,  1.3156e-02, -1.3086e-01,  3.0184e-03, -1.2807e-01,\n",
      "          -8.4130e-02,  2.8061e-01,  1.2512e-01, -1.7849e-02,  2.4567e-01],\n",
      "         [-2.5928e-02, -9.7324e-03, -1.5935e-01,  1.1899e-02, -2.2654e-02,\n",
      "          -9.3030e-02,  1.9113e-01,  1.4522e-01,  7.0525e-02,  2.8610e-01],\n",
      "         [-1.1952e-01, -1.5992e-02, -1.1576e-01, -2.1405e-02, -6.7021e-02,\n",
      "          -8.3768e-02,  1.2123e-01,  4.2976e-02,  3.8130e-02,  1.8663e-01],\n",
      "         [-5.0223e-02, -5.3334e-03, -1.0266e-01, -6.3217e-02, -3.3279e-02,\n",
      "          -1.2166e-01,  1.5243e-01,  6.9959e-02, -1.2434e-01,  1.6179e-01],\n",
      "         [-1.3210e-02, -6.3620e-02, -3.8500e-02,  5.2647e-02, -2.6561e-01,\n",
      "          -1.5702e-01,  2.5903e-01, -8.9341e-03, -4.4549e-02,  2.0245e-01],\n",
      "         [-1.7827e-01, -3.5490e-02, -1.0132e-01,  2.1031e-01, -2.6636e-01,\n",
      "          -9.8618e-02,  2.1400e-01,  5.4484e-02,  6.4513e-02,  4.1391e-01],\n",
      "         [-1.0851e-01, -5.2660e-02, -4.8586e-02,  9.6431e-02, -4.8757e-02,\n",
      "          -9.6577e-02,  1.1963e-01,  1.1696e-02, -5.4953e-02,  1.9743e-01],\n",
      "         [-4.8735e-02, -3.0142e-02, -1.3586e-01, -4.0961e-02, -1.1014e-01,\n",
      "          -1.1868e-01,  1.3343e-01,  2.2206e-02,  5.6243e-02,  2.0808e-01],\n",
      "         [-1.8342e-01, -8.0658e-02, -1.3846e-01,  8.8036e-02, -1.8041e-03,\n",
      "          -6.6929e-02,  1.7731e-01,  9.5611e-02, -1.3330e-03,  2.5644e-01]]],\n",
      "       grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Given inputs\n",
    "word_rep = torch.randn(1, 6, 10)  # Assuming 6 tokens with each token represented by a 10-dimensional vector\n",
    "span_idx = torch.LongTensor([[[0, 0],\n",
    "                              [0, 1],\n",
    "                              [0, 2],\n",
    "                              [1, 1],\n",
    "                              [1, 2],\n",
    "                              [1, 3],\n",
    "                              [2, 2],\n",
    "                              [2, 3],\n",
    "                              [2, 4],\n",
    "                              [3, 3],\n",
    "                              [3, 4],\n",
    "                              [3, 5],\n",
    "                              [4, 4],\n",
    "                              [4, 5],\n",
    "                              [5, 5]]])\n",
    "\n",
    "# Create an instance of SpanMarkerV0\n",
    "span_marker = SpanMarkerV0(hidden_size=10, max_width=3, dropout=0.4)\n",
    "\n",
    "# Compute span_rep\n",
    "span_rep = span_marker.forward(word_rep, span_idx)\n",
    "\n",
    "print(\"###############\")\n",
    "print(span_rep.shape)  # Output shape\n",
    "print(span_rep)         # Output tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "my_project_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
