{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mixtral 8X7B \n",
    "\n",
    "Tiny Mixtral is used for checking the computations (Small model, same architercture)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.trainer_utils import set_seed\n",
    "import torch\n",
    "SEED = 6\n",
    "set_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MixtralConfig {\n",
       "  \"_name_or_path\": \"NickyNicky/LocutusqueXFelladrin-TinyMistral248M-Instruct_oasst2_chatML_V4\",\n",
       "  \"architectures\": [\n",
       "    \"MixtralForCausalLM\"\n",
       "  ],\n",
       "  \"attention_dropout\": 0.0,\n",
       "  \"bos_token_id\": 1,\n",
       "  \"eos_token_id\": 2,\n",
       "  \"hidden_act\": \"silu\",\n",
       "  \"hidden_size\": 8,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 8,\n",
       "  \"max_position_embeddings\": 32768,\n",
       "  \"model_type\": \"mixtral\",\n",
       "  \"num_attention_heads\": 4,\n",
       "  \"num_experts_per_tok\": 2,\n",
       "  \"num_hidden_layers\": 1,\n",
       "  \"num_key_value_heads\": 2,\n",
       "  \"num_local_experts\": 4,\n",
       "  \"output_hidden_states\": true,\n",
       "  \"output_router_logits\": false,\n",
       "  \"pretraining_tp\": 1,\n",
       "  \"rms_norm_eps\": 1e-06,\n",
       "  \"rope_theta\": 10000.0,\n",
       "  \"router_aux_loss_coef\": 0.001,\n",
       "  \"sliding_window\": 3,\n",
       "  \"tie_word_embeddings\": false,\n",
       "  \"torch_dtype\": \"bfloat16\",\n",
       "  \"transformers_version\": \"4.38.0.dev0\",\n",
       "  \"use_cache\": false,\n",
       "  \"vocab_size\": 32005\n",
       "}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoConfig, MixtralConfig\n",
    "\n",
    "\n",
    "mixtral_config = MixtralConfig.from_pretrained(\"NickyNicky/Mixtral-TinyMistral-8x248M-Instruct_oasst2_chatML_Intel_orca_dpo_pairs_DPO_V1\", num_hidden_layers = 1, use_cache = False, hidden_size = 8, num_attention_heads = 4, \n",
    "                                           output_hidden_states=True,  num_key_value_heads = 2, past_key_values = True, intermediate_size = 8, sliding_window = 3, dropout_p = 0, \n",
    "                                           \n",
    "                                           num_local_experts = 4, num_experts_per_tok = 2)\n",
    "\n",
    "mixtral_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel\n",
    "\n",
    "tinymixtral = AutoModel.from_config(mixtral_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "src_sent = \"hi how are you doing\"\n",
    "\n",
    "mistal_tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-v0.1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    1, 12014,   910,   460,   368,  2548]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_src_dict = mistal_tokenizer.encode_plus(src_sent, return_tensors='pt')\n",
    "tokenized_src_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    1, 12014,   910,   460,   368,  2548]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_src_dict = mistal_tokenizer.encode_plus(src_sent, return_tensors='pt')\n",
    "tokenized_src_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    1, 12014,   910,   460,   368,  2548]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_tokenized = tokenized_src_dict[\"input_ids\"]\n",
    "src_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s> hi how are you doing'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mistal_tokenizer.decode(*src_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 0., 0., 0.],\n",
       "        [0., 1., 1., 1., 0., 0.],\n",
       "        [0., 0., 1., 1., 1., 0.],\n",
       "        [0., 0., 0., 1., 1., 1.]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_length = src_tokenized.shape[1]\n",
    "sliding_window_len = 3\n",
    "\n",
    "sliding_window_mask = 1 - (torch.triu(torch.ones(seq_length, seq_length), diagonal=1))\n",
    "\n",
    "\n",
    "for i in range(sliding_window_mask.shape[0]-1, -1, -1):\n",
    "\n",
    "    li = i - sliding_window_len + 1\n",
    "\n",
    "\n",
    "    if li > 0:\n",
    "\n",
    "        sliding_window_mask[i][0:li] = 0\n",
    "\n",
    "sliding_window_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 1, 6, 6]),\n",
       " tensor([[[[1., 0., 0., 0., 0., 0.],\n",
       "           [1., 1., 0., 0., 0., 0.],\n",
       "           [1., 1., 1., 0., 0., 0.],\n",
       "           [0., 1., 1., 1., 0., 0.],\n",
       "           [0., 0., 1., 1., 1., 0.],\n",
       "           [0., 0., 0., 1., 1., 1.]]]]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sliding_window_mask = sliding_window_mask.unsqueeze(0)\n",
    "sliding_window_mask = sliding_window_mask.unsqueeze(0)\n",
    "sliding_window_mask.shape, sliding_window_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'attention_mask': tensor([[[[1., 0., 0., 0., 0., 0.],\n",
      "          [1., 1., 0., 0., 0., 0.],\n",
      "          [1., 1., 1., 0., 0., 0.],\n",
      "          [0., 1., 1., 1., 0., 0.],\n",
      "          [0., 0., 1., 1., 1., 0.],\n",
      "          [0., 0., 0., 1., 1., 1.]]]]),\n",
      " 'input_ids': tensor([[    1, 12014,   910,   460,   368,  2548]])}\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint \n",
    "\n",
    "tokenized_src_dict[\"attention_mask\"] = sliding_window_mask\n",
    "\n",
    "pprint(tokenized_src_dict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **SLIDING WINDOW ATTENTION** and **GROUPED-QUERY ATTENTION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_rep =  2\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters of the model we loaded\n",
    "\n",
    "# Hidden size\n",
    "embed_dim = 8\n",
    "\n",
    "num_heads = 4\n",
    "\n",
    "# For grouped query attention \n",
    "# No.of query heds \n",
    "n_heads_q = num_heads\n",
    "\n",
    "# No.of key and values heads \n",
    "n_kv_heads = 2\n",
    "\n",
    "# Sequence length of the input passed \n",
    "seq_len = 6\n",
    "\n",
    "# Dimension per head\n",
    "head_dim = embed_dim // num_heads\n",
    "\n",
    "# No.of time the K and V matrices needs to be repeated\n",
    "n_rep = n_heads_q//n_kv_heads\n",
    "print(\"n_rep = \", n_rep)\n",
    "\n",
    "\n",
    "# MOE hyperparameters \n",
    "num_local_experts = 4\n",
    "num_experts_per_tok = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict = tinymixtral.state_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function for getting the word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def look_up_table(sentence, vocab_embeds, embedding):\n",
    "\n",
    "    for i in range(sentence.size(0)):\n",
    "        for j in range(sentence.size(1)):\n",
    "            \n",
    "            # Get the index for the current word token index in the sequence\n",
    "            word_index = sentence[i, j].item()\n",
    "\n",
    "            if word_index < 0 or word_index >= vocab_embeds.size(0):\n",
    "                raise ValueError(f\"Invalid word index: {word_index}\")\n",
    "\n",
    "            # Lookup the corresponding embedding vector for the word\n",
    "            embedding[i, j, :] = vocab_embeds[word_index, :]\n",
    "\n",
    "            print(f\"Word index: {word_index}, Embedding: {vocab_embeds[word_index, :]}\")\n",
    "    print()\n",
    "\n",
    "    return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source sentence embedding\n",
      "Word index: 1, Embedding: tensor([-0.0205,  0.0033, -0.0227,  0.0090, -0.0157, -0.0196,  0.0259,  0.0032])\n",
      "Word index: 12014, Embedding: tensor([ 0.0156,  0.0015,  0.0287, -0.0089,  0.0089,  0.0155, -0.0087, -0.0101])\n",
      "Word index: 910, Embedding: tensor([-0.0133, -0.0164,  0.0127, -0.0108,  0.0175, -0.0043, -0.0097,  0.0199])\n",
      "Word index: 460, Embedding: tensor([ 0.0042, -0.0034,  0.0021, -0.0117, -0.0083,  0.0120,  0.0334,  0.0175])\n",
      "Word index: 368, Embedding: tensor([-0.0078,  0.0300, -0.0113,  0.0074,  0.0170,  0.0120, -0.0012, -0.0383])\n",
      "Word index: 2548, Embedding: tensor([-0.0140,  0.0294, -0.0047,  0.0060,  0.0086, -0.0286, -0.0195,  0.0427])\n",
      "\n",
      "torch.Size([1, 6, 8])\n",
      "Source embeddings : \n",
      "\n",
      "tensor([[[-0.0205,  0.0033, -0.0227,  0.0090, -0.0157, -0.0196,  0.0259,\n",
      "           0.0032],\n",
      "         [ 0.0156,  0.0015,  0.0287, -0.0089,  0.0089,  0.0155, -0.0087,\n",
      "          -0.0101],\n",
      "         [-0.0133, -0.0164,  0.0127, -0.0108,  0.0175, -0.0043, -0.0097,\n",
      "           0.0199],\n",
      "         [ 0.0042, -0.0034,  0.0021, -0.0117, -0.0083,  0.0120,  0.0334,\n",
      "           0.0175],\n",
      "         [-0.0078,  0.0300, -0.0113,  0.0074,  0.0170,  0.0120, -0.0012,\n",
      "          -0.0383],\n",
      "         [-0.0140,  0.0294, -0.0047,  0.0060,  0.0086, -0.0286, -0.0195,\n",
      "           0.0427]]])\n"
     ]
    }
   ],
   "source": [
    "def get_embedding_outputs(src_tokens, state_dict, d_model):\n",
    "\n",
    "    src_vocab_embeds = state_dict[\"embed_tokens.weight\"]\n",
    "\n",
    "    src_embedding = torch.zeros(src_tokens.size(0), src_tokens.size(1), d_model)\n",
    "    print(\"Source sentence embedding\")\n",
    "    src_embedding =  look_up_table(src_tokens, src_vocab_embeds, src_embedding)\n",
    "    print(src_embedding.shape)\n",
    "\n",
    "\n",
    "    print(\"Source embeddings : \\n\")\n",
    "    print(src_embedding)\n",
    "\n",
    "    return src_embedding\n",
    "\n",
    "\n",
    "input_embeddings = get_embedding_outputs(src_tokenized, state_dict, d_model = embed_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "residual = input_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-normlization in the transformer block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_layernorm(hidden_states, wt, variance_epsilon = 1e-06):\n",
    "\n",
    "    dtype =  hidden_states.dtype\n",
    "    hidden_states = hidden_states.to(torch.float32)\n",
    "    variance = hidden_states.pow(2).mean(-1, keepdim=True)\n",
    "    hidden_states = hidden_states * torch.rsqrt(variance + variance_epsilon)\n",
    "    op = wt * hidden_states\n",
    "    return op.to(dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.1963,  0.1901, -1.3278,  0.5259, -0.9184, -1.1431,  1.5101,\n",
       "           0.1872],\n",
       "         [ 1.0852,  0.1033,  1.9964, -0.6169,  0.6195,  1.0772, -0.6076,\n",
       "          -0.7026],\n",
       "         [-0.9566, -1.1778,  0.9121, -0.7751,  1.2597, -0.3058, -0.6990,\n",
       "           1.4334],\n",
       "         [ 0.2787, -0.2234,  0.1394, -0.7768, -0.5505,  0.7990,  2.2188,\n",
       "           1.1619],\n",
       "         [-0.4003,  1.5348, -0.5801,  0.3767,  0.8682,  0.6159, -0.0632,\n",
       "          -1.9615],\n",
       "         [-0.6097,  1.2791, -0.2058,  0.2616,  0.3740, -1.2438, -0.8486,\n",
       "           1.8599]]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_state = apply_layernorm(input_embeddings, state_dict[\"layers.0.input_layernorm.weight\"], variance_epsilon = 1e-06)\n",
    "\n",
    "hidden_state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['embed_tokens.weight', 'layers.0.self_attn.q_proj.weight', 'layers.0.self_attn.k_proj.weight', 'layers.0.self_attn.v_proj.weight', 'layers.0.self_attn.o_proj.weight', 'layers.0.block_sparse_moe.gate.weight', 'layers.0.block_sparse_moe.experts.0.w1.weight', 'layers.0.block_sparse_moe.experts.0.w2.weight', 'layers.0.block_sparse_moe.experts.0.w3.weight', 'layers.0.block_sparse_moe.experts.1.w1.weight', 'layers.0.block_sparse_moe.experts.1.w2.weight', 'layers.0.block_sparse_moe.experts.1.w3.weight', 'layers.0.block_sparse_moe.experts.2.w1.weight', 'layers.0.block_sparse_moe.experts.2.w2.weight', 'layers.0.block_sparse_moe.experts.2.w3.weight', 'layers.0.block_sparse_moe.experts.3.w1.weight', 'layers.0.block_sparse_moe.experts.3.w2.weight', 'layers.0.block_sparse_moe.experts.3.w3.weight', 'layers.0.input_layernorm.weight', 'layers.0.post_attention_layernorm.weight', 'norm.weight'])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_dict.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to get the Q, K and V vectors from input embeddings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_qkv(hidden_state ,Wq, Wk, Wv):\n",
    "\n",
    "\n",
    "    q_matmul = hidden_state@Wq.T\n",
    "    k_matmul = hidden_state@Wk.T\n",
    "    v_matmul = hidden_state@Wv.T\n",
    "\n",
    "    return q_matmul, k_matmul, v_matmul\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "Wq = state_dict[\"layers.0.self_attn.q_proj.weight\"]\n",
    "Wk = state_dict[\"layers.0.self_attn.k_proj.weight\"]\n",
    "Wv = state_dict[\"layers.0.self_attn.v_proj.weight\"]\n",
    "query, key, value = get_qkv(hidden_state ,Wq, Wk, Wv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[-2.4317e-02,  3.0005e-03,  1.0064e-02, -1.0727e-01, -3.9087e-02,\n",
       "            1.3240e-03, -5.9916e-02,  8.2977e-02],\n",
       "          [-1.2701e-02,  6.1144e-03, -1.4088e-02,  9.4415e-02,  8.5527e-02,\n",
       "           -4.3866e-02,  4.6483e-02, -3.9903e-02],\n",
       "          [ 9.6255e-03,  1.2447e-01,  6.6383e-02,  3.3577e-03, -2.5601e-02,\n",
       "            1.2348e-04,  9.0148e-02, -6.5317e-02],\n",
       "          [ 3.0088e-02,  6.6576e-02,  8.3207e-03, -6.3840e-02, -1.6522e-02,\n",
       "           -8.4684e-02,  1.7307e-02,  2.8523e-02],\n",
       "          [-4.5867e-02, -2.2584e-02, -6.2673e-02,  5.3766e-02,  1.0826e-01,\n",
       "           -2.7717e-02, -5.8768e-02,  9.9162e-02],\n",
       "          [ 1.8099e-02,  6.8153e-02,  6.8070e-02,  4.5351e-02, -4.9041e-02,\n",
       "           -3.6117e-02, -4.7914e-02, -8.1207e-02]]]),\n",
       " tensor([[[ 0.0002, -0.0163, -0.0740,  0.1318],\n",
       "          [ 0.0107,  0.0231,  0.1412, -0.1121],\n",
       "          [-0.0582, -0.0852, -0.0010, -0.0617],\n",
       "          [ 0.0354, -0.0031,  0.0278,  0.0185],\n",
       "          [ 0.0854,  0.0305,  0.0678, -0.0290],\n",
       "          [-0.0050, -0.0115,  0.0315,  0.0075]]]),\n",
       " tensor([[[ 0.0112, -0.0299,  0.0317, -0.0445],\n",
       "          [ 0.0384,  0.0472, -0.0398,  0.0336],\n",
       "          [-0.0358,  0.0217,  0.0193,  0.0889],\n",
       "          [ 0.0085,  0.1030, -0.0138,  0.0682],\n",
       "          [-0.0063, -0.1437, -0.0108, -0.0527],\n",
       "          [ 0.0063, -0.0075,  0.0422,  0.0699]]]))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query, key, value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rotary postional embeddings fucntions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sin_cos(dim, seq_len, max_seq_len, base = 10000):\n",
    "\n",
    "    inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2, dtype=torch.int64).float() / dim))\n",
    "\n",
    "    t = torch.arange(max_seq_len, dtype=torch.int64).type_as(inv_freq)\n",
    "\n",
    "    freqs = torch.outer(t, inv_freq)\n",
    "    \n",
    "    # Uses a different permutation in order to obtain the same calculation\n",
    "    emb = torch.cat((freqs, freqs), dim=-1)    \n",
    "\n",
    "    return  emb.cos()[:seq_len], emb.sin()[:seq_len]\n",
    "\n",
    "\n",
    "def rotate_half(x):\n",
    "\n",
    "    x1 = x[..., : x.shape[-1] // 2]\n",
    "    x2 = x[..., x.shape[-1] // 2 :]\n",
    "\n",
    "    # print(x1.shape, x2.shape)\n",
    "    # x1 = x[ : , : x.shape[-1] // 2]\n",
    "    # x2 = x[ : , x.shape[-1] // 2 :]\n",
    "\n",
    "    return torch.cat((-x2, x1), dim=-1)\n",
    "\n",
    "def apply_rotary_pos_emb(q, k, cos, sin, unsqueeze_dim=1):\n",
    "\n",
    "    cos = cos.unsqueeze(unsqueeze_dim)\n",
    "    sin = sin.unsqueeze(unsqueeze_dim)\n",
    "\n",
    "    # print(\"HALF ROT SHAPES = \")\n",
    "    # print((rotate_half(q).shape,   sin.shape))\n",
    "    # print((rotate_half(k).shape , sin.shape))\n",
    "\n",
    "    q_embed = (q * cos) + (rotate_half(q) * sin)\n",
    "    k_embed = (k * cos) + (rotate_half(k) * sin)\n",
    "\n",
    "    return q_embed, k_embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 6, 8]), torch.Size([1, 6, 4]), torch.Size([1, 6, 4]))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query.shape, key.shape, value.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "bsz, q_len, _ = query.shape\n",
    "\n",
    "query = query.view(bsz, q_len, n_heads_q, head_dim).transpose(1, 2)\n",
    "key = key.view(bsz, q_len, n_kv_heads, head_dim).transpose(1, 2)\n",
    "value = value.view(bsz, q_len, n_kv_heads, head_dim).transpose(1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 4, 6, 2]), torch.Size([1, 2, 6, 2]), torch.Size([1, 2, 6, 2]))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query.shape, key.shape, value.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_Rope(query, key, head_dim, seq_len,  num_heads):\n",
    "\n",
    "    cos, sin = get_sin_cos(dim = head_dim, seq_len = seq_len, max_seq_len = 2048, base = 10000)\n",
    "\n",
    "    cos = cos.unsqueeze(0)\n",
    "    sin = sin.unsqueeze(0)\n",
    "\n",
    "    q_rotated, k_rotated = apply_rotary_pos_emb(query, key, cos, sin, unsqueeze_dim=1)\n",
    "\n",
    "    return q_rotated, k_rotated\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rotating the query and key before self attention\n",
    "\n",
    "query_rotated, key_rotated = get_Rope(query, key, head_dim, seq_len,  num_heads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[[-0.0243,  0.0030],\n",
       "           [-0.0120, -0.0074],\n",
       "           [-0.1172, -0.0430],\n",
       "           [-0.0392, -0.0617],\n",
       "           [ 0.0129,  0.0495],\n",
       "           [ 0.0705,  0.0020]],\n",
       " \n",
       "          [[ 0.0101, -0.1073],\n",
       "           [-0.0871,  0.0392],\n",
       "           [-0.0307,  0.0590],\n",
       "           [ 0.0008,  0.0644],\n",
       "           [ 0.0817,  0.0123],\n",
       "           [ 0.0628, -0.0524]],\n",
       " \n",
       "          [[-0.0391,  0.0013],\n",
       "           [ 0.0831,  0.0483],\n",
       "           [ 0.0105, -0.0233],\n",
       "           [ 0.0283,  0.0815],\n",
       "           [-0.0917, -0.0638],\n",
       "           [-0.0485,  0.0368]],\n",
       " \n",
       "          [[-0.0599,  0.0830],\n",
       "           [ 0.0587,  0.0176],\n",
       "           [ 0.0219,  0.1092],\n",
       "           [-0.0212, -0.0258],\n",
       "           [ 0.1135, -0.0203],\n",
       "           [-0.0915,  0.0229]]]]),\n",
       " tensor([[[[ 0.0002, -0.0163],\n",
       "           [-0.0137,  0.0215],\n",
       "           [ 0.1017, -0.0174],\n",
       "           [-0.0346,  0.0080],\n",
       "           [-0.0327, -0.0845],\n",
       "           [-0.0124,  0.0015]],\n",
       " \n",
       "          [[-0.0740,  0.1318],\n",
       "           [ 0.1706,  0.0582],\n",
       "           [ 0.0566,  0.0248],\n",
       "           [-0.0302, -0.0144],\n",
       "           [-0.0662, -0.0323],\n",
       "           [ 0.0162, -0.0281]]]]))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_rotated, key_rotated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.0112, -0.0299],\n",
       "          [ 0.0384,  0.0472],\n",
       "          [-0.0358,  0.0217],\n",
       "          [ 0.0085,  0.1030],\n",
       "          [-0.0063, -0.1437],\n",
       "          [ 0.0063, -0.0075]],\n",
       "\n",
       "         [[ 0.0317, -0.0445],\n",
       "          [-0.0398,  0.0336],\n",
       "          [ 0.0193,  0.0889],\n",
       "          [-0.0138,  0.0682],\n",
       "          [-0.0108, -0.0527],\n",
       "          [ 0.0422,  0.0699]]]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fucntion to repeat the K and V matrices for GQA (Grouped Query Attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def repeat_kv(x, n_rep):\n",
    "\n",
    "    bsz, num_key_value_heads, seq_len, head_dim = x.shape\n",
    "\n",
    "    if n_rep == 1:\n",
    "        return x\n",
    "    \n",
    "    x = x[:, :, None, :, :].expand(bsz, num_key_value_heads, n_rep, seq_len, head_dim)\n",
    "    return x.reshape(bsz, num_key_value_heads * n_rep, seq_len, head_dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_rotated = repeat_kv(key_rotated, n_rep)\n",
    "value = repeat_kv(value, n_rep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 4, 6, 2]), torch.Size([1, 4, 6, 2]), torch.Size([1, 4, 6, 2]))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query.shape, key_rotated.shape, value.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Self attention (GQA + Sliding window attention)\n",
    "\n",
    "#### The mask is passed as required by the sliding window attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def self_attention_rope(query, key, value, attn_mask = None, scale = None, is_causal=False):\n",
    "\n",
    "    L, S = query.size(-2), key.size(-2)\n",
    "\n",
    "    scale_factor = 1 / math.sqrt(query.size(-1)) if scale is None else scale\n",
    "    attn_bias = torch.zeros(L, S, dtype=query.dtype)\n",
    "\n",
    "    if is_causal:\n",
    "        assert attn_mask is None\n",
    "        temp_mask = torch.ones(L, S, dtype=torch.bool).tril(diagonal=0)\n",
    "        attn_bias.masked_fill_(temp_mask.logical_not(), float(\"-inf\"))\n",
    "        attn_bias.to(query.dtype)\n",
    "\n",
    "    \n",
    "    if attn_mask is not None:\n",
    "        if attn_mask.dtype == torch.bool:\n",
    "            attn_bias.masked_fill_(attn_mask.logical_not(), float(\"-inf\"))\n",
    "        else:\n",
    "            attn_bias += attn_mask\n",
    "\n",
    "\n",
    "\n",
    "    # (bsz, num_heads, tgt_len, head_dim) @ (bsz, num_heads, head_dim, tgt_len) -> (bsz, num_heads, tgt_len, tgt_len) \n",
    "    attn_weight = query @ key.transpose(-2, -1) * scale_factor\n",
    "    attn_weight += attn_bias\n",
    "\n",
    "    # (bsz, num_heads, tgt_len, tgt_len) \n",
    "    attn_weight = torch.softmax(attn_weight, dim=-1)\n",
    "\n",
    "\n",
    "    sum_last_dim = attn_weight.sum(dim=-1)\n",
    "    tolerance = 1e-6  \n",
    "    assert torch.allclose(sum_last_dim, torch.ones_like(sum_last_dim), atol=tolerance), \"Attention weights sum is not approximately equal to 1\"\n",
    "\n",
    "\n",
    "    # # (bsz, num_heads, tgt_len, tgt_len) @ (bsz, num_heads, tgt_len, head_dim) -> (bsz, num_heads, tgt_len, head_dim) \n",
    "    attn_output = attn_weight @ value\n",
    "\n",
    "    print(\"ATTEN OUTPUT = \", attn_output)\n",
    "\n",
    "    return attn_output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the attention mask for SWA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ True, False, False, False, False, False],\n",
       "        [ True,  True, False, False, False, False],\n",
       "        [ True,  True,  True, False, False, False],\n",
       "        [False,  True,  True,  True, False, False],\n",
       "        [False, False,  True,  True,  True, False],\n",
       "        [False, False, False,  True,  True,  True]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_length = src_tokenized.shape[1]\n",
    "sliding_window_len = 3\n",
    "\n",
    "sliding_window_mask =  (1 - torch.triu(torch.ones(seq_length, seq_length), diagonal=1)).bool()\n",
    "\n",
    "# print(sliding_window_mask)\n",
    "\n",
    "for i in range(sliding_window_mask.shape[0]-1, -1, -1):\n",
    "\n",
    "    li = i - sliding_window_len + 1\n",
    "\n",
    "    if li > 0:\n",
    "\n",
    "        sliding_window_mask[i][0:li] = False\n",
    "\n",
    "sliding_window_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ATTEN OUTPUT =  tensor([[[[ 0.0112, -0.0299],\n",
      "          [ 0.0248,  0.0086],\n",
      "          [ 0.0047,  0.0130],\n",
      "          [ 0.0037,  0.0573],\n",
      "          [-0.0112, -0.0062],\n",
      "          [ 0.0028, -0.0160]],\n",
      "\n",
      "         [[ 0.0112, -0.0299],\n",
      "          [ 0.0248,  0.0087],\n",
      "          [ 0.0047,  0.0130],\n",
      "          [ 0.0037,  0.0573],\n",
      "          [-0.0113, -0.0062],\n",
      "          [ 0.0028, -0.0162]],\n",
      "\n",
      "         [[ 0.0317, -0.0445],\n",
      "          [-0.0043, -0.0052],\n",
      "          [ 0.0037,  0.0260],\n",
      "          [-0.0115,  0.0635],\n",
      "          [-0.0018,  0.0346],\n",
      "          [ 0.0059,  0.0284]],\n",
      "\n",
      "         [[ 0.0317, -0.0445],\n",
      "          [-0.0042, -0.0053],\n",
      "          [ 0.0037,  0.0258],\n",
      "          [-0.0114,  0.0636],\n",
      "          [-0.0017,  0.0350],\n",
      "          [ 0.0058,  0.0284]]]])\n"
     ]
    }
   ],
   "source": [
    "self_attn_op = self_attention_rope(query_rotated, key_rotated, value, attn_mask = sliding_window_mask, is_causal=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 6, 2])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "self_attn_op.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def out_proj_self_attn(self_attn_op, W, embed_dim):\n",
    "\n",
    "    self_attn_op = self_attn_op.transpose(1, 2).contiguous()\n",
    "    self_attn_op = self_attn_op.reshape(bsz, q_len, embed_dim)\n",
    "    return self_attn_op@W.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 8])\n"
     ]
    }
   ],
   "source": [
    "Wo = state_dict[\"layers.0.self_attn.o_proj.weight\"]\n",
    "print(Wo.shape)\n",
    "\n",
    "sa_output = out_proj_self_attn(self_attn_op, Wo, embed_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 8.5693e-04, -8.4662e-04,  1.4658e-03,  1.2231e-03,  1.6009e-03,\n",
       "           3.5980e-04, -1.0710e-03,  1.5701e-03],\n",
       "         [-8.5480e-04, -4.6229e-04, -2.8992e-04,  7.1789e-05, -4.6940e-04,\n",
       "           3.3089e-04,  3.2598e-05,  1.1919e-03],\n",
       "         [-4.4477e-04,  7.0532e-04, -5.9762e-04, -7.9699e-04, -1.9645e-04,\n",
       "          -6.6960e-04,  6.7209e-04, -4.9457e-04],\n",
       "         [-2.1304e-03,  2.0274e-03, -1.6052e-03, -2.0819e-03, -1.4704e-03,\n",
       "          -1.8510e-03,  1.5074e-03, -1.6401e-03],\n",
       "         [ 7.1337e-04,  5.0488e-04, -8.2343e-04, -8.1675e-04, -3.3030e-05,\n",
       "          -1.8039e-04,  9.0701e-04, -1.2630e-03],\n",
       "         [ 8.8190e-04,  6.5856e-05, -7.8390e-04, -5.9789e-04,  2.5963e-04,\n",
       "           1.4621e-04,  8.8148e-04, -4.5098e-04]]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sa_output\n",
    "\n",
    "# check and verified +- 0.002 diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Residual connection \n",
    "\n",
    "hidden_state = sa_output + residual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.1940,  0.1465, -1.2939,  0.6222, -0.8592, -1.1688,  1.5078,\n",
       "           0.2904],\n",
       "         [ 1.0467,  0.0726,  2.0168, -0.6245,  0.5989,  1.1229, -0.6177,\n",
       "          -0.6323],\n",
       "         [-1.0018, -1.1421,  0.8807, -0.8434,  1.2622, -0.3587, -0.6593,\n",
       "           1.4165],\n",
       "         [ 0.1351, -0.0874,  0.0323, -0.9003, -0.6377,  0.6653,  2.2816,\n",
       "           1.0361],\n",
       "         [-0.3569,  1.5310, -0.6104,  0.3285,  0.8500,  0.5952, -0.0165,\n",
       "          -1.9877],\n",
       "         [-0.5778,  1.2966, -0.2427,  0.2382,  0.3897, -1.2515, -0.8194,\n",
       "           1.8612]]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_state = apply_layernorm(hidden_state, state_dict[\"layers.0.post_attention_layernorm.weight\"], variance_epsilon = 1e-06)\n",
    "\n",
    "hidden_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mixtral Feed forward network \n",
    "\n",
    "Generic structure of an expert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only when 'pretraining_tp == 1'\n",
    "\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "def mixtral_mlp(x, W1, W2, W3):\n",
    "    \n",
    "    w1_proj =  x@W1.T\n",
    "    w3_proj =  x@W3.T\n",
    "\n",
    "    print(\"W1 PROJ = \", w1_proj)\n",
    "    print()\n",
    "\n",
    "    print(\"W3 PROJ = \", w3_proj)\n",
    "    print()\n",
    "\n",
    "    temp_proj = w1_proj * w3_proj\n",
    "\n",
    "    silu = nn.SiLU()\n",
    "\n",
    "    # sigm = nn.Sigmoid()\n",
    "\n",
    "    # sigm_proj = sigm(temp_proj)\n",
    "    # sigm_proj = torch.special.expit(temp_proj)\n",
    "\n",
    "    # temp_proj = sigm_proj*temp_proj\n",
    "\n",
    "    temp_proj = silu(temp_proj)\n",
    "    print(temp_proj.dtype, x.dtype)\n",
    "\n",
    "    print(\"ACT = \",temp_proj)\n",
    "\n",
    "    w_final_proj = temp_proj@W2.T\n",
    "\n",
    "    print(\"W2 proj = \", w_final_proj)\n",
    "\n",
    "    return w_final_proj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mixtrue of experts (MoE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to get the logits from gate for all the tokens in the hidden state\n",
    "\n",
    "For every token in the sequence "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0032, -0.0482,  0.0332, -0.0160],\n",
       "        [-0.0102,  0.0712, -0.0517,  0.0003],\n",
       "        [ 0.0885, -0.0172, -0.0019, -0.0231],\n",
       "        [-0.0117,  0.0283,  0.0430, -0.0417],\n",
       "        [-0.0454,  0.0197, -0.0536,  0.0346],\n",
       "        [ 0.0254, -0.0019,  0.0736, -0.0205]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_logits(hidden_states, W_logits):\n",
    "\n",
    "    hidden_dim = hidden_states.shape[2]\n",
    "\n",
    "    hidden_states = hidden_states.view(-1, hidden_dim)\n",
    "\n",
    "    logits = hidden_states@W_logits.T\n",
    "\n",
    "    return logits \n",
    "  \n",
    "\n",
    "\n",
    "W_logit = state_dict[\"layers.0.block_sparse_moe.gate.weight\"]\n",
    "logits = get_logits(hidden_state, W_logit)\n",
    "\n",
    "logits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalised logits "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2512, 0.2402, 0.2605, 0.2480],\n",
       "        [0.2466, 0.2675, 0.2366, 0.2492],\n",
       "        [0.2697, 0.2427, 0.2464, 0.2412],\n",
       "        [0.2458, 0.2559, 0.2597, 0.2386],\n",
       "        [0.2414, 0.2576, 0.2394, 0.2615],\n",
       "        [0.2514, 0.2446, 0.2638, 0.2401]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits_norm = torch.softmax(logits, dim=1)\n",
    "logits_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting the top 2 experts per token from the 4 of them \n",
    "\n",
    "These can be changed (hyperparameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "routing_weights, selected_experts = torch.topk(logits_norm, num_experts_per_tok, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[2, 0],\n",
       "         [1, 3],\n",
       "         [0, 2],\n",
       "         [2, 1],\n",
       "         [3, 1],\n",
       "         [2, 0]]),\n",
       " tensor([[0.2605, 0.2512],\n",
       "         [0.2675, 0.2492],\n",
       "         [0.2697, 0.2464],\n",
       "         [0.2597, 0.2559],\n",
       "         [0.2615, 0.2576],\n",
       "         [0.2638, 0.2514]]))"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_experts, routing_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalising the weights for weighted sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5091, 0.4909],\n",
       "        [0.5177, 0.4823],\n",
       "        [0.5226, 0.4774],\n",
       "        [0.5037, 0.4963],\n",
       "        [0.5037, 0.4963],\n",
       "        [0.5121, 0.4879]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "routing_weights /= routing_weights.sum(dim=-1, keepdim=True)\n",
    "routing_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intialising the torch vectors for final state\n",
    "final_hidden_states = torch.zeros(\n",
    "            (bsz * seq_len, embed_dim), dtype=hidden_state.dtype\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 8])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_hidden_states.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One hot encoded vectors for each of the experts (for all 4 of them)\n",
    "\n",
    "expert_mask = torch.nn.functional.one_hot(selected_experts, num_classes=num_local_experts).permute(2, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0, 0, 1, 0, 0, 0],\n",
       "         [1, 0, 0, 0, 0, 1]],\n",
       "\n",
       "        [[0, 1, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 1, 1, 0]],\n",
       "\n",
       "        [[1, 0, 0, 1, 0, 1],\n",
       "         [0, 0, 1, 0, 0, 0]],\n",
       "\n",
       "        [[0, 0, 0, 0, 1, 0],\n",
       "         [0, 1, 0, 0, 0, 0]]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expert_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['embed_tokens.weight', 'layers.0.self_attn.q_proj.weight', 'layers.0.self_attn.k_proj.weight', 'layers.0.self_attn.v_proj.weight', 'layers.0.self_attn.o_proj.weight', 'layers.0.block_sparse_moe.gate.weight', 'layers.0.block_sparse_moe.experts.0.w1.weight', 'layers.0.block_sparse_moe.experts.0.w2.weight', 'layers.0.block_sparse_moe.experts.0.w3.weight', 'layers.0.block_sparse_moe.experts.1.w1.weight', 'layers.0.block_sparse_moe.experts.1.w2.weight', 'layers.0.block_sparse_moe.experts.1.w3.weight', 'layers.0.block_sparse_moe.experts.2.w1.weight', 'layers.0.block_sparse_moe.experts.2.w2.weight', 'layers.0.block_sparse_moe.experts.2.w3.weight', 'layers.0.block_sparse_moe.experts.3.w1.weight', 'layers.0.block_sparse_moe.experts.3.w2.weight', 'layers.0.block_sparse_moe.experts.3.w3.weight', 'layers.0.input_layernorm.weight', 'layers.0.post_attention_layernorm.weight', 'norm.weight'])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_dict.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Storing the W1, W2 and W3 weights for each of the experts \n",
    "\n",
    "To compute the outputs for the tokens assigned for specific feedforwad units "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 - w1, w2, w3\n",
    "# 2 - w1, w2, w3\n",
    "# 3 - w1, w2, w3\n",
    "# 4 - w1, w2, w3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "exp_prefix = \"layers.0.block_sparse_moe.experts.\" \n",
    "\n",
    "expert_weights = []\n",
    "\n",
    "for i in range(num_local_experts):\n",
    "\n",
    "    exp_wt_pref = exp_prefix + str(i)\n",
    "\n",
    "    temp_wts = {\"w1\":None, \"w2\":None, \"w3\":None}\n",
    "\n",
    "    for j in range(1,4):\n",
    "\n",
    "        exp_wt_pref_new = exp_wt_pref + \".w\" + str(j) + \".weight\"\n",
    "\n",
    "        temp_wts[\"w\" + str(j)] = state_dict[exp_wt_pref_new]\n",
    "\n",
    "    expert_weights.append(temp_wts)\n",
    "\n",
    "\n",
    "\n",
    "len(expert_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_states_moe = hidden_state.view(-1, embed_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing the feedforwad outputs for the **experts assigned for each token**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1 PROJ =  tensor([[ 0.0018, -0.0021,  0.0020,  0.0039, -0.0791, -0.0755, -0.0111,  0.0611],\n",
      "        [-0.0258, -0.0187, -0.0983,  0.0030,  0.0440,  0.0357, -0.0696, -0.0077],\n",
      "        [-0.0298, -0.0790, -0.0499, -0.0019, -0.0149, -0.0616, -0.0725, -0.0390]])\n",
      "\n",
      "W3 PROJ =  tensor([[-0.0082, -0.0883, -0.0946,  0.0140, -0.0667, -0.1311,  0.0647,  0.0161],\n",
      "        [ 0.0225, -0.0679, -0.0663, -0.0125,  0.0879,  0.0320, -0.0503,  0.0111],\n",
      "        [-0.0190,  0.0059, -0.0476, -0.0095,  0.0509, -0.0089, -0.1948,  0.0007]])\n",
      "\n",
      "torch.float32 torch.float32\n",
      "ACT =  tensor([[-7.2249e-06,  9.2548e-05, -9.2995e-05,  2.7197e-05,  2.6439e-03,\n",
      "          4.9699e-03, -3.5835e-04,  4.9337e-04],\n",
      "        [-2.9021e-04,  6.3659e-04,  3.2708e-03, -1.8742e-05,  1.9381e-03,\n",
      "          5.7198e-04,  1.7519e-03, -4.2701e-05],\n",
      "        [ 2.8234e-04, -2.3260e-04,  1.1896e-03,  9.1454e-06, -3.7784e-04,\n",
      "          2.7400e-04,  7.1150e-03, -1.4499e-05]])\n",
      "W2 proj =  tensor([[ 1.2205e-04,  1.4340e-04, -6.8711e-05, -1.5961e-04, -5.3765e-05,\n",
      "          2.0748e-04,  1.4622e-05,  4.5323e-05],\n",
      "        [-3.2115e-06,  8.9715e-05,  8.7064e-05, -3.0135e-05,  1.4843e-04,\n",
      "          5.9632e-05, -2.3822e-06, -4.6461e-05],\n",
      "        [-1.3268e-04,  2.8575e-05, -2.2582e-05, -1.0409e-04,  2.0324e-04,\n",
      "          1.2238e-04,  1.3912e-04, -2.6826e-04]])\n",
      "W1 PROJ =  tensor([[-0.0170, -0.0030,  0.0234,  0.0128, -0.0712,  0.0239,  0.0940, -0.0030],\n",
      "        [-0.0043, -0.0466, -0.0338,  0.0387,  0.0099,  0.0241, -0.0796, -0.0051],\n",
      "        [ 0.0578,  0.0061, -0.0064, -0.0402, -0.0403,  0.0153, -0.0575, -0.0108]])\n",
      "\n",
      "W3 PROJ =  tensor([[ 0.0018,  0.0318,  0.0590, -0.0220, -0.0412,  0.0841,  0.0700, -0.0398],\n",
      "        [-0.0650,  0.0590,  0.0342,  0.0255,  0.0729, -0.0164,  0.0119,  0.0300],\n",
      "        [ 0.0737, -0.0093, -0.0581, -0.0170, -0.0341,  0.0643, -0.0147,  0.0449]])\n",
      "\n",
      "torch.float32 torch.float32\n",
      "ACT =  tensor([[-1.5650e-05, -4.7541e-05,  6.9078e-04, -1.4010e-04,  1.4678e-03,\n",
      "          1.0055e-03,  3.2980e-03,  5.9317e-05],\n",
      "        [ 1.4043e-04, -1.3705e-03, -5.7918e-04,  4.9252e-04,  3.5993e-04,\n",
      "         -1.9794e-04, -4.7510e-04, -7.6263e-05],\n",
      "        [ 2.1340e-03, -2.8203e-05,  1.8608e-04,  3.4255e-04,  6.8712e-04,\n",
      "          4.9185e-04,  4.2343e-04, -2.4248e-04]])\n",
      "W2 proj =  tensor([[-7.6112e-05, -2.4554e-05,  6.1715e-05,  5.0130e-05, -5.2519e-06,\n",
      "          2.7219e-05, -1.7254e-04,  9.5200e-05],\n",
      "        [-1.4633e-05, -5.2324e-05,  1.7202e-05, -3.5815e-06,  3.4736e-05,\n",
      "         -5.2539e-05,  4.4467e-05, -2.1953e-05],\n",
      "        [ 1.9727e-05, -7.2891e-05,  7.2711e-05,  8.2084e-05,  2.7926e-05,\n",
      "          2.1889e-05, -6.1411e-05,  1.4948e-06]])\n",
      "W1 PROJ =  tensor([[-0.0521, -0.0186, -0.0367, -0.0807,  0.0016, -0.1046,  0.0332, -0.0155],\n",
      "        [ 0.0439,  0.0259,  0.0260, -0.0723, -0.0733,  0.0220, -0.0418,  0.0730],\n",
      "        [-0.0586,  0.0523, -0.0233, -0.0470,  0.0094,  0.0628,  0.0268,  0.0222],\n",
      "        [ 0.0393, -0.0216,  0.0318,  0.0564,  0.0294, -0.0179, -0.0600, -0.0245]])\n",
      "\n",
      "W3 PROJ =  tensor([[-0.0478,  0.0182,  0.0502, -0.2013, -0.0212,  0.0347, -0.0216, -0.0172],\n",
      "        [-0.1180,  0.0218, -0.0074, -0.0868, -0.0214,  0.0011, -0.0339, -0.0940],\n",
      "        [ 0.0665,  0.0074,  0.0828, -0.0106, -0.0139, -0.0785,  0.0302,  0.0414],\n",
      "        [ 0.0636, -0.1025, -0.0261,  0.0660, -0.0629, -0.0454,  0.0455,  0.0747]])\n",
      "\n",
      "torch.float32 torch.float32\n",
      "ACT =  tensor([[ 1.2474e-03, -1.6939e-04, -9.1955e-04,  8.1858e-03, -1.6546e-05,\n",
      "         -1.8102e-03, -3.5739e-04,  1.3313e-04],\n",
      "        [-2.5833e-03,  2.8310e-04, -9.5936e-05,  3.1467e-03,  7.8658e-04,\n",
      "          1.1970e-05,  7.0870e-04, -3.4219e-03],\n",
      "        [-1.9421e-03,  1.9308e-04, -9.6406e-04,  2.4968e-04, -6.5224e-05,\n",
      "         -2.4613e-03,  4.0494e-04,  4.6026e-04],\n",
      "        [ 1.2516e-03,  1.1090e-03, -4.1568e-04,  1.8630e-03, -9.2359e-04,\n",
      "          4.0537e-04, -1.3639e-03, -9.1551e-04]])\n",
      "W2 proj =  tensor([[ 2.4661e-05,  4.7906e-05, -2.3578e-04,  1.0442e-04, -1.2459e-04,\n",
      "         -2.5134e-04, -1.5388e-04, -4.5820e-05],\n",
      "        [-6.2189e-06, -6.6178e-06, -8.3770e-05,  1.3842e-04, -1.6430e-04,\n",
      "         -9.0269e-05, -1.2215e-04, -5.0591e-05],\n",
      "        [ 7.2908e-05, -1.1331e-04, -1.0265e-04, -2.8303e-05, -3.6790e-06,\n",
      "          5.6507e-06, -1.5634e-06, -4.8450e-05],\n",
      "        [ 5.9532e-05,  3.8044e-05, -6.7013e-05,  7.4082e-05, -1.2353e-04,\n",
      "         -1.0198e-06, -6.4021e-05, -1.6124e-05]])\n",
      "W1 PROJ =  tensor([[ 0.0131,  0.0240,  0.0099,  0.0751,  0.0339,  0.0192,  0.0469,  0.0531],\n",
      "        [ 0.0071, -0.0741,  0.0475,  0.0404,  0.0771, -0.0460,  0.0060,  0.0260]])\n",
      "\n",
      "W3 PROJ =  tensor([[ 0.0854, -0.0562,  0.0539,  0.0114, -0.0115,  0.0346,  0.0126, -0.0018],\n",
      "        [-0.0606, -0.0048,  0.0588,  0.0985,  0.0088,  0.0983, -0.0336, -0.1446]])\n",
      "\n",
      "torch.float32 torch.float32\n",
      "ACT =  tensor([[ 5.6100e-04, -6.7459e-04,  2.6839e-04,  4.2822e-04, -1.9425e-04,\n",
      "          3.3312e-04,  2.9616e-04, -4.9054e-05],\n",
      "        [-2.1452e-04,  1.7687e-04,  1.3981e-03,  1.9919e-03,  3.3776e-04,\n",
      "         -2.2551e-03, -1.0025e-04, -1.8721e-03]])\n",
      "W2 proj =  tensor([[ 1.8895e-06,  4.0897e-06, -3.6938e-05,  4.8415e-05, -2.1987e-05,\n",
      "         -3.2681e-05, -3.1658e-05,  3.0346e-06],\n",
      "        [ 5.1656e-05,  1.0922e-04, -1.0310e-04,  4.7437e-05, -6.3214e-05,\n",
      "         -8.9166e-05,  6.8527e-05, -1.4183e-05]])\n"
     ]
    }
   ],
   "source": [
    "for expert_idx in range(num_local_experts):\n",
    "\n",
    "    expert_layer_wts = expert_weights[expert_idx]\n",
    "\n",
    "    idx, top_x = torch.where(expert_mask[expert_idx])\n",
    "\n",
    "    if top_x.shape[0] == 0:\n",
    "        continue\n",
    "\n",
    "    # in torch it is faster to index using lists than torch tensors\n",
    "    top_x_list = top_x.tolist()\n",
    "    idx_list = idx.tolist()\n",
    "\n",
    "    # Index the correct hidden states and compute the expert hidden state for\n",
    "    # the current expert. We need to make sure to multiply the output hidden\n",
    "    # states by `routing_weights` on the corresponding tokens (top-1 and top-2)\n",
    "    current_state = hidden_states_moe[None, top_x_list].reshape(-1, embed_dim)\n",
    "\n",
    "    current_hidden_states = mixtral_mlp(current_state, expert_layer_wts[\"w1\"], expert_layer_wts[\"w2\"], expert_layer_wts[\"w3\"]) * routing_weights[top_x_list, idx_list, None]\n",
    "\n",
    "    # However `index_add_` only support torch tensors for indexing so we'll use\n",
    "    # the `top_x` tensor here.\n",
    "    final_hidden_states.index_add_(0, top_x, current_hidden_states.to(hidden_state.dtype))\n",
    "\n",
    "\n",
    "final_hidden_states = final_hidden_states.reshape(bsz, seq_len, embed_dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.0978e-05,  6.8431e-05, -7.7291e-05,  3.8363e-05,  9.4358e-06,\n",
       "          -9.8680e-05, -7.9507e-05, -4.6135e-05],\n",
       "         [-1.4491e-05,  3.9965e-05, -1.7774e-05,  4.8831e-05, -3.3206e-05,\n",
       "          -2.8912e-05, -5.6277e-05,  4.2447e-05],\n",
       "         [ 9.2201e-05,  9.3103e-05, -6.7900e-05, -4.8041e-05, -8.7071e-05,\n",
       "           1.0794e-04, -2.2923e-05,  1.5988e-05],\n",
       "         [-1.0395e-05, -2.9303e-05, -3.3655e-05,  6.7940e-05, -6.5514e-05,\n",
       "          -7.1542e-05, -3.9455e-05, -3.6377e-05],\n",
       "         [ 1.0742e-05, -3.4113e-05,  1.7477e-05,  6.5124e-05,  2.7828e-06,\n",
       "          -5.6000e-06, -4.6424e-05,  2.2704e-06],\n",
       "         [-2.7409e-05, -4.4076e-05, -6.3579e-05, -6.5281e-05,  9.7286e-05,\n",
       "           6.2610e-05,  6.7082e-05, -1.5571e-04]]])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_hidden_states\n",
    "\n",
    "# Small perturbation due to SiLU floating point precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Residual connection \n",
    "\n",
    "hidden_state = final_hidden_states + residual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0205,  0.0033, -0.0228,  0.0090, -0.0157, -0.0197,  0.0258,\n",
       "           0.0032],\n",
       "         [ 0.0156,  0.0015,  0.0287, -0.0088,  0.0089,  0.0154, -0.0088,\n",
       "          -0.0100],\n",
       "         [-0.0132, -0.0163,  0.0126, -0.0108,  0.0174, -0.0041, -0.0097,\n",
       "           0.0200],\n",
       "         [ 0.0042, -0.0034,  0.0021, -0.0116, -0.0084,  0.0120,  0.0334,\n",
       "           0.0175],\n",
       "         [-0.0078,  0.0300, -0.0113,  0.0074,  0.0170,  0.0120, -0.0013,\n",
       "          -0.0383],\n",
       "         [-0.0140,  0.0293, -0.0048,  0.0059,  0.0087, -0.0285, -0.0194,\n",
       "           0.0426]]])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_state\n",
    "\n",
    "# +- 0.01 floating point error due to the SiLU activation function precision "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "my_project_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
