{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mixtral 8X7B \n",
    "\n",
    "Tiny Mixtral is used for checking the computations (Small model, same architercture)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.trainer_utils import set_seed\n",
    "import torch\n",
    "SEED = 6\n",
    "set_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MixtralConfig {\n",
       "  \"_name_or_path\": \"NickyNicky/LocutusqueXFelladrin-TinyMistral248M-Instruct_oasst2_chatML_V4\",\n",
       "  \"architectures\": [\n",
       "    \"MixtralForCausalLM\"\n",
       "  ],\n",
       "  \"attention_dropout\": 0.0,\n",
       "  \"bos_token_id\": 1,\n",
       "  \"eos_token_id\": 2,\n",
       "  \"hidden_act\": \"silu\",\n",
       "  \"hidden_size\": 8,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 8,\n",
       "  \"max_position_embeddings\": 32768,\n",
       "  \"model_type\": \"mixtral\",\n",
       "  \"num_attention_heads\": 4,\n",
       "  \"num_experts_per_tok\": 2,\n",
       "  \"num_hidden_layers\": 1,\n",
       "  \"num_key_value_heads\": 2,\n",
       "  \"num_local_experts\": 4,\n",
       "  \"output_hidden_states\": true,\n",
       "  \"output_router_logits\": false,\n",
       "  \"pretraining_tp\": 1,\n",
       "  \"rms_norm_eps\": 1e-06,\n",
       "  \"rope_theta\": 10000.0,\n",
       "  \"router_aux_loss_coef\": 0.001,\n",
       "  \"sliding_window\": 3,\n",
       "  \"tie_word_embeddings\": false,\n",
       "  \"torch_dtype\": \"bfloat16\",\n",
       "  \"transformers_version\": \"4.38.0.dev0\",\n",
       "  \"use_cache\": false,\n",
       "  \"vocab_size\": 32005\n",
       "}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoConfig, MixtralConfig\n",
    "\n",
    "\n",
    "mixtral_config = MixtralConfig.from_pretrained(\"NickyNicky/Mixtral-TinyMistral-8x248M-Instruct_oasst2_chatML_Intel_orca_dpo_pairs_DPO_V1\", num_hidden_layers = 1, use_cache = False, hidden_size = 8, num_attention_heads = 4, \n",
    "                                           output_hidden_states=True,  num_key_value_heads = 2, past_key_values = True, intermediate_size = 8, sliding_window = 3, dropout_p = 0, \n",
    "                                           \n",
    "                                           num_local_experts = 4, num_experts_per_tok = 2)\n",
    "\n",
    "mixtral_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel\n",
    "\n",
    "tinymixtral = AutoModel.from_config(mixtral_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "src_sent = \"hi how are you doing\"\n",
    "\n",
    "mistal_tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-v0.1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    1, 12014,   910,   460,   368,  2548]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_src_dict = mistal_tokenizer.encode_plus(src_sent, return_tensors='pt')\n",
    "tokenized_src_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    1, 12014,   910,   460,   368,  2548]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_src_dict = mistal_tokenizer.encode_plus(src_sent, return_tensors='pt')\n",
    "tokenized_src_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    1, 12014,   910,   460,   368,  2548]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_tokenized = tokenized_src_dict[\"input_ids\"]\n",
    "src_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s> hi how are you doing'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mistal_tokenizer.decode(*src_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6,)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_tokenized_np = src_tokenized.numpy()[0]\n",
    "src_tokenized_np.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **SLIDING WINDOW ATTENTION** and **GROUPED-QUERY ATTENTION**\n",
    "\n",
    "### Defining hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_rep =  2\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters of the model we loaded\n",
    "\n",
    "# Hidden size\n",
    "embed_dim = d_model = 8\n",
    "\n",
    "num_heads = 4\n",
    "\n",
    "# For grouped query attention \n",
    "# No.of query heds \n",
    "n_heads_q = num_heads\n",
    "\n",
    "# No.of key and values heads \n",
    "n_kv_heads = 2\n",
    "\n",
    "# Sequence length of the input passed \n",
    "seq_len = 6\n",
    "\n",
    "# Dimension per head\n",
    "head_dim = embed_dim // num_heads\n",
    "\n",
    "# No.of time the K and V matrices needs to be repeated\n",
    "n_rep = n_heads_q//n_kv_heads\n",
    "print(\"n_rep = \", n_rep)\n",
    "\n",
    "\n",
    "# MOE hyperparameters \n",
    "num_local_experts = 4\n",
    "num_experts_per_tok = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict = tinymixtral.state_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Source token embeddings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word index: 1, Embedding: [-0.02049089  0.00325612 -0.02274372  0.00900858 -0.01572997 -0.01957951\n",
      "  0.02586581  0.00320597]\n",
      "Word index: 12014, Embedding: [ 0.01558615  0.00148417  0.02867359 -0.00886034  0.0088985   0.0154724\n",
      " -0.00872641 -0.01009076]\n",
      "Word index: 910, Embedding: [-0.0133071  -0.01638352  0.01268725 -0.01078121  0.0175229  -0.00425425\n",
      " -0.00972286  0.01993921]\n",
      "Word index: 460, Embedding: [ 0.00419843 -0.0033648   0.00210035 -0.01170026 -0.00829275  0.01203581\n",
      "  0.03342133  0.01750135]\n",
      "Word index: 368, Embedding: [-0.00782214  0.02998792 -0.01133393  0.00735941  0.01696292  0.01203462\n",
      " -0.0012349  -0.03832581]\n",
      "Word index: 2548, Embedding: [-0.01400485  0.029383   -0.0047281   0.00600842  0.00859148 -0.02857112\n",
      " -0.01949297  0.04272375]\n",
      "\n",
      "(6, 8)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "src_vocab_embeds = state_dict[\"embed_tokens.weight\"].numpy()\n",
    "\n",
    "src_embedding = np.zeros((src_tokenized_np.shape[0], d_model))\n",
    "\n",
    "for i in range(src_tokenized_np.shape[0]):\n",
    "        word_index = src_tokenized_np[i]\n",
    "        if word_index < 0 or word_index >= src_vocab_embeds.shape[0]:\n",
    "            raise ValueError(f\"Invalid word index: {word_index}\")\n",
    "        src_embedding[i, :] = src_vocab_embeds[word_index, :]\n",
    "\n",
    "        print(f\"Word index: {word_index}, Embedding: {src_vocab_embeds[word_index, :]}\")\n",
    "print()\n",
    "print(src_embedding.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "residual = src_embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Pre-normalization (RMSNorm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[-1.1783826 ,  0.1872517 , -1.3079375 ,  0.5180622 , -0.90459317,\n",
       "         -1.1259714 ,  1.4874815 ,  0.18436776],\n",
       "        [ 1.062237  ,  0.10115016,  1.9541806 , -0.60385543,  0.6064558 ,\n",
       "          1.0544846 , -0.59472793, -0.687712  ],\n",
       "        [-0.9351401 , -1.1513319 ,  0.8915809 , -0.7576364 ,  1.2314006 ,\n",
       "         -0.29896203, -0.6832625 ,  1.4012038 ],\n",
       "        [ 0.27335456, -0.21907791,  0.13675119, -0.7617898 , -0.53993076,\n",
       "          0.7836371 ,  2.1760218 ,  1.1394916 ],\n",
       "        [-0.39570653,  1.5170282 , -0.57336086,  0.37229788,  0.85811996,\n",
       "          0.60880727, -0.06247089, -1.9388255 ],\n",
       "        [-0.60453886,  1.2683583 , -0.20409523,  0.25936186,  0.37086317,\n",
       "         -1.2333126 , -0.84144133,  1.8442307 ]], dtype=float32),\n",
       " (6, 8))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "variance_epsilon = 1e-05\n",
    "\n",
    "wt = state_dict[\"layers.0.input_layernorm.weight\"].numpy()\n",
    "\n",
    "dtype = src_embedding.dtype\n",
    "src_embedding = src_embedding.astype(np.float32)\n",
    "variance = np.mean(src_embedding**2, axis=-1, keepdims=True)\n",
    "src_embedding = src_embedding * (1/np.sqrt(variance + variance_epsilon))\n",
    "\n",
    "hidden_state = wt * src_embedding\n",
    "\n",
    "hidden_state, hidden_state.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Getting the Q,K and V matrices for attention calcualtion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "Wq = state_dict[\"layers.0.self_attn.q_proj.weight\"].numpy()\n",
    "Wk = state_dict[\"layers.0.self_attn.k_proj.weight\"].numpy()\n",
    "Wv = state_dict[\"layers.0.self_attn.v_proj.weight\"].numpy()\n",
    "\n",
    "\n",
    "query = np.matmul(hidden_state, Wq.T)\n",
    "key = np.matmul(hidden_state, Wk.T)\n",
    "value = np.matmul(hidden_state, Wv.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[-2.39523221e-02,  2.95548211e-03,  9.91353579e-03,\n",
       "         -1.05658919e-01, -3.85008045e-02,  1.30411331e-03,\n",
       "         -5.90171143e-02,  8.17328617e-02],\n",
       "        [-1.24330502e-02,  5.98522369e-03, -1.37904333e-02,\n",
       "          9.24208090e-02,  8.37197974e-02, -4.29389700e-02,\n",
       "          4.55013290e-02, -3.90602089e-02],\n",
       "        [ 9.40914825e-03,  1.21670708e-01,  6.48907349e-02,\n",
       "          3.28224525e-03, -2.50251535e-02,  1.20714554e-04,\n",
       "          8.81221592e-02, -6.38489798e-02],\n",
       "        [ 2.95085814e-02,  6.52939826e-02,  8.16039462e-03,\n",
       "         -6.26104698e-02, -1.62040964e-02, -8.30524042e-02,\n",
       "          1.69737469e-02,  2.79735588e-02],\n",
       "        [-4.53358404e-02, -2.23228615e-02, -6.19471893e-02,\n",
       "          5.31434007e-02,  1.07010707e-01, -2.73960289e-02,\n",
       "         -5.80873974e-02,  9.80136469e-02],\n",
       "        [ 1.79467648e-02,  6.75790384e-02,  6.74972013e-02,\n",
       "          4.49689254e-02, -4.86278459e-02, -3.58133242e-02,\n",
       "         -4.75102328e-02, -8.05234313e-02]], dtype=float32),\n",
       " array([[ 0.00020003, -0.01606483, -0.07288229,  0.1298013 ],\n",
       "        [ 0.01043612,  0.02261915,  0.13818644, -0.10976848],\n",
       "        [-0.05685725, -0.08331263, -0.0009732 , -0.06036117],\n",
       "        [ 0.0347448 , -0.00301749,  0.02731169,  0.01810934],\n",
       "        [ 0.08436264,  0.03014495,  0.06698818, -0.02866209],\n",
       "        [-0.00491377, -0.01141234,  0.031265  ,  0.00745543]],\n",
       "       dtype=float32),\n",
       " array([[ 0.01107843, -0.02947902,  0.03121278, -0.04385417],\n",
       "        [ 0.03758795,  0.04618904, -0.03898475,  0.03285853],\n",
       "        [-0.03502538,  0.02122178,  0.01888887,  0.08686603],\n",
       "        [ 0.00834949,  0.10100346, -0.01351706,  0.06688891],\n",
       "        [-0.00625654, -0.14200555, -0.01066491, -0.05208423],\n",
       "        [ 0.00622982, -0.00740614,  0.04187129,  0.0693242 ]],\n",
       "       dtype=float32))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query, key, value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query_shape =  (6, 8)\n",
      "key_shape =  (6, 4)\n",
      "value_shape =  (6, 4)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"query_shape = \", query.shape)\n",
    "print(\"key_shape = \", key.shape)\n",
    "print(\"value_shape = \", value.shape)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After reshaping... \n",
      "\n",
      "query_shape =  (4, 6, 2)\n",
      "key_shape =  (2, 6, 2)\n",
      "value_shape =  (2, 6, 2)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "q_len, _ = query.shape\n",
    "\n",
    "\n",
    "print(\"After reshaping... \\n\")\n",
    "\n",
    "query1 = np.transpose(np.reshape(query, (q_len, num_heads, head_dim)), (1, 0, 2))\n",
    "key1 = np.transpose(np.reshape(key, (q_len, n_kv_heads, head_dim)), (1, 0, 2))\n",
    "value1 = np.transpose(np.reshape(value, (q_len, n_kv_heads, head_dim)), (1, 0, 2))\n",
    "\n",
    "print(\"query_shape = \", query1.shape)\n",
    "print(\"key_shape = \", key1.shape)\n",
    "print(\"value_shape = \", value1.shape)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Obtaining the rotary embeddings \n",
    "\n",
    "#### 4.1 Pre-computing the sin and cos values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "base = 10000\n",
    "max_seq_len = 2048\n",
    "dim = head_dim\n",
    "\n",
    "\n",
    "inv_freq = 1.0 / (base ** (np.arange(0, dim, 2, dtype=np.float32) / dim))\n",
    "t = np.arange(max_seq_len, dtype=np.float32)\n",
    "freqs = np.outer(t, inv_freq)\n",
    "# emb = np.concatenate((np.cos(freqs), np.sin(freqs)), axis=-1)\n",
    "emb = np.concatenate((freqs,freqs), axis=-1)\n",
    "\n",
    "cos, sin =  np.cos(emb[:seq_len]), np.sin(emb[:seq_len])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 1.        ,  1.        ],\n",
       "        [ 0.5403023 ,  0.5403023 ],\n",
       "        [-0.4161468 , -0.4161468 ],\n",
       "        [-0.9899925 , -0.9899925 ],\n",
       "        [-0.6536436 , -0.6536436 ],\n",
       "        [ 0.28366217,  0.28366217]], dtype=float32),\n",
       " array([[ 0.       ,  0.       ],\n",
       "        [ 0.841471 ,  0.841471 ],\n",
       "        [ 0.9092974,  0.9092974],\n",
       "        [ 0.14112  ,  0.14112  ],\n",
       "        [-0.7568025, -0.7568025],\n",
       "        [-0.9589243, -0.9589243]], dtype=float32))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cos, sin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 Applying the rotations on the Q and K matrices "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q matrix rotation\n",
    "\n",
    "unsqueeze_dim = 0\n",
    "\n",
    "cos_exp = np.expand_dims(cos, axis=unsqueeze_dim)\n",
    "sin_exp = np.expand_dims(sin, axis=unsqueeze_dim)\n",
    "\n",
    "\n",
    "# Half rotation \n",
    "q1 = query1[..., :query1.shape[-1] // 2]\n",
    "q2 = query1[..., query1.shape[-1] // 2:]\n",
    "q_half_rot = np.concatenate((-q2, q1), axis=-1)\n",
    "\n",
    "\n",
    "query_rotated = query1*cos_exp + q_half_rot*sin_exp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K matrix rotation\n",
    "\n",
    "unsqueeze_dim = 0\n",
    "\n",
    "cos_exp = np.expand_dims(cos, axis=unsqueeze_dim)\n",
    "sin_exp = np.expand_dims(sin, axis=unsqueeze_dim)\n",
    "\n",
    "\n",
    "# Half rotation \n",
    "k1 = key1[..., :key1.shape[-1] // 2]\n",
    "k2 = key1[..., key1.shape[-1] // 2:]\n",
    "key_half_rot = np.concatenate((-k2, k1), axis=-1)\n",
    "\n",
    "\n",
    "key_rotated = key1*cos_exp + key_half_rot*sin_exp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[[-0.02395232,  0.00295548],\n",
       "         [-0.011754  , -0.00722822],\n",
       "         [-0.11455044, -0.04207717],\n",
       "         [-0.03842756, -0.0604763 ],\n",
       "         [ 0.01273949,  0.04890147],\n",
       "         [ 0.069894  ,  0.00196003]],\n",
       " \n",
       "        [[ 0.00991354, -0.10565892],\n",
       "         [-0.08522043,  0.03833092],\n",
       "         [-0.02998861,  0.05763908],\n",
       "         [ 0.00075686,  0.06313549],\n",
       "         [ 0.08071044,  0.01214494],\n",
       "         [ 0.0622682 , -0.05196872]],\n",
       " \n",
       "        [[-0.0385008 ,  0.00130411],\n",
       "         [ 0.0813659 ,  0.04724776],\n",
       "         [ 0.01030437, -0.02280554],\n",
       "         [ 0.02776229,  0.07993453],\n",
       "         [-0.09068024, -0.06307873],\n",
       "         [-0.04813614,  0.03647154]],\n",
       " \n",
       "        [[-0.05901711,  0.08173286],\n",
       "         [ 0.05745251,  0.01718373],\n",
       "         [ 0.02138595,  0.1066998 ],\n",
       "         [-0.02075151, -0.02529828],\n",
       "         [ 0.11214543, -0.02010531],\n",
       "         [-0.09069273,  0.02271727]]], dtype=float32),\n",
       " array([[[ 0.00020003, -0.01606483],\n",
       "         [-0.0133947 ,  0.02100287],\n",
       "         [ 0.09941693, -0.01702986],\n",
       "         [-0.03397126,  0.00789048],\n",
       "         [-0.03232932, -0.08354992],\n",
       "         [-0.01233742,  0.00147469]],\n",
       " \n",
       "        [[-0.07288229,  0.1298013 ],\n",
       "         [ 0.16702944,  0.05697173],\n",
       "         [ 0.05529125,  0.02423418],\n",
       "         [-0.02959396, -0.01407389],\n",
       "         [-0.06547794, -0.03196203],\n",
       "         [ 0.01601789, -0.02786594]]], dtype=float32))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_rotated, key_rotated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 0.01107843, -0.02947902],\n",
       "        [ 0.03758795,  0.04618904],\n",
       "        [-0.03502538,  0.02122178],\n",
       "        [ 0.00834949,  0.10100346],\n",
       "        [-0.00625654, -0.14200555],\n",
       "        [ 0.00622982, -0.00740614]],\n",
       "\n",
       "       [[ 0.03121278, -0.04385417],\n",
       "        [-0.03898475,  0.03285853],\n",
       "        [ 0.01888887,  0.08686603],\n",
       "        [-0.01351706,  0.06688891],\n",
       "        [-0.01066491, -0.05208423],\n",
       "        [ 0.04187129,  0.0693242 ]]], dtype=float32)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Sliding window attention\n",
    "### (Grouped Query Attention)\n",
    "\n",
    "\n",
    "#### 5.1 Repating the K and V values for the GQA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repeating the Value vector \n",
    "\n",
    "num_value_value_heads, seq_len, head_dim = value1.shape[0], value1.shape[1], value1.shape[2]\n",
    "\n",
    "if n_rep > 1:\n",
    "\n",
    "    value1 = np.broadcast_to(value1[:, np.newaxis, :, :], (num_value_value_heads, n_rep, seq_len, head_dim))\n",
    "    value1 =  value1.reshape(num_value_value_heads * n_rep, seq_len, head_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 0.01107843, -0.02947902],\n",
       "        [ 0.03758795,  0.04618904],\n",
       "        [-0.03502538,  0.02122178],\n",
       "        [ 0.00834949,  0.10100346],\n",
       "        [-0.00625654, -0.14200555],\n",
       "        [ 0.00622982, -0.00740614]],\n",
       "\n",
       "       [[ 0.01107843, -0.02947902],\n",
       "        [ 0.03758795,  0.04618904],\n",
       "        [-0.03502538,  0.02122178],\n",
       "        [ 0.00834949,  0.10100346],\n",
       "        [-0.00625654, -0.14200555],\n",
       "        [ 0.00622982, -0.00740614]],\n",
       "\n",
       "       [[ 0.03121278, -0.04385417],\n",
       "        [-0.03898475,  0.03285853],\n",
       "        [ 0.01888887,  0.08686603],\n",
       "        [-0.01351706,  0.06688891],\n",
       "        [-0.01066491, -0.05208423],\n",
       "        [ 0.04187129,  0.0693242 ]],\n",
       "\n",
       "       [[ 0.03121278, -0.04385417],\n",
       "        [-0.03898475,  0.03285853],\n",
       "        [ 0.01888887,  0.08686603],\n",
       "        [-0.01351706,  0.06688891],\n",
       "        [-0.01066491, -0.05208423],\n",
       "        [ 0.04187129,  0.0693242 ]]], dtype=float32)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repeating the Key vector \n",
    "\n",
    "num_key_value_heads, seq_len, head_dim = key_rotated.shape[0], key_rotated.shape[1], key_rotated.shape[2]\n",
    "\n",
    "if n_rep > 1:\n",
    "\n",
    "    key_rotated = np.broadcast_to(key_rotated[:, np.newaxis, :, :], (num_key_value_heads, n_rep, seq_len, head_dim))\n",
    "    key_rotated =  key_rotated.reshape(num_key_value_heads * n_rep, seq_len, head_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 0.00020003, -0.01606483],\n",
       "        [-0.0133947 ,  0.02100287],\n",
       "        [ 0.09941693, -0.01702986],\n",
       "        [-0.03397126,  0.00789048],\n",
       "        [-0.03232932, -0.08354992],\n",
       "        [-0.01233742,  0.00147469]],\n",
       "\n",
       "       [[ 0.00020003, -0.01606483],\n",
       "        [-0.0133947 ,  0.02100287],\n",
       "        [ 0.09941693, -0.01702986],\n",
       "        [-0.03397126,  0.00789048],\n",
       "        [-0.03232932, -0.08354992],\n",
       "        [-0.01233742,  0.00147469]],\n",
       "\n",
       "       [[-0.07288229,  0.1298013 ],\n",
       "        [ 0.16702944,  0.05697173],\n",
       "        [ 0.05529125,  0.02423418],\n",
       "        [-0.02959396, -0.01407389],\n",
       "        [-0.06547794, -0.03196203],\n",
       "        [ 0.01601789, -0.02786594]],\n",
       "\n",
       "       [[-0.07288229,  0.1298013 ],\n",
       "        [ 0.16702944,  0.05697173],\n",
       "        [ 0.05529125,  0.02423418],\n",
       "        [-0.02959396, -0.01407389],\n",
       "        [-0.06547794, -0.03196203],\n",
       "        [ 0.01601789, -0.02786594]]], dtype=float32)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "key_rotated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4, 6, 2), (4, 6, 2), (4, 6, 2))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_rotated.shape, key_rotated.shape, value1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Self attention (GQA + Sliding window attention)\n",
    "\n",
    "#### The mask is passed as required by the sliding window attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[False,  True,  True,  True,  True,  True],\n",
       "       [False, False,  True,  True,  True,  True],\n",
       "       [False, False, False,  True,  True,  True],\n",
       "       [ True, False, False, False,  True,  True],\n",
       "       [ True,  True, False, False, False,  True],\n",
       "       [ True,  True,  True, False, False, False]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_length = src_tokenized_np.shape[0]\n",
    "sliding_window_len = 3\n",
    "\n",
    "sliding_window_mask = np.triu(np.ones((seq_length, seq_length)), k=1).astype('bool')\n",
    "\n",
    "# print(sliding_window_mask)\n",
    "\n",
    "for i in range(sliding_window_mask.shape[0]-1, -1, -1):\n",
    "\n",
    "    li = i - sliding_window_len + 1\n",
    "\n",
    "    if li > 0:\n",
    "\n",
    "        sliding_window_mask[i][0:li] = True\n",
    "\n",
    "attn_mask = sliding_window_mask\n",
    "\n",
    "sliding_window_mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention bias = \n",
      "[[  0. -inf -inf -inf -inf -inf]\n",
      " [  0.   0. -inf -inf -inf -inf]\n",
      " [  0.   0.   0. -inf -inf -inf]\n",
      " [-inf   0.   0.   0. -inf -inf]\n",
      " [-inf -inf   0.   0.   0. -inf]\n",
      " [-inf -inf -inf   0.   0.   0.]]\n"
     ]
    }
   ],
   "source": [
    "L, S = query.shape[-2], key.shape[-2]\n",
    "\n",
    "scale_factor = 1 / np.sqrt(query.shape[-1])\n",
    "attn_bias = np.zeros((L, S), dtype=query.dtype)\n",
    "\n",
    "if attn_mask is not None:\n",
    "    if attn_mask.dtype == np.bool_:\n",
    "        attn_bias[attn_mask] = -np.inf\n",
    "    else:\n",
    "        attn_bias += attn_mask\n",
    "\n",
    "    print(\"Attention bias = \")\n",
    "    print(attn_bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SoftMax (Scaled Dot Product of Q and K) = \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[1.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        ],\n",
       "        [0.50000954, 0.49999046, 0.        , 0.        , 0.        ,\n",
       "         0.        ],\n",
       "        [0.333778  , 0.3337777 , 0.33244425, 0.        , 0.        ,\n",
       "         0.        ],\n",
       "        [0.        , 0.33335105, 0.33311126, 0.33353773, 0.        ,\n",
       "         0.        ],\n",
       "        [0.        , 0.        , 0.33354586, 0.3334892 , 0.33296487,\n",
       "         0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.33327696, 0.33326936,\n",
       "         0.33345369]],\n",
       "\n",
       "       [[1.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        ],\n",
       "        [0.49977204, 0.500228  , 0.        , 0.        , 0.        ,\n",
       "         0.        ],\n",
       "        [0.33335233, 0.33365232, 0.3329953 , 0.        , 0.        ,\n",
       "         0.        ],\n",
       "        [0.        , 0.33345747, 0.33318454, 0.33335802, 0.        ,\n",
       "         0.        ],\n",
       "        [0.        , 0.        , 0.33419436, 0.3329604 , 0.33284527,\n",
       "         0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.33307663, 0.33364877,\n",
       "         0.3332746 ]],\n",
       "\n",
       "       [[1.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        ],\n",
       "        [0.4985787 , 0.5014212 , 0.        , 0.        , 0.        ,\n",
       "         0.        ],\n",
       "        [0.33302456, 0.33351153, 0.33346385, 0.        , 0.        ,\n",
       "         0.        ],\n",
       "        [0.        , 0.3339958 , 0.33332112, 0.33268315, 0.        ,\n",
       "         0.        ],\n",
       "        [0.        , 0.        , 0.33236718, 0.3335578 , 0.33407497,\n",
       "         0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.33339706, 0.3335238 ,\n",
       "         0.33307913]],\n",
       "\n",
       "       [[1.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        ],\n",
       "        [0.4988923 , 0.5011077 , 0.        , 0.        , 0.        ,\n",
       "         0.        ],\n",
       "        [0.33377188, 0.33346048, 0.33276764, 0.        , 0.        ,\n",
       "         0.        ],\n",
       "        [0.        , 0.33297887, 0.3333496 , 0.33367157, 0.        ,\n",
       "         0.        ],\n",
       "        [0.        , 0.        , 0.33416516, 0.33313307, 0.3327017 ,\n",
       "         0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.33339608, 0.33373195,\n",
       "         0.33287197]]], dtype=float32)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "key_rotated_T = np.transpose(key_rotated, axes=(0, 2, 1))\n",
    "\n",
    "attn_weight = query_rotated @ key_rotated_T * scale_factor\n",
    "attn_weight += attn_bias\n",
    "\n",
    "exp_attn_weight = np.exp(attn_weight)\n",
    "sum_exp_attn_weight = np.sum(exp_attn_weight, axis=-1, keepdims=True)\n",
    "softmax_attn_weight = exp_attn_weight / sum_exp_attn_weight\n",
    "\n",
    "print(\"SoftMax (Scaled Dot Product of Q and K) = \")\n",
    "softmax_attn_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_output = softmax_attn_weight @ value1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 0.01107843, -0.02947902],\n",
       "        [ 0.02433294,  0.00835429],\n",
       "        [ 0.00459977,  0.01263248],\n",
       "        [ 0.0036475 ,  0.05615484],\n",
       "        [-0.01098131, -0.00652086],\n",
       "        [ 0.00277493, -0.01613358]],\n",
       "\n",
       "       [[ 0.01107843, -0.02947902],\n",
       "        [ 0.02433924,  0.00837226],\n",
       "        [ 0.00457104,  0.01265093],\n",
       "        [ 0.00364744,  0.05614316],\n",
       "        [-0.0110077 , -0.00654352],\n",
       "        [ 0.00276977, -0.01620636]],\n",
       "\n",
       "       [[ 0.03121278, -0.04385417],\n",
       "        [-0.00398575, -0.00538879],\n",
       "        [ 0.00369151,  0.02532087],\n",
       "        [-0.01122158,  0.06218171],\n",
       "        [-0.00179356,  0.0337827 ],\n",
       "        [ 0.0058829 ,  0.02801968]],\n",
       "\n",
       "       [[ 0.03121278, -0.04385417],\n",
       "        [-0.00396374, -0.00541284],\n",
       "        [ 0.00370368,  0.02522594],\n",
       "        [-0.01119476,  0.06221688],\n",
       "        [-0.00173921,  0.033982  ],\n",
       "        [ 0.00587202,  0.02799441]]], dtype=float32)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_output\n",
    "\n",
    "# +- 0.005 difference due to numpy precision "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Post attention "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "Wo = state_dict[\"layers.0.self_attn.o_proj.weight\"].numpy()\n",
    "\n",
    "self_attn_op = np.transpose(attn_output, (1, 0, 2)).copy()\n",
    "self_attn_op = self_attn_op.reshape(q_len, embed_dim)\n",
    "\n",
    "sa_output = np.matmul(self_attn_op, Wo.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 8.4407721e-04, -8.3392626e-04,  1.4437779e-03,  1.2047621e-03,\n",
       "          1.5769361e-03,  3.5440541e-04, -1.0549710e-03,  1.5465862e-03],\n",
       "        [-8.3491480e-04, -4.5491473e-04, -2.7456848e-04,  7.6034143e-05,\n",
       "         -4.5097884e-04,  3.2300162e-04,  2.5672549e-05,  1.1732047e-03],\n",
       "        [-4.3571822e-04,  6.8573118e-04, -5.8262644e-04, -7.7831530e-04,\n",
       "         -1.8956563e-04, -6.5275497e-04,  6.5544032e-04, -4.8372874e-04],\n",
       "        [-2.0873179e-03,  1.9857292e-03, -1.5717335e-03, -2.0375180e-03,\n",
       "         -1.4400799e-03, -1.8136515e-03,  1.4754506e-03, -1.6038014e-03],\n",
       "        [ 7.1967638e-04,  4.8155471e-04, -8.0577575e-04, -7.8643003e-04,\n",
       "         -2.6787560e-05, -1.6236477e-04,  8.8560878e-04, -1.2233036e-03],\n",
       "        [ 8.8070869e-04,  5.9564245e-05, -7.7368139e-04, -5.8852532e-04,\n",
       "          2.6537999e-04,  1.4871509e-04,  8.6985884e-04, -4.4311673e-04]],\n",
       "       dtype=float32),\n",
       " (6, 8))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sa_output, sa_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.01964682,  0.00242219, -0.02129994,  0.01021334, -0.01415303,\n",
       "        -0.01922511,  0.02481084,  0.00475256],\n",
       "       [ 0.01475124,  0.00102926,  0.02839903, -0.00878431,  0.00844752,\n",
       "         0.0157954 , -0.00870074, -0.00891756],\n",
       "       [-0.01374281, -0.01569779,  0.01210462, -0.01155952,  0.01733334,\n",
       "        -0.004907  , -0.00906742,  0.01945548],\n",
       "       [ 0.00211111, -0.00137907,  0.00052862, -0.01373778, -0.00973283,\n",
       "         0.01022216,  0.03489678,  0.01589755],\n",
       "       [-0.00710247,  0.03046947, -0.01213971,  0.00657298,  0.01693613,\n",
       "         0.01187226, -0.00034929, -0.03954912],\n",
       "       [-0.01312414,  0.02944256, -0.00550178,  0.00541989,  0.00885686,\n",
       "        -0.02842241, -0.01862311,  0.04228063]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_states = residual + sa_output\n",
    "\n",
    "hidden_states\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "residual = hidden_states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. LayerNorm 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Layer norm\n",
    "\n",
    "variance_epsilon = 1e-05\n",
    "\n",
    "wt = state_dict[\"layers.0.post_attention_layernorm.weight\"].numpy()\n",
    "\n",
    "dtype = hidden_states.dtype\n",
    "hidden_states = hidden_states.astype(np.float32)\n",
    "variance = np.mean(hidden_states**2, axis=-1, keepdims=True)\n",
    "hidden_states = hidden_states * (1/np.sqrt(variance + variance_epsilon))\n",
    "\n",
    "hidden_states = wt * hidden_states\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Mixtral Feed forward network \n",
    "\n",
    "### 8.1 Logits from the MoE layer \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.00319363, -0.04745055,  0.0325985 , -0.01575745],\n",
       "       [-0.0099739 ,  0.06966699, -0.05062584,  0.00032955],\n",
       "       [ 0.08649002, -0.01674007, -0.00187917, -0.02254649],\n",
       "       [-0.01136106,  0.02795726,  0.0422231 , -0.04087061],\n",
       "       [-0.0448232 ,  0.01951255, -0.0529776 ,  0.03426894],\n",
       "       [ 0.02523708, -0.00186044,  0.07299722, -0.02028412]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W_logit = state_dict[\"layers.0.block_sparse_moe.gate.weight\"].numpy()\n",
    "\n",
    "logits = np.matmul(hidden_states, W_logit.T)\n",
    "\n",
    "logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.2512144 , 0.24033889, 0.26036876, 0.24807794],\n",
       "       [0.24670537, 0.26715678, 0.23687743, 0.24926043],\n",
       "       [0.26924857, 0.24284053, 0.24647631, 0.24143457],\n",
       "       [0.2459378 , 0.25580028, 0.25947565, 0.23878632],\n",
       "       [0.24150895, 0.2575573 , 0.23954761, 0.2613861 ],\n",
       "       [0.25140253, 0.2446816 , 0.2637009 , 0.24021494]], dtype=float32)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp_x = np.exp(logits - np.max(logits, axis=1, keepdims=True))\n",
    "logits_norm = exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
    "logits_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2 Selecting the top 2 experts per token from the 4 of them \n",
    "\n",
    "These can be changed (hyperparameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# routing_weights, selected_experts = torch.topk(logits_norm, num_experts_per_tok, dim=-1)\n",
    "num_experts_per_tok = 2\n",
    "\n",
    "selected_experts = np.argsort(logits_norm, axis=-1)[ :, -num_experts_per_tok:]\n",
    "selected_experts = np.flip(selected_experts, axis=-1)\n",
    "routing_weights = np.take_along_axis(logits_norm, selected_experts, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[2, 0],\n",
       "        [1, 3],\n",
       "        [0, 2],\n",
       "        [2, 1],\n",
       "        [3, 1],\n",
       "        [2, 0]]),\n",
       " array([[0.26036876, 0.2512144 ],\n",
       "        [0.26715678, 0.24926043],\n",
       "        [0.26924857, 0.24647631],\n",
       "        [0.25947565, 0.25580028],\n",
       "        [0.2613861 , 0.2575573 ],\n",
       "        [0.2637009 , 0.25140253]], dtype=float32))"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_experts, routing_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.3 Normalising the weights for weighted sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.5089471 , 0.4910529 ],\n",
       "       [0.5173274 , 0.48267257],\n",
       "       [0.5220779 , 0.47792205],\n",
       "       [0.5035664 , 0.49643356],\n",
       "       [0.503689  , 0.49631095],\n",
       "       [0.51193774, 0.4880622 ]], dtype=float32)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "routing_weights /= np.sum(routing_weights, axis=-1, keepdims=True)\n",
    "\n",
    "routing_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6, 8)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Intialising the torch vectors for final state\n",
    "\n",
    "final_hidden_states = np.zeros(( seq_len, embed_dim), dtype=hidden_state.dtype)\n",
    "\n",
    "final_hidden_states.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "expert_mask = np.eye(num_local_experts)[selected_experts].transpose(2, 1, 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0., 0., 1., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 1.]],\n",
       "\n",
       "       [[0., 1., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 1., 1., 0.]],\n",
       "\n",
       "       [[1., 0., 0., 1., 0., 1.],\n",
       "        [0., 0., 1., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0., 0., 1., 0.],\n",
       "        [0., 1., 0., 0., 0., 0.]]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expert_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.4 Storing the W1, W2 and W3 weights for each of the experts \n",
    "\n",
    "To compute the outputs for the tokens assigned for specific feedforwad units "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp_prefix = \"layers.0.block_sparse_moe.experts.\" \n",
    "\n",
    "expert_weights = []\n",
    "\n",
    "for i in range(num_local_experts):\n",
    "\n",
    "    exp_wt_pref = exp_prefix + str(i)\n",
    "\n",
    "    temp_wts = {\"w1\":None, \"w2\":None, \"w3\":None}\n",
    "\n",
    "    for j in range(1,4):\n",
    "\n",
    "        exp_wt_pref_new = exp_wt_pref + \".w\" + str(j) + \".weight\"\n",
    "\n",
    "        temp_wts[\"w\" + str(j)] = state_dict[exp_wt_pref_new].numpy()\n",
    "\n",
    "    expert_weights.append(temp_wts)\n",
    "\n",
    "\n",
    "\n",
    "len(expert_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_states_moe = hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.5 Computing the feedforwad outputs for the **experts assigned for each token**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UP PROJ =  [[ 2.0779315e-03 -4.1524232e-05  5.3748214e-03  4.9661640e-03\n",
      "  -7.7020541e-02 -7.4964218e-02 -1.0597602e-02  5.9146583e-02]\n",
      " [-2.5118727e-02 -2.0864358e-02 -9.6453838e-02  2.0347063e-03\n",
      "   4.5176938e-02  4.0917356e-02 -6.4834498e-02 -7.2856932e-03]\n",
      " [-2.9358903e-02 -7.7371515e-02 -4.8153035e-02 -9.4482111e-04\n",
      "  -1.3781639e-02 -6.1157711e-02 -7.1766920e-02 -3.9092816e-02]]\n",
      "\n",
      "GATE PROJ =  [[-0.00769947 -0.08333034 -0.09006876  0.009973   -0.0639486  -0.12828517\n",
      "   0.06382238  0.01461877]\n",
      " [ 0.02198876 -0.06551665 -0.06325372 -0.0090334   0.08448258  0.03772091\n",
      "  -0.04802711  0.01377582]\n",
      " [-0.01560306  0.00607488 -0.04853232 -0.01146191  0.04907023 -0.00804746\n",
      "  -0.19398391  0.0003019 ]]\n",
      "\n",
      "ACT =  [[-7.9994243e-06  1.7301171e-06 -2.4199316e-04  2.4764393e-05\n",
      "   2.4687427e-03  4.8315194e-03 -3.3806774e-04  4.3251214e-04]\n",
      " [-2.7608857e-04  6.8394857e-04  3.0598375e-03 -9.1900756e-06\n",
      "   1.9119739e-03  7.7231554e-04  1.5593307e-03 -5.0180675e-05]\n",
      " [ 2.2909684e-04 -2.3495614e-04  1.1698545e-03  5.4147545e-06\n",
      "  -3.3801972e-04  2.4614268e-04  7.0092659e-03 -5.9009903e-06]]\n",
      "UP PROJ =  [[-0.01633196 -0.00366721  0.02141405  0.01236947 -0.0698545   0.02196908\n",
      "   0.09029647 -0.00109647]\n",
      " [-0.00897679 -0.04730923 -0.02348294  0.04032223  0.0022121   0.03227955\n",
      "  -0.0691267  -0.01635708]\n",
      " [ 0.05739697  0.00738401 -0.00314105 -0.04075788 -0.03959345  0.0152338\n",
      "  -0.0545961  -0.01381274]]\n",
      "\n",
      "GATE PROJ =  [[ 0.00392067  0.03141821  0.05570094 -0.02419446 -0.04147033  0.08408598\n",
      "   0.06733935 -0.03816378]\n",
      " [-0.06429052  0.05263996  0.03729872  0.03417428  0.07000287 -0.01279369\n",
      "   0.02020491  0.01994004]\n",
      " [ 0.07282655 -0.01067709 -0.05677946 -0.01390608 -0.03506543  0.06209364\n",
      "  -0.01322481  0.04372888]]\n",
      "\n",
      "ACT =  [[-3.2015054e-05 -5.7605310e-05  5.9674686e-04 -1.4961396e-04\n",
      "   1.4505425e-03  9.2449889e-04  3.0494961e-03  2.0923158e-05]\n",
      " [ 2.8864437e-04 -1.2436279e-03 -4.3775004e-04  6.8946631e-04\n",
      "   7.7432735e-05 -2.0644466e-04 -6.9786172e-04 -1.6305383e-04]\n",
      " [ 2.0943799e-03 -3.9418319e-05  8.9181551e-05  2.8347140e-04\n",
      "   6.9466245e-04  4.7318486e-04  3.6114187e-04 -3.0191671e-04]]\n",
      "UP PROJ =  [[-0.05120672 -0.01814655 -0.03487625 -0.0774817  -0.00246566 -0.10403867\n",
      "   0.02932719 -0.01313617]\n",
      " [ 0.04783004  0.02691704  0.0248977  -0.07180887 -0.06703263  0.03166964\n",
      "  -0.03642978  0.07100003]\n",
      " [-0.05717193  0.04959687 -0.02338124 -0.04369082  0.01104234  0.0607057\n",
      "   0.02673106  0.01982858]\n",
      " [ 0.0394175  -0.02135401  0.02988893  0.05534603  0.03099303 -0.01461302\n",
      "  -0.05581572 -0.02505191]]\n",
      "\n",
      "GATE PROJ =  [[-0.04990004  0.02395726  0.04563273 -0.19853528 -0.0143195   0.03715624\n",
      "  -0.02522428 -0.0219267 ]\n",
      " [-0.11253069  0.01037272 -0.0031889  -0.07330894 -0.03106271 -0.0030058\n",
      "  -0.02355756 -0.08716021]\n",
      " [ 0.06539712  0.00551515  0.08196067 -0.00879584 -0.014495   -0.07893759\n",
      "   0.0297855   0.04292386]\n",
      " [ 0.06258187 -0.10315299 -0.02341375  0.06831063 -0.06404765 -0.04542702\n",
      "   0.04743043  0.07469823]]\n",
      "\n",
      "ACT =  [[ 1.2792411e-03 -2.1732360e-04 -7.9511618e-04  7.7505829e-03\n",
      "   1.7653792e-05 -1.9291072e-03 -3.6974176e-04  1.4403723e-04]\n",
      " [-2.6839313e-03  1.3962087e-04 -3.9696526e-05  2.6390441e-03\n",
      "   1.0421913e-03 -4.7593952e-05  4.2928246e-04 -3.0846151e-03]\n",
      " [-1.8659451e-03  1.3678576e-04 -9.5725287e-04  1.9218566e-04\n",
      "  -8.0022975e-05 -2.3902403e-03  3.9825740e-04  4.2574061e-04]\n",
      " [ 1.2349316e-03  1.1025779e-03 -3.4978357e-04  1.8939346e-03\n",
      "  -9.9153025e-04  3.3202310e-04 -1.3219297e-03 -9.3479117e-04]]\n",
      "UP PROJ =  [[ 0.01245213  0.02467591  0.0079376   0.07543384  0.03603991  0.02063259\n",
      "   0.04652455  0.05060221]\n",
      " [ 0.006541   -0.07307161  0.04918135  0.04054043  0.07406688 -0.04464553\n",
      "   0.00607491  0.02657355]]\n",
      "\n",
      "GATE PROJ =  [[ 0.0828982  -0.05286994  0.05382144  0.00926946 -0.01267909  0.03257506\n",
      "   0.01049591 -0.00474919]\n",
      " [-0.05541257 -0.0104251   0.05753436  0.09692159  0.00666642  0.09732149\n",
      "  -0.0318852  -0.14036413]]\n",
      "\n",
      "ACT =  [[ 5.16396016e-04 -6.51881273e-04  2.13652136e-04  3.49737704e-04\n",
      "  -2.28424469e-04  3.36166850e-04  2.44218361e-04 -1.20145254e-04]\n",
      " [-1.81193871e-04  3.81034624e-04  1.41681032e-03  1.96848111e-03\n",
      "   2.46941490e-04 -2.16776505e-03 -9.68404638e-05 -1.86150800e-03]]\n"
     ]
    }
   ],
   "source": [
    "for expert_idx in range(num_local_experts):\n",
    "    \n",
    "    expert_layer_wts = expert_weights[expert_idx]\n",
    "    idx, top_x = np.where(expert_mask[expert_idx])\n",
    "\n",
    "    if top_x.shape[0] == 0:\n",
    "        continue\n",
    "\n",
    "    current_state = hidden_states_moe[top_x]\n",
    "\n",
    "\n",
    "    W_up_proj = expert_layer_wts[\"w1\"]  \n",
    "    W_gate_proj = expert_layer_wts[\"w3\"]\n",
    "    W_down_proj = expert_layer_wts[\"w2\"]\n",
    "\n",
    "    up_proj = np.matmul(current_state, W_up_proj.T)\n",
    "    gate_proj = np.matmul(current_state, W_gate_proj.T)\n",
    "\n",
    "    print(\"UP PROJ = \", up_proj)\n",
    "    print()\n",
    "\n",
    "    print(\"GATE PROJ = \", gate_proj)\n",
    "    print()\n",
    "\n",
    "    temp_proj = up_proj * gate_proj\n",
    "\n",
    "\n",
    "    # SilU \n",
    "    temp_proj = temp_proj / (1 + np.exp(-temp_proj))\n",
    "\n",
    "    print(\"ACT = \", temp_proj)\n",
    "\n",
    "    down_proj = np.matmul(temp_proj, W_down_proj.T)\n",
    "\n",
    "    current_hidden_states = down_proj\n",
    "\n",
    "\n",
    "\n",
    "    for i, x in enumerate(top_x):\n",
    "        final_hidden_states[x] += current_hidden_states[i]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 3.02542794e-05,  1.35043272e-04, -1.54088426e-04,\n",
       "         5.34421670e-05,  8.43894668e-06, -1.77466209e-04,\n",
       "        -1.46090271e-04, -8.37543193e-05],\n",
       "       [-2.19882131e-05,  8.97436694e-05, -2.92263467e-05,\n",
       "         8.74870675e-05, -6.94078117e-05, -4.85683049e-05,\n",
       "        -8.66210903e-05,  7.54174980e-05],\n",
       "       [ 1.81148906e-04,  1.70211293e-04, -1.39261247e-04,\n",
       "        -8.31647048e-05, -1.85562851e-04,  1.98851703e-04,\n",
       "        -4.84626544e-05,  2.80959830e-05],\n",
       "       [-1.30659228e-05, -6.35682591e-05, -6.33936070e-05,\n",
       "         1.08051478e-04, -1.01529673e-04, -1.31385939e-04,\n",
       "        -5.35686850e-05, -7.56333829e-05],\n",
       "       [ 2.67640462e-05, -6.88876753e-05,  4.22416051e-05,\n",
       "         1.30987188e-04,  3.85967724e-06, -5.55036058e-06,\n",
       "        -8.86259659e-05,  1.81700739e-06],\n",
       "       [-6.00403946e-05, -7.89116166e-05, -1.21364224e-04,\n",
       "        -1.32052272e-04,  2.00030801e-04,  1.27674823e-04,\n",
       "         1.37436858e-04, -3.09652125e-04]], dtype=float32)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Residual connection \n",
    "\n",
    "hidden_state = final_hidden_states + residual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.01961656,  0.00255724, -0.02145403,  0.01026679, -0.01414459,\n",
       "        -0.01940257,  0.02466475,  0.0046688 ],\n",
       "       [ 0.01472925,  0.001119  ,  0.0283698 , -0.00869682,  0.00837811,\n",
       "         0.01574683, -0.00878736, -0.00884214],\n",
       "       [-0.01356167, -0.01552758,  0.01196536, -0.01164269,  0.01714777,\n",
       "        -0.00470815, -0.00911589,  0.01948358],\n",
       "       [ 0.00209804, -0.00144264,  0.00046522, -0.01362973, -0.00983436,\n",
       "         0.01009078,  0.03484321,  0.01582192],\n",
       "       [-0.0070757 ,  0.03040059, -0.01209747,  0.00670397,  0.01693999,\n",
       "         0.01186671, -0.00043791, -0.0395473 ],\n",
       "       [-0.01318418,  0.02936365, -0.00562315,  0.00528784,  0.00905689,\n",
       "        -0.02829473, -0.01848567,  0.04197098]])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "my_project_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
