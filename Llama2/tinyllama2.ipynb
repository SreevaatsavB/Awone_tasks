{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.trainer_utils import set_seed\n",
    "\n",
    "SEED = 6\n",
    "set_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaConfig {\n",
       "  \"architectures\": [\n",
       "    \"LlamaForCausalLM\"\n",
       "  ],\n",
       "  \"attention_bias\": false,\n",
       "  \"attention_dropout\": 0.0,\n",
       "  \"bos_token_id\": 1,\n",
       "  \"eos_token_id\": 2,\n",
       "  \"hidden_act\": \"silu\",\n",
       "  \"hidden_size\": 8,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 5632,\n",
       "  \"max_position_embeddings\": 2048,\n",
       "  \"model_type\": \"llama\",\n",
       "  \"num_attention_heads\": 2,\n",
       "  \"num_hidden_layers\": 1,\n",
       "  \"num_key_value_heads\": 2,\n",
       "  \"output_hidden_states\": true,\n",
       "  \"pretraining_tp\": 1,\n",
       "  \"rms_norm_eps\": 1e-05,\n",
       "  \"rope_scaling\": null,\n",
       "  \"rope_theta\": 10000.0,\n",
       "  \"tie_word_embeddings\": false,\n",
       "  \"torch_dtype\": \"bfloat16\",\n",
       "  \"transformers_version\": \"4.38.0.dev0\",\n",
       "  \"use_cache\": false,\n",
       "  \"vocab_size\": 32000\n",
       "}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoConfig, LlamaConfig\n",
    "\n",
    "\n",
    "# Llama_config = LlamaConfig.from_pretrained(\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\", num_hidden_layers = 1, use_cache = False, hidden_size = 4, num_attention_heads = 1, \n",
    "#                                            output_hidden_states=True,  num_key_value_heads = 1, past_key_values = True)\n",
    "\n",
    "\n",
    "Llama_config = LlamaConfig.from_pretrained(\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\", num_hidden_layers = 1, use_cache = False, hidden_size = 8, num_attention_heads = 2, \n",
    "                                           output_hidden_states=True,  num_key_value_heads = 2, past_key_values = True)\n",
    "\n",
    "\n",
    "Llama_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel\n",
    "\n",
    "tinyllama = AutoModel.from_config(Llama_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import LlamaTokenizer\n",
    "\n",
    "src_sent = \"hi how are\"\n",
    "\n",
    "llama_tokenizer = LlamaTokenizer.from_pretrained(\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[   1, 7251,  920,  526]]), 'attention_mask': tensor([[1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_src_dict = llama_tokenizer.encode_plus(src_sent, return_tensors='pt')\n",
    "tokenized_src_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   1, 7251,  920,  526]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_tokenized = tokenized_src_dict[\"input_ids\"]\n",
    "src_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s> hi how are'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llama_tokenizer.decode(*src_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###########################################################################\n",
      "LLAMA DECODER FWD START\n",
      "\n",
      "Attention mask =  None\n",
      "\n",
      "Input (hidden states) =  tensor([[[-0.0334, -0.0280, -0.0287,  0.0086, -0.0090, -0.0133,  0.0122,\n",
      "          -0.0035],\n",
      "         [ 0.0080,  0.0184,  0.0083,  0.0189,  0.0256, -0.0133, -0.0215,\n",
      "          -0.0009],\n",
      "         [ 0.0525,  0.0237, -0.0210, -0.0215,  0.0180,  0.0015,  0.0290,\n",
      "          -0.0205],\n",
      "         [-0.0094, -0.0067, -0.0051, -0.0405,  0.0052, -0.0106, -0.0160,\n",
      "           0.0159]]], grad_fn=<EmbeddingBackward0>)\n",
      "\n",
      "LayerNorm(hidden states) =  tensor([[[-1.6458, -1.3806, -1.4139,  0.4244, -0.4413, -0.6542,  0.5997,\n",
      "          -0.1725],\n",
      "         [ 0.4819,  1.1102,  0.4978,  1.1398,  1.5423, -0.8012, -1.2943,\n",
      "          -0.0517],\n",
      "         [ 1.9347,  0.8732, -0.7735, -0.7947,  0.6625,  0.0553,  1.0699,\n",
      "          -0.7544],\n",
      "         [-0.5286, -0.3752, -0.2874, -2.2779,  0.2938, -0.5967, -0.9011,\n",
      "           0.8947]]], grad_fn=<MulBackward0>)\n",
      "\n",
      "position_ids =  tensor([[0, 1, 2, 3]])\n",
      "bsz, query_len  =  1 4\n",
      "\n",
      "query =  tensor([[[-0.0394, -0.0600, -0.0235, -0.1077, -0.0176, -0.0679,  0.0625,\n",
      "           0.0028],\n",
      "         [ 0.0181, -0.0323,  0.0318, -0.0450, -0.0237,  0.0436, -0.0671,\n",
      "          -0.0137],\n",
      "         [ 0.0511,  0.0496, -0.0387,  0.1762,  0.0113,  0.0426, -0.0325,\n",
      "           0.0862],\n",
      "         [ 0.0717, -0.0031,  0.0471,  0.0010, -0.0151,  0.0288, -0.1334,\n",
      "          -0.0413]]], grad_fn=<UnsafeViewBackward0>)\n",
      "key =  tensor([[[ 0.0698, -0.0077, -0.0222, -0.0392, -0.0634,  0.0465,  0.0072,\n",
      "          -0.0172],\n",
      "         [-0.0269, -0.0502, -0.0530, -0.0392,  0.0087, -0.1056, -0.0026,\n",
      "           0.1114],\n",
      "         [ 0.0329, -0.0071,  0.0435,  0.0644,  0.0375, -0.0744,  0.0895,\n",
      "           0.0708],\n",
      "         [ 0.0050,  0.0169,  0.0753, -0.1054,  0.0291,  0.0531,  0.0078,\n",
      "          -0.0332]]], grad_fn=<UnsafeViewBackward0>)\n",
      "value =  tensor([[[ 0.0317, -0.0136,  0.0647,  0.0628, -0.0717, -0.0558,  0.0196,\n",
      "           0.0803],\n",
      "         [ 0.0189, -0.0083, -0.0210,  0.0649,  0.0463,  0.0822, -0.0375,\n",
      "          -0.0235],\n",
      "         [ 0.0314,  0.0382, -0.1098, -0.1415,  0.1071, -0.0132,  0.0508,\n",
      "           0.0340],\n",
      "         [ 0.0395,  0.1032,  0.0332, -0.0572,  0.0280, -0.0223,  0.0267,\n",
      "           0.0051]]], grad_fn=<UnsafeViewBackward0>)\n",
      "\n",
      "torch.Size([1, 4, 8]) torch.Size([1, 4, 8]) torch.Size([1, 4, 8])\n",
      "\n",
      "torch.Size([1, 2, 4, 4]) torch.Size([1, 2, 4, 4]) torch.Size([1, 2, 4, 4])\n",
      "\n",
      "kv_seq_len =  4\n",
      "\n",
      "max_position_embeddings =  2048\n",
      "cos cached=  tensor([[ 1.0000,  1.0000,  1.0000,  1.0000],\n",
      "        [ 0.5403,  0.9999,  0.5403,  0.9999],\n",
      "        [-0.4161,  0.9998, -0.4161,  0.9998],\n",
      "        [-0.9900,  0.9996, -0.9900,  0.9996]])\n",
      "sin cached=  tensor([[0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.8415, 0.0100, 0.8415, 0.0100],\n",
      "        [0.9093, 0.0200, 0.9093, 0.0200],\n",
      "        [0.1411, 0.0300, 0.1411, 0.0300]])\n",
      "\n",
      "Cos =  tensor([[ 1.0000,  1.0000,  1.0000,  1.0000],\n",
      "        [ 0.5403,  0.9999,  0.5403,  0.9999],\n",
      "        [-0.4161,  0.9998, -0.4161,  0.9998],\n",
      "        [-0.9900,  0.9996, -0.9900,  0.9996]])\n",
      "\n",
      "Sin =  tensor([[0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.8415, 0.0100, 0.8415, 0.0100],\n",
      "        [0.9093, 0.0200, 0.9093, 0.0200],\n",
      "        [0.1411, 0.0300, 0.1411, 0.0300]])\n",
      "\n",
      "HALF ROT Q = \n",
      "tensor([[[[ 0.0000e+00,  0.0000e+00, -0.0000e+00, -0.0000e+00],\n",
      "          [-2.6734e-02,  4.4966e-04,  1.5255e-02, -3.2336e-04],\n",
      "          [ 3.5166e-02, -3.5235e-03,  4.6478e-02,  9.9285e-04],\n",
      "          [-6.6406e-03, -3.0816e-05,  1.0115e-02, -9.1583e-05]],\n",
      "\n",
      "         [[-0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00],\n",
      "          [ 5.6455e-02,  1.3747e-04, -1.9909e-02,  4.3553e-04],\n",
      "          [ 2.9551e-02, -1.7230e-03,  1.0236e-02,  8.5137e-04],\n",
      "          [ 1.8820e-02,  1.2392e-03, -2.1379e-03,  8.6458e-04]]]],\n",
      "       grad_fn=<MulBackward0>)\n",
      "torch.Size([1, 2, 4, 4]) torch.Size([1, 1, 4, 4])\n",
      "\n",
      "HALF ROT K = \n",
      "tensor([[[[ 0.0000,  0.0000,  0.0000, -0.0000],\n",
      "          [ 0.0446,  0.0004, -0.0227, -0.0005],\n",
      "          [-0.0395, -0.0013,  0.0299, -0.0001],\n",
      "          [-0.0106,  0.0032,  0.0007,  0.0005]],\n",
      "\n",
      "         [[-0.0000,  0.0000, -0.0000,  0.0000],\n",
      "          [ 0.0022, -0.0011,  0.0073, -0.0011],\n",
      "          [-0.0814, -0.0014,  0.0341, -0.0015],\n",
      "          [-0.0011,  0.0010,  0.0041,  0.0016]]]], grad_fn=<MulBackward0>)\n",
      "torch.Size([1, 2, 4, 4]) torch.Size([1, 1, 4, 4])\n",
      "\n",
      "Rotated query =  tensor([[[[-0.0394, -0.0600, -0.0235, -0.1077],\n",
      "          [-0.0169, -0.0319,  0.0324, -0.0453],\n",
      "          [ 0.0139,  0.0461,  0.0626,  0.1771],\n",
      "          [-0.0776, -0.0031, -0.0365,  0.0009]],\n",
      "\n",
      "         [[-0.0176, -0.0679,  0.0625,  0.0028],\n",
      "          [ 0.0437,  0.0437, -0.0562, -0.0133],\n",
      "          [ 0.0249,  0.0408,  0.0238,  0.0870],\n",
      "          [ 0.0338,  0.0300,  0.1299, -0.0404]]]], grad_fn=<AddBackward0>)\n",
      "\n",
      "Rotated key =  tensor([[[[ 0.0698, -0.0077, -0.0222, -0.0392],\n",
      "          [ 0.0300, -0.0498, -0.0513, -0.0397],\n",
      "          [-0.0532, -0.0084,  0.0118,  0.0642],\n",
      "          [-0.0156,  0.0201, -0.0738, -0.1049]],\n",
      "\n",
      "         [[-0.0634,  0.0465,  0.0072, -0.0172],\n",
      "          [ 0.0069, -0.1067,  0.0059,  0.1103],\n",
      "          [-0.0970, -0.0758, -0.0032,  0.0693],\n",
      "          [-0.0299,  0.0541, -0.0036, -0.0316]]]], grad_fn=<AddBackward0>)\n",
      "\n",
      "RIGHT BEFORE ATTN :- \n",
      "\n",
      "Q =  tensor([[[[-0.0394, -0.0600, -0.0235, -0.1077],\n",
      "          [-0.0169, -0.0319,  0.0324, -0.0453],\n",
      "          [ 0.0139,  0.0461,  0.0626,  0.1771],\n",
      "          [-0.0776, -0.0031, -0.0365,  0.0009]],\n",
      "\n",
      "         [[-0.0176, -0.0679,  0.0625,  0.0028],\n",
      "          [ 0.0437,  0.0437, -0.0562, -0.0133],\n",
      "          [ 0.0249,  0.0408,  0.0238,  0.0870],\n",
      "          [ 0.0338,  0.0300,  0.1299, -0.0404]]]], grad_fn=<AddBackward0>)\n",
      "\n",
      "K =  tensor([[[[ 0.0698, -0.0077, -0.0222, -0.0392],\n",
      "          [ 0.0300, -0.0498, -0.0513, -0.0397],\n",
      "          [-0.0532, -0.0084,  0.0118,  0.0642],\n",
      "          [-0.0156,  0.0201, -0.0738, -0.1049]],\n",
      "\n",
      "         [[-0.0634,  0.0465,  0.0072, -0.0172],\n",
      "          [ 0.0069, -0.1067,  0.0059,  0.1103],\n",
      "          [-0.0970, -0.0758, -0.0032,  0.0693],\n",
      "          [-0.0299,  0.0541, -0.0036, -0.0316]]]], grad_fn=<AddBackward0>)\n",
      "\n",
      "V =  tensor([[[[ 0.0317, -0.0136,  0.0647,  0.0628],\n",
      "          [ 0.0189, -0.0083, -0.0210,  0.0649],\n",
      "          [ 0.0314,  0.0382, -0.1098, -0.1415],\n",
      "          [ 0.0395,  0.1032,  0.0332, -0.0572]],\n",
      "\n",
      "         [[-0.0717, -0.0558,  0.0196,  0.0803],\n",
      "          [ 0.0463,  0.0822, -0.0375, -0.0235],\n",
      "          [ 0.1071, -0.0132,  0.0508,  0.0340],\n",
      "          [ 0.0280, -0.0223,  0.0267,  0.0051]]]],\n",
      "       grad_fn=<TransposeBackward0>)\n",
      "\n",
      "True\n",
      "None\n",
      "ATTN OUTPUT =  tensor([[[[ 0.0317, -0.0136,  0.0647,  0.0628],\n",
      "          [ 0.0253, -0.0110,  0.0219,  0.0638],\n",
      "          [ 0.0274,  0.0055, -0.0223, -0.0051],\n",
      "          [ 0.0304,  0.0299, -0.0083, -0.0179]],\n",
      "\n",
      "         [[-0.0717, -0.0558,  0.0196,  0.0803],\n",
      "          [-0.0128,  0.0131, -0.0089,  0.0285],\n",
      "          [ 0.0273,  0.0045,  0.0109,  0.0302],\n",
      "          [ 0.0273, -0.0023,  0.0149,  0.0240]]]],\n",
      "       grad_fn=<ScaledDotProductFlashAttentionBackward0>)\n",
      "\n",
      "SA OUTPUT =  tensor([[[ 5.3733e-04, -1.7497e-05,  3.1351e-04,  3.3713e-03, -3.0710e-03,\n",
      "           3.1266e-03,  3.8834e-03, -1.0336e-03],\n",
      "         [ 1.8780e-04,  9.9296e-04,  2.1365e-03,  1.2946e-04, -2.4825e-03,\n",
      "           9.5584e-04,  1.0146e-03, -5.4013e-04],\n",
      "         [ 2.2841e-03,  5.6324e-05,  1.6864e-03, -6.4167e-04,  4.1008e-04,\n",
      "           2.3058e-03,  1.9699e-03, -9.3434e-04],\n",
      "         [ 2.0368e-03, -1.0943e-03,  7.3408e-04, -1.7564e-04,  1.0480e-03,\n",
      "           2.5588e-03,  2.1456e-03, -1.2697e-04]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "output = tinyllama(**tokenized_src_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verifying the RoPE of Llama2 \n",
    "\n",
    "ONLY for MULTI-HEAD ATTENTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# head_dim = 4\n",
    "# num_heads = 1\n",
    "# seq_len = 4\n",
    "\n",
    "embed_dim = 8\n",
    "num_heads = 2\n",
    "seq_len = 4\n",
    "\n",
    "head_dim = embed_dim // num_heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict = tinyllama.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def look_up_table(sentence, vocab_embeds, embedding):\n",
    "\n",
    "    for i in range(sentence.size(0)):\n",
    "        for j in range(sentence.size(1)):\n",
    "            \n",
    "            # Get the index for the current word token index in the sequence\n",
    "            word_index = sentence[i, j].item()\n",
    "\n",
    "            if word_index < 0 or word_index >= vocab_embeds.size(0):\n",
    "                raise ValueError(f\"Invalid word index: {word_index}\")\n",
    "\n",
    "            # Lookup the corresponding embedding vector for the word\n",
    "            embedding[i, j, :] = vocab_embeds[word_index, :]\n",
    "\n",
    "            print(f\"Word index: {word_index}, Embedding: {vocab_embeds[word_index, :]}\")\n",
    "    print()\n",
    "\n",
    "    return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source sentence embedding\n",
      "Word index: 1, Embedding: tensor([-0.0334, -0.0280, -0.0287,  0.0086, -0.0090, -0.0133,  0.0122, -0.0035])\n",
      "Word index: 7251, Embedding: tensor([ 0.0080,  0.0184,  0.0083,  0.0189,  0.0256, -0.0133, -0.0215, -0.0009])\n",
      "Word index: 920, Embedding: tensor([ 0.0525,  0.0237, -0.0210, -0.0215,  0.0180,  0.0015,  0.0290, -0.0205])\n",
      "Word index: 526, Embedding: tensor([-0.0094, -0.0067, -0.0051, -0.0405,  0.0052, -0.0106, -0.0160,  0.0159])\n",
      "\n",
      "torch.Size([1, 4, 8])\n",
      "Source embeddings : \n",
      "\n",
      "tensor([[[-0.0334, -0.0280, -0.0287,  0.0086, -0.0090, -0.0133,  0.0122,\n",
      "          -0.0035],\n",
      "         [ 0.0080,  0.0184,  0.0083,  0.0189,  0.0256, -0.0133, -0.0215,\n",
      "          -0.0009],\n",
      "         [ 0.0525,  0.0237, -0.0210, -0.0215,  0.0180,  0.0015,  0.0290,\n",
      "          -0.0205],\n",
      "         [-0.0094, -0.0067, -0.0051, -0.0405,  0.0052, -0.0106, -0.0160,\n",
      "           0.0159]]])\n"
     ]
    }
   ],
   "source": [
    "def get_embedding_outputs(src_tokens, state_dict, d_model):\n",
    "\n",
    "    src_vocab_embeds = state_dict[\"embed_tokens.weight\"]\n",
    "\n",
    "    src_embedding = torch.zeros(src_tokens.size(0), src_tokens.size(1), d_model)\n",
    "    print(\"Source sentence embedding\")\n",
    "    src_embedding =  look_up_table(src_tokens, src_vocab_embeds, src_embedding)\n",
    "    print(src_embedding.shape)\n",
    "\n",
    "\n",
    "    print(\"Source embeddings : \\n\")\n",
    "    print(src_embedding)\n",
    "\n",
    "    return src_embedding\n",
    "\n",
    "\n",
    "input_embeddings = get_embedding_outputs(src_tokenized, state_dict, d_model = embed_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_layernorm(hidden_states, wt, variance_epsilon = 1e-05):\n",
    "\n",
    "    dtype =  hidden_states.dtype\n",
    "    hidden_states = hidden_states.to(torch.float32)\n",
    "    variance = hidden_states.pow(2).mean(-1, keepdim=True)\n",
    "    hidden_states = hidden_states * torch.rsqrt(variance + variance_epsilon)\n",
    "    op = wt * hidden_states\n",
    "    return op.to(dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.6458, -1.3806, -1.4139,  0.4244, -0.4413, -0.6542,  0.5997,\n",
       "          -0.1725],\n",
       "         [ 0.4819,  1.1102,  0.4978,  1.1398,  1.5423, -0.8012, -1.2943,\n",
       "          -0.0517],\n",
       "         [ 1.9347,  0.8732, -0.7735, -0.7947,  0.6625,  0.0553,  1.0699,\n",
       "          -0.7544],\n",
       "         [-0.5286, -0.3752, -0.2874, -2.2779,  0.2938, -0.5967, -0.9011,\n",
       "           0.8947]]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_state = apply_layernorm(input_embeddings, state_dict[\"layers.0.input_layernorm.weight\"], variance_epsilon = 1e-05)\n",
    "\n",
    "residual = hidden_state\n",
    "\n",
    "hidden_state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['embed_tokens.weight', 'layers.0.self_attn.q_proj.weight', 'layers.0.self_attn.k_proj.weight', 'layers.0.self_attn.v_proj.weight', 'layers.0.self_attn.o_proj.weight', 'layers.0.mlp.gate_proj.weight', 'layers.0.mlp.up_proj.weight', 'layers.0.mlp.down_proj.weight', 'layers.0.input_layernorm.weight', 'layers.0.post_attention_layernorm.weight', 'norm.weight'])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_qkv(hidden_state ,Wq, Wk, Wv):\n",
    "\n",
    "\n",
    "    q_matmul = hidden_state@Wq.T\n",
    "    k_matmul = hidden_state@Wk.T\n",
    "    v_matmul = hidden_state@Wv.T\n",
    "\n",
    "    return q_matmul, k_matmul, v_matmul\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "Wq = state_dict[\"layers.0.self_attn.q_proj.weight\"]\n",
    "Wk = state_dict[\"layers.0.self_attn.k_proj.weight\"]\n",
    "Wv = state_dict[\"layers.0.self_attn.v_proj.weight\"]\n",
    "query, key, value = get_qkv(hidden_state ,Wq, Wk, Wv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[-0.0394, -0.0600, -0.0235, -0.1077, -0.0176, -0.0679,  0.0625,\n",
       "            0.0028],\n",
       "          [ 0.0181, -0.0323,  0.0318, -0.0450, -0.0237,  0.0436, -0.0671,\n",
       "           -0.0137],\n",
       "          [ 0.0511,  0.0496, -0.0387,  0.1762,  0.0113,  0.0426, -0.0325,\n",
       "            0.0862],\n",
       "          [ 0.0717, -0.0031,  0.0471,  0.0010, -0.0151,  0.0288, -0.1334,\n",
       "           -0.0413]]]),\n",
       " tensor([[[ 0.0698, -0.0077, -0.0222, -0.0392, -0.0634,  0.0465,  0.0072,\n",
       "           -0.0172],\n",
       "          [-0.0269, -0.0502, -0.0530, -0.0392,  0.0087, -0.1056, -0.0026,\n",
       "            0.1114],\n",
       "          [ 0.0329, -0.0071,  0.0435,  0.0644,  0.0375, -0.0744,  0.0895,\n",
       "            0.0708],\n",
       "          [ 0.0050,  0.0169,  0.0753, -0.1054,  0.0291,  0.0531,  0.0078,\n",
       "           -0.0332]]]),\n",
       " tensor([[[ 0.0317, -0.0136,  0.0647,  0.0628, -0.0717, -0.0558,  0.0196,\n",
       "            0.0803],\n",
       "          [ 0.0189, -0.0083, -0.0210,  0.0649,  0.0463,  0.0822, -0.0375,\n",
       "           -0.0235],\n",
       "          [ 0.0314,  0.0382, -0.1098, -0.1415,  0.1071, -0.0132,  0.0508,\n",
       "            0.0340],\n",
       "          [ 0.0395,  0.1032,  0.0332, -0.0572,  0.0280, -0.0223,  0.0267,\n",
       "            0.0051]]]))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query, key, value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sin_cos(dim, seq_len, max_seq_len, base = 10000):\n",
    "\n",
    "    inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2, dtype=torch.int64).float() / dim))\n",
    "\n",
    "    t = torch.arange(max_seq_len, dtype=torch.int64).type_as(inv_freq)\n",
    "\n",
    "    freqs = torch.outer(t, inv_freq)\n",
    "    \n",
    "    # Uses a different permutation in order to obtain the same calculation\n",
    "    emb = torch.cat((freqs, freqs), dim=-1)    \n",
    "\n",
    "    return  emb.cos()[:seq_len], emb.sin()[:seq_len]\n",
    "\n",
    "\n",
    "def rotate_half(x):\n",
    "\n",
    "    x1 = x[..., : x.shape[-1] // 2]\n",
    "    x2 = x[..., x.shape[-1] // 2 :]\n",
    "\n",
    "    # print(x1.shape, x2.shape)\n",
    "    # x1 = x[ : , : x.shape[-1] // 2]\n",
    "    # x2 = x[ : , x.shape[-1] // 2 :]\n",
    "\n",
    "    return torch.cat((-x2, x1), dim=-1)\n",
    "\n",
    "def apply_rotary_pos_emb(q, k, cos, sin, unsqueeze_dim=1):\n",
    "\n",
    "    cos = cos.unsqueeze(unsqueeze_dim)\n",
    "    sin = sin.unsqueeze(unsqueeze_dim)\n",
    "\n",
    "    # print(\"HLAF ROT SHAPES = \")\n",
    "    # print((rotate_half(q).shape,   sin.shape))\n",
    "    # print((rotate_half(k).shape , sin.shape))\n",
    "\n",
    "    q_embed = (q * cos) + (rotate_half(q) * sin)\n",
    "    k_embed = (k * cos) + (rotate_half(k) * sin)\n",
    "\n",
    "    return q_embed, k_embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "bsz, q_len, _ = query.shape\n",
    "\n",
    "query = query.view(bsz, q_len, num_heads, head_dim).transpose(1, 2)\n",
    "key = key.view(bsz, q_len, num_heads, head_dim).transpose(1, 2)\n",
    "value = value.view(bsz, q_len, num_heads, head_dim).transpose(1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_Rope(query, key, head_dim, seq_len,  num_heads):\n",
    "\n",
    "    cos, sin = get_sin_cos(dim = head_dim, seq_len = seq_len, max_seq_len = 2048, base = 10000)\n",
    "\n",
    "    cos = cos.unsqueeze(0)\n",
    "    sin = sin.unsqueeze(0)\n",
    "\n",
    "    q_rotated, k_rotated = apply_rotary_pos_emb(query, key, cos, sin, unsqueeze_dim=1)\n",
    "\n",
    "    return q_rotated, k_rotated\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_rotated, key_rotated = get_Rope(query, key, head_dim, seq_len,  num_heads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[[-0.0394, -0.0600, -0.0235, -0.1077],\n",
       "           [-0.0169, -0.0319,  0.0324, -0.0453],\n",
       "           [ 0.0139,  0.0461,  0.0626,  0.1771],\n",
       "           [-0.0776, -0.0031, -0.0365,  0.0009]],\n",
       " \n",
       "          [[-0.0176, -0.0679,  0.0625,  0.0028],\n",
       "           [ 0.0437,  0.0437, -0.0562, -0.0133],\n",
       "           [ 0.0249,  0.0408,  0.0238,  0.0870],\n",
       "           [ 0.0338,  0.0300,  0.1299, -0.0404]]]]),\n",
       " tensor([[[[ 0.0698, -0.0077, -0.0222, -0.0392],\n",
       "           [ 0.0300, -0.0498, -0.0513, -0.0397],\n",
       "           [-0.0532, -0.0084,  0.0118,  0.0642],\n",
       "           [-0.0156,  0.0201, -0.0738, -0.1049]],\n",
       " \n",
       "          [[-0.0634,  0.0465,  0.0072, -0.0172],\n",
       "           [ 0.0069, -0.1067,  0.0059,  0.1103],\n",
       "           [-0.0970, -0.0758, -0.0032,  0.0693],\n",
       "           [-0.0299,  0.0541, -0.0036, -0.0316]]]]))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_rotated, key_rotated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.0317, -0.0136,  0.0647,  0.0628],\n",
       "          [ 0.0189, -0.0083, -0.0210,  0.0649],\n",
       "          [ 0.0314,  0.0382, -0.1098, -0.1415],\n",
       "          [ 0.0395,  0.1032,  0.0332, -0.0572]],\n",
       "\n",
       "         [[-0.0717, -0.0558,  0.0196,  0.0803],\n",
       "          [ 0.0463,  0.0822, -0.0375, -0.0235],\n",
       "          [ 0.1071, -0.0132,  0.0508,  0.0340],\n",
       "          [ 0.0280, -0.0223,  0.0267,  0.0051]]]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def self_attention_rope(query, key, value, attn_mask = None, scale = None, is_causal=False):\n",
    "\n",
    "    L, S = query.size(-2), key.size(-2)\n",
    "\n",
    "    scale_factor = 1 / math.sqrt(query.size(-1)) if scale is None else scale\n",
    "    attn_bias = torch.zeros(L, S, dtype=query.dtype)\n",
    "\n",
    "    if is_causal:\n",
    "        assert attn_mask is None\n",
    "        temp_mask = torch.ones(L, S, dtype=torch.bool).tril(diagonal=0)\n",
    "        attn_bias.masked_fill_(temp_mask.logical_not(), float(\"-inf\"))\n",
    "        attn_bias.to(query.dtype)\n",
    "\n",
    "    \n",
    "    if attn_mask is not None:\n",
    "        if attn_mask.dtype == torch.bool:\n",
    "            attn_bias.masked_fill_(attn_mask.logical_not(), float(\"-inf\"))\n",
    "        else:\n",
    "            attn_bias += attn_mask\n",
    "\n",
    "\n",
    "\n",
    "    # (bsz, num_heads, tgt_len, head_dim) @ (bsz, num_heads, head_dim, tgt_len) -> (bsz, num_heads, tgt_len, tgt_len) \n",
    "    attn_weight = query @ key.transpose(-2, -1) * scale_factor\n",
    "    attn_weight += attn_bias\n",
    "\n",
    "    # (bsz, num_heads, tgt_len, tgt_len) \n",
    "    attn_weight = torch.softmax(attn_weight, dim=-1)\n",
    "\n",
    "\n",
    "    sum_last_dim = attn_weight.sum(dim=-1)\n",
    "    tolerance = 1e-6  \n",
    "    assert torch.allclose(sum_last_dim, torch.ones_like(sum_last_dim), atol=tolerance), \"Attention weights sum is not approximately equal to 1\"\n",
    "\n",
    "\n",
    "    # # (bsz, num_heads, tgt_len, tgt_len) @ (bsz, num_heads, tgt_len, head_dim) -> (bsz, num_heads, tgt_len, head_dim) \n",
    "    attn_output = attn_weight @ value\n",
    "\n",
    "    print(\"ATTEN OUTPUT = \", attn_output)\n",
    "\n",
    "    return attn_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ATTEN OUTPUT =  tensor([[[[ 0.0317, -0.0136,  0.0647,  0.0628],\n",
      "          [ 0.0253, -0.0110,  0.0219,  0.0638],\n",
      "          [ 0.0274,  0.0055, -0.0223, -0.0051],\n",
      "          [ 0.0304,  0.0299, -0.0083, -0.0179]],\n",
      "\n",
      "         [[-0.0717, -0.0558,  0.0196,  0.0803],\n",
      "          [-0.0128,  0.0131, -0.0089,  0.0285],\n",
      "          [ 0.0273,  0.0045,  0.0109,  0.0302],\n",
      "          [ 0.0273, -0.0023,  0.0149,  0.0240]]]])\n"
     ]
    }
   ],
   "source": [
    "self_attn_op = self_attention_rope(query_rotated, key_rotated, value, attn_mask = None, is_causal=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2, 4, 4])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "self_attn_op.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def out_proj_self_attn(self_attn_op, W, embed_dim):\n",
    "\n",
    "    self_attn_op = self_attn_op.transpose(1, 2).contiguous()\n",
    "    self_attn_op = self_attn_op.reshape(bsz, q_len, embed_dim)\n",
    "    return self_attn_op@W.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 8])\n"
     ]
    }
   ],
   "source": [
    "Wo = state_dict[\"layers.0.self_attn.o_proj.weight\"]\n",
    "print(Wo.shape)\n",
    "\n",
    "sa_output = out_proj_self_attn(self_attn_op, Wo, embed_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 5.3733e-04, -1.7497e-05,  3.1351e-04,  3.3713e-03, -3.0710e-03,\n",
       "           3.1266e-03,  3.8834e-03, -1.0336e-03],\n",
       "         [ 1.8780e-04,  9.9296e-04,  2.1365e-03,  1.2946e-04, -2.4825e-03,\n",
       "           9.5584e-04,  1.0146e-03, -5.4013e-04],\n",
       "         [ 2.2841e-03,  5.6324e-05,  1.6864e-03, -6.4167e-04,  4.1008e-04,\n",
       "           2.3058e-03,  1.9699e-03, -9.3434e-04],\n",
       "         [ 2.0368e-03, -1.0943e-03,  7.3408e-04, -1.7564e-04,  1.0480e-03,\n",
       "           2.5588e-03,  2.1456e-03, -1.2697e-04]]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sa_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "my_project_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
