{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.trainer_utils import set_seed\n",
    "\n",
    "SEED = 6\n",
    "set_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaConfig {\n",
       "  \"architectures\": [\n",
       "    \"LlamaForCausalLM\"\n",
       "  ],\n",
       "  \"attention_bias\": false,\n",
       "  \"attention_dropout\": 0.0,\n",
       "  \"bos_token_id\": 1,\n",
       "  \"eos_token_id\": 2,\n",
       "  \"hidden_act\": \"silu\",\n",
       "  \"hidden_size\": 8,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 5632,\n",
       "  \"max_position_embeddings\": 2048,\n",
       "  \"model_type\": \"llama\",\n",
       "  \"num_attention_heads\": 4,\n",
       "  \"num_hidden_layers\": 1,\n",
       "  \"num_key_value_heads\": 2,\n",
       "  \"output_hidden_states\": true,\n",
       "  \"pretraining_tp\": 1,\n",
       "  \"rms_norm_eps\": 1e-05,\n",
       "  \"rope_scaling\": null,\n",
       "  \"rope_theta\": 10000.0,\n",
       "  \"tie_word_embeddings\": false,\n",
       "  \"torch_dtype\": \"bfloat16\",\n",
       "  \"transformers_version\": \"4.38.0.dev0\",\n",
       "  \"use_cache\": false,\n",
       "  \"vocab_size\": 32000\n",
       "}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoConfig, LlamaConfig\n",
    "\n",
    "\n",
    "# Llama_config = LlamaConfig.from_pretrained(\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\", num_hidden_layers = 1, use_cache = False, hidden_size = 4, num_attention_heads = 1, \n",
    "#                                            output_hidden_states=True,  num_key_value_heads = 1, past_key_values = True)\n",
    "\n",
    "\n",
    "Llama_config = LlamaConfig.from_pretrained(\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\", num_hidden_layers = 1, use_cache = False, hidden_size = 8, num_attention_heads = 4, \n",
    "                                           output_hidden_states=True,  num_key_value_heads = 2, past_key_values = True)\n",
    "\n",
    "\n",
    "Llama_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel\n",
    "\n",
    "tinyllama = AutoModel.from_config(Llama_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import LlamaTokenizer\n",
    "\n",
    "src_sent = \"hi how are\"\n",
    "\n",
    "llama_tokenizer = LlamaTokenizer.from_pretrained(\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[   1, 7251,  920,  526]]), 'attention_mask': tensor([[1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_src_dict = llama_tokenizer.encode_plus(src_sent, return_tensors='pt')\n",
    "tokenized_src_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   1, 7251,  920,  526]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_tokenized = tokenized_src_dict[\"input_ids\"]\n",
    "src_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s> hi how are'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llama_tokenizer.decode(*src_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###########################################################################\n",
      "LLAMA DECODER FWD START\n",
      "\n",
      "Attention mask =  None\n",
      "\n",
      "Input (hidden states) =  tensor([[[-0.0258,  0.0210, -0.0483, -0.0079, -0.0164, -0.0140, -0.0164,\n",
      "          -0.0006],\n",
      "         [ 0.0074, -0.0400,  0.0304, -0.0152,  0.0153,  0.0289,  0.0037,\n",
      "           0.0250],\n",
      "         [ 0.0291,  0.0036, -0.0103, -0.0144,  0.0057, -0.0030, -0.0159,\n",
      "          -0.0409],\n",
      "         [ 0.0214, -0.0114, -0.0003,  0.0414,  0.0012,  0.0080, -0.0227,\n",
      "           0.0104]]], grad_fn=<EmbeddingBackward0>)\n",
      "\n",
      "LayerNorm(hidden states) =  tensor([[[-1.1118,  0.9048, -2.0794, -0.3405, -0.7058, -0.6022, -0.7040,\n",
      "          -0.0255],\n",
      "         [ 0.3099, -1.6691,  1.2685, -0.6328,  0.6387,  1.2049,  0.1544,\n",
      "           1.0419],\n",
      "         [ 1.4492,  0.1812, -0.5150, -0.7174,  0.2862, -0.1514, -0.7926,\n",
      "          -2.0385],\n",
      "         [ 1.0946, -0.5810, -0.0146,  2.1127,  0.0634,  0.4081, -1.1568,\n",
      "           0.5321]]], grad_fn=<MulBackward0>)\n",
      "\n",
      "position_ids =  tensor([[0, 1, 2, 3]])\n",
      "bsz, query_len  =  1 4\n",
      "\n",
      "query =  tensor([[[ 0.0507,  0.0144,  0.0718, -0.0079, -0.0578,  0.0141, -0.1129,\n",
      "           0.0667],\n",
      "         [-0.0108,  0.0590, -0.0587,  0.0913,  0.0326, -0.0691,  0.0018,\n",
      "          -0.0266],\n",
      "         [-0.0745,  0.0044,  0.0250, -0.1372,  0.0829,  0.0847,  0.0965,\n",
      "          -0.1339],\n",
      "         [-0.0155, -0.0212, -0.0371,  0.0443, -0.0387, -0.0826,  0.0557,\n",
      "           0.0384]]], grad_fn=<UnsafeViewBackward0>)\n",
      "key =  tensor([[[-0.0798, -0.0299, -0.0730, -0.0716],\n",
      "         [ 0.0677,  0.0232,  0.0885,  0.0634],\n",
      "         [-0.0026, -0.0251, -0.0366,  0.0742],\n",
      "         [-0.0431, -0.0219,  0.0021, -0.0356]]], grad_fn=<UnsafeViewBackward0>)\n",
      "value =  tensor([[[ 0.0016, -0.0609, -0.0340, -0.0412],\n",
      "         [ 0.0243,  0.0421, -0.0112, -0.0031],\n",
      "         [-0.0515,  0.0227, -0.0062,  0.0636],\n",
      "         [ 0.0180,  0.0050,  0.0095, -0.0304]]], grad_fn=<UnsafeViewBackward0>)\n",
      "\n",
      "torch.Size([1, 4, 8]) torch.Size([1, 4, 4]) torch.Size([1, 4, 4])\n",
      "\n",
      "torch.Size([1, 4, 4, 2]) torch.Size([1, 2, 4, 2]) torch.Size([1, 2, 4, 2])\n",
      "\n",
      "kv_seq_len =  4\n",
      "\n",
      "max_position_embeddings =  2048\n",
      "cos cached=  tensor([[ 1.0000,  1.0000],\n",
      "        [ 0.5403,  0.5403],\n",
      "        [-0.4161, -0.4161],\n",
      "        [-0.9900, -0.9900]])\n",
      "sin cached=  tensor([[0.0000, 0.0000],\n",
      "        [0.8415, 0.8415],\n",
      "        [0.9093, 0.9093],\n",
      "        [0.1411, 0.1411]])\n",
      "\n",
      "Cos =  tensor([[ 1.0000,  1.0000],\n",
      "        [ 0.5403,  0.5403],\n",
      "        [-0.4161, -0.4161],\n",
      "        [-0.9900, -0.9900]])\n",
      "\n",
      "Sin =  tensor([[0.0000, 0.0000],\n",
      "        [0.8415, 0.8415],\n",
      "        [0.9093, 0.9093],\n",
      "        [0.1411, 0.1411]])\n",
      "\n",
      "HALF ROT Q = \n",
      "tensor([[[[-0.0000,  0.0000],\n",
      "          [-0.0496, -0.0091],\n",
      "          [-0.0040, -0.0677],\n",
      "          [ 0.0030, -0.0022]],\n",
      "\n",
      "         [[ 0.0000,  0.0000],\n",
      "          [-0.0769, -0.0494],\n",
      "          [ 0.1247,  0.0227],\n",
      "          [-0.0063, -0.0052]],\n",
      "\n",
      "         [[-0.0000, -0.0000],\n",
      "          [ 0.0582,  0.0274],\n",
      "          [-0.0770,  0.0753],\n",
      "          [ 0.0117, -0.0055]],\n",
      "\n",
      "         [[-0.0000, -0.0000],\n",
      "          [ 0.0224,  0.0015],\n",
      "          [ 0.1218,  0.0877],\n",
      "          [-0.0054,  0.0079]]]], grad_fn=<MulBackward0>)\n",
      "torch.Size([1, 4, 4, 2]) torch.Size([1, 1, 4, 2])\n",
      "\n",
      "HALF ROT K = \n",
      "tensor([[[[ 0.0000, -0.0000],\n",
      "          [-0.0195,  0.0570],\n",
      "          [ 0.0228, -0.0023],\n",
      "          [ 0.0031, -0.0061]],\n",
      "\n",
      "         [[ 0.0000, -0.0000],\n",
      "          [-0.0534,  0.0745],\n",
      "          [-0.0675, -0.0332],\n",
      "          [ 0.0050,  0.0003]]]], grad_fn=<MulBackward0>)\n",
      "torch.Size([1, 2, 4, 2]) torch.Size([1, 1, 4, 2])\n",
      "\n",
      "Rotated query =  tensor([[[[ 5.0677e-02,  1.4430e-02],\n",
      "          [-5.5456e-02,  2.2768e-02],\n",
      "          [ 2.7001e-02, -6.9541e-02],\n",
      "          [ 1.8380e-02,  1.8795e-02]],\n",
      "\n",
      "         [[ 7.1790e-02, -7.9177e-03],\n",
      "          [-1.0855e-01, -2.3097e-06],\n",
      "          [ 1.1433e-01,  7.9771e-02],\n",
      "          [ 3.0465e-02, -4.9116e-02]],\n",
      "\n",
      "         [[-5.7788e-02,  1.4150e-02],\n",
      "          [ 7.5767e-02, -9.9350e-03],\n",
      "          [-1.1151e-01,  4.0088e-02],\n",
      "          [ 5.0002e-02,  7.6326e-02]],\n",
      "\n",
      "         [[-1.1291e-01,  6.6661e-02],\n",
      "          [ 2.3330e-02, -1.2850e-02],\n",
      "          [ 8.1624e-02,  1.4343e-01],\n",
      "          [-6.0592e-02, -3.0113e-02]]]], grad_fn=<AddBackward0>)\n",
      "\n",
      "Rotated key =  tensor([[[[-0.0798, -0.0299],\n",
      "          [ 0.0171,  0.0695],\n",
      "          [ 0.0239,  0.0081],\n",
      "          [ 0.0458,  0.0156]],\n",
      "\n",
      "         [[-0.0730, -0.0716],\n",
      "          [-0.0055,  0.1087],\n",
      "          [-0.0523, -0.0641],\n",
      "          [ 0.0030,  0.0355]]]], grad_fn=<AddBackward0>)\n",
      "\n",
      "RIGHT BEFORE ATTN :- \n",
      "\n",
      "Q =  tensor([[[[ 5.0677e-02,  1.4430e-02],\n",
      "          [-5.5456e-02,  2.2768e-02],\n",
      "          [ 2.7001e-02, -6.9541e-02],\n",
      "          [ 1.8380e-02,  1.8795e-02]],\n",
      "\n",
      "         [[ 7.1790e-02, -7.9177e-03],\n",
      "          [-1.0855e-01, -2.3097e-06],\n",
      "          [ 1.1433e-01,  7.9771e-02],\n",
      "          [ 3.0465e-02, -4.9116e-02]],\n",
      "\n",
      "         [[-5.7788e-02,  1.4150e-02],\n",
      "          [ 7.5767e-02, -9.9350e-03],\n",
      "          [-1.1151e-01,  4.0088e-02],\n",
      "          [ 5.0002e-02,  7.6326e-02]],\n",
      "\n",
      "         [[-1.1291e-01,  6.6661e-02],\n",
      "          [ 2.3330e-02, -1.2850e-02],\n",
      "          [ 8.1624e-02,  1.4343e-01],\n",
      "          [-6.0592e-02, -3.0113e-02]]]], grad_fn=<AddBackward0>)\n",
      "\n",
      "K =  tensor([[[[-0.0798, -0.0299],\n",
      "          [ 0.0171,  0.0695],\n",
      "          [ 0.0239,  0.0081],\n",
      "          [ 0.0458,  0.0156]],\n",
      "\n",
      "         [[-0.0798, -0.0299],\n",
      "          [ 0.0171,  0.0695],\n",
      "          [ 0.0239,  0.0081],\n",
      "          [ 0.0458,  0.0156]],\n",
      "\n",
      "         [[-0.0730, -0.0716],\n",
      "          [-0.0055,  0.1087],\n",
      "          [-0.0523, -0.0641],\n",
      "          [ 0.0030,  0.0355]],\n",
      "\n",
      "         [[-0.0730, -0.0716],\n",
      "          [-0.0055,  0.1087],\n",
      "          [-0.0523, -0.0641],\n",
      "          [ 0.0030,  0.0355]]]], grad_fn=<UnsafeViewBackward0>)\n",
      "\n",
      "V =  tensor([[[[ 0.0016, -0.0609],\n",
      "          [ 0.0243,  0.0421],\n",
      "          [-0.0515,  0.0227],\n",
      "          [ 0.0180,  0.0050]],\n",
      "\n",
      "         [[ 0.0016, -0.0609],\n",
      "          [ 0.0243,  0.0421],\n",
      "          [-0.0515,  0.0227],\n",
      "          [ 0.0180,  0.0050]],\n",
      "\n",
      "         [[-0.0340, -0.0412],\n",
      "          [-0.0112, -0.0031],\n",
      "          [-0.0062,  0.0636],\n",
      "          [ 0.0095, -0.0304]],\n",
      "\n",
      "         [[-0.0340, -0.0412],\n",
      "          [-0.0112, -0.0031],\n",
      "          [-0.0062,  0.0636],\n",
      "          [ 0.0095, -0.0304]]]], grad_fn=<UnsafeViewBackward0>)\n",
      "\n",
      "True\n",
      "None\n",
      "ATTN OUTPUT =  tensor([[[[ 0.0016, -0.0609],\n",
      "          [ 0.0129, -0.0094],\n",
      "          [-0.0086,  0.0013],\n",
      "          [-0.0019,  0.0023]],\n",
      "\n",
      "         [[ 0.0016, -0.0609],\n",
      "          [ 0.0129, -0.0096],\n",
      "          [-0.0085,  0.0016],\n",
      "          [-0.0019,  0.0022]],\n",
      "\n",
      "         [[-0.0340, -0.0412],\n",
      "          [-0.0226, -0.0221],\n",
      "          [-0.0172,  0.0064],\n",
      "          [-0.0104, -0.0028]],\n",
      "\n",
      "         [[-0.0340, -0.0412],\n",
      "          [-0.0226, -0.0221],\n",
      "          [-0.0171,  0.0064],\n",
      "          [-0.0105, -0.0027]]]],\n",
      "       grad_fn=<ScaledDotProductFlashAttentionBackward0>)\n",
      "\n",
      "SA OUTPUT =  tensor([[[ 7.2769e-03, -2.6772e-03,  5.2673e-03, -8.7639e-05,  5.1127e-05,\n",
      "           2.1528e-03, -5.8406e-03, -1.3345e-03],\n",
      "         [ 3.0233e-03, -1.5681e-03,  2.1389e-03,  1.0173e-03,  4.9285e-04,\n",
      "           6.7958e-05, -3.4745e-03, -3.2432e-04],\n",
      "         [-1.2038e-03,  3.6274e-04,  3.0120e-04, -1.1266e-04, -1.5346e-04,\n",
      "           3.7689e-04, -2.5342e-04, -1.3245e-03],\n",
      "         [ 2.0138e-05, -1.6477e-04,  4.0039e-04,  1.5589e-04,  6.1682e-05,\n",
      "           3.0537e-05, -6.1216e-04, -4.5421e-04]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n",
      "\n",
      "ATTN DONE\n",
      "\n",
      "residual + hidden_states :-  tensor([[[-0.0186,  0.0183, -0.0430, -0.0080, -0.0163, -0.0118, -0.0222,\n",
      "          -0.0019],\n",
      "         [ 0.0104, -0.0415,  0.0325, -0.0141,  0.0158,  0.0289,  0.0002,\n",
      "           0.0246],\n",
      "         [ 0.0279,  0.0040, -0.0100, -0.0145,  0.0056, -0.0027, -0.0162,\n",
      "          -0.0422],\n",
      "         [ 0.0215, -0.0115,  0.0001,  0.0415,  0.0013,  0.0080, -0.0233,\n",
      "           0.0100]]], grad_fn=<AddBackward0>)\n",
      "\n",
      "Post attention hidden_states :-  tensor([[[-0.8769,  0.8670, -2.0344, -0.3781, -0.7726, -0.5595, -1.0491,\n",
      "          -0.0911],\n",
      "         [ 0.4236, -1.6847,  1.3187, -0.5734,  0.6403,  1.1730,  0.0090,\n",
      "           0.9987],\n",
      "         [ 1.3795,  0.1979, -0.4964, -0.7179,  0.2766, -0.1317, -0.7995,\n",
      "          -2.0897],\n",
      "         [ 1.0891, -0.5859,  0.0058,  2.1080,  0.0661,  0.4072, -1.1810,\n",
      "           0.5059]]], grad_fn=<MulBackward0>)\n",
      "\n",
      "SiLU()\n",
      "UP PROJ =  tensor([[[-0.0796, -0.0497, -0.1191,  ..., -0.0259,  0.0210, -0.0127],\n",
      "         [ 0.0585,  0.0547,  0.0888,  ...,  0.0021, -0.0128,  0.0241],\n",
      "         [-0.0024, -0.0741, -0.0656,  ..., -0.1007, -0.0907, -0.0512],\n",
      "         [ 0.0014,  0.0548,  0.0600,  ..., -0.0253, -0.0084,  0.0511]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n",
      "\n",
      "GATE PROJ =  tensor([[[ 0.0527,  0.0807,  0.1029,  ..., -0.0380, -0.0192,  0.0034],\n",
      "         [-0.0608, -0.0465, -0.0959,  ..., -0.0405, -0.0408,  0.0139],\n",
      "         [ 0.0068,  0.0973, -0.0276,  ...,  0.0418,  0.0148,  0.0371],\n",
      "         [-0.0763, -0.0806, -0.0333,  ...,  0.0304,  0.0270,  0.0448]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n",
      "\n",
      "ACT (UP PROJ * GATE PROJ) =  tensor([[[-2.1528e-03, -2.0859e-03, -6.4430e-03,  ...,  4.8162e-04,\n",
      "          -2.0018e-04, -2.1373e-05],\n",
      "         [-1.7241e-03, -1.2422e-03, -4.0539e-03,  ..., -4.2444e-05,\n",
      "           2.5505e-04,  1.6923e-04],\n",
      "         [-8.1328e-06, -3.7824e-03,  8.9243e-04,  ..., -2.1506e-03,\n",
      "          -6.7440e-04, -9.6759e-04],\n",
      "         [-5.0396e-05, -2.1172e-03, -9.8260e-04,  ..., -3.9011e-04,\n",
      "          -1.1523e-04,  1.1691e-03]]], grad_fn=<MulBackward0>)\n",
      "\n",
      "DOWN PROJ =  tensor([[[-7.9168e-04, -8.5359e-04, -6.1779e-04,  1.5167e-03, -4.5981e-04,\n",
      "           1.1554e-03,  4.4039e-03,  1.1778e-03],\n",
      "         [ 2.6692e-03, -1.3403e-03, -3.1494e-03, -1.5276e-03, -3.0114e-05,\n",
      "           3.6384e-03,  1.9843e-03,  1.3003e-04],\n",
      "         [-7.0545e-04, -3.6622e-04, -4.9341e-03,  1.3888e-03, -1.7656e-03,\n",
      "           4.4448e-03,  1.0788e-03, -6.0451e-04],\n",
      "         [-3.1228e-04,  8.7655e-04,  1.8007e-03,  1.2018e-03,  1.4708e-03,\n",
      "          -1.1259e-03,  9.5736e-04, -1.9959e-03]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n",
      "MLP output :-  tensor([[[-7.9168e-04, -8.5359e-04, -6.1779e-04,  1.5167e-03, -4.5981e-04,\n",
      "           1.1554e-03,  4.4039e-03,  1.1778e-03],\n",
      "         [ 2.6692e-03, -1.3403e-03, -3.1494e-03, -1.5276e-03, -3.0114e-05,\n",
      "           3.6384e-03,  1.9843e-03,  1.3003e-04],\n",
      "         [-7.0545e-04, -3.6622e-04, -4.9341e-03,  1.3888e-03, -1.7656e-03,\n",
      "           4.4448e-03,  1.0788e-03, -6.0451e-04],\n",
      "         [-3.1228e-04,  8.7655e-04,  1.8007e-03,  1.2018e-03,  1.4708e-03,\n",
      "          -1.1259e-03,  9.5736e-04, -1.9959e-03]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n",
      "\n",
      "OUTPUTS =  tensor([[[-0.0193,  0.0175, -0.0437, -0.0065, -0.0168, -0.0107, -0.0178,\n",
      "          -0.0007],\n",
      "         [ 0.0131, -0.0429,  0.0294, -0.0157,  0.0158,  0.0326,  0.0022,\n",
      "           0.0248],\n",
      "         [ 0.0272,  0.0036, -0.0150, -0.0131,  0.0038,  0.0018, -0.0151,\n",
      "          -0.0428],\n",
      "         [ 0.0212, -0.0107,  0.0019,  0.0427,  0.0028,  0.0069, -0.0223,\n",
      "           0.0080]]], grad_fn=<AddBackward0>)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "output = tinyllama(**tokenized_src_dict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "my_project_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
