{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additions in llama-2 model compared ot the original transformer:- \n",
    "\n",
    "1. Rotary Positional Embeddings\n",
    "2. SwiGLU activation function\n",
    "3. RMSProp\n",
    "4. KV Caching \n",
    "5. Grouped Query attention "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RMS Norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self, dim: int, eps: float = 1e-6):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.weight = nn.Parameter(torch.ones(dim))\n",
    "\n",
    "    def _norm(self, x: torch.Tensor):\n",
    "        # (bsz, seq_len, dim) * (bsz, seq_len, 1) = (bsz, seq_len, dim)\n",
    "        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        # (dim) * (bsz, seq_len, dim) = (bsz, seq_len, dim)\n",
    "        return self.weight * self._norm(x.float()).type_as(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rotary Embeddings for positional encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "def precompute_theta_pos_freq(head_dim, seq_len, theta = 10000):\n",
    "\n",
    "    assert head_dim%2 == 0, \"Dimension of head must by divissible by 2\"\n",
    "\n",
    "    # theta_i = 10000^(-2(i-1)/dim) for i = [1,2,3.....dim/2]\n",
    "\n",
    "    theta_numerator = torch.arange(0, head_dim, 2).float()\n",
    "\n",
    "    theta = 1.0/ (theta **(theta_numerator/head_dim))\n",
    "\n",
    "    m = torch.arange(seq_len)\n",
    "\n",
    "    freqs = torch.outer(m,theta).float()\n",
    "\n",
    "    #                                   magnitude       angle\n",
    "    freqs_complex = torch.polar(torch.ones_like(freqs), freqs) \n",
    "\n",
    "    return freqs_complex\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_rotary_embeds(x, freqs_complex):\n",
    "\n",
    "    # Separate the last dimension pairs of two values, representing the real and imaginary parts of the complex number\n",
    "    # Two consecutive values will become a single complex number\n",
    "\n",
    "\n",
    "    # H -> no.of heads; can be num_heads for Query and num_kv_heads for Key\n",
    "\n",
    "    # (bsz, seq_len, H , head_dim) -> (bsz, seq_len, H, head_dim/2)\n",
    "    x_complex = torch.view_as_complex(x.float().reshape(*x.shape[:-1], -1, 2))\n",
    "\n",
    "\n",
    "    # Reshape the freqs_complex tensor to match the shape of the x_complex tensor. \n",
    "    # (seq_len, head_dim/2) --> (1, seq_len, 1, head_dim/2)\n",
    "    freqs_complex = freqs_complex.unsqueeze(0).unsqueeze(2)\n",
    "\n",
    "\n",
    "    # Multiply each complex number in the x_complex tensor by the corresponding complex number in the freqs_complex tensor\n",
    "    # Which results in the rotation of the complex number as shown in the Figure 1 of the paper\n",
    "    # (bsz, seq_len, H, head_dim/2) * (1, seq_len, 1, head_dim/2) = (bsz, seq_len, H, head_dim/2)\n",
    "    x_rotated = x_complex * freqs_complex\n",
    "\n",
    "\n",
    "    # Convert the complex number back to the real number\n",
    "    # (bsz, seq_len, H, head_dim/2) -> (bsz, seq_len, H, head_dim/2, 2)\n",
    "    x_out = torch.view_as_real(x_rotated)\n",
    "\n",
    "\n",
    "    # (bsz, seq_len, H, head_dim/2, 2) -> (bsz, seq_len, H, head_dim)\n",
    "    x_out = x_out.reshape(*x.shape)\n",
    "    \n",
    "\n",
    "    return x_out.type_as(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.0000+0.0000j],\n",
       "        [ 0.5403+0.8415j],\n",
       "        [-0.4161+0.9093j]])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freqs_complex = precompute_theta_pos_freq(head_dim = 2, seq_len = 3, theta = 10000)\n",
    "freqs_complex\n",
    "\n",
    "# head_dim = embed_dim // n_heads\n",
    "# here, 8//4 = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 3, 4, 2]),\n",
       " tensor([[[[0.8417, 0.5510],\n",
       "           [0.0214, 0.3450],\n",
       "           [0.3619, 0.2223],\n",
       "           [0.6088, 0.2576]],\n",
       " \n",
       "          [[0.0726, 0.3607],\n",
       "           [0.2889, 0.7267],\n",
       "           [0.5482, 0.7990],\n",
       "           [0.6860, 0.6757]],\n",
       " \n",
       "          [[0.6538, 0.7353],\n",
       "           [0.9691, 0.1316],\n",
       "           [0.7166, 0.7213],\n",
       "           [0.0201, 0.1098]]]]))"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.rand(1,3,4,2)\n",
    "\n",
    "# batch_size, seq_len, n_kv_heads, head_dim\n",
    "x.shape, x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.8417,  0.5510],\n",
       "          [ 0.0214,  0.3450],\n",
       "          [ 0.3619,  0.2223],\n",
       "          [ 0.6088,  0.2576]],\n",
       "\n",
       "         [[-0.2643,  0.2560],\n",
       "          [-0.4555,  0.6357],\n",
       "          [-0.3761,  0.8930],\n",
       "          [-0.1979,  0.9423]],\n",
       "\n",
       "         [[-0.9407,  0.2885],\n",
       "          [-0.5230,  0.8264],\n",
       "          [-0.9541,  0.3514],\n",
       "          [-0.1082, -0.0274]]]])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "apply_rotary_embeds(x, freqs_complex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grouped Query Attention\n",
    "\n",
    "Without KV caching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to expand the vector 'x' for grouped query attention\n",
    "\n",
    "def repeat_kv(x, n_rep):\n",
    "\n",
    "    batch_size, seq_len, n_kv_heads, head_dim = x.shape\n",
    "\n",
    "    if n_rep == 1:\n",
    "        return x\n",
    "    \n",
    "    else:\n",
    "        # (bsz, seq_len, n_kv_heads, 1, head_dim)\n",
    "        # --> (bsz, seq_len, n_kv_heads, n_rep, head_dim)\n",
    "        # --> (bsz, seq_len, n_kv_heads * n_rep, head_dim)\n",
    "        return (\n",
    "            x[:, :, :, None, :]\n",
    "            .expand(batch_size, seq_len, n_kv_heads, n_rep, head_dim)\n",
    "            .reshape(batch_size, seq_len, n_kv_heads * n_rep, head_dim)\n",
    "        )\n",
    "\n",
    "\n",
    "# Grouped query attention \n",
    "def GQ_attention_fwd(x, n_heads, n_kv_heads, embed_dim):\n",
    "\n",
    "    n_kv_heads = n_heads if n_kv_heads is None else n_kv_heads\n",
    "\n",
    "    n_heads_q = n_heads\n",
    "\n",
    "    n_rep = n_heads_q//n_kv_heads\n",
    "\n",
    "    head_dim = embed_dim//n_heads\n",
    "\n",
    "    Wq = nn.Linear(embed_dim, n_heads*head_dim, bias=False)\n",
    "    Wk = nn.Linear(embed_dim, n_kv_heads*head_dim, bias=False)\n",
    "    Wv = nn.Linear(embed_dim, n_kv_heads*head_dim, bias=False)\n",
    "    Wo = nn.Linear(n_heads*head_dim, embed_dim, bias=False)\n",
    "\n",
    "    batch_size, seq_len, _ = x.shape\n",
    "\n",
    "    # (bsz, seq_len, embed_dim)\n",
    "    xq = Wq(x)\n",
    "\n",
    "    # (bsz, seq_len, h_kv * head_dim)\n",
    "    xk = Wk(x)\n",
    "    xv = Wv(x)\n",
    "\n",
    "    # (bsz, seq_len, n_heads, head_dim)\n",
    "    xq = xq.view(batch_size, seq_len, n_heads_q, head_dim)\n",
    "\n",
    "    # (bsz, seq_len, h_kv, head_dim)\n",
    "    xk = xk.view(batch_size, seq_len, n_kv_heads, head_dim)\n",
    "    xv = xv.view(batch_size, seq_len, n_kv_heads, head_dim)\n",
    "\n",
    "\n",
    "    print(\"Before applying Rotary embeddings :- \")\n",
    "    print(\"Q = \", xq)\n",
    "    print(xq.shape)\n",
    "    print()\n",
    "    print(\"K = \", xk)\n",
    "    print(xk.shape)\n",
    "    print()\n",
    "\n",
    "    ##################################\n",
    "    ### Applying rotary embeddings ###\n",
    "\n",
    "    freqs_complex = precompute_theta_pos_freq(head_dim = head_dim, seq_len = seq_len, theta = 10000)\n",
    "\n",
    "    # (bsz, seq_len, n_heads, head_dim) -> (bsz, seq_len, n_heads, head_dim)\n",
    "    xq = apply_rotary_embeds(xq, freqs_complex)\n",
    "\n",
    "    # (bsz, seq_len, n_kv_heads, head_dim) -> (bsz, seq_len, n_kv_heads, head_dim)\n",
    "    xk = apply_rotary_embeds(xk, freqs_complex)\n",
    "\n",
    "    #####################################\n",
    "\n",
    "    print(\"After applying Rotary embeddings :- \")\n",
    "    print(\"Q = \", xq)\n",
    "    print(xq.shape)\n",
    "    print()\n",
    "    print(\"K = \", xk)\n",
    "    print(xk.shape)\n",
    "    print()\n",
    "\n",
    "\n",
    "    keys = repeat_kv(xk, n_rep)\n",
    "    values = repeat_kv(xv, n_rep)\n",
    "\n",
    "    print(\"Keys and Values after repeating for GQA\")\n",
    "    print(\"keys = \",keys.shape,keys)\n",
    "    print()\n",
    "    print(\"values = \",values.shape,values)\n",
    "    print()\n",
    "\n",
    "    xq = xq.transpose(1, 2)\n",
    "\n",
    "    # (bsz, n_heads, seq_len, head_dim)\n",
    "    keys = keys.transpose(1, 2)\n",
    "    values = values.transpose(1, 2)\n",
    "\n",
    "    # (bsz, n_heads, seq_len_q, head_dim) MATMUL (bsz, n_heads, head_dim, seq_len) -> (bsz, n_heads, seq_len_q, seq_len)\n",
    "    scores = torch.matmul(xq, keys.transpose(2, 3)) / math.sqrt(head_dim)\n",
    "\n",
    "    # (bsz, n_heads, seq_len_q, seq_len)\n",
    "    scores = F.softmax(scores.float(), dim=-1).type_as(xq)\n",
    "    print(\"Attention scores = \", scores)\n",
    "    print()\n",
    "\n",
    "    # (bsz, n_heads, seq_len_q, seq_len) MATMUL (bsz, n_heads, seq_len, head_dim) -> (bsz, n_heads, seq_len_q, head_dim)\n",
    "    output = torch.matmul(scores, values)\n",
    "\n",
    "    # ((bsz, n_heads, seq_len_q, head_dim) -> (bsz, seq_len_q, dim)\n",
    "    output = (output.transpose(1, 2).contiguous().view(batch_size, seq_len, -1))\n",
    "    print(\"Attention values = \",output)\n",
    "    print()\n",
    "\n",
    "    # (bsz, seq_len_q, dim)\n",
    "    return Wo(output)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 3, 8]),\n",
       " tensor([[[0.3885, 0.6270, 0.3206, 0.4409, 0.3074, 0.2796, 0.3976, 0.3765],\n",
       "          [0.2976, 0.8556, 0.0485, 0.8291, 0.5157, 0.7897, 0.8054, 0.7738],\n",
       "          [0.3322, 0.6595, 0.4512, 0.7347, 0.6833, 0.5190, 0.5468, 0.0774]]]))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.rand(1,3,8)\n",
    "\n",
    "# batch_size, seq_len, embedding_dim\n",
    "x.shape, x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before applying Rotary embeddings :- \n",
      "Q =  tensor([[[[-0.1299,  0.2407],\n",
      "          [-0.1696,  0.0881],\n",
      "          [-0.1522, -0.0535],\n",
      "          [-0.5082, -0.1717]],\n",
      "\n",
      "         [[ 0.0579,  0.1978],\n",
      "          [-0.2157,  0.2064],\n",
      "          [-0.0879,  0.0266],\n",
      "          [-0.4180,  0.3014]],\n",
      "\n",
      "         [[-0.0530,  0.4617],\n",
      "          [-0.1664,  0.1750],\n",
      "          [-0.4898, -0.0712],\n",
      "          [-0.6477,  0.2386]]]], grad_fn=<ViewBackward0>)\n",
      "torch.Size([1, 3, 4, 2])\n",
      "\n",
      "K =  tensor([[[[-0.1565, -0.1516],\n",
      "          [-0.3590,  0.3951]],\n",
      "\n",
      "         [[ 0.1555, -0.3181],\n",
      "          [-0.1116,  0.2460]],\n",
      "\n",
      "         [[-0.0209, -0.1935],\n",
      "          [-0.2571,  0.4130]]]], grad_fn=<ViewBackward0>)\n",
      "torch.Size([1, 3, 2, 2])\n",
      "\n",
      "After applying Rotary embeddings :- \n",
      "Q =  tensor([[[[-0.1299,  0.2407],\n",
      "          [-0.1696,  0.0881],\n",
      "          [-0.1522, -0.0535],\n",
      "          [-0.5082, -0.1717]],\n",
      "\n",
      "         [[-0.1351,  0.1556],\n",
      "          [-0.2903, -0.0700],\n",
      "          [-0.0698, -0.0596],\n",
      "          [-0.4795, -0.1889]],\n",
      "\n",
      "         [[-0.3978, -0.2403],\n",
      "          [-0.0899, -0.2241],\n",
      "          [ 0.2686, -0.4158],\n",
      "          [ 0.0526, -0.6882]]]], grad_fn=<ViewBackward0>)\n",
      "torch.Size([1, 3, 4, 2])\n",
      "\n",
      "K =  tensor([[[[-0.1565, -0.1516],\n",
      "          [-0.3590,  0.3951]],\n",
      "\n",
      "         [[ 0.3517, -0.0410],\n",
      "          [-0.2673,  0.0390]],\n",
      "\n",
      "         [[ 0.1846,  0.0615],\n",
      "          [-0.2686, -0.4056]]]], grad_fn=<ViewBackward0>)\n",
      "torch.Size([1, 3, 2, 2])\n",
      "\n",
      "Keys and Values after repeating for GQA\n",
      "keys =  torch.Size([1, 3, 4, 2]) tensor([[[[-0.1565, -0.1516],\n",
      "          [-0.1565, -0.1516],\n",
      "          [-0.3590,  0.3951],\n",
      "          [-0.3590,  0.3951]],\n",
      "\n",
      "         [[ 0.3517, -0.0410],\n",
      "          [ 0.3517, -0.0410],\n",
      "          [-0.2673,  0.0390],\n",
      "          [-0.2673,  0.0390]],\n",
      "\n",
      "         [[ 0.1846,  0.0615],\n",
      "          [ 0.1846,  0.0615],\n",
      "          [-0.2686, -0.4056],\n",
      "          [-0.2686, -0.4056]]]], grad_fn=<UnsafeViewBackward0>)\n",
      "\n",
      "values =  torch.Size([1, 3, 4, 2]) tensor([[[[ 1.9356e-02, -2.5811e-01],\n",
      "          [ 1.9356e-02, -2.5811e-01],\n",
      "          [-2.7033e-04,  1.7361e-01],\n",
      "          [-2.7033e-04,  1.7361e-01]],\n",
      "\n",
      "         [[ 1.2156e-01, -2.5068e-01],\n",
      "          [ 1.2156e-01, -2.5068e-01],\n",
      "          [-2.0269e-01,  2.8963e-02],\n",
      "          [-2.0269e-01,  2.8963e-02]],\n",
      "\n",
      "         [[ 2.3913e-01, -4.8252e-01],\n",
      "          [ 2.3913e-01, -4.8252e-01],\n",
      "          [-3.7998e-01,  1.1804e-01],\n",
      "          [-3.7998e-01,  1.1804e-01]]]], grad_fn=<UnsafeViewBackward0>)\n",
      "\n",
      "Attention scores =  tensor([[[[0.3359, 0.3266, 0.3375],\n",
      "          [0.3384, 0.3263, 0.3353],\n",
      "          [0.3667, 0.3120, 0.3213]],\n",
      "\n",
      "         [[0.3424, 0.3244, 0.3331],\n",
      "          [0.3548, 0.3179, 0.3273],\n",
      "          [0.3451, 0.3284, 0.3265]],\n",
      "\n",
      "         [[0.3306, 0.3318, 0.3375],\n",
      "          [0.3289, 0.3324, 0.3387],\n",
      "          [0.2926, 0.3306, 0.3767]],\n",
      "\n",
      "         [[0.3250, 0.3283, 0.3467],\n",
      "          [0.3230, 0.3284, 0.3486],\n",
      "          [0.2721, 0.3247, 0.4031]]]], grad_fn=<SoftmaxBackward0>)\n",
      "\n",
      "Attention values =  tensor([[[ 0.1269, -0.3314,  0.1257, -0.3305, -0.1956,  0.1069, -0.1984,\n",
      "           0.1069],\n",
      "         [ 0.1264, -0.3309,  0.1238, -0.3292, -0.1962,  0.1067, -0.1991,\n",
      "           0.1067],\n",
      "         [ 0.1219, -0.3279,  0.1247, -0.3289, -0.2102,  0.1048, -0.2191,\n",
      "           0.1042]]], grad_fn=<ViewBackward0>)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "atten_values = GQ_attention_fwd(x, n_heads = 4, n_kv_heads = 2, embed_dim = 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0505,  0.1723, -0.0181, -0.3009, -0.0628, -0.0622, -0.0380,\n",
       "          -0.1810],\n",
       "         [ 0.0535,  0.1749, -0.0165, -0.3008, -0.0634, -0.0638, -0.0403,\n",
       "          -0.1846],\n",
       "         [ 0.0523,  0.1754, -0.0178, -0.2965, -0.0640, -0.0633, -0.0405,\n",
       "          -0.1850]]], grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Projected attetion values\n",
    "atten_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Tensor Shape: torch.Size([1, 3, 2, 4])\n",
      "\n",
      "Original Tensor : tensor([[[[0.3553, 0.5873, 0.9951, 0.3988],\n",
      "          [0.3021, 0.7420, 0.4973, 0.3956]],\n",
      "\n",
      "         [[0.6649, 0.3215, 0.3432, 0.2998],\n",
      "          [0.2185, 0.9958, 0.5237, 0.6992]],\n",
      "\n",
      "         [[0.1545, 0.1401, 0.7353, 0.0985],\n",
      "          [0.5105, 0.2776, 0.4774, 0.7729]]]])\n",
      "\n",
      "x[:, :, :, None, :] =  tensor([[[[[0.3553, 0.5873, 0.9951, 0.3988]],\n",
      "\n",
      "          [[0.3021, 0.7420, 0.4973, 0.3956]]],\n",
      "\n",
      "\n",
      "         [[[0.6649, 0.3215, 0.3432, 0.2998]],\n",
      "\n",
      "          [[0.2185, 0.9958, 0.5237, 0.6992]]],\n",
      "\n",
      "\n",
      "         [[[0.1545, 0.1401, 0.7353, 0.0985]],\n",
      "\n",
      "          [[0.5105, 0.2776, 0.4774, 0.7729]]]]])\n",
      "\n",
      ".expand =  tensor([[[[[0.3553, 0.5873, 0.9951, 0.3988],\n",
      "           [0.3553, 0.5873, 0.9951, 0.3988]],\n",
      "\n",
      "          [[0.3021, 0.7420, 0.4973, 0.3956],\n",
      "           [0.3021, 0.7420, 0.4973, 0.3956]]],\n",
      "\n",
      "\n",
      "         [[[0.6649, 0.3215, 0.3432, 0.2998],\n",
      "           [0.6649, 0.3215, 0.3432, 0.2998]],\n",
      "\n",
      "          [[0.2185, 0.9958, 0.5237, 0.6992],\n",
      "           [0.2185, 0.9958, 0.5237, 0.6992]]],\n",
      "\n",
      "\n",
      "         [[[0.1545, 0.1401, 0.7353, 0.0985],\n",
      "           [0.1545, 0.1401, 0.7353, 0.0985]],\n",
      "\n",
      "          [[0.5105, 0.2776, 0.4774, 0.7729],\n",
      "           [0.5105, 0.2776, 0.4774, 0.7729]]]]])\n",
      "\n",
      "Result Tensor Values:\n",
      " tensor([[[[0.3553, 0.5873, 0.9951, 0.3988],\n",
      "          [0.3553, 0.5873, 0.9951, 0.3988],\n",
      "          [0.3021, 0.7420, 0.4973, 0.3956],\n",
      "          [0.3021, 0.7420, 0.4973, 0.3956]],\n",
      "\n",
      "         [[0.6649, 0.3215, 0.3432, 0.2998],\n",
      "          [0.6649, 0.3215, 0.3432, 0.2998],\n",
      "          [0.2185, 0.9958, 0.5237, 0.6992],\n",
      "          [0.2185, 0.9958, 0.5237, 0.6992]],\n",
      "\n",
      "         [[0.1545, 0.1401, 0.7353, 0.0985],\n",
      "          [0.1545, 0.1401, 0.7353, 0.0985],\n",
      "          [0.5105, 0.2776, 0.4774, 0.7729],\n",
      "          [0.5105, 0.2776, 0.4774, 0.7729]]]])\n"
     ]
    }
   ],
   "source": [
    "# repeat_kv :- Operations and intermediate outputs\n",
    "\n",
    "import torch\n",
    "\n",
    "batch_size = 1\n",
    "seq_len = 3\n",
    "n_kv_heads = 2\n",
    "n_rep = 2\n",
    "head_dim = 4\n",
    "\n",
    "# random 4D tensor\n",
    "x = torch.rand(batch_size, seq_len, n_kv_heads, head_dim)\n",
    "\n",
    "result = (\n",
    "    x[:, :, :, None, :]\n",
    "    .expand(batch_size, seq_len, n_kv_heads, n_rep, head_dim)\n",
    "    .reshape(batch_size, seq_len, n_kv_heads * n_rep, head_dim)\n",
    ")\n",
    "\n",
    "# Displaying shapes and values\n",
    "print(\"Original Tensor Shape:\", x.shape)\n",
    "print()\n",
    "\n",
    "print(\"Original Tensor :\", x)\n",
    "print()\n",
    "\n",
    "print(\"x[:, :, :, None, :] = \", x[:, :, :, None, :])\n",
    "print()\n",
    "\n",
    "print(\".expand = \", x[:, :, :, None, :].expand(batch_size, seq_len, n_kv_heads, n_rep, head_dim))\n",
    "print()\n",
    "\n",
    "print(\"Result Tensor Values:\\n\", result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LlamaMLP(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.hidden_size = config.hidden_size\n",
    "        self.intermediate_size = config.intermediate_size\n",
    "        self.gate_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)\n",
    "        self.up_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)\n",
    "        self.down_proj = nn.Linear(self.intermediate_size, self.hidden_size, bias=False)\n",
    "        self.act_fn = ACT2FN[config.hidden_act]\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.config.pretraining_tp > 1:\n",
    "            slice = self.intermediate_size // self.config.pretraining_tp\n",
    "            gate_proj_slices = self.gate_proj.weight.split(slice, dim=0)\n",
    "            up_proj_slices = self.up_proj.weight.split(slice, dim=0)\n",
    "            down_proj_slices = self.down_proj.weight.split(slice, dim=1)\n",
    "\n",
    "            gate_proj = torch.cat(\n",
    "                [F.linear(x, gate_proj_slices[i]) for i in range(self.config.pretraining_tp)], dim=-1\n",
    "            )\n",
    "            up_proj = torch.cat([F.linear(x, up_proj_slices[i]) for i in range(self.config.pretraining_tp)], dim=-1)\n",
    "\n",
    "            intermediate_states = (self.act_fn(gate_proj) * up_proj).split(slice, dim=2)\n",
    "\n",
    "            down_proj = [\n",
    "                F.linear(intermediate_states[i], down_proj_slices[i]) for i in range(self.config.pretraining_tp)\n",
    "            ]\n",
    "            down_proj = sum(down_proj)\n",
    "            \n",
    "        else:\n",
    "            down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
    "\n",
    "        return down_proj\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'LlamaConfig' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[73], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mLlamaDecoderLayer\u001b[39;00m(nn\u001b[38;5;241m.\u001b[39mModule):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, config: LlamaConfig, layer_idx: \u001b[38;5;28mint\u001b[39m):\n\u001b[1;32m      3\u001b[0m         \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n",
      "Cell \u001b[0;32mIn[73], line 2\u001b[0m, in \u001b[0;36mLlamaDecoderLayer\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mLlamaDecoderLayer\u001b[39;00m(nn\u001b[38;5;241m.\u001b[39mModule):\n\u001b[0;32m----> 2\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, config: \u001b[43mLlamaConfig\u001b[49m, layer_idx: \u001b[38;5;28mint\u001b[39m):\n\u001b[1;32m      3\u001b[0m         \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[1;32m      4\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhidden_size \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mhidden_size\n",
      "\u001b[0;31mNameError\u001b[0m: name 'LlamaConfig' is not defined"
     ]
    }
   ],
   "source": [
    "class LlamaDecoderLayer(nn.Module):\n",
    "    def __init__(self, config: LlamaConfig, layer_idx: int):\n",
    "        super().__init__()\n",
    "        self.hidden_size = config.hidden_size\n",
    "\n",
    "        self.self_attn = LLAMA_ATTENTION_CLASSES[config._attn_implementation](config=config, layer_idx=layer_idx)\n",
    "\n",
    "        self.mlp = LlamaMLP(config)\n",
    "        self.input_layernorm = LlamaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n",
    "        self.post_attention_layernorm = LlamaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states: torch.Tensor,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        position_ids: Optional[torch.LongTensor] = None,\n",
    "        past_key_value: Optional[Tuple[torch.Tensor]] = None,\n",
    "        output_attentions: Optional[bool] = False,\n",
    "        use_cache: Optional[bool] = False,\n",
    "        **kwargs,\n",
    "    ) -> Tuple[torch.FloatTensor, Optional[Tuple[torch.FloatTensor, torch.FloatTensor]]]:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            hidden_states (`torch.FloatTensor`): input to the layer of shape `(batch, seq_len, embed_dim)`\n",
    "            attention_mask (`torch.FloatTensor`, *optional*):\n",
    "                attention mask of size `(batch_size, sequence_length)` if flash attention is used or `(batch_size, 1,\n",
    "                query_sequence_length, key_sequence_length)` if default attention is used.\n",
    "            output_attentions (`bool`, *optional*):\n",
    "                Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n",
    "                returned tensors for more detail.\n",
    "            use_cache (`bool`, *optional*):\n",
    "                If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding\n",
    "                (see `past_key_values`).\n",
    "            past_key_value (`Tuple(torch.FloatTensor)`, *optional*): cached past key and value projection states\n",
    "        \"\"\"\n",
    "        if \"padding_mask\" in kwargs:\n",
    "            warnings.warn(\n",
    "                \"Passing `padding_mask` is deprecated and will be removed in v4.37. Please make sure use `attention_mask` instead.`\"\n",
    "            )\n",
    "\n",
    "        residual = hidden_states\n",
    "\n",
    "        hidden_states = self.input_layernorm(hidden_states)\n",
    "\n",
    "        # Self Attention\n",
    "        hidden_states, self_attn_weights, present_key_value = self.self_attn(\n",
    "            hidden_states=hidden_states,\n",
    "            attention_mask=attention_mask,\n",
    "            position_ids=position_ids,\n",
    "            past_key_value=past_key_value,\n",
    "            output_attentions=output_attentions,\n",
    "            use_cache=use_cache,\n",
    "            **kwargs,\n",
    "        )\n",
    "        hidden_states = residual + hidden_states\n",
    "\n",
    "        # Fully Connected\n",
    "        residual = hidden_states\n",
    "        hidden_states = self.post_attention_layernorm(hidden_states)\n",
    "        hidden_states = self.mlp(hidden_states)\n",
    "        hidden_states = residual + hidden_states\n",
    "\n",
    "        outputs = (hidden_states,)\n",
    "\n",
    "        if output_attentions:\n",
    "            outputs += (self_attn_weights,)\n",
    "\n",
    "        if use_cache:\n",
    "            outputs += (present_key_value,)\n",
    "\n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLama Configurations "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- vocab_size (int, optional, defaults to 32000) — Vocabulary size of the LLaMA model. Defines the number of different tokens that can be represented by the inputs_ids passed when calling LlamaModel\n",
    "\n",
    "- hidden_size (int, optional, defaults to 4096) — Dimension of the hidden representations.\n",
    "\n",
    "- intermediate_size (int, optional, defaults to 11008) — Dimension of the MLP representations.\n",
    "\n",
    "- num_hidden_layers (int, optional, defaults to 32) — Number of hidden layers in the Transformer decoder.\n",
    "\n",
    "- num_attention_heads (int, optional, defaults to 32) — Number of attention heads for each attention layer in the Transformer decoder.\n",
    "\n",
    "- num_key_value_heads (int, optional) — This is the number of key_value heads that should be used to implement Grouped Query Attention. If num_key_value_heads=num_attention_heads, the model will use Multi Head Attention (MHA), if num_key_value_heads=1 the model will use Multi Query Attention (MQA) otherwise GQA is used. When converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed by meanpooling all the original heads within that group. For more details checkout [this paper](https://arxiv.org/pdf/2305.13245.pdf). If it is not specified, will default to num_attention_heads`.\n",
    "\n",
    "- hidden_act (str or function, optional, defaults to \"silu\") — The non-linear activation function (function or string) in the decoder.\n",
    "\n",
    "- max_position_embeddings (int, optional, defaults to 2048) — The maximum sequence length that this model might ever be used with. Llama 1 supports up to 2048 tokens, Llama 2 up to 4096, CodeLlama up to 16384.\n",
    "\n",
    "- initializer_range (float, optional, defaults to 0.02) — The standard deviation of the truncated_normal_initializer for initializing all weight matrices.\n",
    "\n",
    "- rms_norm_eps (float, optional, defaults to 1e-06) — The epsilon used by the rms normalization layers.\n",
    "\n",
    "- use_cache (bool, optional, defaults to True) — Whether or not the model should return the last key/values attentions (not used by all models). Only relevant if config.is_decoder=True.\n",
    "\n",
    "- pad_token_id (int, optional) — Padding token id.\n",
    "- bos_token_id (int, optional, defaults to 1) — Beginning of stream token id.\n",
    "- eos_token_id (int, optional, defaults to 2) — End of stream token id.\n",
    "\n",
    "- pretraining_tp (int, optional, defaults to 1) — Experimental feature. Tensor parallelism rank used during pretraining. Please refer to this document to understand more about it. This value is necessary to ensure exact reproducibility of the pretraining results. Please refer to this issue.\n",
    "\n",
    "- tie_word_embeddings (bool, optional, defaults to False) — Whether to tie weight embeddings\n",
    "\n",
    "- rope_theta (float, optional, defaults to 10000.0) — The base period of the RoPE embeddings.\n",
    "- rope_scaling (Dict, optional) — Dictionary containing the scaling configuration for the RoPE embeddings. Currently supports two scaling strategies: linear and dynamic. Their scaling factor must be a float greater than 1. The expected format is {\"type\": strategy name, \"factor\": scaling factor}. When using this flag, don’t update max_position_embeddings to the expected new maximum. See the following thread for more information on how these scaling strategies behave: https://www.reddit.com/r/LocalLLaMA/comments/14mrgpr/dynamically_scaled_rope_further_increases/. This is an experimental feature, subject to breaking API changes in future versions.\n",
    "\n",
    "- attention_bias (bool, defaults to False, optional, defaults to False) — Whether to use a bias in the query, key, value and output projection layers during self-attention.\n",
    "- attention_dropout (float, optional, defaults to 0.0) — The dropout ratio for the attention probabilities.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "my_project_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
