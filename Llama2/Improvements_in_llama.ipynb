{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additions in llama-2 model compared ot the original transformer:- \n",
    "\n",
    "1. Rotary Positional Embeddings\n",
    "2. SwiGLU activation function\n",
    "3. RMSProp\n",
    "4. KV Caching \n",
    "5. Grouped Query attention "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RMS Norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self, dim: int, eps: float = 1e-6):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.weight = nn.Parameter(torch.ones(dim))\n",
    "\n",
    "    def _norm(self, x: torch.Tensor):\n",
    "        # (bsz, seq_len, dim) * (bsz, seq_len, 1) = (bsz, seq_len, dim)\n",
    "        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        # (dim) * (bsz, seq_len, dim) = (bsz, seq_len, dim)\n",
    "        return self.weight * self._norm(x.float()).type_as(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rotary Embeddings for positional encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "def precompute_theta_pos_freq(head_dim, seq_len, theta = 10000):\n",
    "\n",
    "    assert head_dim%2 == 0, \"Dimension of head must by divissible by 2\"\n",
    "\n",
    "    # theta_i = 10000^(-2(i-1)/dim) for i = [1,2,3.....dim/2]\n",
    "\n",
    "    theta_numerator = torch.arange(0, head_dim, 2).float()\n",
    "\n",
    "    theta = 1.0/ (theta **(theta_numerator/head_dim))\n",
    "\n",
    "    m = torch.arange(seq_len)\n",
    "\n",
    "    freqs = torch.outer(m,theta).float()\n",
    "\n",
    "    #                                   magnitude       angle\n",
    "    freqs_complex = torch.polar(torch.ones_like(freqs), freqs) \n",
    "\n",
    "    return freqs_complex\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_rotary_embeds(x, freqs_complex):\n",
    "\n",
    "    # Separate the last dimension pairs of two values, representing the real and imaginary parts of the complex number\n",
    "    # Two consecutive values will become a single complex number\n",
    "\n",
    "\n",
    "    # H -> no.of heads; can be num_heads for Query and num_kv_heads for Key\n",
    "\n",
    "    # (bsz, seq_len, H , head_dim) -> (bsz, seq_len, H, head_dim/2)\n",
    "    x_complex = torch.view_as_complex(x.float().reshape(*x.shape[:-1], -1, 2))\n",
    "\n",
    "\n",
    "    # Reshape the freqs_complex tensor to match the shape of the x_complex tensor. \n",
    "    # (seq_len, head_dim/2) --> (1, seq_len, 1, head_dim/2)\n",
    "    freqs_complex = freqs_complex.unsqueeze(0).unsqueeze(2)\n",
    "\n",
    "\n",
    "    # Multiply each complex number in the x_complex tensor by the corresponding complex number in the freqs_complex tensor\n",
    "    # Which results in the rotation of the complex number as shown in the Figure 1 of the paper\n",
    "    # (bsz, seq_len, H, head_dim/2) * (1, seq_len, 1, head_dim/2) = (bsz, seq_len, H, head_dim/2)\n",
    "    x_rotated = x_complex * freqs_complex\n",
    "\n",
    "\n",
    "    # Convert the complex number back to the real number\n",
    "    # (bsz, seq_len, H, head_dim/2) -> (bsz, seq_len, H, head_dim/2, 2)\n",
    "    x_out = torch.view_as_real(x_rotated)\n",
    "\n",
    "\n",
    "    # (bsz, seq_len, H, head_dim/2, 2) -> (bsz, seq_len, H, head_dim)\n",
    "    x_out = x_out.reshape(*x.shape)\n",
    "    \n",
    "\n",
    "    return x_out.type_as(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.0000+0.0000j],\n",
       "        [ 0.5403+0.8415j],\n",
       "        [-0.4161+0.9093j]])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freqs_complex = precompute_theta_pos_freq(head_dim = 2, seq_len = 3, theta = 10000)\n",
    "freqs_complex\n",
    "\n",
    "# head_dim = embed_dim // n_heads\n",
    "# here, 8//4 = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 3, 4, 2]),\n",
       " tensor([[[[0.8417, 0.5510],\n",
       "           [0.0214, 0.3450],\n",
       "           [0.3619, 0.2223],\n",
       "           [0.6088, 0.2576]],\n",
       " \n",
       "          [[0.0726, 0.3607],\n",
       "           [0.2889, 0.7267],\n",
       "           [0.5482, 0.7990],\n",
       "           [0.6860, 0.6757]],\n",
       " \n",
       "          [[0.6538, 0.7353],\n",
       "           [0.9691, 0.1316],\n",
       "           [0.7166, 0.7213],\n",
       "           [0.0201, 0.1098]]]]))"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.rand(1,3,4,2)\n",
    "\n",
    "# batch_size, seq_len, n_kv_heads, head_dim\n",
    "x.shape, x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.8417,  0.5510],\n",
       "          [ 0.0214,  0.3450],\n",
       "          [ 0.3619,  0.2223],\n",
       "          [ 0.6088,  0.2576]],\n",
       "\n",
       "         [[-0.2643,  0.2560],\n",
       "          [-0.4555,  0.6357],\n",
       "          [-0.3761,  0.8930],\n",
       "          [-0.1979,  0.9423]],\n",
       "\n",
       "         [[-0.9407,  0.2885],\n",
       "          [-0.5230,  0.8264],\n",
       "          [-0.9541,  0.3514],\n",
       "          [-0.1082, -0.0274]]]])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "apply_rotary_embeds(x, freqs_complex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grouped Query Attention\n",
    "\n",
    "Without KV caching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to expand the vector 'x' for grouped query attention\n",
    "\n",
    "def repeat_kv(x, n_rep):\n",
    "\n",
    "    batch_size, seq_len, n_kv_heads, head_dim = x.shape\n",
    "\n",
    "    if n_rep == 1:\n",
    "        return x\n",
    "    \n",
    "    else:\n",
    "        # (bsz, seq_len, n_kv_heads, 1, head_dim)\n",
    "        # --> (bsz, seq_len, n_kv_heads, n_rep, head_dim)\n",
    "        # --> (bsz, seq_len, n_kv_heads * n_rep, head_dim)\n",
    "        return (\n",
    "            x[:, :, :, None, :]\n",
    "            .expand(batch_size, seq_len, n_kv_heads, n_rep, head_dim)\n",
    "            .reshape(batch_size, seq_len, n_kv_heads * n_rep, head_dim)\n",
    "        )\n",
    "\n",
    "\n",
    "# Grouped query attention \n",
    "def GQ_attention_fwd(x, n_heads, n_kv_heads, embed_dim):\n",
    "\n",
    "    n_kv_heads = n_heads if n_kv_heads is None else n_kv_heads\n",
    "\n",
    "    n_heads_q = n_heads\n",
    "\n",
    "    n_rep = n_heads_q//n_kv_heads\n",
    "\n",
    "    head_dim = embed_dim//n_heads\n",
    "\n",
    "    Wq = nn.Linear(embed_dim, n_heads*head_dim, bias=False)\n",
    "    Wk = nn.Linear(embed_dim, n_kv_heads*head_dim, bias=False)\n",
    "    Wv = nn.Linear(embed_dim, n_kv_heads*head_dim, bias=False)\n",
    "    Wo = nn.Linear(n_heads*head_dim, embed_dim, bias=False)\n",
    "\n",
    "    batch_size, seq_len, _ = x.shape\n",
    "\n",
    "    # (bsz, seq_len, embed_dim)\n",
    "    xq = Wq(x)\n",
    "\n",
    "    # (bsz, seq_len, h_kv * head_dim)\n",
    "    xk = Wk(x)\n",
    "    xv = Wv(x)\n",
    "\n",
    "    # (bsz, seq_len, n_heads, head_dim)\n",
    "    xq = xq.view(batch_size, seq_len, n_heads_q, head_dim)\n",
    "\n",
    "    # (bsz, seq_len, h_kv, head_dim)\n",
    "    xk = xk.view(batch_size, seq_len, n_kv_heads, head_dim)\n",
    "    xv = xv.view(batch_size, seq_len, n_kv_heads, head_dim)\n",
    "\n",
    "\n",
    "    print(\"Before applying Rotary embeddings :- \")\n",
    "    print(\"Q = \", xq)\n",
    "    print(xq.shape)\n",
    "    print()\n",
    "    print(\"K = \", xk)\n",
    "    print(xk.shape)\n",
    "    print()\n",
    "\n",
    "    ##################################\n",
    "    ### Applying rotary embeddings ###\n",
    "\n",
    "    freqs_complex = precompute_theta_pos_freq(head_dim = head_dim, seq_len = seq_len, theta = 10000)\n",
    "\n",
    "    # (bsz, seq_len, n_heads, head_dim) -> (bsz, seq_len, n_heads, head_dim)\n",
    "    xq = apply_rotary_embeds(xq, freqs_complex)\n",
    "\n",
    "    # (bsz, seq_len, n_kv_heads, head_dim) -> (bsz, seq_len, n_kv_heads, head_dim)\n",
    "    xk = apply_rotary_embeds(xk, freqs_complex)\n",
    "\n",
    "    #####################################\n",
    "\n",
    "    print(\"After applying Rotary embeddings :- \")\n",
    "    print(\"Q = \", xq)\n",
    "    print(xq.shape)\n",
    "    print()\n",
    "    print(\"K = \", xk)\n",
    "    print(xk.shape)\n",
    "    print()\n",
    "\n",
    "\n",
    "    keys = repeat_kv(xk, n_rep)\n",
    "    values = repeat_kv(xv, n_rep)\n",
    "\n",
    "    print(\"Keys and Values after repeating for GQA\")\n",
    "    print(\"keys = \",keys.shape,keys)\n",
    "    print()\n",
    "    print(\"values = \",values.shape,values)\n",
    "    print()\n",
    "\n",
    "    xq = xq.transpose(1, 2)\n",
    "\n",
    "    # (bsz, n_heads, seq_len, head_dim)\n",
    "    keys = keys.transpose(1, 2)\n",
    "    values = values.transpose(1, 2)\n",
    "\n",
    "    # (bsz, n_heads, seq_len_q, head_dim) MATMUL (bsz, n_heads, head_dim, seq_len) -> (bsz, n_heads, seq_len_q, seq_len)\n",
    "    scores = torch.matmul(xq, keys.transpose(2, 3)) / math.sqrt(head_dim)\n",
    "\n",
    "    # (bsz, n_heads, seq_len_q, seq_len)\n",
    "    scores = F.softmax(scores.float(), dim=-1).type_as(xq)\n",
    "    print(\"Attention scores = \", scores)\n",
    "    print()\n",
    "\n",
    "    # (bsz, n_heads, seq_len_q, seq_len) MATMUL (bsz, n_heads, seq_len, head_dim) -> (bsz, n_heads, seq_len_q, head_dim)\n",
    "    output = torch.matmul(scores, values)\n",
    "\n",
    "    # ((bsz, n_heads, seq_len_q, head_dim) -> (bsz, seq_len_q, dim)\n",
    "    output = (output.transpose(1, 2).contiguous().view(batch_size, seq_len, -1))\n",
    "    print(\"Attention values = \",output)\n",
    "    print()\n",
    "\n",
    "    # (bsz, seq_len_q, dim)\n",
    "    return Wo(output)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 3, 8]),\n",
       " tensor([[[0.3885, 0.6270, 0.3206, 0.4409, 0.3074, 0.2796, 0.3976, 0.3765],\n",
       "          [0.2976, 0.8556, 0.0485, 0.8291, 0.5157, 0.7897, 0.8054, 0.7738],\n",
       "          [0.3322, 0.6595, 0.4512, 0.7347, 0.6833, 0.5190, 0.5468, 0.0774]]]))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.rand(1,3,8)\n",
    "\n",
    "# batch_size, seq_len, embedding_dim\n",
    "x.shape, x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before applying Rotary embeddings :- \n",
      "Q =  tensor([[[[-0.1299,  0.2407],\n",
      "          [-0.1696,  0.0881],\n",
      "          [-0.1522, -0.0535],\n",
      "          [-0.5082, -0.1717]],\n",
      "\n",
      "         [[ 0.0579,  0.1978],\n",
      "          [-0.2157,  0.2064],\n",
      "          [-0.0879,  0.0266],\n",
      "          [-0.4180,  0.3014]],\n",
      "\n",
      "         [[-0.0530,  0.4617],\n",
      "          [-0.1664,  0.1750],\n",
      "          [-0.4898, -0.0712],\n",
      "          [-0.6477,  0.2386]]]], grad_fn=<ViewBackward0>)\n",
      "torch.Size([1, 3, 4, 2])\n",
      "\n",
      "K =  tensor([[[[-0.1565, -0.1516],\n",
      "          [-0.3590,  0.3951]],\n",
      "\n",
      "         [[ 0.1555, -0.3181],\n",
      "          [-0.1116,  0.2460]],\n",
      "\n",
      "         [[-0.0209, -0.1935],\n",
      "          [-0.2571,  0.4130]]]], grad_fn=<ViewBackward0>)\n",
      "torch.Size([1, 3, 2, 2])\n",
      "\n",
      "After applying Rotary embeddings :- \n",
      "Q =  tensor([[[[-0.1299,  0.2407],\n",
      "          [-0.1696,  0.0881],\n",
      "          [-0.1522, -0.0535],\n",
      "          [-0.5082, -0.1717]],\n",
      "\n",
      "         [[-0.1351,  0.1556],\n",
      "          [-0.2903, -0.0700],\n",
      "          [-0.0698, -0.0596],\n",
      "          [-0.4795, -0.1889]],\n",
      "\n",
      "         [[-0.3978, -0.2403],\n",
      "          [-0.0899, -0.2241],\n",
      "          [ 0.2686, -0.4158],\n",
      "          [ 0.0526, -0.6882]]]], grad_fn=<ViewBackward0>)\n",
      "torch.Size([1, 3, 4, 2])\n",
      "\n",
      "K =  tensor([[[[-0.1565, -0.1516],\n",
      "          [-0.3590,  0.3951]],\n",
      "\n",
      "         [[ 0.3517, -0.0410],\n",
      "          [-0.2673,  0.0390]],\n",
      "\n",
      "         [[ 0.1846,  0.0615],\n",
      "          [-0.2686, -0.4056]]]], grad_fn=<ViewBackward0>)\n",
      "torch.Size([1, 3, 2, 2])\n",
      "\n",
      "Keys and Values after repeating for GQA\n",
      "keys =  torch.Size([1, 3, 4, 2]) tensor([[[[-0.1565, -0.1516],\n",
      "          [-0.1565, -0.1516],\n",
      "          [-0.3590,  0.3951],\n",
      "          [-0.3590,  0.3951]],\n",
      "\n",
      "         [[ 0.3517, -0.0410],\n",
      "          [ 0.3517, -0.0410],\n",
      "          [-0.2673,  0.0390],\n",
      "          [-0.2673,  0.0390]],\n",
      "\n",
      "         [[ 0.1846,  0.0615],\n",
      "          [ 0.1846,  0.0615],\n",
      "          [-0.2686, -0.4056],\n",
      "          [-0.2686, -0.4056]]]], grad_fn=<UnsafeViewBackward0>)\n",
      "\n",
      "values =  torch.Size([1, 3, 4, 2]) tensor([[[[ 1.9356e-02, -2.5811e-01],\n",
      "          [ 1.9356e-02, -2.5811e-01],\n",
      "          [-2.7033e-04,  1.7361e-01],\n",
      "          [-2.7033e-04,  1.7361e-01]],\n",
      "\n",
      "         [[ 1.2156e-01, -2.5068e-01],\n",
      "          [ 1.2156e-01, -2.5068e-01],\n",
      "          [-2.0269e-01,  2.8963e-02],\n",
      "          [-2.0269e-01,  2.8963e-02]],\n",
      "\n",
      "         [[ 2.3913e-01, -4.8252e-01],\n",
      "          [ 2.3913e-01, -4.8252e-01],\n",
      "          [-3.7998e-01,  1.1804e-01],\n",
      "          [-3.7998e-01,  1.1804e-01]]]], grad_fn=<UnsafeViewBackward0>)\n",
      "\n",
      "Attention scores =  tensor([[[[0.3359, 0.3266, 0.3375],\n",
      "          [0.3384, 0.3263, 0.3353],\n",
      "          [0.3667, 0.3120, 0.3213]],\n",
      "\n",
      "         [[0.3424, 0.3244, 0.3331],\n",
      "          [0.3548, 0.3179, 0.3273],\n",
      "          [0.3451, 0.3284, 0.3265]],\n",
      "\n",
      "         [[0.3306, 0.3318, 0.3375],\n",
      "          [0.3289, 0.3324, 0.3387],\n",
      "          [0.2926, 0.3306, 0.3767]],\n",
      "\n",
      "         [[0.3250, 0.3283, 0.3467],\n",
      "          [0.3230, 0.3284, 0.3486],\n",
      "          [0.2721, 0.3247, 0.4031]]]], grad_fn=<SoftmaxBackward0>)\n",
      "\n",
      "Attention values =  tensor([[[ 0.1269, -0.3314,  0.1257, -0.3305, -0.1956,  0.1069, -0.1984,\n",
      "           0.1069],\n",
      "         [ 0.1264, -0.3309,  0.1238, -0.3292, -0.1962,  0.1067, -0.1991,\n",
      "           0.1067],\n",
      "         [ 0.1219, -0.3279,  0.1247, -0.3289, -0.2102,  0.1048, -0.2191,\n",
      "           0.1042]]], grad_fn=<ViewBackward0>)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "atten_values = GQ_attention_fwd(x, n_heads = 4, n_kv_heads = 2, embed_dim = 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0505,  0.1723, -0.0181, -0.3009, -0.0628, -0.0622, -0.0380,\n",
       "          -0.1810],\n",
       "         [ 0.0535,  0.1749, -0.0165, -0.3008, -0.0634, -0.0638, -0.0403,\n",
       "          -0.1846],\n",
       "         [ 0.0523,  0.1754, -0.0178, -0.2965, -0.0640, -0.0633, -0.0405,\n",
       "          -0.1850]]], grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Projected attetion values\n",
    "atten_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Tensor Shape: torch.Size([1, 3, 2, 4])\n",
      "\n",
      "Original Tensor : tensor([[[[0.3553, 0.5873, 0.9951, 0.3988],\n",
      "          [0.3021, 0.7420, 0.4973, 0.3956]],\n",
      "\n",
      "         [[0.6649, 0.3215, 0.3432, 0.2998],\n",
      "          [0.2185, 0.9958, 0.5237, 0.6992]],\n",
      "\n",
      "         [[0.1545, 0.1401, 0.7353, 0.0985],\n",
      "          [0.5105, 0.2776, 0.4774, 0.7729]]]])\n",
      "\n",
      "x[:, :, :, None, :] =  tensor([[[[[0.3553, 0.5873, 0.9951, 0.3988]],\n",
      "\n",
      "          [[0.3021, 0.7420, 0.4973, 0.3956]]],\n",
      "\n",
      "\n",
      "         [[[0.6649, 0.3215, 0.3432, 0.2998]],\n",
      "\n",
      "          [[0.2185, 0.9958, 0.5237, 0.6992]]],\n",
      "\n",
      "\n",
      "         [[[0.1545, 0.1401, 0.7353, 0.0985]],\n",
      "\n",
      "          [[0.5105, 0.2776, 0.4774, 0.7729]]]]])\n",
      "\n",
      ".expand =  tensor([[[[[0.3553, 0.5873, 0.9951, 0.3988],\n",
      "           [0.3553, 0.5873, 0.9951, 0.3988]],\n",
      "\n",
      "          [[0.3021, 0.7420, 0.4973, 0.3956],\n",
      "           [0.3021, 0.7420, 0.4973, 0.3956]]],\n",
      "\n",
      "\n",
      "         [[[0.6649, 0.3215, 0.3432, 0.2998],\n",
      "           [0.6649, 0.3215, 0.3432, 0.2998]],\n",
      "\n",
      "          [[0.2185, 0.9958, 0.5237, 0.6992],\n",
      "           [0.2185, 0.9958, 0.5237, 0.6992]]],\n",
      "\n",
      "\n",
      "         [[[0.1545, 0.1401, 0.7353, 0.0985],\n",
      "           [0.1545, 0.1401, 0.7353, 0.0985]],\n",
      "\n",
      "          [[0.5105, 0.2776, 0.4774, 0.7729],\n",
      "           [0.5105, 0.2776, 0.4774, 0.7729]]]]])\n",
      "\n",
      "Result Tensor Values:\n",
      " tensor([[[[0.3553, 0.5873, 0.9951, 0.3988],\n",
      "          [0.3553, 0.5873, 0.9951, 0.3988],\n",
      "          [0.3021, 0.7420, 0.4973, 0.3956],\n",
      "          [0.3021, 0.7420, 0.4973, 0.3956]],\n",
      "\n",
      "         [[0.6649, 0.3215, 0.3432, 0.2998],\n",
      "          [0.6649, 0.3215, 0.3432, 0.2998],\n",
      "          [0.2185, 0.9958, 0.5237, 0.6992],\n",
      "          [0.2185, 0.9958, 0.5237, 0.6992]],\n",
      "\n",
      "         [[0.1545, 0.1401, 0.7353, 0.0985],\n",
      "          [0.1545, 0.1401, 0.7353, 0.0985],\n",
      "          [0.5105, 0.2776, 0.4774, 0.7729],\n",
      "          [0.5105, 0.2776, 0.4774, 0.7729]]]])\n"
     ]
    }
   ],
   "source": [
    "# repeat_kv :- Operations and intermediate outputs\n",
    "\n",
    "import torch\n",
    "\n",
    "batch_size = 1\n",
    "seq_len = 3\n",
    "n_kv_heads = 2\n",
    "n_rep = 2\n",
    "head_dim = 4\n",
    "\n",
    "# random 4D tensor\n",
    "x = torch.rand(batch_size, seq_len, n_kv_heads, head_dim)\n",
    "\n",
    "result = (\n",
    "    x[:, :, :, None, :]\n",
    "    .expand(batch_size, seq_len, n_kv_heads, n_rep, head_dim)\n",
    "    .reshape(batch_size, seq_len, n_kv_heads * n_rep, head_dim)\n",
    ")\n",
    "\n",
    "# Displaying shapes and values\n",
    "print(\"Original Tensor Shape:\", x.shape)\n",
    "print()\n",
    "\n",
    "print(\"Original Tensor :\", x)\n",
    "print()\n",
    "\n",
    "print(\"x[:, :, :, None, :] = \", x[:, :, :, None, :])\n",
    "print()\n",
    "\n",
    "print(\".expand = \", x[:, :, :, None, :].expand(batch_size, seq_len, n_kv_heads, n_rep, head_dim))\n",
    "print()\n",
    "\n",
    "print(\"Result Tensor Values:\\n\", result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import einops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "my_project_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
