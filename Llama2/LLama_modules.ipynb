{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Llama 2 modules\n",
    "\n",
    "- **Rotary positional embeddings** -> applied to the computed Q and K vectors in the self_attention part\n",
    "- **Grouped Query Attention** -> Tradeoff between Mutli-Query attention and MHA, balances memory bandwidth requirements and speedup\n",
    "- **KV Caching** -> For faster computation and better memory management\n",
    "- **SwiGLU activation function**\n",
    "- **RMS Norm**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rotary positional embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precompute_theta_pos_freq(head_dim, seq_len, theta = 10000):\n",
    "\n",
    "    assert head_dim%2 == 0, \"Dimension of head must by divisible by 2\"\n",
    "\n",
    "    # theta_i = 10000^(-2(i-1)/dim) for i = [1,2,3.....dim/2]\n",
    "\n",
    "    theta_numerator = torch.arange(0, head_dim, 2).float()\n",
    "\n",
    "    theta = 1.0/ (theta **(theta_numerator/head_dim))\n",
    "\n",
    "    m = torch.arange(seq_len)\n",
    "\n",
    "    freqs = torch.outer(m,theta).float()\n",
    "\n",
    "    #                                   magnituda       angle\n",
    "    freqs_complex = torch.polar(torch.ones_like(freqs), freqs) \n",
    "\n",
    "    return freqs_complex\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_rotary_embeds(x, freqs_complex):\n",
    "\n",
    "    # Separate the last dimension pairs of two values, representing the real and imaginary parts of the complex number\n",
    "    # Two consecutive values will become a single complex number\n",
    "\n",
    "\n",
    "    # H -> no.of heads; can be num_heads for Query and num_kv_heads for Key\n",
    "\n",
    "    # (bsz, seq_len, H , head_dim) -> (bsz, seq_len, H, head_dim/2)\n",
    "\n",
    "    # (bsz, seq_len, H , head_dim) -> (bsz, seq_len, H, head_dim/2)\n",
    "    x_complex = torch.view_as_complex(x.float().reshape(*x.shape[:-1], -1, 2))\n",
    "\n",
    "\n",
    "    # Reshape the freqs_complex tensor to match the shape of the x_complex tensor. \n",
    "    # (seq_len, head_dim/2) --> (1, seq_len, 1, head_dim/2)\n",
    "    freqs_complex = freqs_complex.unsqueeze(0).unsqueeze(2)\n",
    "\n",
    "\n",
    "    # (bsz, seq_len, H, head_dim/2) * (1, seq_len, 1, head_dim/2) = (bsz, seq_len, H, head_dim/2)\n",
    "    x_rotated = x_complex * freqs_complex\n",
    "\n",
    "\n",
    "    # Convert the complex number back to the real number\n",
    "    # (bsz, seq_len, H, head_dim/2) -> (bsz, seq_len, H, head_dim/2, 2)\n",
    "    x_out = torch.view_as_real(x_rotated)\n",
    "\n",
    "\n",
    "    # (bsz, seq_len, H, head_dim/2, 2) -> (bsz, seq_len, H, head_dim)\n",
    "    x_out = x_out.reshape(*x.shape)\n",
    "    \n",
    "\n",
    "    return x_out.type_as(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, n_heads ,embed_dim, ):\n",
    "        super().__init__()\n",
    "\n",
    "        # Indicates the number of heads for the Queries\n",
    "        self.num_heads = n_heads\n",
    "\n",
    "        # Indicates the dimension of each head, that is, the part of the embedding that each head will be responsible for\n",
    "        self.head_dim = embed_dim // n_heads\n",
    "\n",
    "        self.wq = nn.Linear(embed_dim, n_heads * self.head_dim, bias=False)\n",
    "        self.wk = nn.Linear(embed_dim, n_heads * self.head_dim, bias=False)\n",
    "        self.wv = nn.Linear(embed_dim, n_heads * self.head_dim, bias=False)\n",
    "        self.wo = nn.Linear(n_heads * self.head_dim, embed_dim, bias=False)\n",
    "\n",
    "\n",
    "    def precompute_theta_pos_freq(self, head_dim, seq_len, theta = 10000):\n",
    "\n",
    "        assert head_dim%2 == 0, \"Dimension of head must by divisible by 2\"\n",
    "\n",
    "        # theta_i = 10000^(-2(i-1)/dim) for i = [1,2,3.....dim/2]\n",
    "\n",
    "        theta_numerator = torch.arange(0, head_dim, 2).float()\n",
    "\n",
    "        theta = 1.0/ (theta **(theta_numerator/head_dim))\n",
    "\n",
    "        m = torch.arange(seq_len)\n",
    "\n",
    "        freqs = torch.outer(m,theta).float()\n",
    "\n",
    "        #                                   magnituda       angle\n",
    "        freqs_complex = torch.polar(torch.ones_like(freqs), freqs) \n",
    "\n",
    "        return freqs_complex\n",
    "    \n",
    "\n",
    "    def apply_rotary_embeds(self, x, freqs_complex):\n",
    "\n",
    "        # Separate the last dimension pairs of two values, representing the real and imaginary parts of the complex number\n",
    "        # Two consecutive values will become a single complex number\n",
    "\n",
    "\n",
    "        # H -> no.of heads; can be num_heads for Query and num_kv_heads for Key\n",
    "\n",
    "        # (bsz, seq_len, H , head_dim) -> (bsz, seq_len, H, head_dim/2)\n",
    "        x_complex = torch.view_as_complex(x.float().reshape(*x.shape[:-1], -1, 2))\n",
    "\n",
    "\n",
    "        # (seq_len, head_dim/2) --> (1, seq_len, 1, head_dim/2)\n",
    "        freqs_complex = freqs_complex.unsqueeze(0).unsqueeze(2)\n",
    "\n",
    "\n",
    "\n",
    "        # (bsz, seq_len, H, head_dim/2) * (1, seq_len, 1, head_dim/2) = (bsz, seq_len, H, head_dim/2)\n",
    "        x_rotated = x_complex * freqs_complex\n",
    "\n",
    "\n",
    "        # Convert the complex number back to the real number\n",
    "        # (bsz, seq_len, H, head_dim/2) -> (bsz, seq_len, H, head_dim/2, 2)\n",
    "        x_out = torch.view_as_real(x_rotated)\n",
    "\n",
    "        # (bsz, seq_len, H, head_dim/2, 2) -> (bsz, seq_len, H, head_dim)\n",
    "        x_out = x_out.reshape(*x.shape)\n",
    "        \n",
    "        return x_out.type_as(x)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x,\n",
    "        freqs_complex,\n",
    "        attn_mask\n",
    "    ):                                # While inferencing:- \n",
    "        bsz, seq_len, _ = x.shape   # (bsz, 1, embed_dim)\n",
    "\n",
    "        Q = self.wq(x)\n",
    "        K = self.wk(x)\n",
    "        V = self.wv(x)\n",
    "\n",
    "\n",
    "        if attn_mask is not None:\n",
    "        # ensure attn_mask's dim is 3\n",
    "            if attn_mask.dim() == 2:\n",
    "                correct_2d_size = (seq_len, seq_len)\n",
    "                if attn_mask.shape != correct_2d_size:\n",
    "                    raise RuntimeError(f\"The shape of the 2D attn_mask is {attn_mask.shape}, but should be {correct_2d_size}.\")\n",
    "                attn_mask = attn_mask.unsqueeze(0)\n",
    "            elif attn_mask.dim() == 3:\n",
    "                correct_3d_size = (bsz * self.num_heads, seq_len, seq_len)\n",
    "                if attn_mask.shape != correct_3d_size:\n",
    "                    raise RuntimeError(f\"The shape of the 3D attn_mask is {attn_mask.shape}, but should be {correct_3d_size}.\")\n",
    "            else:\n",
    "                raise RuntimeError(f\"attn_mask's dimension {attn_mask.dim()} is not supported\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        Q = Q.view(bsz, seq_len, self.n_heads, self.head_dim)\n",
    "        K = K.view(bsz, seq_len, self.n_heads, self.head_dim)\n",
    "        V = V.view(bsz, seq_len, self.n_heads, self.head_dim)\n",
    "\n",
    "        Q = self.apply_rotary_embeddings(Q, freqs_complex)\n",
    "        K = self.apply_rotary_embeddings(K, freqs_complex)\n",
    "\n",
    "        # (bsz, 1, H_Q, Head_Dim) -> (bsz, H_Q, 1, Head_Dim)\n",
    "        Q = Q.transpose(1, 2)\n",
    "        # (bsz, Seq_Len_KV, H_Q, Head_Dim) -> (bsz, H_Q, Seq_Len_KV, Head_Dim)\n",
    "        K = K.transpose(1, 2)\n",
    "        # (bsz, Seq_Len_KV, H_Q, Head_Dim) -> (bsz, H_Q, Seq_Len_KV, Head_Dim)\n",
    "        V = V.transpose(1, 2)\n",
    "\n",
    "        # (bsz, H_Q, 1, Head_Dim) @ (bsz, H_Q, Head_Dim, Seq_Len_KV) -> (bsz, H_Q, 1, Seq_Len_KV)\n",
    "        scores = torch.matmul(Q, K.transpose(2, 3)) / math.sqrt(self.head_dim)\n",
    "\n",
    "        if attn_mask is not None:\n",
    "            scores = scores + attn_mask \n",
    "\n",
    "        # (bsz, H_Q, 1, Seq_Len_KV) -> (bsz, H_Q, 1, Seq_Len_KV)\n",
    "        scores = F.softmax(scores.float(), dim=-1).type_as(Q)\n",
    "\n",
    "        # (bsz, H_Q, 1, Seq_Len) @ (bsz, H_Q, Seq_Len_KV, Head_Dim) -> (bsz, H_Q, 1, Head_Dim)\n",
    "        output = torch.matmul(scores, V)\n",
    "        # (bsz, H_Q, 1, Head_Dim) -> (bsz, 1, H_Q, Head_Dim) -> (bsz, 1, Dim)\n",
    "        output = (output.transpose(1, 2).contiguous().view(bsz, seq_len, -1))\n",
    "        return self.wo(output) # (bsz, 1, Dim) -> (bsz, 1, Dim)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# freqs_complex = precompute_thaeta_pos_freq(head_dim = 4, seq_len = 4, theta = 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Huggingface implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def rotate_half(x):\n",
    "#     \"\"\"Rotates half the hidden dims of the input.\"\"\"\n",
    "#     x1 = x[..., : x.shape[-1] // 2]\n",
    "#     x2 = x[..., x.shape[-1] // 2 :]\n",
    "    \n",
    "#     return torch.cat((-x2, x1), dim=-1)\n",
    "\n",
    "# def apply_rotary_pos_emb(q, k, cos, sin, position_ids, unsqueeze_dim=1):\n",
    "#     \"\"\"Applies Rotary Position Embedding to the query and key tensors.\n",
    "\n",
    "#     Args:\n",
    "#         q (`torch.Tensor`): The query tensor.\n",
    "#         k (`torch.Tensor`): The key tensor.\n",
    "#         cos (`torch.Tensor`): The cosine part of the rotary embedding.\n",
    "#         sin (`torch.Tensor`): The sine part of the rotary embedding.\n",
    "#         position_ids (`torch.Tensor`):\n",
    "#             The position indices of the tokens corresponding to the query and key tensors. For example, this can be\n",
    "#             used to pass offsetted position ids when working with a KV-cache.\n",
    "#         unsqueeze_dim (`int`, *optional*, defaults to 1):\n",
    "#             The 'unsqueeze_dim' argument specifies the dimension along which to unsqueeze cos[position_ids] and\n",
    "#             sin[position_ids] so that they can be properly broadcasted to the dimensions of q and k. For example, note\n",
    "#             that cos[position_ids] and sin[position_ids] have the shape [batch_size, seq_len, head_dim]. Then, if q and\n",
    "#             k have the shape [batch_size, heads, seq_len, head_dim], then setting unsqueeze_dim=1 makes\n",
    "#             cos[position_ids] and sin[position_ids] broadcastable to the shapes of q and k. Similarly, if q and k have\n",
    "#             the shape [batch_size, seq_len, heads, head_dim], then set unsqueeze_dim=2.\n",
    "#     Returns:\n",
    "#         `tuple(torch.Tensor)` comprising of the query and key tensors rotated using the Rotary Position Embedding.\n",
    "#     \"\"\"\n",
    "#     cos = cos[position_ids].unsqueeze(unsqueeze_dim)\n",
    "#     sin = sin[position_ids].unsqueeze(unsqueeze_dim)\n",
    "\n",
    "#     q_embed = (q * cos) + (rotate_half(q) * sin)\n",
    "#     k_embed = (k * cos) + (rotate_half(k) * sin)\n",
    "\n",
    "#     return q_embed, k_embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RoPE implementation (LLama2 implementation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sin_cos(dim, seq_len, max_seq_len, base = 10000):\n",
    "\n",
    "    inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2, dtype=torch.int64).float() / dim))\n",
    "\n",
    "    t = torch.arange(max_seq_len, dtype=torch.int64).type_as(inv_freq)\n",
    "\n",
    "    freqs = torch.outer(t, inv_freq)\n",
    "    \n",
    "    # Uses a different permutation in order to obtain the same calculation\n",
    "    emb = torch.cat((freqs, freqs), dim=-1)    \n",
    "\n",
    "    return  emb.cos()[:seq_len], emb.sin()[:seq_len]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = 4\n",
    "\n",
    "max_seq_len = 2048\n",
    "seq_len = 4\n",
    "\n",
    "num_heads = 2\n",
    "num_kv_heads = num_heads\n",
    "\n",
    "cos, sin = get_sin_cos(dim, seq_len, max_seq_len, base = 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 1.0000,  1.0000,  1.0000,  1.0000],\n",
       "         [ 0.5403,  0.9999,  0.5403,  0.9999],\n",
       "         [-0.4161,  0.9998, -0.4161,  0.9998],\n",
       "         [-0.9900,  0.9996, -0.9900,  0.9996]]),\n",
       " tensor([[0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.8415, 0.0100, 0.8415, 0.0100],\n",
       "         [0.9093, 0.0200, 0.9093, 0.0200],\n",
       "         [0.1411, 0.0300, 0.1411, 0.0300]]),\n",
       " torch.Size([4, 4]),\n",
       " torch.Size([4, 4]))"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cos, sin, cos.shape, sin.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rotate_half(x):\n",
    "\n",
    "    x1 = x[..., : x.shape[-1] // 2]\n",
    "    x2 = x[..., x.shape[-1] // 2 :]\n",
    "\n",
    "    # print(x1.shape, x2.shape)\n",
    "    # x1 = x[ : , : x.shape[-1] // 2]\n",
    "    # x2 = x[ : , x.shape[-1] // 2 :]\n",
    "\n",
    "    return torch.cat((-x2, x1), dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_rotary_pos_emb(q, k, cos, sin, unsqueeze_dim=1):\n",
    "\n",
    "    cos = cos.unsqueeze(unsqueeze_dim)\n",
    "    sin = sin.unsqueeze(unsqueeze_dim)\n",
    "\n",
    "    # print(\"HLAF ROT SHAPES = \")\n",
    "    # print((rotate_half(q).shape,   sin.shape))\n",
    "    # print((rotate_half(k).shape , sin.shape))\n",
    "\n",
    "    q_embed = (q * cos) + (rotate_half(q) * sin)\n",
    "    k_embed = (k * cos) + (rotate_half(k) * sin)\n",
    "\n",
    "    return q_embed, k_embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 1, 4, 4]), torch.Size([1, 1, 4, 4]), torch.Size([1, 1, 4, 4]))"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# query =   torch.tensor([[[-0.0437,  0.0253, -0.0413,  0.0103],\n",
    "#          [-0.0459, -0.0178,  0.0348, -0.0220],\n",
    "#          [ 0.0150, -0.0052, -0.0289,  0.0082],\n",
    "#          [ 0.0364, -0.0503,  0.0622, -0.0376]]])\n",
    "\n",
    "# key =  torch.tensor([[[ 0.0332,  0.0166,  0.0118, -0.0467],\n",
    "#          [ 0.0217,  0.0474, -0.0396, -0.0136],\n",
    "#          [ 0.0500,  0.0030,  0.0491, -0.0210],\n",
    "#          [ 0.0077,  0.0465, -0.0323,  0.0301]]])\n",
    "\n",
    "# value =  torch.tensor([[[ 0.0046, -0.0319, -0.0447, -0.0426],\n",
    "#          [-0.0408, -0.0341, -0.0696, -0.0315],\n",
    "#          [ 0.0233,  0.0008, -0.0235, -0.0119],\n",
    "#          [-0.0386, -0.0073, -0.0242,  0.0268]]])\n",
    "\n",
    "\n",
    "query =  torch.tensor([[[ 0.0504,  0.0288,  0.0344,  0.0388, -0.0436, -0.0319, -0.0429,\n",
    "          -0.0098],\n",
    "         [ 0.1113,  0.0207,  0.0186, -0.0005, -0.0560, -0.0844,  0.0346,\n",
    "           0.0186],\n",
    "         [-0.1298, -0.0652, -0.0048,  0.0499,  0.0185,  0.0948, -0.0852,\n",
    "          -0.0162],\n",
    "         [-0.0612,  0.0323, -0.0151,  0.0323,  0.0426,  0.0375, -0.0656,\n",
    "           0.0103]]])\n",
    "\n",
    "key = torch.tensor([[[ 2.4886e-02,  7.6303e-02, -5.6346e-02, -2.4803e-02,  1.1542e-01,\n",
    "          -7.3159e-02, -3.0394e-02, -5.5170e-02],\n",
    "         [ 1.7044e-02, -5.8096e-02,  1.6804e-02, -2.4497e-03,  8.4145e-02,\n",
    "          -3.4248e-02, -3.9195e-02, -2.2824e-02],\n",
    "         [-5.4661e-02, -7.4460e-03,  8.9877e-03,  1.3472e-02, -5.5859e-02,\n",
    "          -8.4246e-05, -1.3683e-02, -1.8631e-03],\n",
    "         [-2.3927e-02,  5.0750e-02, -6.1120e-03, -5.3097e-03, -2.8093e-02,\n",
    "           1.1956e-01, -3.7372e-02, -3.3173e-02]]])\n",
    "\n",
    "value = torch.tensor([[[-0.0186,  0.0089,  0.0408, -0.0983, -0.0562, -0.1053, -0.0689,\n",
    "          -0.0754],\n",
    "         [-0.0603,  0.0383,  0.0621,  0.0291,  0.0002, -0.0553, -0.0520,\n",
    "          -0.1110],\n",
    "         [ 0.0389, -0.0035, -0.0284, -0.0129, -0.0393,  0.0243, -0.0281,\n",
    "           0.0394],\n",
    "         [-0.0880, -0.1116, -0.0291, -0.1508, -0.0056, -0.0027,  0.0898,\n",
    "           0.1074]]])\n",
    "\n",
    "\n",
    "\n",
    "bsz, q_len, _ = query.shape\n",
    "\n",
    "# num_heads = 1\n",
    "\n",
    "# head_dim = 4\n",
    "\n",
    "query = query.view(bsz, q_len, num_heads, head_dim).transpose(1, 2)\n",
    "key = key.view(bsz, q_len, num_heads, head_dim).transpose(1, 2)\n",
    "value = value.view(bsz, q_len, num_heads, head_dim).transpose(1, 2)\n",
    "\n",
    "\n",
    "query.shape, key.shape, value.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 4, 4]), torch.Size([1, 4, 4]))"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cos = cos.unsqueeze(0)\n",
    "sin = sin.unsqueeze(0)\n",
    "cos.shape, sin.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[[-0.0437,  0.0253, -0.0413,  0.0103],\n",
       "           [-0.0541, -0.0176, -0.0198, -0.0222],\n",
       "           [ 0.0200, -0.0054,  0.0257,  0.0081],\n",
       "           [-0.0448, -0.0491, -0.0564, -0.0391]]]]),\n",
       " tensor([[[[ 0.0332,  0.0166,  0.0118, -0.0467],\n",
       "           [ 0.0450,  0.0475, -0.0031, -0.0131],\n",
       "           [-0.0655,  0.0034,  0.0250, -0.0209],\n",
       "           [-0.0031,  0.0456,  0.0331,  0.0315]]]]))"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "apply_rotary_pos_emb(query, key, cos, sin, unsqueeze_dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LlamaRMSNorm(hidden_states, wt, variance_epsilon = 1e-6):\n",
    "\n",
    "    hidden_states = hidden_states.to(torch.float32)\n",
    "    variance = hidden_states.pow(2).mean(-1, keepdim=True)\n",
    "    hidden_states = hidden_states * torch.rsqrt(variance + variance_epsilon)\n",
    "    return wt * hidden_states\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LlamaRMSNorm(nn.Module):\n",
    "    def __init__(self, hidden_size, eps=1e-6):\n",
    "        \"\"\"\n",
    "        LlamaRMSNorm is equivalent to T5LayerNorm\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(hidden_size))\n",
    "        self.variance_epsilon = eps\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        input_dtype = hidden_states.dtype\n",
    "        hidden_states = hidden_states.to(torch.float32)\n",
    "        variance = hidden_states.pow(2).mean(-1, keepdim=True)\n",
    "        hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n",
    "        return self.weight * hidden_states.to(input_dtype)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        embed_dim, \n",
    "\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        hidden_dim = 4 * embed_dim\n",
    "        hidden_dim = int(2 * hidden_dim / 3)\n",
    "        if ffn_dim_multiplier is not None:\n",
    "            hidden_dim = int(ffn_dim_multiplier * hidden_dim)\n",
    "        # Round the hidden_dim to the nearest multiple of the multiple_of parameter\n",
    "        hidden_dim = multiple_of * ((hidden_dim + multiple_of - 1) // multiple_of)\n",
    "\n",
    "        self.w1 = nn.Linear(embed_dim, hidden_dim, bias=False)\n",
    "        self.w2 = nn.Linear(hidden_dim, embed_dim, bias=False)\n",
    "        self.w3 = nn.Linear(embed_dim, hidden_dim, bias=False)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        # (B, Seq_Len, Dim) --> (B, Seq_Len, Hidden_Dim)\n",
    "        swish = F.silu(self.w1(x))\n",
    "        # (B, Seq_Len, Dim) --> (B, Seq_Len, Hidden_Dim)\n",
    "        x_V = self.w3(x)\n",
    "        # (B, Seq_Len, Hidden_Dim) * (B, Seq_Len, Hidden_Dim) --> (B, Seq_Len, Hidden_Dim)\n",
    "        x = swish * x_V\n",
    "        # (B, Seq_Len, Hidden_Dim) --> (B, Seq_Len, Dim)\n",
    "        x = self.w2(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, nhead: int, dim_feedforward: int = 2048, dropout: float = 0.1,\n",
    "                 activation: Union[str, Callable[[Tensor], Tensor]] = F.relu,\n",
    "                 layer_norm_eps: float = 1e-5, batch_first: bool = False, norm_first: bool = False,\n",
    "                 bias: bool = True, device=None, dtype=None) -> None:\n",
    "        factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        super().__init__()\n",
    "        self.self_attn = MultiheadAttention(d_model, nhead, dropout=dropout,\n",
    "                                            bias=bias, batch_first=batch_first,\n",
    "                                            **factory_kwargs)\n",
    "        # Implementation of Feedforward model\n",
    "        self.linear1 = Linear(d_model, dim_feedforward, bias=bias, **factory_kwargs)\n",
    "        self.dropout = Dropout(dropout)\n",
    "        self.linear2 = Linear(dim_feedforward, d_model, bias=bias, **factory_kwargs)\n",
    "\n",
    "        self.norm_first = norm_first\n",
    "        self.norm1 = LayerNorm(d_model, eps=layer_norm_eps, bias=bias, **factory_kwargs)\n",
    "        self.norm2 = LayerNorm(d_model, eps=layer_norm_eps, bias=bias, **factory_kwargs)\n",
    "        self.dropout1 = Dropout(dropout)\n",
    "        self.dropout2 = Dropout(dropout)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def __init__(self, n_heads, embed_dim, norm_eps = 1e-6, ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.n_heads = n_heads\n",
    "        self.embed_dim = embed_dim\n",
    "        self.head_embed_dim = embed_dim // n_heads\n",
    "\n",
    "        self.attention = SelfAttention()\n",
    "        self.feed_forward = FeedForward()\n",
    "\n",
    "        # Normalization BEFORE the attention block\n",
    "        self.attention_norm = RMSNorm(embed_dim, eps=norm_eps)\n",
    "        # Normalization BEFORE the feed forward block\n",
    "        self.ffn_norm = RMSNorm(embed_dim, eps=norm_eps)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor, start_pos: int, freqs_complex: torch.Tensor):\n",
    "        # (B, Seq_Len, embed_dim) + (B, Seq_Len, embed_dim) --> (B, Seq_Len, embed_dim)\n",
    "        h = x + self.attention.forward(\n",
    "            self.attention_norm(x), start_pos, freqs_complex\n",
    "        )\n",
    "        # (B, Seq_Len, embed_dim) + (B, Seq_Len, embed_dim) --> (B, Seq_Len, embed_dim)\n",
    "        out = h + self.feed_forward.forward(self.ffn_norm(h))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "my_project_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
