{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Llama 2 modules\n",
    "\n",
    "- **Rotary positional embeddings** -> applied to the computed Q and K vectors in the self_attention part\n",
    "- **Grouped Query Attention** -> Tradeoff between Mutli-Query attention and MHA, balances memory bandwidth requirements and speedup\n",
    "- **KV Caching** -> For faster computation and better memory management\n",
    "- **SwiGLU activation function**\n",
    "- **RMS Norm**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rotary positional embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precompute_theta_pos_freq(head_dim, seq_len, theta = 10000):\n",
    "\n",
    "    assert head_dim%2 == 0, \"Dimension of head must by divisible by 2\"\n",
    "\n",
    "    # theta_i = 10000^(-2(i-1)/dim) for i = [1,2,3.....dim/2]\n",
    "\n",
    "    theta_numerator = torch.arange(0, head_dim, 2).float()\n",
    "\n",
    "    theta = 1.0/ (theta **(theta_numerator/head_dim))\n",
    "\n",
    "    m = torch.arange(seq_len)\n",
    "\n",
    "    freqs = torch.outer(m,theta).float()\n",
    "\n",
    "    #                                   magnituda       angle\n",
    "    freqs_complex = torch.polar(torch.ones_like(freqs), freqs) \n",
    "\n",
    "    return freqs_complex\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_rotary_embeds(x, freqs_complex):\n",
    "\n",
    "    # Separate the last dimension pairs of two values, representing the real and imaginary parts of the complex number\n",
    "    # Two consecutive values will become a single complex number\n",
    "\n",
    "\n",
    "    # H -> no.of heads; can be num_heads for Query and num_kv_heads for Key\n",
    "\n",
    "    # (bsz, seq_len, H , head_dim) -> (bsz, seq_len, H, head_dim/2)\n",
    "\n",
    "    # (bsz, seq_len, H , head_dim) -> (bsz, seq_len, H, head_dim/2)\n",
    "    x_complex = torch.view_as_complex(x.float().reshape(*x.shape[:-1], -1, 2))\n",
    "\n",
    "\n",
    "    # Reshape the freqs_complex tensor to match the shape of the x_complex tensor. \n",
    "    # (seq_len, head_dim/2) --> (1, seq_len, 1, head_dim/2)\n",
    "    freqs_complex = freqs_complex.unsqueeze(0).unsqueeze(2)\n",
    "\n",
    "\n",
    "    # (bsz, seq_len, H, head_dim/2) * (1, seq_len, 1, head_dim/2) = (bsz, seq_len, H, head_dim/2)\n",
    "    x_rotated = x_complex * freqs_complex\n",
    "\n",
    "\n",
    "    # Convert the complex number back to the real number\n",
    "    # (bsz, seq_len, H, head_dim/2) -> (bsz, seq_len, H, head_dim/2, 2)\n",
    "    x_out = torch.view_as_real(x_rotated)\n",
    "\n",
    "\n",
    "    # (bsz, seq_len, H, head_dim/2, 2) -> (bsz, seq_len, H, head_dim)\n",
    "    x_out = x_out.reshape(*x.shape)\n",
    "    \n",
    "\n",
    "    return x_out.type_as(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ModelArgs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mSelfAttention\u001b[39;00m(nn\u001b[38;5;241m.\u001b[39mModule):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: ModelArgs):\n\u001b[1;32m      3\u001b[0m         \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n",
      "Cell \u001b[0;32mIn[4], line 2\u001b[0m, in \u001b[0;36mSelfAttention\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mSelfAttention\u001b[39;00m(nn\u001b[38;5;241m.\u001b[39mModule):\n\u001b[0;32m----> 2\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: \u001b[43mModelArgs\u001b[49m):\n\u001b[1;32m      3\u001b[0m         \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[1;32m      5\u001b[0m         \u001b[38;5;66;03m# Indicates the number of heads for the Keys and Values\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ModelArgs' is not defined"
     ]
    }
   ],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, n_heads ,embed_dim, ):\n",
    "        super().__init__()\n",
    "\n",
    "        # Indicates the number of heads for the Queries\n",
    "        self.num_heads = n_heads\n",
    "\n",
    "        # Indicates the dimension of each head, that is, the part of the embedding that each head will be responsible for\n",
    "        self.head_dim = embed_dim // n_heads\n",
    "\n",
    "        self.wq = nn.Linear(embed_dim, n_heads * self.head_dim, bias=False)\n",
    "        self.wk = nn.Linear(embed_dim, n_heads * self.head_dim, bias=False)\n",
    "        self.wv = nn.Linear(embed_dim, n_heads * self.head_dim, bias=False)\n",
    "        self.wo = nn.Linear(n_heads * self.head_dim, embed_dim, bias=False)\n",
    "\n",
    "\n",
    "    def precompute_theta_pos_freq(self, head_dim, seq_len, theta = 10000):\n",
    "\n",
    "        assert head_dim%2 == 0, \"Dimension of head must by divisible by 2\"\n",
    "\n",
    "        # theta_i = 10000^(-2(i-1)/dim) for i = [1,2,3.....dim/2]\n",
    "\n",
    "        theta_numerator = torch.arange(0, head_dim, 2).float()\n",
    "\n",
    "        theta = 1.0/ (theta **(theta_numerator/head_dim))\n",
    "\n",
    "        m = torch.arange(seq_len)\n",
    "\n",
    "        freqs = torch.outer(m,theta).float()\n",
    "\n",
    "        #                                   magnituda       angle\n",
    "        freqs_complex = torch.polar(torch.ones_like(freqs), freqs) \n",
    "\n",
    "        return freqs_complex\n",
    "    \n",
    "\n",
    "    def apply_rotary_embeds(self, x, freqs_complex):\n",
    "\n",
    "        # Separate the last dimension pairs of two values, representing the real and imaginary parts of the complex number\n",
    "        # Two consecutive values will become a single complex number\n",
    "\n",
    "\n",
    "        # H -> no.of heads; can be num_heads for Query and num_kv_heads for Key\n",
    "\n",
    "        # (bsz, seq_len, H , head_dim) -> (bsz, seq_len, H, head_dim/2)\n",
    "        x_complex = torch.view_as_complex(x.float().reshape(*x.shape[:-1], -1, 2))\n",
    "\n",
    "\n",
    "        # (seq_len, head_dim/2) --> (1, seq_len, 1, head_dim/2)\n",
    "        freqs_complex = freqs_complex.unsqueeze(0).unsqueeze(2)\n",
    "\n",
    "\n",
    "\n",
    "        # (bsz, seq_len, H, head_dim/2) * (1, seq_len, 1, head_dim/2) = (bsz, seq_len, H, head_dim/2)\n",
    "        x_rotated = x_complex * freqs_complex\n",
    "\n",
    "\n",
    "        # Convert the complex number back to the real number\n",
    "        # (bsz, seq_len, H, head_dim/2) -> (bsz, seq_len, H, head_dim/2, 2)\n",
    "        x_out = torch.view_as_real(x_rotated)\n",
    "\n",
    "        # (bsz, seq_len, H, head_dim/2, 2) -> (bsz, seq_len, H, head_dim)\n",
    "        x_out = x_out.reshape(*x.shape)\n",
    "        \n",
    "        return x_out.type_as(x)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x,\n",
    "        freqs_complex,\n",
    "        attn_mask\n",
    "    ):                                # While inferencing:- \n",
    "        bsz, seq_len, _ = x.shape   # (bsz, 1, embed_dim)\n",
    "\n",
    "        Q = self.wq(x)\n",
    "        K = self.wk(x)\n",
    "        V = self.wv(x)\n",
    "\n",
    "\n",
    "        if attn_mask is not None:\n",
    "        # ensure attn_mask's dim is 3\n",
    "            if attn_mask.dim() == 2:\n",
    "                correct_2d_size = (seq_len, seq_len)\n",
    "                if attn_mask.shape != correct_2d_size:\n",
    "                    raise RuntimeError(f\"The shape of the 2D attn_mask is {attn_mask.shape}, but should be {correct_2d_size}.\")\n",
    "                attn_mask = attn_mask.unsqueeze(0)\n",
    "            elif attn_mask.dim() == 3:\n",
    "                correct_3d_size = (bsz * self.num_heads, seq_len, seq_len)\n",
    "                if attn_mask.shape != correct_3d_size:\n",
    "                    raise RuntimeError(f\"The shape of the 3D attn_mask is {attn_mask.shape}, but should be {correct_3d_size}.\")\n",
    "            else:\n",
    "                raise RuntimeError(f\"attn_mask's dimension {attn_mask.dim()} is not supported\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        Q = Q.view(bsz, seq_len, self.n_heads, self.head_dim)\n",
    "        K = K.view(bsz, seq_len, self.n_heads, self.head_dim)\n",
    "        V = V.view(bsz, seq_len, self.n_heads, self.head_dim)\n",
    "\n",
    "        Q = self.apply_rotary_embeddings(Q, freqs_complex)\n",
    "        K = self.apply_rotary_embeddings(K, freqs_complex)\n",
    "\n",
    "        # (bsz, 1, H_Q, Head_Dim) -> (bsz, H_Q, 1, Head_Dim)\n",
    "        Q = Q.transpose(1, 2)\n",
    "        # (bsz, Seq_Len_KV, H_Q, Head_Dim) -> (bsz, H_Q, Seq_Len_KV, Head_Dim)\n",
    "        K = K.transpose(1, 2)\n",
    "        # (bsz, Seq_Len_KV, H_Q, Head_Dim) -> (bsz, H_Q, Seq_Len_KV, Head_Dim)\n",
    "        V = V.transpose(1, 2)\n",
    "\n",
    "        # (bsz, H_Q, 1, Head_Dim) @ (bsz, H_Q, Head_Dim, Seq_Len_KV) -> (bsz, H_Q, 1, Seq_Len_KV)\n",
    "        scores = torch.matmul(Q, K.transpose(2, 3)) / math.sqrt(self.head_dim)\n",
    "\n",
    "        if attn_mask is not None:\n",
    "            scores = scores + attn_mask \n",
    "\n",
    "        # (bsz, H_Q, 1, Seq_Len_KV) -> (bsz, H_Q, 1, Seq_Len_KV)\n",
    "        scores = F.softmax(scores.float(), dim=-1).type_as(Q)\n",
    "\n",
    "        # (bsz, H_Q, 1, Seq_Len) @ (bsz, H_Q, Seq_Len_KV, Head_Dim) -> (bsz, H_Q, 1, Head_Dim)\n",
    "        output = torch.matmul(scores, V)\n",
    "        # (bsz, H_Q, 1, Head_Dim) -> (bsz, 1, H_Q, Head_Dim) -> (bsz, 1, Dim)\n",
    "        output = (output.transpose(1, 2).contiguous().view(bsz, seq_len, -1))\n",
    "        return self.wo(output) # (bsz, 1, Dim) -> (bsz, 1, Dim)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        embed_dim, \n",
    "\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        hidden_dim = 4 * embed_dim\n",
    "        hidden_dim = int(2 * hidden_dim / 3)\n",
    "        if ffn_dim_multiplier is not None:\n",
    "            hidden_dim = int(ffn_dim_multiplier * hidden_dim)\n",
    "        # Round the hidden_dim to the nearest multiple of the multiple_of parameter\n",
    "        hidden_dim = multiple_of * ((hidden_dim + multiple_of - 1) // multiple_of)\n",
    "\n",
    "        self.w1 = nn.Linear(embed_dim, hidden_dim, bias=False)\n",
    "        self.w2 = nn.Linear(hidden_dim, embed_dim, bias=False)\n",
    "        self.w3 = nn.Linear(embed_dim, hidden_dim, bias=False)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        # (B, Seq_Len, Dim) --> (B, Seq_Len, Hidden_Dim)\n",
    "        swish = F.silu(self.w1(x))\n",
    "        # (B, Seq_Len, Dim) --> (B, Seq_Len, Hidden_Dim)\n",
    "        x_V = self.w3(x)\n",
    "        # (B, Seq_Len, Hidden_Dim) * (B, Seq_Len, Hidden_Dim) --> (B, Seq_Len, Hidden_Dim)\n",
    "        x = swish * x_V\n",
    "        # (B, Seq_Len, Hidden_Dim) --> (B, Seq_Len, Dim)\n",
    "        x = self.w2(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, nhead: int, dim_feedforward: int = 2048, dropout: float = 0.1,\n",
    "                 activation: Union[str, Callable[[Tensor], Tensor]] = F.relu,\n",
    "                 layer_norm_eps: float = 1e-5, batch_first: bool = False, norm_first: bool = False,\n",
    "                 bias: bool = True, device=None, dtype=None) -> None:\n",
    "        factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        super().__init__()\n",
    "        self.self_attn = MultiheadAttention(d_model, nhead, dropout=dropout,\n",
    "                                            bias=bias, batch_first=batch_first,\n",
    "                                            **factory_kwargs)\n",
    "        # Implementation of Feedforward model\n",
    "        self.linear1 = Linear(d_model, dim_feedforward, bias=bias, **factory_kwargs)\n",
    "        self.dropout = Dropout(dropout)\n",
    "        self.linear2 = Linear(dim_feedforward, d_model, bias=bias, **factory_kwargs)\n",
    "\n",
    "        self.norm_first = norm_first\n",
    "        self.norm1 = LayerNorm(d_model, eps=layer_norm_eps, bias=bias, **factory_kwargs)\n",
    "        self.norm2 = LayerNorm(d_model, eps=layer_norm_eps, bias=bias, **factory_kwargs)\n",
    "        self.dropout1 = Dropout(dropout)\n",
    "        self.dropout2 = Dropout(dropout)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def __init__(self, n_heads, embed_dim, norm_eps = 1e-6, ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.n_heads = n_heads\n",
    "        self.embed_dim = embed_dim\n",
    "        self.head_embed_dim = embed_dim // n_heads\n",
    "\n",
    "        self.attention = SelfAttention()\n",
    "        self.feed_forward = FeedForward()\n",
    "\n",
    "        # Normalization BEFORE the attention block\n",
    "        self.attention_norm = RMSNorm(embed_dim, eps=norm_eps)\n",
    "        # Normalization BEFORE the feed forward block\n",
    "        self.ffn_norm = RMSNorm(embed_dim, eps=norm_eps)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor, start_pos: int, freqs_complex: torch.Tensor):\n",
    "        # (B, Seq_Len, embed_dim) + (B, Seq_Len, embed_dim) --> (B, Seq_Len, embed_dim)\n",
    "        h = x + self.attention.forward(\n",
    "            self.attention_norm(x), start_pos, freqs_complex\n",
    "        )\n",
    "        # (B, Seq_Len, embed_dim) + (B, Seq_Len, embed_dim) --> (B, Seq_Len, embed_dim)\n",
    "        out = h + self.feed_forward.forward(self.ffn_norm(h))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "my_project_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
