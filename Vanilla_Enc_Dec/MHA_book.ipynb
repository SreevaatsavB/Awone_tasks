{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-head attention transformer\n",
    "### Encoder and Decoder\n",
    "### (With masking)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inilialsing an Encoder-Decoder transformer with different modules from torch (such as nn.Transformer, nn.Linear, etc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import math\n",
    "import copy\n",
    "import numpy as np\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=512, dropout=0):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "\n",
    "        self.encoding = torch.zeros(max_len, d_model)\n",
    "\n",
    "        # Create a tensor with positions [0, 1, 2, ..., max_len-1]\n",
    "        position = torch.arange(0, max_len).unsqueeze(1).float()\n",
    "\n",
    "        # Compute the division term based on the dimension of the model\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(torch.log(torch.tensor(10000.0)) / d_model))\n",
    "\n",
    "        # Apply sine to even indices in the positional encoding\n",
    "        self.encoding[:, 0::2] = torch.sin(position * div_term)\n",
    "\n",
    "        # Apply cosine to odd indices in the positional encoding\n",
    "        self.encoding[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "        # Add an extra dimension to match the batch size (1, max_len, d_model)\n",
    "        self.encoding = self.encoding.unsqueeze(0)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        return self.encoding[:, :x.size(1)].detach()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerModel_Enc_Dec(nn.Module):\n",
    "\n",
    "    def __init__(self, src_vocab_size, tgt_vocab_size, d_model, num_heads, num_encoder_layers, num_decoder_layers, max_seq_len, d_ff, dropout = 0):\n",
    "\n",
    "        super(TransformerModel_Enc_Dec, self).__init__()\n",
    "\n",
    "        # Initialize source and target embeddings\n",
    "        self.src_embedding = nn.Embedding(src_vocab_size, d_model)\n",
    "        self.tgt_embedding = nn.Embedding(tgt_vocab_size, d_model)\n",
    "\n",
    "        # Initialize positional encoding\n",
    "        self.positional_encoding = PositionalEncoding(d_model, dropout=0, max_len=max_seq_len)\n",
    "\n",
    "        # Initialize the transformer model\n",
    "        self.transformer = nn.Transformer(\n",
    "            d_model=d_model,\n",
    "            nhead=num_heads,\n",
    "            num_encoder_layers=num_encoder_layers,\n",
    "            num_decoder_layers=num_decoder_layers,\n",
    "            dropout=dropout,\n",
    "            dim_feedforward=d_ff,\n",
    "        )\n",
    "\n",
    "        # Initialize the final fully connected layer to project the output to the target vocabulary size\n",
    "        self.fc = nn.Linear(d_model, tgt_vocab_size)\n",
    "\n",
    "\n",
    "\n",
    "    def generate_mask(self, src, tgt):\n",
    "        \n",
    "        # Generate masks for source and target sequences\n",
    "        src_mask = None # No mask for the source sequence\n",
    "        seq_length = tgt.size(0)\n",
    "        \n",
    "        # Generate a no-peak mask for the target sequence to prevent attending to future tokens\n",
    "        nopeak_mask = (torch.triu(torch.ones(seq_length, seq_length), diagonal=1)).bool()\n",
    "\n",
    "        return src_mask, nopeak_mask\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "\n",
    "        # Generate source and target masks\n",
    "        src_mask, tgt_mask = self.generate_mask(src, tgt)\n",
    "\n",
    "        # Embed the source and target sequences and add positional encodings\n",
    "        src = self.src_embedding(src) + self.positional_encoding(src)\n",
    "        tgt = self.tgt_embedding(tgt) + self.positional_encoding(tgt)\n",
    "\n",
    "        # Pass the embedded sequences through the transformer model\n",
    "        output = self.transformer(src, tgt, src_mask = src_mask, tgt_mask = tgt_mask)\n",
    "\n",
    "        # Apply the final fully connected layer to project the output to the target vocabulary size\n",
    "        output = self.fc(output)\n",
    "        \n",
    "        return output\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "src_vocab_size = 20\n",
    "tgt_vocab_size = 20\n",
    "\n",
    "d_model = 16\n",
    "num_heads = 4\n",
    "num_encoder_layers = 1\n",
    "num_decoder_layers = 1\n",
    "d_ff = 20\n",
    "max_seq_len = 5\n",
    "dropout = 0\n",
    "\n",
    "\n",
    "src_data = torch.tensor([[2], [1], [5], [4]])\n",
    "tgt_data = torch.tensor([[1], [16], [5], [3], [9]]) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "head_dim = d_model//num_heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "transformer = TransformerModel_Enc_Dec(src_vocab_size, tgt_vocab_size, d_model, num_heads, num_encoder_layers, num_decoder_layers, max_seq_len, d_ff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "tgt_data1 = tgt_data[:-1,:]\n",
    "\n",
    "output = transformer.forward(src=src_data, tgt=tgt_data1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.3775,  0.0734, -0.8096,  0.1469, -0.4905,  0.3739, -0.2705,\n",
      "          -0.9242,  0.6274, -0.6122,  0.2593, -0.1688,  0.1323,  1.3099,\n",
      "           0.7643, -0.6106, -0.1371,  0.7552, -0.5415,  0.7423]],\n",
      "\n",
      "        [[ 0.4344,  0.3561, -0.4982, -0.4326, -0.2377,  0.8758, -1.0641,\n",
      "          -0.3423,  0.3772, -0.4452,  0.6519, -0.2666, -0.0646,  0.7807,\n",
      "           0.5272, -0.6573,  0.0976,  1.0804, -0.9153,  1.1743]],\n",
      "\n",
      "        [[-0.5873,  0.2161, -0.6100, -0.0774, -0.5958,  0.6877, -0.2677,\n",
      "          -0.4463,  0.5172,  1.1137,  0.1040, -0.3007, -1.1872, -0.2548,\n",
      "           0.6388,  0.0914,  0.0290, -0.0519,  0.3834, -0.1068]],\n",
      "\n",
      "        [[ 0.7888,  0.7641, -0.1245, -0.0249,  0.1865, -0.0143, -0.9726,\n",
      "          -0.9177,  0.3889, -0.3268,  0.1702, -0.8667,  0.2136,  0.3825,\n",
      "           0.2635, -0.9925, -0.4424,  1.0395, -1.2948,  0.7781]]],\n",
      "       grad_fn=<ViewBackward0>) torch.Size([4, 1, 20])\n"
     ]
    }
   ],
   "source": [
    "print(output, output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict = transformer.state_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Manual computation of each layer of the transformer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming these are the induvidual token indices after tokenzing the input and ouput\n",
    "src_data = np.array([[2], [1], [5], [4]])\n",
    "tgt_data = np.array([[1], [16], [5], [3], [9]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Source token embeddings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word index: [2], Embedding: tensor([[-0.6136,  0.0316, -0.4927,  0.2484,  0.4397,  0.1124,  0.6408,  0.4412,\n",
      "         -0.1023,  0.7924, -0.2897,  0.0525,  0.5229,  2.3022, -1.4689, -1.5867]])\n",
      "Word index: [1], Embedding: tensor([[-1.3527, -1.6959,  0.5667,  0.7935,  0.5988, -1.5551, -0.3414,  1.8530,\n",
      "          0.7502, -0.5855, -0.1734,  0.1835,  1.3894,  1.5863,  0.9463, -0.8437]])\n",
      "Word index: [5], Embedding: tensor([[-9.3348e-02,  6.8705e-01, -8.3832e-01,  8.9182e-04,  8.4189e-01,\n",
      "         -4.0003e-01,  1.0395e+00,  3.5815e-01, -2.4600e-01,  2.3025e+00,\n",
      "         -1.8817e+00, -4.9727e-02, -1.0450e+00, -9.5650e-01,  3.3532e-02,\n",
      "          7.1009e-01]])\n",
      "Word index: [4], Embedding: tensor([[-0.5692,  0.9200,  1.1108,  1.2899, -1.4782,  2.5672, -0.4731,  0.3356,\n",
      "         -1.6293, -0.5497, -0.4798, -0.4997, -1.0670,  1.1149, -0.1407,  0.8058]])\n",
      "\n",
      "(4, 16)\n"
     ]
    }
   ],
   "source": [
    "# Load the source vocabulary embeddings from the state dictionary\n",
    "src_vocab_embeds = state_dict[\"src_embedding.weight\"]\n",
    "\n",
    "# Initialize an array to hold the embeddings for the source data\n",
    "# Shape: (source sequence length , embedding dimension)\n",
    "src_embedding = np.zeros((src_data.shape[0], d_model))\n",
    "\n",
    "# Iterate over each token index in the source data\n",
    "for i in range(src_data.shape[0]):\n",
    "        \n",
    "        word_index = src_data[i]\n",
    "\n",
    "        # Check if the word index is valid\n",
    "        if word_index < 0 or word_index >= src_vocab_embeds.shape[0]:\n",
    "            raise ValueError(f\"Invalid word index: {word_index}\")\n",
    "        \n",
    "         # Retrieve the embedding for the current word index and store it in src_embedding\n",
    "        src_embedding[i, :] = src_vocab_embeds[word_index, :]\n",
    "\n",
    "        print(f\"Word index: {word_index}, Embedding: {src_vocab_embeds[word_index, :]}\")\n",
    "\n",
    "print()\n",
    "print(src_embedding.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Source token embeddings + positional embeddings \n",
    "\n",
    "Redefining the Positonal Embeddings class into the numpy version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class PositionalEncoding_np:\n",
    "\n",
    "    def __init__(self, d_model, max_len=512, dropout=0):\n",
    "\n",
    "        self.encoding = np.zeros((max_len, d_model))\n",
    "\n",
    "        position = np.arange(0, max_len).reshape(-1, 1).astype(np.float32)\n",
    "\n",
    "        div_term = np.exp(np.arange(0, d_model, 2).astype(np.float32) * -(np.log(10000.0) / d_model))\n",
    "\n",
    "        self.encoding[:, 0::2] = np.sin(position * div_term)\n",
    "        self.encoding[:, 1::2] = np.cos(position * div_term)\n",
    "        self.encoding = self.encoding[np.newaxis, :]\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.encoding[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4, 16),\n",
       " array([[-0.61358309,  1.03159274, -0.49267703,  1.24841475,  0.43969586,\n",
       "          1.11241119,  0.64079237,  1.44115627, -0.10230965,  1.79244399,\n",
       "         -0.28966758,  1.05250749,  0.52286041,  3.30220532, -1.46889389,\n",
       "         -0.58668876],\n",
       "        [-1.35265374, -0.69593132,  0.56665051,  1.79350841,  0.59883946,\n",
       "         -0.55509508, -0.3413603 ,  2.85300612,  0.75018942,  0.41450286,\n",
       "         -0.17339702,  1.18347792,  1.38936615,  2.58633435,  0.94629836,\n",
       "          0.15632319],\n",
       "        [-0.09334823,  1.68705022, -0.83831537,  1.00089182,  0.84189409,\n",
       "          0.59996545,  1.03946197,  1.3581531 , -0.24600095,  3.30251646,\n",
       "         -1.88168919,  0.95027298, -1.04497862,  0.04349947,  0.03353186,\n",
       "          1.71008658],\n",
       "        [-0.56924802,  1.91997129,  1.11081612,  2.28987384, -1.47817433,\n",
       "          3.56723285, -0.4731198 ,  1.33555073, -1.62932599,  0.45025635,\n",
       "         -0.47983426,  0.50031784, -1.06698   ,  2.11493957, -0.14067143,\n",
       "          1.80575365]]))"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set the model dimension (number of features in the embedding)\n",
    "d_model = 16\n",
    "\n",
    "# Set the maximum sequence length\n",
    "max_seq_len = 5\n",
    "\n",
    "\n",
    "pe = PositionalEncoding_np(d_model=d_model, max_len=max_seq_len)\n",
    "\n",
    "# Compute the positional encodings for the source data and add them to the source embeddings\n",
    "pe_src_embeds = src_embedding + pe.forward(src_data)\n",
    "\n",
    "pe_src_embeds.shape, pe_src_embeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_enc = pe_src_embeds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Q,K,V matrices from the model's intialised weights\n",
    "\n",
    "We define a function for getting these vectors in any number of layers in the encoder or decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def get_QKV_matrices(x, layer_num, state_dict, num_heads, is_decoder=False):\n",
    "    \"\"\"\n",
    "    Extracts the Q, K, and V matrices from the encoded input for a given transformer layer.\n",
    "\n",
    "    Args:\n",
    "    - x: Encoded input tensor (numpy array).\n",
    "    - layer_num: Layer number from which to extract the Q, K, and V matrices.\n",
    "    - state_dict: Dictionary containing the model weights.\n",
    "    - num_heads: Number of attention heads.\n",
    "    - is_decoder: Boolean indicating if the layer is in the decoder.\n",
    "\n",
    "    Returns:\n",
    "    - Q: Query matrix after reshaping.\n",
    "    - K: Key matrix after reshaping.\n",
    "    - V: Value matrix after reshaping.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Set query, key, and value encoding to the input encoding\n",
    "    query = key = value = x\n",
    "\n",
    "    # Determine the target length and embedding dimension from the input encoding\n",
    "    tgt_len, embed_dim = x.shape\n",
    "\n",
    "    # Determine the layer type (encoder or decoder) and extract the weights and biases accordingly\n",
    "    layer_type = \"decoder\" if is_decoder else \"encoder\"\n",
    "    W = state_dict[f\"transformer.{layer_type}.layers.{layer_num}.self_attn.in_proj_weight\"].numpy()\n",
    "    b = state_dict[f\"transformer.{layer_type}.layers.{layer_num}.self_attn.in_proj_bias\"].numpy()\n",
    "\n",
    "    # Calculate the dimension for each attention head\n",
    "    head_dim = embed_dim // num_heads\n",
    "\n",
    "    # Compute the product of the input encoding and the transpose of the weight matrix\n",
    "    tempop1 = np.matmul(query, W.T)\n",
    "\n",
    "    # Split the resulting matrix into Q, K, and V matrices\n",
    "    Q = tempop1[:, 0:embed_dim]\n",
    "    K = tempop1[:, embed_dim:2*embed_dim]\n",
    "    V = tempop1[:, 2*embed_dim:3*embed_dim]\n",
    "\n",
    "    # Print the shapes of the Q, K, and V matrices\n",
    "    print(\"Q_shape = \", Q.shape)\n",
    "    print(\"K_shape = \", K.shape)\n",
    "    print(\"V_shape = \", V.shape)\n",
    "    print()\n",
    "\n",
    "    print(\"After reshaping... \\n\")\n",
    "\n",
    "    # Reshape and transpose the Q, K, and V matrices for multi-head attention\n",
    "    Q = np.transpose(np.reshape(Q, (tgt_len, num_heads, head_dim)), (1, 0, 2))\n",
    "    K = np.transpose(np.reshape(K, (K.shape[0], num_heads, head_dim)), (1, 0, 2))\n",
    "    V = np.transpose(np.reshape(V, (V.shape[0], num_heads, head_dim)), (1, 0, 2))\n",
    "\n",
    "    # Print the shapes of the reshaped Q, K, and V matrices\n",
    "    print(\"Q_shape = \", Q.shape)\n",
    "    print(\"K_shape = \", K.shape)\n",
    "    print(\"V_shape = \", V.shape)\n",
    "\n",
    "    return Q, K, V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q_shape =  (4, 16)\n",
      "K_shape =  (4, 16)\n",
      "V_shape =  (4, 16)\n",
      "\n",
      "After reshaping... \n",
      "\n",
      "Q_shape =  (4, 4, 4)\n",
      "K_shape =  (4, 4, 4)\n",
      "V_shape =  (4, 4, 4)\n"
     ]
    }
   ],
   "source": [
    "tgt_len, embed_dim = x_enc.shape\n",
    "\n",
    "Q_enc , K_enc, V_enc =  get_QKV_matrices(x = x_enc, layer_num = 0, state_dict = state_dict, num_heads = num_heads, is_decoder=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Self-Attention in the Enocder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "\n",
    "def calculate_attention(Q, K, V, attn_mask = None):\n",
    "    \"\"\"\n",
    "    Calculates the scaled dot-product attention for a given set of Q, K, and V matrices.\n",
    "\n",
    "    Args:\n",
    "    - Q: Query matrix.\n",
    "    - K: Key matrix.\n",
    "    - V: Value matrix.\n",
    "\n",
    "    Returns:\n",
    "    - attn_output: Output of the attention calculation.\n",
    "    \"\"\"\n",
    "\n",
    "    # Calculate the scale factor for the dot product\n",
    "    scale_factor = 1 / math.sqrt(Q.shape[-1]) \n",
    "\n",
    "    # Transpose the K matrix for matrix multiplication\n",
    "    K_T = np.transpose(K, axes=(0, 2, 1))\n",
    "\n",
    "    # Calculate the scaled dot product of Q and K\n",
    "    attn_weight = Q @ K_T * scale_factor\n",
    "\n",
    "    if attn_mask is not None:\n",
    "        attn_weight += attn_mask\n",
    "\n",
    "    # Apply softmax to the attention weights\n",
    "    exp_attn_weight = np.exp(attn_weight)\n",
    "\n",
    "    sum_exp_attn_weight = np.sum(exp_attn_weight, axis=-1, keepdims=True)\n",
    "\n",
    "    softmax_attn_weight = exp_attn_weight / sum_exp_attn_weight\n",
    "\n",
    "    # Calculate the attention output\n",
    "    attn_output = softmax_attn_weight @ V\n",
    "\n",
    "    return attn_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate attention\n",
    "attn_output_enc = calculate_attention(Q_enc, K_enc, V_enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[[ 0.71996273,  0.37472799,  0.19898526, -0.09682794],\n",
       "         [ 0.74778944,  0.41424775,  0.14055336, -0.09750267],\n",
       "         [ 0.6835169 ,  0.43522334,  0.44485319, -0.15091043],\n",
       "         [ 0.55486831, -0.18135746, -0.03144971,  0.15490705]],\n",
       " \n",
       "        [[ 0.72312542,  0.11965131, -0.6167594 ,  0.41149312],\n",
       "         [ 0.8335319 ,  0.10672248, -0.50738735,  0.41318483],\n",
       "         [ 0.68540564,  0.1174512 , -0.58773575,  0.70770555],\n",
       "         [ 0.65851092,  0.1460082 , -0.62973259,  0.47600019]],\n",
       " \n",
       "        [[-0.17895638, -0.97088941,  0.21564848,  0.34026238],\n",
       "         [-0.15921383, -0.95981101,  0.22985974,  0.33606614],\n",
       "         [ 0.17079554, -0.57683205,  0.32521921,  0.41776349],\n",
       "         [-0.21387928, -0.85617058,  0.13442093,  0.39797621]],\n",
       " \n",
       "        [[ 0.45370406,  0.97827887,  0.88422871, -0.11814313],\n",
       "         [ 0.39785863,  0.99090849,  0.84784164, -0.16750662],\n",
       "         [ 0.97294568,  0.8204363 ,  1.46646822, -0.43123428],\n",
       "         [ 0.54188754,  0.98235485,  0.95371994, -0.1078992 ]]]),\n",
       " (4, 4, 4))"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_output_enc, attn_output_enc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape_attention_op(attn_output, num_heads, tgt_len, head_dim):\n",
    "    \"\"\"\n",
    "    Reshapes the attention output matrix to match the desired shape.\n",
    "\n",
    "    Args:\n",
    "    - attn_output: Attention output matrix.\n",
    "    - num_heads: Number of attention heads.\n",
    "    - tgt_len: Length of the target sequence.\n",
    "    - head_dim: Dimension of each attention head.\n",
    "\n",
    "    Returns:\n",
    "    - final_attn_sa_op: Reshaped attention output.\n",
    "    \"\"\"\n",
    "\n",
    "    # Transpose the attention output dimensions\n",
    "    attn_output_permuted_sa = np.transpose(attn_output, axes=(0, 1, 2))\n",
    "\n",
    "    # Compute the number of elements in the reshaped output\n",
    "    numh_tgt_len, embed_dim = num_heads * tgt_len, head_dim\n",
    "\n",
    "    # Reshape the attention output\n",
    "    attn_output_reshaped_sa = attn_output_permuted_sa.reshape(numh_tgt_len, embed_dim)\n",
    "\n",
    "    # Initialize the final attention output matrix with zeros\n",
    "    final_attn_sa_op = np.zeros(attn_output_reshaped_sa.shape)\n",
    "\n",
    "    i = 0\n",
    "\n",
    "    while i < attn_output_reshaped_sa.shape[1]:\n",
    "        for j in range(attn_output_reshaped_sa.shape[1]):\n",
    "\n",
    "            # Compute the position in the final matrix\n",
    "            pos = i * attn_output_reshaped_sa.shape[1] + j\n",
    "\n",
    "            # Compute the block index and offset for the current column\n",
    "            blk = j * attn_output_reshaped_sa.shape[1]\n",
    "\n",
    "            offset = i\n",
    "\n",
    "            # Assign the corresponding value from the reshaped attention output to the final matrix\n",
    "            final_attn_sa_op[pos] = attn_output_reshaped_sa[blk + offset]\n",
    "\n",
    "        i += 1\n",
    "\n",
    "    final_attn_sa_op = final_attn_sa_op.reshape(attn_output_reshaped_sa.shape[1], -1)\n",
    "\n",
    "    return final_attn_sa_op\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_sa_enc = reshape_attention_op(attn_output_enc, num_heads, tgt_len, head_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 0.71996273,  0.37472799,  0.19898526, -0.09682794,  0.72312542,\n",
       "          0.11965131, -0.6167594 ,  0.41149312, -0.17895638, -0.97088941,\n",
       "          0.21564848,  0.34026238,  0.45370406,  0.97827887,  0.88422871,\n",
       "         -0.11814313],\n",
       "        [ 0.74778944,  0.41424775,  0.14055336, -0.09750267,  0.8335319 ,\n",
       "          0.10672248, -0.50738735,  0.41318483, -0.15921383, -0.95981101,\n",
       "          0.22985974,  0.33606614,  0.39785863,  0.99090849,  0.84784164,\n",
       "         -0.16750662],\n",
       "        [ 0.6835169 ,  0.43522334,  0.44485319, -0.15091043,  0.68540564,\n",
       "          0.1174512 , -0.58773575,  0.70770555,  0.17079554, -0.57683205,\n",
       "          0.32521921,  0.41776349,  0.97294568,  0.8204363 ,  1.46646822,\n",
       "         -0.43123428],\n",
       "        [ 0.55486831, -0.18135746, -0.03144971,  0.15490705,  0.65851092,\n",
       "          0.1460082 , -0.62973259,  0.47600019, -0.21387928, -0.85617058,\n",
       "          0.13442093,  0.39797621,  0.54188754,  0.98235485,  0.95371994,\n",
       "         -0.1078992 ]]),\n",
       " (4, 16))"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_sa_enc, attn_sa_enc.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Post self attention in the encoder block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_layer_forward(x_enc, final_attn_sa_op, state_dict, layer_num, is_decoder = False):\n",
    "    \"\"\"\n",
    "    Forward pass through an encoder layer of a Transformer model.\n",
    "\n",
    "    Args:\n",
    "    - x_enc: Input tensor for the encoder layer.\n",
    "    - final_attn_sa_op: Reshaped attention output tensor.\n",
    "    - state_dict: State dictionary containing the parameters of the model.\n",
    "    - layer_num: Index of the encoder layer.\n",
    "\n",
    "    Returns:\n",
    "    - output_enc_final: Output tensor of the encoder layer.\n",
    "    \"\"\"\n",
    "\n",
    "    # Obtain the parameters for the attention output projection\n",
    "    layer_type = \"decoder\" if is_decoder else \"encoder\"\n",
    "    weight_enc = state_dict[\"transformer.{layer_type}.layers.{layer_num}.self_attn.out_proj.weight\"].numpy()\n",
    "    bias_enc = state_dict[\"transformer.{layer_type}.layers.{layer_num}.self_attn.out_proj.bias\"].numpy()\n",
    "\n",
    "    # Output projection of the attention values\n",
    "    op_enc_1 = np.matmul(final_attn_sa_op, weight_enc.T) + bias_enc\n",
    "\n",
    "    # Residual connection 1\n",
    "    output_enc_1 = op_enc_1 + x_enc\n",
    "    \n",
    "\n",
    "    #### Layer Norm 1 ####\n",
    "    norm_weight = state_dict[\"transformer.{layer_type}.layers.{layer_num}.norm1.weight\"].numpy()\n",
    "    norm_bias = state_dict[\"transformer.{layer_type}.layers.{layer_num}.norm1.bias\"].numpy()\n",
    "    linear_result_enc_1 = output_enc_1 * norm_weight + norm_bias\n",
    "\n",
    "    # Compute mean and standard deviation for Layer Norm\n",
    "    mean = np.mean(linear_result_enc_1, axis=-1, keepdims=True)\n",
    "    std = np.std(linear_result_enc_1, axis=-1, keepdims=True)\n",
    "    epsilon = 1e-5 \n",
    "    linear_op_enc_1 = (linear_result_enc_1 - mean) / (std + epsilon)\n",
    "    \n",
    "\n",
    "    # Obtain the parameters for the linear projections\n",
    "    linear1_weight = state_dict[\"transformer.{layer_type}.layers.{layer_num}.linear1.weight\"].numpy()\n",
    "    linear1_bias = state_dict[\"transformer.{layer_type}.layers.{layer_num}.linear1.bias\"].numpy()\n",
    "\n",
    "    linear2_weight = state_dict[\"transformer.{layer_type}.layers.{layer_num}.linear2.weight\"].numpy()\n",
    "    linear2_bias = state_dict[\"transformer.{layer_type}.layers.{layer_num}.linear2.bias\"].numpy()\n",
    "\n",
    "    # Linear projection 1\n",
    "    op_enc_1 = np.matmul(linear_op_enc_1, linear1_weight.T) + linear1_bias\n",
    "    # ReLU activation\n",
    "    op_enc_1_relu = np.maximum(op_enc_1, 0)\n",
    "    # Linear projection 2\n",
    "    op_enc_2 = np.matmul(op_enc_1_relu, linear2_weight.T) + linear2_bias\n",
    "\n",
    "    # Residual connection 2\n",
    "    output_enc_2 = op_enc_2 + linear_op_enc_1\n",
    "\n",
    "    # Layer Norm 2\n",
    "    norm_weight = state_dict[\"transformer.{layer_type}.layers.{layer_num}.norm2.weight\"].numpy()\n",
    "    norm_bias = state_dict[\"transformer.{layer_type}.layers.{layer_num}.norm1.bias\"].numpy()\n",
    "    linear_result_enc_2 = output_enc_2 * norm_weight + norm_bias\n",
    "\n",
    "    # Compute mean and standard deviation for Layer Norm\n",
    "    mean = np.mean(linear_result_enc_2, axis=-1, keepdims=True)\n",
    "    std = np.std(linear_result_enc_2, axis=-1, keepdims=True)\n",
    "    epsilon = 1e-5 \n",
    "    linear_op_enc_2 = (linear_result_enc_2 - mean) / (std + epsilon)\n",
    "\n",
    "    # Output of the encoder layer\n",
    "    output_enc_final = linear_op_enc_2\n",
    "\n",
    "    return output_enc_final\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def attention_output_projection(final_attn_sa_op, state_dict, layer_num, is_decoder = False, is_crossattn = False):\n",
    "    \"\"\"\n",
    "    Perform the attention output projection.\n",
    "\n",
    "    Args:\n",
    "    - final_attn_sa_op: Reshaped attention output tensor.\n",
    "    - state_dict: State dictionary containing the parameters of the model.\n",
    "    - layer_num: Index of the encoder layer.\n",
    "\n",
    "    Returns:\n",
    "    - output_enc_1: Output of the attention output projection.\n",
    "    \"\"\"\n",
    "    # Obtain the parameters for the attention output projection\n",
    "    layer_type = \"decoder\" if is_decoder else \"encoder\"\n",
    "    attn_type = \"multihead_attn\" if is_crossattn else \"self_attn\"\n",
    "\n",
    "    weight_key = \"transformer.{}.layers.{}.{}.out_proj.weight\".format(layer_type, layer_num, attn_type)\n",
    "    bias_key = \"transformer.{}.layers.{}.{}.out_proj.bias\".format(layer_type, layer_num, attn_type)\n",
    "\n",
    "    weight_op = state_dict[weight_key].numpy()\n",
    "    bias_op = state_dict[bias_key].numpy()\n",
    "\n",
    "    # Output projection of the attention values\n",
    "    output_1 = np.matmul(final_attn_sa_op, weight_op.T) + bias_op\n",
    "\n",
    "    return output_1\n",
    "\n",
    "def layer_norm(input_tensor, state_dict, layer_num, suffix, is_decoder=False):\n",
    "    \"\"\"\n",
    "    Apply layer normalization.\n",
    "\n",
    "    Args:\n",
    "    - input_tensor: Input tensor to be normalized.\n",
    "    - state_dict: State dictionary containing the parameters of the model.\n",
    "    - layer_num: Index of the encoder layer.\n",
    "    - suffix: Suffix for parameter keys.\n",
    "\n",
    "    Returns:\n",
    "    - normalized_tensor: Normalized tensor.\n",
    "    \"\"\"\n",
    "\n",
    "    layer_type = \"decoder\" if is_decoder else \"encoder\"\n",
    "\n",
    "    weight_key = \"transformer.{}.layers.{}.{}.weight\".format(layer_type, layer_num, suffix)\n",
    "    bias_key = \"transformer.{}.layers.{}.{}.bias\".format(layer_type, layer_num, suffix)\n",
    "\n",
    "    norm_weight = state_dict[weight_key].numpy()\n",
    "    norm_bias = state_dict[bias_key].numpy()\n",
    "\n",
    "    linear_result = input_tensor * norm_weight + norm_bias\n",
    "\n",
    "    # Compute mean and standard deviation for Layer Norm\n",
    "    mean = np.mean(linear_result, axis=-1, keepdims=True)\n",
    "    std = np.std(linear_result, axis=-1, keepdims=True)\n",
    "    epsilon = 1e-5 \n",
    "    normalized_tensor = (linear_result - mean) / (std + epsilon)\n",
    "\n",
    "    return normalized_tensor\n",
    "\n",
    "def linear_relu_linear(input_tensor, state_dict, layer_num, is_decoder=False):\n",
    "    \"\"\"\n",
    "    Apply Linear-Relu-Linear projection.\n",
    "\n",
    "    Args:\n",
    "    - input_tensor: Input tensor to be projected.\n",
    "    - state_dict: State dictionary containing the parameters of the model.\n",
    "    - layer_num: Index of the encoder layer.\n",
    "\n",
    "    Returns:\n",
    "    - output_enc_final: Output tensor after Linear-Relu-Linear projection.\n",
    "    \"\"\"\n",
    "\n",
    "    # Obtain the parameters for the linear projections\n",
    "    layer_type = \"decoder\" if is_decoder else \"encoder\"\n",
    "\n",
    "    weight1_key = \"transformer.{}.layers.{}.linear1.weight\".format(layer_type, layer_num)\n",
    "    bias1_key = \"transformer.{}.layers.{}.linear1.bias\".format(layer_type, layer_num)\n",
    "    weight2_key = \"transformer.{}.layers.{}.linear2.weight\".format(layer_type, layer_num)\n",
    "    bias2_key = \"transformer.{}.layers.{}.linear2.bias\".format(layer_type, layer_num)\n",
    "\n",
    "    linear1_weight = state_dict[weight1_key].numpy()\n",
    "    linear1_bias = state_dict[bias1_key].numpy()\n",
    "    linear2_weight = state_dict[weight2_key].numpy()\n",
    "    linear2_bias = state_dict[bias2_key].numpy()\n",
    "\n",
    "    # Linear projection 1\n",
    "    op_enc_1 = np.matmul(input_tensor, linear1_weight.T) + linear1_bias\n",
    "    # ReLU activation\n",
    "    op_enc_1_relu = np.maximum(op_enc_1, 0)\n",
    "    # Linear projection 2\n",
    "    op_enc_2 = np.matmul(op_enc_1_relu, linear2_weight.T) + linear2_bias\n",
    "\n",
    "    return op_enc_2\n",
    "\n",
    "def encoder_layer_forward(x_enc, final_attn_sa_op, state_dict, layer_num):\n",
    "    \"\"\"\n",
    "    Forward pass through an encoder layer of a Transformer model.\n",
    "\n",
    "    Args:\n",
    "    - x_enc: Input tensor for the encoder layer.\n",
    "    - final_attn_sa_op: Reshaped attention output tensor.\n",
    "    - state_dict: State dictionary containing the parameters of the model.\n",
    "    - layer_num: Index of the encoder layer.\n",
    "\n",
    "    Returns:\n",
    "    - output_enc_final: Output tensor of the encoder layer.\n",
    "    \"\"\"\n",
    "\n",
    "    # Attention output projection\n",
    "    output_enc_1 = attention_output_projection(final_attn_sa_op, state_dict, layer_num)\n",
    "\n",
    "    # Residual connection 1\n",
    "    output_enc_1 += x_enc\n",
    "\n",
    "    # Layer Norm 1\n",
    "    normalized_tensor_1 = layer_norm(output_enc_1, state_dict, layer_num, \"norm1\")\n",
    "\n",
    "    # Linear-Relu-Linear projection\n",
    "    output_enc_2 = linear_relu_linear(normalized_tensor_1, state_dict, layer_num)\n",
    "\n",
    "    # Residual connection 2\n",
    "    output_enc_2 += normalized_tensor_1\n",
    "\n",
    "    # Layer Norm 2\n",
    "    output_enc_final = layer_norm(output_enc_2, state_dict, layer_num, \"norm2\")\n",
    "\n",
    "    return output_enc_final\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_enc_final = encoder_layer_forward(x_enc, attn_sa_enc, state_dict, layer_num = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 0.07763966, -0.81340241, -0.63372394,  0.23144549, -0.17759687,\n",
       "          0.3632295 , -1.3014882 ,  1.52573876, -0.57451076,  1.58645178,\n",
       "          0.40404961,  0.25715019, -1.01007227,  1.90874712, -1.71313098,\n",
       "         -0.13052668],\n",
       "        [-0.56655927, -1.69010877, -0.29210758,  1.21460146, -0.11027101,\n",
       "          0.00789656, -2.11566028,  1.65359012,  0.08810835,  0.56830719,\n",
       "          0.03861744,  0.5650082 , -0.14632035,  1.70270482, -0.61037473,\n",
       "         -0.30743213],\n",
       "        [ 0.11359249, -0.5709097 , -1.03994491, -0.1699108 ,  0.97662986,\n",
       "         -0.3185164 , -0.41645207,  1.2750162 , -1.37529657,  2.30959525,\n",
       "         -0.54989653,  0.85788181, -1.11259539, -0.98347755, -0.09901768,\n",
       "          1.10330199],\n",
       "        [ 0.18098552,  0.02234932,  0.57643494,  0.78534802, -1.54732829,\n",
       "          0.83277342, -1.94984843,  0.44517712, -0.74449741,  1.16907805,\n",
       "          0.10308054, -0.41452632, -1.45966998,  1.31105764, -0.58765698,\n",
       "          1.27724283]]),\n",
       " (4, 16))"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_enc_final, output_enc_final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_enc = output_enc_final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder block\n",
    "\n",
    "\n",
    "### Self attention outputs from a decoder block\n",
    "\n",
    "### 6. Target token embeddings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Considering only the tokens except the last one for the next word prediction (auto-regressive task)\n",
    "tgt_data1 = tgt_data[:-1,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word index: [1], Embedding: tensor([[ 0.6442,  3.9300, -0.1244,  0.2953,  0.3827, -0.5497, -0.9940,  1.3459,\n",
      "          1.9457, -1.2904, -2.3495, -2.0689,  0.9094, -0.6946,  1.9595, -1.1038]])\n",
      "Word index: [16], Embedding: tensor([[-0.8733,  0.0043, -1.2579, -1.0845,  0.7530,  0.3236, -0.2750,  1.3056,\n",
      "          0.2118,  0.2720, -0.9268, -2.7330, -0.5642, -0.2740,  0.1398,  0.5086]])\n",
      "Word index: [5], Embedding: tensor([[-0.7645,  0.2408,  0.1664, -2.2318,  1.3892, -0.5023,  1.6797, -1.0240,\n",
      "          1.6859, -1.2177,  0.7650,  1.1971, -0.7128, -0.0656,  2.2050,  1.7852]])\n",
      "Word index: [3], Embedding: tensor([[ 0.4990,  0.8780,  0.3894,  1.4625,  0.4795, -0.5334, -0.0347,  0.6573,\n",
      "         -0.3112, -0.5620, -0.4835, -1.2721, -0.1740,  0.5541, -0.1817, -0.2345]])\n",
      "\n",
      "(4, 16)\n"
     ]
    }
   ],
   "source": [
    "# Extract target vocabulary embeddings from the model's state dictionary\n",
    "tgt_vocab_embeds = state_dict[\"tgt_embedding.weight\"]\n",
    "\n",
    "# Initialize a numpy array to store target embeddings\n",
    "tgt_embedding = np.zeros((tgt_data1.shape[0], d_model))\n",
    "\n",
    "# Iterate over each target token in tgt_data1\n",
    "for i in range(tgt_data1.shape[0]):\n",
    "    # Get the word index of the current target token\n",
    "    word_index = tgt_data1[i]\n",
    "    \n",
    "    # Check if the word index is valid\n",
    "    if word_index < 0 or word_index >= tgt_vocab_embeds.shape[0]:\n",
    "        raise ValueError(f\"Invalid word index: {word_index}\")\n",
    "    \n",
    "    # Retrieve the embedding corresponding to the word index and assign it to tgt_embedding\n",
    "    tgt_embedding[i, :] = tgt_vocab_embeds[word_index, :]\n",
    "    \n",
    "    # Print the word index and its corresponding embedding\n",
    "    print(f\"Word index: {word_index}, Embedding: {tgt_vocab_embeds[word_index, :]}\")\n",
    "\n",
    "# Print the shape of tgt_embedding\n",
    "print()\n",
    "print(tgt_embedding.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Target token embeddings + positional embeddings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4, 16),\n",
       " array([[ 0.64423001,  4.93000388, -0.12442428,  1.29534167,  0.38265419,\n",
       "          0.45027864, -0.99403578,  2.34593689,  1.94566822, -0.29036391,\n",
       "         -2.3494761 , -1.06886196,  0.90942109,  0.30537993,  1.95945716,\n",
       "         -0.10382783],\n",
       "        [-0.87330669,  1.00426142, -1.25788677, -0.08446777,  0.7529794 ,\n",
       "          1.32364774, -0.27501002,  2.30561185,  0.21175182,  1.27196231,\n",
       "         -0.92684317, -1.7329998 , -0.5641737 ,  0.72600037,  0.13978058,\n",
       "          1.50856197],\n",
       "        [-0.76447284,  1.24084058,  0.16642573, -1.23181415,  1.38921094,\n",
       "          0.49766743,  1.67969298, -0.02395296,  1.68592429, -0.21769202,\n",
       "          0.76496333,  2.19711864, -0.71278685,  0.93442459,  2.20497036,\n",
       "          2.78517103],\n",
       "        [ 0.49895304,  1.87799746,  0.38944435,  2.4625175 ,  0.47950602,\n",
       "          0.46660012, -0.03465135,  1.65729696, -0.31122431,  0.43799645,\n",
       "         -0.48349261, -0.27211261, -0.17401844,  1.55411685, -0.18165524,\n",
       "          0.76552661]]))"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pe = PositionalEncoding_np(d_model=d_model, max_len=max_seq_len)\n",
    "\n",
    "pe_tgt_embeds = tgt_embedding + pe.forward(tgt_data)\n",
    "\n",
    "\n",
    "pe_tgt_embeds.shape, pe_tgt_embeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_dec = pe_tgt_embeds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Self Attention in Decoder **(with mask)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[False,  True,  True,  True],\n",
       "       [False, False,  True,  True],\n",
       "       [False, False, False,  True],\n",
       "       [False, False, False, False]])"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_length = 4\n",
    "tgt_mask = np.triu(np.ones((seq_length, seq_length)), k=1).astype('bool')\n",
    "\n",
    "tgt_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q_shape =  (4, 16)\n",
      "K_shape =  (4, 16)\n",
      "V_shape =  (4, 16)\n",
      "\n",
      "After reshaping... \n",
      "\n",
      "Q_shape =  (4, 4, 4)\n",
      "K_shape =  (4, 4, 4)\n",
      "V_shape =  (4, 4, 4)\n"
     ]
    }
   ],
   "source": [
    "Q_dec , K_dec, V_dec =  get_QKV_matrices(x = x_dec, layer_num = 0, state_dict = state_dict, num_heads = num_heads, is_decoder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 7.83983860e-01, -2.89777000e+00, -1.99690011e+00,\n",
       "         -8.15224661e-02],\n",
       "        [ 1.19432158e-01, -1.32870414e+00,  6.87518232e-02,\n",
       "          7.77369267e-01],\n",
       "        [ 2.24591093e+00,  1.18072686e-01, -8.42719821e-01,\n",
       "         -5.20880030e-01],\n",
       "        [-2.40537342e-01, -1.17889995e+00, -3.21789001e-01,\n",
       "          3.00278529e-01]],\n",
       "\n",
       "       [[-2.24577325e-01, -1.41271065e-01, -1.54248077e-02,\n",
       "         -4.35614408e-01],\n",
       "        [ 1.03550371e-01, -1.66130565e+00,  5.08676441e-01,\n",
       "         -8.00304722e-01],\n",
       "        [-8.19911916e-01,  9.25537349e-01,  3.23517940e-02,\n",
       "          7.18360881e-01],\n",
       "        [-4.52862487e-01, -1.71429861e-01,  7.39203197e-01,\n",
       "         -2.21240286e-01]],\n",
       "\n",
       "       [[ 4.39262634e-01,  1.05416450e+00,  2.56656161e-03,\n",
       "         -2.55351944e+00],\n",
       "        [-2.82534251e-01,  1.19934027e+00, -4.78174562e-01,\n",
       "         -6.89177429e-01],\n",
       "        [ 5.96152102e-01, -2.51479706e-01,  2.78669174e-01,\n",
       "         -1.73430184e+00],\n",
       "        [ 8.34376828e-01,  4.87032609e-01,  3.73917057e-01,\n",
       "         -3.10194982e-01]],\n",
       "\n",
       "       [[ 2.10363015e-01,  1.35340639e+00, -4.95340383e-01,\n",
       "         -2.21551775e+00],\n",
       "        [ 8.71803466e-01,  7.83756067e-01, -1.03508885e+00,\n",
       "         -1.71702671e-01],\n",
       "        [ 8.39702277e-01,  8.29664186e-01,  1.56272798e-01,\n",
       "         -1.76636739e-01],\n",
       "        [-2.24804283e-01,  1.17050994e+00, -5.14773162e-01,\n",
       "         -4.64646170e-01]]])"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K_dec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing the mask for decoder attention mechanism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an attention mask with zeros having the same shape as tgt_mask\n",
    "attn_mask = np.zeros(tgt_mask.shape)\n",
    "\n",
    "if tgt_mask is not None:\n",
    "\n",
    "    if tgt_mask.dtype == 'bool':\n",
    "\n",
    "        # Convert boolean mask to a float mask with -inf for True and 0 for False\n",
    "        masked_tensor = tgt_mask.astype(float)\n",
    "        masked_tensor[masked_tensor == 1] = -np.inf\n",
    "\n",
    "        tgt_mask = masked_tensor\n",
    "\n",
    "        attn_mask += tgt_mask\n",
    "\n",
    "    else:\n",
    "        # If tgt_mask's dtype is not boolean, directly add tgt_mask to the attention mask\n",
    "        attn_mask += tgt_mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0., -inf, -inf, -inf],\n",
       "       [  0.,   0., -inf, -inf],\n",
       "       [  0.,   0.,   0., -inf],\n",
       "       [  0.,   0.,   0.,   0.]])"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "self_attn_output_dec = calculate_attention(Q_dec, K_dec, V_dec, attn_mask = attn_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[[-1.02595533e+00,  9.66156934e-01, -2.29126390e+00,\n",
       "           1.80570103e+00],\n",
       "         [-4.94813720e-01,  3.63255462e-02, -1.18165437e+00,\n",
       "           1.23624901e+00],\n",
       "         [ 3.36763840e-01, -5.12013597e-01,  8.94194164e-02,\n",
       "           5.29699393e-01],\n",
       "         [ 4.78021100e-01, -4.68884973e-02, -2.54583645e-01,\n",
       "           4.71600683e-01]],\n",
       " \n",
       "        [[-2.39826266e-02, -1.33530231e+00, -1.67510994e+00,\n",
       "          -9.04894019e-01],\n",
       "         [-8.35024099e-02, -1.15296021e+00, -1.09796134e+00,\n",
       "          -5.63298511e-01],\n",
       "         [ 1.44866035e-01, -1.71497100e-01,  2.57711368e-01,\n",
       "          -4.27455299e-01],\n",
       "         [-5.85121465e-02, -7.90012909e-01, -3.01850574e-01,\n",
       "          -4.11758885e-01]],\n",
       " \n",
       "        [[ 1.53036519e+00, -1.85140923e+00, -1.51471969e+00,\n",
       "           9.21301524e-01],\n",
       "         [ 1.37871482e+00, -1.49100273e+00, -1.79188210e+00,\n",
       "           7.92854103e-01],\n",
       "         [ 1.35127614e+00, -8.67529277e-01, -1.43847057e+00,\n",
       "          -6.94138377e-01],\n",
       "         [ 1.29407118e+00, -1.14007402e+00, -1.47584493e+00,\n",
       "           2.71347186e-01]],\n",
       " \n",
       "        [[ 5.28027792e-01,  7.11727409e-01, -2.57351948e-01,\n",
       "           1.51332427e+00],\n",
       "         [ 4.19327992e-01,  7.33425132e-01,  5.74104863e-02,\n",
       "           1.38451382e-01],\n",
       "         [ 1.55118181e-01,  2.82364028e-03,  1.78452212e-01,\n",
       "           6.81186860e-02],\n",
       "         [ 3.91602311e-01,  6.22536065e-01, -5.82463750e-02,\n",
       "           1.05593933e-03]]]),\n",
       " (4, 4, 4))"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "self_attn_output_dec, self_attn_output_dec.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_sa_dec = reshape_attention_op(self_attn_output_dec, num_heads, tgt_len, head_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4, 16),\n",
       " array([[-1.02595533e+00,  9.66156934e-01, -2.29126390e+00,\n",
       "          1.80570103e+00, -2.39826266e-02, -1.33530231e+00,\n",
       "         -1.67510994e+00, -9.04894019e-01,  1.53036519e+00,\n",
       "         -1.85140923e+00, -1.51471969e+00,  9.21301524e-01,\n",
       "          5.28027792e-01,  7.11727409e-01, -2.57351948e-01,\n",
       "          1.51332427e+00],\n",
       "        [-4.94813720e-01,  3.63255462e-02, -1.18165437e+00,\n",
       "          1.23624901e+00, -8.35024099e-02, -1.15296021e+00,\n",
       "         -1.09796134e+00, -5.63298511e-01,  1.37871482e+00,\n",
       "         -1.49100273e+00, -1.79188210e+00,  7.92854103e-01,\n",
       "          4.19327992e-01,  7.33425132e-01,  5.74104863e-02,\n",
       "          1.38451382e-01],\n",
       "        [ 3.36763840e-01, -5.12013597e-01,  8.94194164e-02,\n",
       "          5.29699393e-01,  1.44866035e-01, -1.71497100e-01,\n",
       "          2.57711368e-01, -4.27455299e-01,  1.35127614e+00,\n",
       "         -8.67529277e-01, -1.43847057e+00, -6.94138377e-01,\n",
       "          1.55118181e-01,  2.82364028e-03,  1.78452212e-01,\n",
       "          6.81186860e-02],\n",
       "        [ 4.78021100e-01, -4.68884973e-02, -2.54583645e-01,\n",
       "          4.71600683e-01, -5.85121465e-02, -7.90012909e-01,\n",
       "         -3.01850574e-01, -4.11758885e-01,  1.29407118e+00,\n",
       "         -1.14007402e+00, -1.47584493e+00,  2.71347186e-01,\n",
       "          3.91602311e-01,  6.22536065e-01, -5.82463750e-02,\n",
       "          1.05593933e-03]]))"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_sa_dec.shape, attn_sa_dec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "/Users/sreevaatsav/Downloads/KGs/MHA_book.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Post self attention in the decoder self attention block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoder_layer_forward1(x_dec, final_attn_sa_op, state_dict, layer_num):\n",
    "\n",
    "\n",
    "    # Attention output projection\n",
    "    output_dec_1 = attention_output_projection(final_attn_sa_op, state_dict, layer_num,is_decoder = True)\n",
    "\n",
    "    # Residual connection 1\n",
    "    output_dec_1 += x_dec\n",
    "\n",
    "    # Layer Norm 1\n",
    "    normalized_tensor_1 = layer_norm(output_dec_1, state_dict, layer_num, \"norm1\", is_decoder = True)\n",
    "\n",
    "\n",
    "    return normalized_tensor_1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dec_1 = decoder_layer_forward1(x_dec, attn_sa_dec, state_dict, layer_num = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 0.79080436,  2.61675492,  0.19728347,  0.11469253, -0.84743373,\n",
       "         -0.22291594,  0.25940573,  0.67274157, -0.13527261, -0.38400555,\n",
       "         -1.56532808, -1.85249906,  0.65941381, -0.47682196,  0.56739699,\n",
       "         -0.39421645],\n",
       "        [-0.29441922,  1.09712817, -0.40351996,  0.22616283, -0.75575188,\n",
       "          0.25824741,  0.46990197,  1.71531652, -0.62644138,  0.81040219,\n",
       "         -1.21026324, -2.66499552,  0.36661349,  0.43849546, -0.25539105,\n",
       "          0.8285142 ],\n",
       "        [-2.00405224,  0.13751043,  0.09835611, -1.51830931, -0.80674191,\n",
       "         -0.84878997,  0.94845981,  0.49026421,  1.10373596, -0.70779382,\n",
       "         -0.45183934,  0.9925127 , -0.58471128,  1.09218878,  0.49342051,\n",
       "          1.56578937],\n",
       "        [ 0.31189761,  1.17028354,  0.41392054,  2.03995892, -1.06850928,\n",
       "         -0.54656387, -0.17191765,  1.32368864, -0.99201868, -0.01623896,\n",
       "         -0.9910646 , -1.69925982,  0.12088679,  1.09146899, -0.95770799,\n",
       "         -0.02882418]]),\n",
       " (4, 16))"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_dec_1, output_dec_1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.**Cross attention** in decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = x_enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_dec_ca = output_dec_1\n",
    "\n",
    "key_dec_ca, value_dec_ca = memory, memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_QKV_crossattn(query_tensor, key_tensor, state_dict, layer_num, num_heads):\n",
    "    \"\"\"\n",
    "    Compute the Q, K, and V matrices for cross-attention.\n",
    "\n",
    "    Args:\n",
    "    - query_tensor: The query tensor for cross-attention.\n",
    "    - key_tensor: The key tensor for cross-attention.\n",
    "    - state_dict: State dictionary containing the parameters of the model.\n",
    "    - layer_num: Index of the decoder layer.\n",
    "    - num_heads: Number of attention heads.\n",
    "\n",
    "    Returns:\n",
    "    - Q: Query matrix for cross-attention.\n",
    "    - K: Key matrix for cross-attention.\n",
    "    - V: Value matrix for cross-attention.\n",
    "    \"\"\"\n",
    "    # Get the shape of the query tensor\n",
    "    tgt_len, embed_dim = query_tensor.shape\n",
    "\n",
    "    # Obtain the weight and bias matrices for in-projection\n",
    "    W_dec_ca = state_dict[\"transformer.decoder.layers.{}.multihead_attn.in_proj_weight\".format(layer_num)].numpy()\n",
    "    b_dec_ca = state_dict[\"transformer.decoder.layers.{}.multihead_attn.in_proj_bias\".format(layer_num)].numpy()\n",
    "\n",
    "    # Compute the dimension of each attention head\n",
    "    head_dim = embed_dim // num_heads\n",
    "\n",
    "    # Compute the dimension of embeddings\n",
    "    E = query_tensor.shape[-1]\n",
    "\n",
    "    # Split the weight matrix into W_q and W_kv\n",
    "    split_indices = [E, E * 2]\n",
    "    W_q = W_dec_ca[:split_indices[0], :]\n",
    "    W_kv = W_dec_ca[split_indices[0]:, :]\n",
    "\n",
    "    # Compute Q matrix\n",
    "    Q = np.matmul(query_tensor, W_q.T)\n",
    "\n",
    "    # Compute K and V matrices\n",
    "    KV_op = np.matmul(key_tensor, W_kv.T)\n",
    "    K, V = KV_op[:, 0:embed_dim], KV_op[:, embed_dim:2*embed_dim]\n",
    "\n",
    "    # Reshape Q, K, and V matrices\n",
    "    Q = np.transpose(np.reshape(Q, (tgt_len, num_heads, head_dim)), (1, 0, 2))\n",
    "    K = np.transpose(np.reshape(K, (K.shape[0], num_heads, head_dim)), (1, 0, 2))\n",
    "    V = np.transpose(np.reshape(V, (V.shape[0], num_heads, head_dim)), (1, 0, 2))\n",
    "\n",
    "    return Q, K, V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_num = 0\n",
    "\n",
    "Q_dec1, K_dec1, V_dec1 = get_QKV_crossattn(query_dec_ca, key_dec_ca, state_dict, layer_num, num_heads)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "# V_dec1\n",
    "K_dec_ca1_T = np.transpose(K_dec1, axes=(0, 2, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_attn_output_dec = calculate_attention(Q_dec1, K_dec1, V_dec1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[-0.18553441, -0.57655386, -0.69666788,  0.25312486],\n",
       "        [-0.23444545, -0.61369421, -0.66903157,  0.30315544],\n",
       "        [-0.48267096, -0.72678632, -0.47050186,  0.51914174],\n",
       "        [-0.24223902, -0.64934472, -0.69094346,  0.31772663]],\n",
       "\n",
       "       [[-0.4313023 ,  0.17086466, -0.28128801, -0.18745656],\n",
       "        [-0.39735754,  0.08389441, -0.22194945, -0.27801537],\n",
       "        [-0.32980284,  0.06160475, -0.13966224, -0.39402187],\n",
       "        [-0.40939   ,  0.01804331, -0.23653818, -0.28511785]],\n",
       "\n",
       "       [[ 0.79633162, -0.68718596, -0.10476245,  0.01925739],\n",
       "        [ 0.79108095, -0.68322047, -0.15286889,  0.04382875],\n",
       "        [ 0.79858936, -0.68131396, -0.14482916,  0.03213887],\n",
       "        [ 0.78334287, -0.6882062 , -0.17583734,  0.05549425]],\n",
       "\n",
       "       [[ 0.17863263, -0.71823765, -0.38514964, -0.2301647 ],\n",
       "        [ 0.23649462, -0.8250898 , -0.31696894, -0.29983348],\n",
       "        [ 0.19556961, -0.76403825, -0.31572912, -0.2474797 ],\n",
       "        [ 0.23015667, -0.83680755, -0.29030636, -0.29460031]]])"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_attn_output_dec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_ca_dec = reshape_attention_op(cross_attn_output_dec, num_heads, tgt_len, head_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[-0.18553441, -0.57655386, -0.69666788,  0.25312486, -0.4313023 ,\n",
       "          0.17086466, -0.28128801, -0.18745656,  0.79633162, -0.68718596,\n",
       "         -0.10476245,  0.01925739,  0.17863263, -0.71823765, -0.38514964,\n",
       "         -0.2301647 ],\n",
       "        [-0.23444545, -0.61369421, -0.66903157,  0.30315544, -0.39735754,\n",
       "          0.08389441, -0.22194945, -0.27801537,  0.79108095, -0.68322047,\n",
       "         -0.15286889,  0.04382875,  0.23649462, -0.8250898 , -0.31696894,\n",
       "         -0.29983348],\n",
       "        [-0.48267096, -0.72678632, -0.47050186,  0.51914174, -0.32980284,\n",
       "          0.06160475, -0.13966224, -0.39402187,  0.79858936, -0.68131396,\n",
       "         -0.14482916,  0.03213887,  0.19556961, -0.76403825, -0.31572912,\n",
       "         -0.2474797 ],\n",
       "        [-0.24223902, -0.64934472, -0.69094346,  0.31772663, -0.40939   ,\n",
       "          0.01804331, -0.23653818, -0.28511785,  0.78334287, -0.6882062 ,\n",
       "         -0.17583734,  0.05549425,  0.23015667, -0.83680755, -0.29030636,\n",
       "         -0.29460031]]),\n",
       " (4, 16))"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_ca_dec, attn_ca_dec.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11. Post cross attention in the decoder block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoder_layer_forward2(self_attn_dec, final_attn_ca_op, state_dict, layer_num):\n",
    "\n",
    "\n",
    "    # Attention output projection\n",
    "    output_dec_1 = attention_output_projection(final_attn_ca_op, state_dict, layer_num,is_decoder = True, is_crossattn = True)\n",
    "\n",
    "    # Residual connection 2\n",
    "    output_dec_1 += self_attn_dec\n",
    "\n",
    "    # Layer Norm 1\n",
    "    normalized_tensor_2 = layer_norm(output_dec_1, state_dict, layer_num, \"norm2\", is_decoder = True)\n",
    "\n",
    "    output_dec_2 = linear_relu_linear(normalized_tensor_2, state_dict, layer_num, is_decoder = True)\n",
    "\n",
    "    ff_dec = output_dec_2\n",
    "\n",
    "    # Residual connection 3\n",
    "    output_dec_3 = normalized_tensor_2 + ff_dec\n",
    "\n",
    "    # Layer Norm 3\n",
    "    output_dec_3 = layer_norm(output_dec_3, state_dict, layer_num, \"norm3\", is_decoder = True)\n",
    "\n",
    "\n",
    "\n",
    "    return output_dec_3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_op_dec_3 = decoder_layer_forward2(output_dec_1, attn_ca_dec, state_dict, layer_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.5356827 ,  1.93180892, -0.51183151, -0.06886474, -0.35834298,\n",
       "         0.006337  ,  0.74473562,  0.38577164,  0.36656192,  0.24361149,\n",
       "        -1.51696656, -1.19693871,  1.24391757, -1.00695612,  1.61225937,\n",
       "        -1.33942023],\n",
       "       [-0.95041354,  0.78181972, -0.90229085, -0.16324231, -0.3592311 ,\n",
       "         0.16954442,  0.85607012,  1.72101943, -0.56157297,  1.35115966,\n",
       "        -1.33890043, -2.14980348,  0.58189228,  0.48414693,  0.76356128,\n",
       "        -0.28375916],\n",
       "       [-1.7267202 ,  0.12770976, -0.20487979, -1.94317655,  0.58656848,\n",
       "        -1.09442334,  0.84953304,  0.56638679,  0.32088158, -0.55749931,\n",
       "        -1.0709999 ,  0.7707259 , -0.08021092,  1.04794369,  1.6733852 ,\n",
       "         0.73477555],\n",
       "       [-0.18048601,  1.14506299, -0.14022405,  1.80746055, -0.66768442,\n",
       "        -0.59171444,  0.12012847,  1.24848012, -1.23003434,  0.65160384,\n",
       "        -1.4823933 , -1.11823431,  0.06295639,  1.53559277, -0.04201097,\n",
       "        -1.11850329]])"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_op_dec_3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12. Feed Forward Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "dec_output_final = linear_op_dec_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_ff = state_dict[\"fc.weight\"].numpy()\n",
    "b_ff = state_dict[\"fc.bias\"].numpy()\n",
    "\n",
    "final_op = dec_output_final@W_ff.T + b_ff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 0.37746685,  0.07337387, -0.80958893,  0.14691591, -0.49054451,\n",
       "          0.37394687, -0.27048344, -0.92419295,  0.62737914, -0.61220299,\n",
       "          0.25932976, -0.16877907,  0.13227677,  1.30987519,  0.76432395,\n",
       "         -0.61062077, -0.13705934,  0.7552388 , -0.54154864,  0.74224788],\n",
       "        [ 0.43438884,  0.35607952, -0.49823122, -0.43256203, -0.23772529,\n",
       "          0.87583749, -1.06413742, -0.34228145,  0.37721902, -0.44517008,\n",
       "          0.65188803, -0.26663242, -0.06459797,  0.7806719 ,  0.52723765,\n",
       "         -0.65729075,  0.09763294,  1.08035112, -0.91532704,  1.17432552],\n",
       "        [-0.58731561,  0.21614686, -0.60998807, -0.07736154, -0.59575491,\n",
       "          0.68770998, -0.26769391, -0.44631606,  0.51722351,  1.11366001,\n",
       "          0.1039768 , -0.30067784, -1.18722492, -0.25477752,  0.63879546,\n",
       "          0.09141405,  0.02895858, -0.05193275,  0.38341005, -0.10680178],\n",
       "        [ 0.78883248,  0.76411429, -0.12451219, -0.02487891,  0.18648671,\n",
       "         -0.01425053, -0.97255592, -0.91765565,  0.38885112, -0.32676804,\n",
       "          0.17021863, -0.86673849,  0.21357193,  0.38245737,  0.26350688,\n",
       "         -0.99247295, -0.44237076,  1.03947443, -1.29483286,  0.77811196]]),\n",
       " (4, 20))"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_op, final_op.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This is the output from nn.Transformers :- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.3775,  0.0734, -0.8096,  0.1469, -0.4905,  0.3739, -0.2705,\n",
      "          -0.9242,  0.6274, -0.6122,  0.2593, -0.1688,  0.1323,  1.3099,\n",
      "           0.7643, -0.6106, -0.1371,  0.7552, -0.5415,  0.7423]],\n",
      "\n",
      "        [[ 0.4344,  0.3561, -0.4982, -0.4326, -0.2377,  0.8758, -1.0641,\n",
      "          -0.3423,  0.3772, -0.4452,  0.6519, -0.2666, -0.0646,  0.7807,\n",
      "           0.5272, -0.6573,  0.0976,  1.0804, -0.9153,  1.1743]],\n",
      "\n",
      "        [[-0.5873,  0.2161, -0.6100, -0.0774, -0.5958,  0.6877, -0.2677,\n",
      "          -0.4463,  0.5172,  1.1137,  0.1040, -0.3007, -1.1872, -0.2548,\n",
      "           0.6388,  0.0914,  0.0290, -0.0519,  0.3834, -0.1068]],\n",
      "\n",
      "        [[ 0.7888,  0.7641, -0.1245, -0.0249,  0.1865, -0.0143, -0.9726,\n",
      "          -0.9177,  0.3889, -0.3268,  0.1702, -0.8667,  0.2136,  0.3825,\n",
      "           0.2635, -0.9925, -0.4424,  1.0395, -1.2948,  0.7781]]],\n",
      "       grad_fn=<ViewBackward0>) torch.Size([4, 1, 20])\n"
     ]
    }
   ],
   "source": [
    "print(output, output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
