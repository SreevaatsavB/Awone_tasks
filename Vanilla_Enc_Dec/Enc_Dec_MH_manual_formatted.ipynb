{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-head attention transformer\n",
    "### Encoder and Decoder\n",
    "### (With masking)\n",
    "\n",
    "### Initialising the same transformer as in the other notebook\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import math\n",
    "import copy\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=512, dropout=0):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.encoding = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1).float()\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(torch.log(torch.tensor(10000.0)) / d_model))\n",
    "        self.encoding[:, 0::2] = torch.sin(position * div_term)\n",
    "        self.encoding[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.encoding = self.encoding.unsqueeze(0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        print(self.encoding)\n",
    "        return self.encoding[:, :x.size(1)].detach()\n",
    "\n",
    "\n",
    "\n",
    "class TransformerModel1(nn.Module):\n",
    "    def __init__(self, src_vocab_size, tgt_vocab_size, d_model, num_heads, num_encoder_layers, num_decoder_layers, max_seq_len, d_ff, dropout = 0):\n",
    "\n",
    "        super(TransformerModel1, self).__init__()\n",
    "\n",
    "        self.src_embedding = nn.Embedding(src_vocab_size, d_model)\n",
    "        self.tgt_embedding = nn.Embedding(tgt_vocab_size, d_model)\n",
    "\n",
    "        self.positional_encoding = PositionalEncoding(d_model, dropout=0, max_len=max_seq_len)\n",
    "\n",
    "        self.transformer = nn.Transformer(\n",
    "            d_model=d_model,\n",
    "            nhead=num_heads,\n",
    "            num_encoder_layers=num_encoder_layers,\n",
    "            num_decoder_layers=num_decoder_layers,\n",
    "            dropout=dropout,\n",
    "            dim_feedforward=d_ff,\n",
    "        )\n",
    "\n",
    "        self.fc = nn.Linear(d_model, tgt_vocab_size)\n",
    "\n",
    "\n",
    "\n",
    "    def generate_mask(self, src, tgt):\n",
    "\n",
    "        src_mask = None\n",
    "        seq_length = tgt.size(0)\n",
    "        \n",
    "        nopeak_mask = (torch.triu(torch.ones(seq_length, seq_length), diagonal=1)).bool()\n",
    "\n",
    "        return src_mask, nopeak_mask\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "\n",
    "        src_mask, tgt_mask = self.generate_mask(src, tgt)\n",
    "\n",
    "        print(\"Tgt mask shape = \", tgt_mask.shape)\n",
    "\n",
    "        src = self.src_embedding(src) + self.positional_encoding(src)\n",
    "        tgt = self.tgt_embedding(tgt) + self.positional_encoding(tgt)\n",
    "\n",
    "\n",
    "        output = self.transformer(src, tgt, src_mask = src_mask, tgt_mask = tgt_mask, tgt_is_causal = False)\n",
    "        output = self.fc(output)\n",
    "        \n",
    "        return output\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters defined "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "src_vocab_size = 20\n",
    "tgt_vocab_size = 20\n",
    "d_model = 16\n",
    "num_heads = 4\n",
    "num_encoder_layers = 1\n",
    "num_decoder_layers = 1\n",
    "d_ff = 20\n",
    "max_seq_len = 5\n",
    "dropout = 0\n",
    "\n",
    "\n",
    "src_data = np.array([[2], [1], [5], [4]])\n",
    "tgt_data = np.array([[1], [16], [5], [3], [9]])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sreevaatsav/.pyenv/versions/project_env/lib/python3.10/site-packages/torch/nn/modules/transformer.py:293: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "\n",
    "transformer = TransformerModel1(src_vocab_size, tgt_vocab_size, d_model, num_heads, num_encoder_layers, num_decoder_layers, max_seq_len, d_ff)\n",
    "\n",
    "state_dict = transformer.state_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Source token embeddings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word index: [2], Embedding: tensor([[-0.6136,  0.0316, -0.4927,  0.2484,  0.4397,  0.1124,  0.6408,  0.4412,\n",
      "         -0.1023,  0.7924, -0.2897,  0.0525,  0.5229,  2.3022, -1.4689, -1.5867]])\n",
      "Word index: [1], Embedding: tensor([[-1.3527, -1.6959,  0.5667,  0.7935,  0.5988, -1.5551, -0.3414,  1.8530,\n",
      "          0.7502, -0.5855, -0.1734,  0.1835,  1.3894,  1.5863,  0.9463, -0.8437]])\n",
      "Word index: [5], Embedding: tensor([[-9.3348e-02,  6.8705e-01, -8.3832e-01,  8.9182e-04,  8.4189e-01,\n",
      "         -4.0003e-01,  1.0395e+00,  3.5815e-01, -2.4600e-01,  2.3025e+00,\n",
      "         -1.8817e+00, -4.9727e-02, -1.0450e+00, -9.5650e-01,  3.3532e-02,\n",
      "          7.1009e-01]])\n",
      "Word index: [4], Embedding: tensor([[-0.5692,  0.9200,  1.1108,  1.2899, -1.4782,  2.5672, -0.4731,  0.3356,\n",
      "         -1.6293, -0.5497, -0.4798, -0.4997, -1.0670,  1.1149, -0.1407,  0.8058]])\n",
      "\n",
      "(4, 16)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "src_vocab_embeds = state_dict[\"src_embedding.weight\"]\n",
    "\n",
    "src_embedding = np.zeros((src_data.shape[0], d_model))\n",
    "\n",
    "for i in range(src_data.shape[0]):\n",
    "        word_index = src_data[i]\n",
    "        if word_index < 0 or word_index >= src_vocab_embeds.shape[0]:\n",
    "            raise ValueError(f\"Invalid word index: {word_index}\")\n",
    "        src_embedding[i, :] = src_vocab_embeds[word_index, :]\n",
    "\n",
    "        print(f\"Word index: {word_index}, Embedding: {src_vocab_embeds[word_index, :]}\")\n",
    "print()\n",
    "print(src_embedding.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 16)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class PositionalEncoding:\n",
    "\n",
    "    def __init__(self, d_model, max_len=512, dropout=0):\n",
    "\n",
    "        self.encoding = np.zeros((max_len, d_model))\n",
    "\n",
    "        position = np.arange(0, max_len).reshape(-1, 1).astype(np.float32)\n",
    "\n",
    "        div_term = np.exp(np.arange(0, d_model, 2).astype(np.float32) * -(np.log(10000.0) / d_model))\n",
    "\n",
    "        self.encoding[:, 0::2] = np.sin(position * div_term)\n",
    "        self.encoding[:, 1::2] = np.cos(position * div_term)\n",
    "        self.encoding = self.encoding[np.newaxis, :]\n",
    "\n",
    "    def forward(self, x):\n",
    "        enc = self.encoding\n",
    "        # print(enc[0][0].shape)\n",
    "        return self.encoding[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Source token embeddings + positional embeddings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4, 16),\n",
       " array([[-0.61358309,  1.03159274, -0.49267703,  1.24841475,  0.43969586,\n",
       "          1.11241119,  0.64079237,  1.44115627, -0.10230965,  1.79244399,\n",
       "         -0.28966758,  1.05250749,  0.52286041,  3.30220532, -1.46889389,\n",
       "         -0.58668876],\n",
       "        [-1.35265374, -0.69593132,  0.56665051,  1.79350841,  0.59883946,\n",
       "         -0.55509508, -0.3413603 ,  2.85300612,  0.75018942,  0.41450286,\n",
       "         -0.17339702,  1.18347792,  1.38936615,  2.58633435,  0.94629836,\n",
       "          0.15632319],\n",
       "        [-0.09334823,  1.68705022, -0.83831537,  1.00089182,  0.84189409,\n",
       "          0.59996545,  1.03946197,  1.3581531 , -0.24600095,  3.30251646,\n",
       "         -1.88168919,  0.95027298, -1.04497862,  0.04349947,  0.03353186,\n",
       "          1.71008658],\n",
       "        [-0.56924802,  1.91997129,  1.11081612,  2.28987384, -1.47817433,\n",
       "          3.56723285, -0.4731198 ,  1.33555073, -1.62932599,  0.45025635,\n",
       "         -0.47983426,  0.50031784, -1.06698   ,  2.11493957, -0.14067143,\n",
       "          1.80575365]]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_model = 16\n",
    "max_seq_len = 5\n",
    "\n",
    "pe = PositionalEncoding(d_model=d_model, max_len=max_seq_len)\n",
    "\n",
    "pe_src_embeds = src_embedding + pe.forward(src_data)\n",
    "\n",
    "\n",
    "pe_src_embeds.shape, pe_src_embeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 16)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_enc = pe_src_embeds\n",
    "x_enc.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Getting the Q,K,V matrices from the model's intialised weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q_enc_shape =  (4, 16)\n",
      "K_enc_shape =  (4, 16)\n",
      "V_enc_shape =  (4, 16)\n",
      "\n",
      "After reshaping... \n",
      "\n",
      "Q_enc_shape =  (4, 4, 4)\n",
      "K_enc_shape =  (4, 4, 4)\n",
      "V_enc_shape =  (4, 4, 4)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "query_enc = key_enc = value_enc = x_enc\n",
    "\n",
    "tgt_len, embed_dim = x_enc.shape\n",
    "\n",
    "layer_num = 0\n",
    "\n",
    "W_enc = state_dict[\"transformer.encoder.layers.{}.self_attn.in_proj_weight\".format(layer_num)].numpy()\n",
    "b_enc = state_dict[\"transformer.encoder.layers.{}.self_attn.in_proj_bias\".format(layer_num)].numpy()\n",
    "\n",
    "head_dim = embed_dim // num_heads\n",
    "\n",
    "# embed_dim\n",
    "E = query_enc.shape[-1]\n",
    "\n",
    "tempop1 = np.matmul(query_enc, W_enc.T)\n",
    "\n",
    "Q_enc, K_enc, V_enc = tempop1[:, 0:embed_dim], tempop1[:, embed_dim:2*embed_dim], tempop1[:, 2*embed_dim:3*embed_dim]\n",
    "\n",
    "print(\"Q_enc_shape = \", Q_enc.shape)\n",
    "print(\"K_enc_shape = \", K_enc.shape)\n",
    "print(\"V_enc_shape = \", V_enc.shape)\n",
    "print()\n",
    "\n",
    "print(\"After reshaping... \\n\")\n",
    "\n",
    "Q_enc = np.transpose(np.reshape(Q_enc, (tgt_len, num_heads, head_dim)), (1, 0, 2))\n",
    "K_enc = np.transpose(np.reshape(K_enc, (K_enc.shape[0], num_heads, head_dim)), (1, 0, 2))\n",
    "V_enc = np.transpose(np.reshape(V_enc, (V_enc.shape[0], num_heads, head_dim)), (1, 0, 2))\n",
    "\n",
    "print(\"Q_enc_shape = \", Q_enc.shape)\n",
    "print(\"K_enc_shape = \", K_enc.shape)\n",
    "print(\"V_enc_shape = \", V_enc.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q_enc_0 = \n",
      "[[[-1.36735794  0.38451974 -0.62079629  0.31211979]\n",
      "  [-1.11743603  0.75657466 -0.97703017  1.40062991]\n",
      "  [-1.38411373  0.49395312 -0.61819885 -1.53646693]\n",
      "  [-0.82986259 -0.55943665 -1.79558619 -0.35191729]]\n",
      "\n",
      " [[ 1.43866634 -0.02124564  0.67543413 -0.15366692]\n",
      "  [ 1.07486503  0.93711323 -0.45114472  0.68147772]\n",
      "  [-0.07431921 -0.52279624  0.71932026 -1.18875829]\n",
      "  [ 0.984559   -0.70037252  1.17993382 -0.53922323]]\n",
      "\n",
      " [[-0.48074855  1.16585093  1.03184366  0.8862985 ]\n",
      "  [-0.296841    1.25991946  0.79888877  0.90739251]\n",
      "  [-0.11831912  0.54421682  2.078344   -0.07897106]\n",
      "  [-0.99450992 -0.40800719  0.73816109  1.63374404]]\n",
      "\n",
      " [[ 0.18291803 -0.66727623 -1.28919706  1.01650888]\n",
      "  [-0.43740222 -0.45591594 -2.20209216  1.09265755]\n",
      "  [ 0.2551974   0.32470998  1.05439451 -1.19979177]\n",
      "  [ 1.92657715  0.53277614  0.60048078  0.66183671]]]\n",
      "\n",
      "K_enc_0 = \n",
      "[[[ 1.38859394 -0.09582228 -0.18015615  1.11433203]\n",
      "  [ 0.85509775 -0.32077069  0.12798366  1.44537783]\n",
      "  [ 1.49764453 -1.35984899 -0.6593829  -0.14276609]\n",
      "  [ 1.33884307  0.8645316  -0.1417011   0.08537486]]\n",
      "\n",
      " [[-2.22331585 -0.40849706  0.56344588 -1.06896687]\n",
      "  [-0.44947668 -0.42407175  0.45235475 -0.02192151]\n",
      "  [-1.49498859 -0.87250455 -0.28113515  0.25473513]\n",
      "  [-0.76704628 -0.55511116 -0.89275307 -1.30669845]]\n",
      "\n",
      " [[-0.20022322 -0.13379021  0.86808601 -0.50484625]\n",
      "  [-1.45524338  0.56021395  0.19505094 -0.21862601]\n",
      "  [ 1.27651511 -0.15901924  1.21543421 -0.86337873]\n",
      "  [ 2.01706736 -1.28238564  1.59056637 -0.36144729]]\n",
      "\n",
      " [[ 0.90714606  0.38808018 -0.2993248  -0.38839941]\n",
      "  [ 0.40639508  0.88627212 -0.12833495 -1.337182  ]\n",
      "  [ 0.72614973  0.63611508 -0.03439877  0.09732204]\n",
      "  [-0.40787507  0.14619892  1.95213116 -1.42312964]]]\n",
      "\n",
      "V_enc_0 = \n",
      "[[[ 5.38625183e-01  3.84000759e-01  2.65635046e-01  4.80285984e-01]\n",
      "  [ 9.06682550e-01  1.10151337e-01 -5.32710667e-01 -2.92984666e-01]\n",
      "  [ 3.01365113e-01 -1.16103365e+00 -4.99844795e-01  5.29224688e-01]\n",
      "  [ 9.35280358e-01  1.71736982e+00  1.52076633e+00 -8.15704089e-01]]\n",
      "\n",
      " [[-2.61039347e-02  3.44468955e-01 -9.45375167e-01  1.03431336e+00]\n",
      "  [ 1.71919757e-01  2.98874608e-01 -1.08148090e+00 -6.98193835e-02]\n",
      "  [ 1.09574605e+00  3.74868134e-01  2.27797720e-01  1.83946630e-01]\n",
      "  [ 1.89000739e+00 -4.93125072e-01 -1.47158451e-01  1.07059058e+00]]\n",
      "\n",
      " [[-3.91307760e-01 -1.50877641e+00 -2.43897054e-01  8.57127284e-01]\n",
      "  [-5.93273280e-01 -1.07384538e+00  1.52562104e-01  9.93056781e-02]\n",
      "  [ 6.33083805e-01 -1.31898162e+00  1.12695556e+00 -1.21709818e-01]\n",
      "  [ 4.95006369e-01  1.07468220e+00  4.24251040e-02  7.71824808e-01]]\n",
      "\n",
      " [[ 3.69309034e-01  4.54802032e-01  1.07564776e+00 -4.01485027e-01]\n",
      "  [-6.21470004e-01  1.31517806e+00  4.95133618e-01 -2.21982671e+00]\n",
      "  [ 8.56031377e-01  1.41221067e+00  7.05630644e-01  1.11808752e+00]\n",
      "  [ 1.74802688e+00  5.93524679e-01  2.06494014e+00 -7.77750417e-04]]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Q_enc_{} = \".format(layer_num))\n",
    "print(Q_enc)\n",
    "print()\n",
    "\n",
    "print(\"K_enc_{} = \".format(layer_num))\n",
    "print(K_enc)\n",
    "print()\n",
    "\n",
    "print(\"V_enc_{} = \".format(layer_num))\n",
    "print(V_enc)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4, 4, 4), (4, 4, 4), (4, 4, 4))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K_enc.shape, Q_enc.shape, V_enc.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Self attention for encoder layer 1\n",
    "\n",
    "#### Attention calculation with Q,K and V matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SoftMax (Scaled Dot Product of Q_enc and K_enc) = \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[0.24622994, 0.32498347, 0.17093051, 0.25785608],\n",
       "        [0.29817845, 0.40020476, 0.09113512, 0.21048167],\n",
       "        [0.14513324, 0.14001347, 0.30002674, 0.41482654],\n",
       "        [0.18297399, 0.17394416, 0.47776105, 0.16532081]],\n",
       "\n",
       "       [[0.14055266, 0.44753894, 0.16198232, 0.24992608],\n",
       "        [0.10363997, 0.39102536, 0.23427295, 0.27106171],\n",
       "        [0.39596284, 0.19193935, 0.14620412, 0.2658937 ],\n",
       "        [0.22779559, 0.38733568, 0.16309258, 0.22177616]],\n",
       "\n",
       "       [[0.26347481, 0.42828542, 0.18578728, 0.1224525 ],\n",
       "        [0.25692131, 0.41706922, 0.19829975, 0.12770972],\n",
       "        [0.23173862, 0.14811665, 0.30689119, 0.31325353],\n",
       "        [0.27611089, 0.44084326, 0.11295147, 0.17009438]],\n",
       "\n",
       "       [[0.3910106 , 0.17491111, 0.38204033, 0.05203796],\n",
       "        [0.39744782, 0.19527263, 0.38060955, 0.02667   ],\n",
       "        [0.11367704, 0.22355699, 0.09936722, 0.56339875],\n",
       "        [0.32765678, 0.17762593, 0.37389094, 0.12082634]]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_len = K_enc.shape[1]\n",
    "\n",
    "Q_enc1 = Q_enc\n",
    "K_enc1 = K_enc\n",
    "V_enc1 = V_enc\n",
    "\n",
    "scale_factor = 1 / math.sqrt(Q_enc1.shape[-1]) \n",
    "\n",
    "K_enc1_T = np.transpose(K_enc1, axes=(0, 2, 1))\n",
    "\n",
    "attn_weight = Q_enc1 @ K_enc1_T * scale_factor\n",
    "\n",
    "\n",
    "exp_attn_weight = np.exp(attn_weight)\n",
    "sum_exp_attn_weight = np.sum(exp_attn_weight, axis=-1, keepdims=True)\n",
    "softmax_attn_weight = exp_attn_weight / sum_exp_attn_weight\n",
    "\n",
    "print(\"SoftMax (Scaled Dot Product of Q_enc and K_enc) = \")\n",
    "softmax_attn_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_output = softmax_attn_weight @ V_enc1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Attention Values = \n",
      "\n",
      "[[[ 0.71996271  0.37472803  0.19898526 -0.096828  ]\n",
      "  [ 0.74778941  0.41424777  0.14055333 -0.09750273]\n",
      "  [ 0.6835169   0.43522338  0.44485323 -0.15091048]\n",
      "  [ 0.55486835 -0.18135736 -0.03144966  0.15490696]]\n",
      "\n",
      " [[ 0.72312544  0.11965125 -0.61675933  0.41149321]\n",
      "  [ 0.83353188  0.10672244 -0.50738729  0.4131849 ]\n",
      "  [ 0.68540562  0.11745112 -0.58773571  0.70770561]\n",
      "  [ 0.65851092  0.14600813 -0.62973254  0.47600028]]\n",
      "\n",
      " [[-0.17895635 -0.97088938  0.21564846  0.34026236]\n",
      "  [-0.15921384 -0.95981102  0.22985971  0.33606611]\n",
      "  [ 0.17079556 -0.57683199  0.32521917  0.4177635 ]\n",
      "  [-0.21387931 -0.8561706   0.1344209   0.39797618]]\n",
      "\n",
      " [[ 0.45370401  0.97827891  0.88422869 -0.1181432 ]\n",
      "  [ 0.39785858  0.99090853  0.84784162 -0.1675067 ]\n",
      "  [ 0.97294562  0.8204363   1.46646829 -0.43123435]\n",
      "  [ 0.54188749  0.98235489  0.95371993 -0.10789927]]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Final Attention Values = \\n\")\n",
    "print(attn_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reshaping the final attention output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 0.71996271,  0.37472803,  0.19898526, -0.096828  ],\n",
       "        [ 0.74778941,  0.41424777,  0.14055333, -0.09750273],\n",
       "        [ 0.6835169 ,  0.43522338,  0.44485323, -0.15091048],\n",
       "        [ 0.55486835, -0.18135736, -0.03144966,  0.15490696]],\n",
       "\n",
       "       [[ 0.72312544,  0.11965125, -0.61675933,  0.41149321],\n",
       "        [ 0.83353188,  0.10672244, -0.50738729,  0.4131849 ],\n",
       "        [ 0.68540562,  0.11745112, -0.58773571,  0.70770561],\n",
       "        [ 0.65851092,  0.14600813, -0.62973254,  0.47600028]],\n",
       "\n",
       "       [[-0.17895635, -0.97088938,  0.21564846,  0.34026236],\n",
       "        [-0.15921384, -0.95981102,  0.22985971,  0.33606611],\n",
       "        [ 0.17079556, -0.57683199,  0.32521917,  0.4177635 ],\n",
       "        [-0.21387931, -0.8561706 ,  0.1344209 ,  0.39797618]],\n",
       "\n",
       "       [[ 0.45370401,  0.97827891,  0.88422869, -0.1181432 ],\n",
       "        [ 0.39785858,  0.99090853,  0.84784162, -0.1675067 ],\n",
       "        [ 0.97294562,  0.8204363 ,  1.46646829, -0.43123435],\n",
       "        [ 0.54188749,  0.98235489,  0.95371993, -0.10789927]]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_output_permuted_sa = np.transpose(attn_output, axes=(0,1,2))\n",
    "\n",
    "attn_output_permuted_sa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 0.71996271,  0.37472803,  0.19898526, -0.096828  ],\n",
       "        [ 0.74778941,  0.41424777,  0.14055333, -0.09750273],\n",
       "        [ 0.6835169 ,  0.43522338,  0.44485323, -0.15091048],\n",
       "        [ 0.55486835, -0.18135736, -0.03144966,  0.15490696],\n",
       "        [ 0.72312544,  0.11965125, -0.61675933,  0.41149321],\n",
       "        [ 0.83353188,  0.10672244, -0.50738729,  0.4131849 ],\n",
       "        [ 0.68540562,  0.11745112, -0.58773571,  0.70770561],\n",
       "        [ 0.65851092,  0.14600813, -0.62973254,  0.47600028],\n",
       "        [-0.17895635, -0.97088938,  0.21564846,  0.34026236],\n",
       "        [-0.15921384, -0.95981102,  0.22985971,  0.33606611],\n",
       "        [ 0.17079556, -0.57683199,  0.32521917,  0.4177635 ],\n",
       "        [-0.21387931, -0.8561706 ,  0.1344209 ,  0.39797618],\n",
       "        [ 0.45370401,  0.97827891,  0.88422869, -0.1181432 ],\n",
       "        [ 0.39785858,  0.99090853,  0.84784162, -0.1675067 ],\n",
       "        [ 0.97294562,  0.8204363 ,  1.46646829, -0.43123435],\n",
       "        [ 0.54188749,  0.98235489,  0.95371993, -0.10789927]]),\n",
       " (16, 4))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numh_tgt_len, embed_dim = num_heads * tgt_len, head_dim\n",
    "attn_output_reshaped_sa = attn_output_permuted_sa.reshape(numh_tgt_len, embed_dim)\n",
    "\n",
    "\n",
    "attn_output_reshaped_sa, attn_output_reshaped_sa.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 0.71996271,  0.37472803,  0.19898526, -0.096828  ,  0.72312544,\n",
       "          0.11965125, -0.61675933,  0.41149321, -0.17895635, -0.97088938,\n",
       "          0.21564846,  0.34026236,  0.45370401,  0.97827891,  0.88422869,\n",
       "         -0.1181432 ],\n",
       "        [ 0.74778941,  0.41424777,  0.14055333, -0.09750273,  0.83353188,\n",
       "          0.10672244, -0.50738729,  0.4131849 , -0.15921384, -0.95981102,\n",
       "          0.22985971,  0.33606611,  0.39785858,  0.99090853,  0.84784162,\n",
       "         -0.1675067 ],\n",
       "        [ 0.6835169 ,  0.43522338,  0.44485323, -0.15091048,  0.68540562,\n",
       "          0.11745112, -0.58773571,  0.70770561,  0.17079556, -0.57683199,\n",
       "          0.32521917,  0.4177635 ,  0.97294562,  0.8204363 ,  1.46646829,\n",
       "         -0.43123435],\n",
       "        [ 0.55486835, -0.18135736, -0.03144966,  0.15490696,  0.65851092,\n",
       "          0.14600813, -0.62973254,  0.47600028, -0.21387931, -0.8561706 ,\n",
       "          0.1344209 ,  0.39797618,  0.54188749,  0.98235489,  0.95371993,\n",
       "         -0.10789927]]),\n",
       " (4, 16))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_attn_sa_op = np.zeros(attn_output_reshaped_sa.shape)\n",
    "\n",
    "i = 0\n",
    "\n",
    "while i < attn_output_reshaped_sa.shape[1]:\n",
    "    for j in range(attn_output_reshaped_sa.shape[1]):\n",
    "\n",
    "        pos = i*attn_output_reshaped_sa.shape[1] + j\n",
    "\n",
    "\n",
    "        blk = (j)*attn_output_reshaped_sa.shape[1]\n",
    "        offset = i\n",
    "        \n",
    "\n",
    "        final_attn_sa_op[pos] = attn_output_reshaped_sa[blk + offset]\n",
    "\n",
    "    i += 1\n",
    "        \n",
    "final_attn_sa_op = final_attn_sa_op.reshape(attn_output_reshaped_sa.shape[1], -1)\n",
    "\n",
    "final_attn_sa_op, final_attn_sa_op.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Post self attention in the encoder block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_num = 0\n",
    "\n",
    "weight_enc = state_dict[\"transformer.encoder.layers.{}.self_attn.out_proj.weight\".format(layer_num)].numpy()\n",
    "bias_enc = state_dict[\"transformer.encoder.layers.{}.self_attn.out_proj.bias\".format(layer_num)].numpy()\n",
    "\n",
    "# Output projection of the attention values\n",
    "op_enc_1 = np.matmul(final_attn_sa_op, weight_enc.T) + bias_enc\n",
    "\n",
    "# Residual connection 1\n",
    "output_enc_1 = op_enc_1 + x_enc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Layer Norm 1\n",
    "\n",
    "\n",
    "norm_weight = state_dict[\"transformer.encoder.layers.{}.norm1.weight\".format(layer_num)].numpy()\n",
    "norm_bias = state_dict[\"transformer.encoder.layers.{}.norm1.bias\".format(layer_num)].numpy()\n",
    "\n",
    "linear_result_enc_1 = output_enc_1 * norm_weight + norm_bias\n",
    "\n",
    "\n",
    "mean = np.mean(linear_result_enc_1, axis=-1, keepdims=True)\n",
    "std = np.std(linear_result_enc_1, axis=-1, keepdims=True)\n",
    "epsilon = 1e-5 \n",
    "normalized_linear_result_enc_1 = (linear_result_enc_1 - mean) / (std + epsilon)\n",
    "\n",
    "\n",
    "layernorm_enc_1 = torch.nn.LayerNorm(normalized_linear_result_enc_1.shape[1:])\n",
    "\n",
    "linear_op_enc_1 = layernorm_enc_1(torch.tensor(normalized_linear_result_enc_1, dtype=torch.float32))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_op_enc_1 = linear_op_enc_1.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear projections \n",
    "\n",
    "linear1_weight = state_dict[\"transformer.encoder.layers.{}.linear1.weight\".format(layer_num)].numpy()\n",
    "linear1_bias = state_dict[\"transformer.encoder.layers.{}.linear1.bias\".format(layer_num)].numpy()\n",
    "linear2_weight = state_dict[\"transformer.encoder.layers.{}.linear2.weight\".format(layer_num)].numpy()\n",
    "linear2_bias = state_dict[\"transformer.encoder.layers.{}.linear2.bias\".format(layer_num)].numpy()\n",
    "\n",
    "\n",
    "op_enc_1 = np.matmul(linear_op_enc_1, linear1_weight.T) + linear1_bias\n",
    "\n",
    "# ReLU activation\n",
    "op_enc_1_relu = np.maximum(op_enc_1, 0)\n",
    "\n",
    "# Linear projection 2\n",
    "op_enc_2 = np.matmul(op_enc_1_relu, linear2_weight.T) + linear2_bias\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Residual conenction 2 \n",
    "\n",
    "output_enc_2 = op_enc_2 + linear_op_enc_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Layer Norm 2\n",
    "\n",
    "norm_weight = state_dict[\"transformer.encoder.layers.{}.norm2.weight\".format(layer_num)].numpy()\n",
    "norm_bias = state_dict[\"transformer.encoder.layers.{}.norm2.bias\".format(layer_num)].numpy()\n",
    "\n",
    "linear_result_enc_2 = output_enc_2 * norm_weight + norm_bias\n",
    "\n",
    "\n",
    "mean = np.mean(linear_result_enc_2, axis=-1, keepdims=True)\n",
    "std = np.std(linear_result_enc_2, axis=-1, keepdims=True)\n",
    "epsilon = 1e-5 \n",
    "normalized_linear_result_enc_2 = (linear_result_enc_2 - mean) / (std + epsilon)\n",
    "\n",
    "layernorm_enc_2 = torch.nn.LayerNorm(normalized_linear_result_enc_2.shape[1:])\n",
    "\n",
    "linear_op_enc_2 = layernorm_enc_2(torch.tensor(normalized_linear_result_enc_2, dtype=torch.float32))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_enc_final =  linear_op_enc_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0776, -0.8134, -0.6337,  0.2314, -0.1776,  0.3632, -1.3015,  1.5257,\n",
       "         -0.5745,  1.5865,  0.4041,  0.2572, -1.0101,  1.9088, -1.7131, -0.1305],\n",
       "        [-0.5666, -1.6901, -0.2921,  1.2146, -0.1103,  0.0079, -2.1157,  1.6536,\n",
       "          0.0881,  0.5683,  0.0386,  0.5650, -0.1463,  1.7027, -0.6104, -0.3074],\n",
       "        [ 0.1136, -0.5709, -1.0399, -0.1699,  0.9766, -0.3185, -0.4165,  1.2750,\n",
       "         -1.3753,  2.3096, -0.5499,  0.8579, -1.1126, -0.9835, -0.0990,  1.1033],\n",
       "        [ 0.1810,  0.0223,  0.5764,  0.7853, -1.5473,  0.8328, -1.9499,  0.4452,\n",
       "         -0.7445,  1.1691,  0.1031, -0.4145, -1.4597,  1.3111, -0.5877,  1.2772]],\n",
       "       grad_fn=<NativeLayerNormBackward0>)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_enc_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_enc_final = output_enc_final.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will be used for the next enocder layer / decoder layer (cross attention)\n",
    "x_enc = output_enc_final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "########################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder block\n",
    "\n",
    "\n",
    "### Self attention outputs from a decoder block"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Target token embeddings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['src_embedding.weight', 'tgt_embedding.weight', 'transformer.encoder.layers.0.self_attn.in_proj_weight', 'transformer.encoder.layers.0.self_attn.in_proj_bias', 'transformer.encoder.layers.0.self_attn.out_proj.weight', 'transformer.encoder.layers.0.self_attn.out_proj.bias', 'transformer.encoder.layers.0.linear1.weight', 'transformer.encoder.layers.0.linear1.bias', 'transformer.encoder.layers.0.linear2.weight', 'transformer.encoder.layers.0.linear2.bias', 'transformer.encoder.layers.0.norm1.weight', 'transformer.encoder.layers.0.norm1.bias', 'transformer.encoder.layers.0.norm2.weight', 'transformer.encoder.layers.0.norm2.bias', 'transformer.encoder.norm.weight', 'transformer.encoder.norm.bias', 'transformer.decoder.layers.0.self_attn.in_proj_weight', 'transformer.decoder.layers.0.self_attn.in_proj_bias', 'transformer.decoder.layers.0.self_attn.out_proj.weight', 'transformer.decoder.layers.0.self_attn.out_proj.bias', 'transformer.decoder.layers.0.multihead_attn.in_proj_weight', 'transformer.decoder.layers.0.multihead_attn.in_proj_bias', 'transformer.decoder.layers.0.multihead_attn.out_proj.weight', 'transformer.decoder.layers.0.multihead_attn.out_proj.bias', 'transformer.decoder.layers.0.linear1.weight', 'transformer.decoder.layers.0.linear1.bias', 'transformer.decoder.layers.0.linear2.weight', 'transformer.decoder.layers.0.linear2.bias', 'transformer.decoder.layers.0.norm1.weight', 'transformer.decoder.layers.0.norm1.bias', 'transformer.decoder.layers.0.norm2.weight', 'transformer.decoder.layers.0.norm2.bias', 'transformer.decoder.layers.0.norm3.weight', 'transformer.decoder.layers.0.norm3.bias', 'transformer.decoder.norm.weight', 'transformer.decoder.norm.bias', 'fc.weight', 'fc.bias'])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "tgt_data1 = tgt_data[:-1,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word index: [1], Embedding: tensor([[ 0.6442,  3.9300, -0.1244,  0.2953,  0.3827, -0.5497, -0.9940,  1.3459,\n",
      "          1.9457, -1.2904, -2.3495, -2.0689,  0.9094, -0.6946,  1.9595, -1.1038]])\n",
      "Word index: [16], Embedding: tensor([[-0.8733,  0.0043, -1.2579, -1.0845,  0.7530,  0.3236, -0.2750,  1.3056,\n",
      "          0.2118,  0.2720, -0.9268, -2.7330, -0.5642, -0.2740,  0.1398,  0.5086]])\n",
      "Word index: [5], Embedding: tensor([[-0.7645,  0.2408,  0.1664, -2.2318,  1.3892, -0.5023,  1.6797, -1.0240,\n",
      "          1.6859, -1.2177,  0.7650,  1.1971, -0.7128, -0.0656,  2.2050,  1.7852]])\n",
      "Word index: [3], Embedding: tensor([[ 0.4990,  0.8780,  0.3894,  1.4625,  0.4795, -0.5334, -0.0347,  0.6573,\n",
      "         -0.3112, -0.5620, -0.4835, -1.2721, -0.1740,  0.5541, -0.1817, -0.2345]])\n",
      "\n",
      "(4, 16)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tgt_vocab_embeds = state_dict[\"tgt_embedding.weight\"]\n",
    "\n",
    "tgt_embedding = np.zeros((tgt_data1.shape[0], d_model))\n",
    "\n",
    "for i in range(tgt_data1.shape[0]):\n",
    "        word_index = tgt_data1[i]\n",
    "        if word_index < 0 or word_index >= tgt_vocab_embeds.shape[0]:\n",
    "            raise ValueError(f\"Invalid word index: {word_index}\")\n",
    "        tgt_embedding[i, :] = tgt_vocab_embeds[word_index, :]\n",
    "\n",
    "        print(f\"Word index: {word_index}, Embedding: {tgt_vocab_embeds[word_index, :]}\")\n",
    "print()\n",
    "print(tgt_embedding.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Target token embeddings + positional embeddings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4, 16),\n",
       " array([[ 0.64423001,  4.93000388, -0.12442428,  1.29534167,  0.38265419,\n",
       "          0.45027864, -0.99403578,  2.34593689,  1.94566822, -0.29036391,\n",
       "         -2.3494761 , -1.06886196,  0.90942109,  0.30537993,  1.95945716,\n",
       "         -0.10382783],\n",
       "        [-0.87330669,  1.00426142, -1.25788677, -0.08446777,  0.7529794 ,\n",
       "          1.32364774, -0.27501002,  2.30561185,  0.21175182,  1.27196231,\n",
       "         -0.92684317, -1.7329998 , -0.5641737 ,  0.72600037,  0.13978058,\n",
       "          1.50856197],\n",
       "        [-0.76447284,  1.24084058,  0.16642573, -1.23181415,  1.38921094,\n",
       "          0.49766743,  1.67969298, -0.02395296,  1.68592429, -0.21769202,\n",
       "          0.76496333,  2.19711864, -0.71278685,  0.93442459,  2.20497036,\n",
       "          2.78517103],\n",
       "        [ 0.49895304,  1.87799746,  0.38944435,  2.4625175 ,  0.47950602,\n",
       "          0.46660012, -0.03465135,  1.65729696, -0.31122431,  0.43799645,\n",
       "         -0.48349261, -0.27211261, -0.17401844,  1.55411685, -0.18165524,\n",
       "          0.76552661]]))"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pe = PositionalEncoding(d_model=d_model, max_len=max_seq_len)\n",
    "\n",
    "pe_tgt_embeds = tgt_embedding + pe.forward(tgt_data)\n",
    "\n",
    "\n",
    "pe_tgt_embeds.shape, pe_tgt_embeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 16)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_dec = pe_tgt_embeds\n",
    "x_dec.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SELF ATTENTION ( with target mask ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[False,  True,  True,  True],\n",
       "       [False, False,  True,  True],\n",
       "       [False, False, False,  True],\n",
       "       [False, False, False, False]])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_length = 4\n",
    "tgt_mask = np.triu(np.ones((seq_length, seq_length)), k=1).astype('bool')\n",
    "\n",
    "tgt_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Getting the Q,K,V matrices from the model's intialised weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q_dec_shape =  (4, 16)\n",
      "K_dec_shape =  (4, 16)\n",
      "V_dec_shape =  (4, 16)\n",
      "\n",
      "After reshaping... \n",
      "\n",
      "Q_dec_shape =  (4, 4, 4)\n",
      "K_dec_shape =  (4, 4, 4)\n",
      "V_dec_shape =  (4, 4, 4)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "query_dec = key_dec = value_dec = x_dec\n",
    "\n",
    "tgt_len, embed_dim = x_dec.shape\n",
    "\n",
    "layer_num = 0\n",
    "\n",
    "W_dec = state_dict[\"transformer.decoder.layers.{}.self_attn.in_proj_weight\".format(layer_num)].numpy()\n",
    "b_dec = state_dict[\"transformer.decoder.layers.{}.self_attn.in_proj_bias\".format(layer_num)].numpy()\n",
    "\n",
    "head_dim = embed_dim // num_heads\n",
    "\n",
    "# embed_dim\n",
    "E = query_dec.shape[-1]\n",
    "\n",
    "tempop1 = np.matmul(query_dec, W_dec.T)\n",
    "\n",
    "Q_dec, K_dec, V_dec = tempop1[:, 0:embed_dim], tempop1[:, embed_dim:2*embed_dim], tempop1[:, 2*embed_dim:3*embed_dim]\n",
    "\n",
    "print(\"Q_dec_shape = \", Q_dec.shape)\n",
    "print(\"K_dec_shape = \", K_dec.shape)\n",
    "print(\"V_dec_shape = \", V_dec.shape)\n",
    "print()\n",
    "\n",
    "print(\"After reshaping... \\n\")\n",
    "\n",
    "Q_dec = np.transpose(np.reshape(Q_dec, (tgt_len, num_heads, head_dim)), (1, 0, 2))\n",
    "K_dec = np.transpose(np.reshape(K_dec, (K_dec.shape[0], num_heads, head_dim)), (1, 0, 2))\n",
    "V_dec = np.transpose(np.reshape(V_dec, (V_dec.shape[0], num_heads, head_dim)), (1, 0, 2))\n",
    "\n",
    "print(\"Q_dec_shape = \", Q_dec.shape)\n",
    "print(\"K_dec_shape = \", K_dec.shape)\n",
    "print(\"V_dec_shape = \", V_dec.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q_dec_0 = \n",
      "[[[-0.4777198   0.69994168 -0.24317164  0.137528  ]\n",
      "  [ 0.81450903 -0.44311232  0.54724521 -0.11311741]\n",
      "  [-0.96026027  2.46727906 -0.2959999   2.93302749]\n",
      "  [-0.20450042  0.66613672  0.35755138 -0.31146674]]\n",
      "\n",
      " [[-0.98550985 -0.75627789  2.19837301 -1.03167234]\n",
      "  [ 0.24665634  0.88958552 -0.30725672 -0.97874922]\n",
      "  [-1.8929974   0.58479245  1.4320325   0.12516218]\n",
      "  [-1.55000703  0.3509483  -0.06880262 -0.52188314]]\n",
      "\n",
      " [[-0.83401293  1.4440153   0.40271761 -0.44414086]\n",
      "  [ 0.43542792 -0.32328431  0.66460001  0.18381045]\n",
      "  [-0.3863224  -1.61058622 -0.63673838  0.19069625]\n",
      "  [-0.50454088  0.39886381 -0.02718482 -0.26922808]]\n",
      "\n",
      " [[ 0.50697246  1.36147471 -0.83323292 -0.15125591]\n",
      "  [-0.10894405  0.72768938 -0.01391727  0.4556665 ]\n",
      "  [ 2.7266259   1.4596128   1.58609343  1.13671036]\n",
      "  [ 0.22424157  1.67205158 -1.2045582   0.0386641 ]]]\n",
      "\n",
      "K_dec_0 = \n",
      "[[[ 7.83983927e-01 -2.89776999e+00 -1.99690021e+00 -8.15225161e-02]\n",
      "  [ 1.19432196e-01 -1.32870406e+00  6.87517271e-02  7.77369245e-01]\n",
      "  [ 2.24591099e+00  1.18072672e-01 -8.42719765e-01 -5.20880032e-01]\n",
      "  [-2.40537293e-01 -1.17889994e+00 -3.21789073e-01  3.00278522e-01]]\n",
      "\n",
      " [[-2.24577290e-01 -1.41271064e-01 -1.54248685e-02 -4.35614381e-01]\n",
      "  [ 1.03550382e-01 -1.66130557e+00  5.08676457e-01 -8.00304741e-01]\n",
      "  [-8.19911919e-01  9.25537387e-01  3.23517338e-02  7.18360723e-01]\n",
      "  [-4.52862468e-01 -1.71429873e-01  7.39203123e-01 -2.21240332e-01]]\n",
      "\n",
      " [[ 4.39262592e-01  1.05416454e+00  2.56657823e-03 -2.55351939e+00]\n",
      "  [-2.82534364e-01  1.19934035e+00 -4.78174551e-01 -6.89177348e-01]\n",
      "  [ 5.96152050e-01 -2.51479668e-01  2.78669153e-01 -1.73430185e+00]\n",
      "  [ 8.34376790e-01  4.87032707e-01  3.73917078e-01 -3.10194949e-01]]\n",
      "\n",
      " [[ 2.10363149e-01  1.35340641e+00 -4.95340483e-01 -2.21551773e+00]\n",
      "  [ 8.71803461e-01  7.83756085e-01 -1.03508882e+00 -1.71702647e-01]\n",
      "  [ 8.39702305e-01  8.29664192e-01  1.56272728e-01 -1.76636695e-01]\n",
      "  [-2.24804272e-01  1.17050994e+00 -5.14773204e-01 -4.64646076e-01]]]\n",
      "\n",
      "V_dec_0 = \n",
      "[[[-1.0259552   0.96615682 -2.29126388  1.80570101]\n",
      "  [ 0.09315591 -0.99299061  0.0466748   0.60586997]\n",
      "  [ 1.15288342  0.63898865  0.46910633  0.18066474]\n",
      "  [ 0.59570744 -0.43632311 -0.61074113  0.17573132]]\n",
      "\n",
      " [[-0.02398255 -1.33530222 -1.6751099  -0.90489411]\n",
      "  [-0.18540205 -0.8407852  -0.10986584  0.02152305]\n",
      "  [ 0.29084197  0.43212431  1.07385359 -0.36105162]\n",
      "  [-0.46698004 -1.84058557 -0.93627077 -0.28056237]]\n",
      "\n",
      " [[ 1.53036526 -1.85140924 -1.51471974  0.92130143]\n",
      "  [ 1.19913914 -1.06422991 -2.12008209  0.64075393]\n",
      "  [ 1.36492474 -0.40778914 -1.05773932 -1.97305199]\n",
      "  [ 0.97233683 -0.88119292 -0.82740185  1.07238705]]\n",
      "\n",
      " [[ 0.52802789  0.71172733 -0.25735192  1.51332413]\n",
      "  [ 0.33262592  0.75073185  0.30847444 -0.95818822]\n",
      "  [ 0.04182323 -0.37421972  0.17793789  0.30422657]\n",
      "  [ 0.49748927  0.94089994 -0.37275955 -0.85913887]]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Q_dec_{} = \".format(layer_num))\n",
    "print(Q_dec)\n",
    "print()\n",
    "\n",
    "print(\"K_dec_{} = \".format(layer_num))\n",
    "print(K_dec)\n",
    "print()\n",
    "\n",
    "print(\"V_dec_{} = \".format(layer_num))\n",
    "print(V_dec)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing the mask for decoder attention mechanisms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_bias = np.zeros(tgt_mask.shape)\n",
    "\n",
    "if tgt_mask is not None:\n",
    "    if tgt_mask.dtype == 'bool':\n",
    "        # tgt_mask.masked_fill_(tgt_mask.logical_not(), float(\"-inf\"))\n",
    "\n",
    "        masked_tensor = tgt_mask.astype(float)\n",
    "        masked_tensor[masked_tensor == 1] = -np.inf\n",
    "        tgt_mask = masked_tensor\n",
    "\n",
    "        attn_bias += tgt_mask \n",
    "\n",
    "\n",
    "    else:\n",
    "        attn_bias += tgt_mask\n",
    "        attn_bias = attn_bias\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention mask = \n",
      " [[  0. -inf -inf -inf]\n",
      " [  0.   0. -inf -inf]\n",
      " [  0.   0.   0. -inf]\n",
      " [  0.   0.   0.   0.]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Attention mask = \\n\", attn_bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Attention calculation with Q,K and V matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SoftMax (Scaled Dot Product of Q_dec and K_dec) = \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[1.        , 0.        , 0.        , 0.        ],\n",
       "        [0.52538981, 0.47461019, 0.        , 0.        ],\n",
       "        [0.02873559, 0.71104075, 0.26022366, 0.        ],\n",
       "        [0.11259072, 0.25719802, 0.34845905, 0.28175221]],\n",
       "\n",
       "       [[1.        , 0.        , 0.        , 0.        ],\n",
       "        [0.63127246, 0.36872754, 0.        , 0.        ],\n",
       "        [0.23052338, 0.15412591, 0.61535071, 0.        ],\n",
       "        [0.24104742, 0.15464001, 0.34063577, 0.2636768 ]],\n",
       "\n",
       "       [[1.        , 0.        , 0.        , 0.        ],\n",
       "        [0.54215452, 0.45784548, 0.        , 0.        ],\n",
       "        [0.19330943, 0.27523381, 0.53145675, 0.        ],\n",
       "        [0.31028955, 0.3000996 , 0.20510235, 0.1845085 ]],\n",
       "\n",
       "       [[1.        , 0.        , 0.        , 0.        ],\n",
       "        [0.44371194, 0.55628806, 0.        , 0.        ],\n",
       "        [0.07702968, 0.2608052 , 0.66216512, 0.        ],\n",
       "        [0.30255118, 0.29141826, 0.14721781, 0.25881275]]])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_len = K_dec.shape[1]\n",
    "\n",
    "Q_dec1 = Q_dec\n",
    "K_dec1 = K_dec\n",
    "V_dec1 = V_dec\n",
    "\n",
    "scale_factor = 1 / math.sqrt(Q_dec1.shape[-1]) \n",
    "\n",
    "K_dec1_T = np.transpose(K_dec1, axes=(0, 2, 1))\n",
    "\n",
    "attn_weight_dec = Q_dec1 @ K_dec1_T * scale_factor\n",
    "attn_weight_dec += attn_bias\n",
    "\n",
    "\n",
    "exp_attn_weight_dec_sa = np.exp(attn_weight_dec)\n",
    "sum_exp_attn_weight_dec_sa = np.sum(exp_attn_weight_dec_sa, axis=-1, keepdims=True)\n",
    "softmax_attn_weight_dec_sa = exp_attn_weight_dec_sa / sum_exp_attn_weight_dec_sa\n",
    "\n",
    "print(\"SoftMax (Scaled Dot Product of Q_dec and K_dec) = \")\n",
    "softmax_attn_weight_dec_sa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_output_dec_sa = softmax_attn_weight_dec_sa @ V_dec1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Attention Values (Self attention decoder)= \n",
      "\n",
      "[[[-1.02595520e+00  9.66156820e-01 -2.29126388e+00  1.80570101e+00]\n",
      "  [-4.94813659e-01  3.63254786e-02 -1.18165435e+00  1.23624897e+00]\n",
      "  [ 3.36763757e-01 -5.12013737e-01  8.94194254e-02  5.29699367e-01]\n",
      "  [ 4.78021036e-01 -4.68885516e-02 -2.54583696e-01  4.71600681e-01]]\n",
      "\n",
      " [[-2.39825494e-02 -1.33530222e+00 -1.67510990e+00 -9.04894110e-01]\n",
      "  [-8.35023637e-02 -1.15296018e+00 -1.09796131e+00 -5.63298594e-01]\n",
      "  [ 1.44866013e-01 -1.71497168e-01  2.57711394e-01 -4.27455359e-01]\n",
      "  [-5.85121327e-02 -7.90012909e-01 -3.01850517e-01 -4.11758949e-01]]\n",
      "\n",
      " [[ 1.53036526e+00 -1.85140924e+00 -1.51471974e+00  9.21301429e-01]\n",
      "  [ 1.37871488e+00 -1.49100274e+00 -1.79188216e+00  7.92854025e-01]\n",
      "  [ 1.35127615e+00 -8.67529220e-01 -1.43847060e+00 -6.94138398e-01]\n",
      "  [ 1.29407121e+00 -1.14007400e+00 -1.47584499e+00  2.71347134e-01]]\n",
      "\n",
      " [[ 5.28027889e-01  7.11727333e-01 -2.57351917e-01  1.51332413e+00]\n",
      "  [ 4.19328104e-01  7.33425081e-01  5.74105293e-02  1.38451307e-01]\n",
      "  [ 1.55118275e-01  2.82365411e-03  1.78452262e-01  6.81186312e-02]\n",
      "  [ 3.91602418e-01  6.22536010e-01 -5.82463390e-02  1.05592326e-03]]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Final Attention Values (Self attention decoder)= \\n\")\n",
    "print(attn_output_dec_sa)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reshaping the final attention output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 4, 4)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_output_dec_sa_permuted = np.transpose(attn_output_dec_sa, axes=(0,1,2))\n",
    "\n",
    "attn_output_dec_sa_permuted.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[-1.02595520e+00,  9.66156820e-01, -2.29126388e+00,\n",
       "          1.80570101e+00],\n",
       "        [-4.94813659e-01,  3.63254786e-02, -1.18165435e+00,\n",
       "          1.23624897e+00],\n",
       "        [ 3.36763757e-01, -5.12013737e-01,  8.94194254e-02,\n",
       "          5.29699367e-01],\n",
       "        [ 4.78021036e-01, -4.68885516e-02, -2.54583696e-01,\n",
       "          4.71600681e-01],\n",
       "        [-2.39825494e-02, -1.33530222e+00, -1.67510990e+00,\n",
       "         -9.04894110e-01],\n",
       "        [-8.35023637e-02, -1.15296018e+00, -1.09796131e+00,\n",
       "         -5.63298594e-01],\n",
       "        [ 1.44866013e-01, -1.71497168e-01,  2.57711394e-01,\n",
       "         -4.27455359e-01],\n",
       "        [-5.85121327e-02, -7.90012909e-01, -3.01850517e-01,\n",
       "         -4.11758949e-01],\n",
       "        [ 1.53036526e+00, -1.85140924e+00, -1.51471974e+00,\n",
       "          9.21301429e-01],\n",
       "        [ 1.37871488e+00, -1.49100274e+00, -1.79188216e+00,\n",
       "          7.92854025e-01],\n",
       "        [ 1.35127615e+00, -8.67529220e-01, -1.43847060e+00,\n",
       "         -6.94138398e-01],\n",
       "        [ 1.29407121e+00, -1.14007400e+00, -1.47584499e+00,\n",
       "          2.71347134e-01],\n",
       "        [ 5.28027889e-01,  7.11727333e-01, -2.57351917e-01,\n",
       "          1.51332413e+00],\n",
       "        [ 4.19328104e-01,  7.33425081e-01,  5.74105293e-02,\n",
       "          1.38451307e-01],\n",
       "        [ 1.55118275e-01,  2.82365411e-03,  1.78452262e-01,\n",
       "          6.81186312e-02],\n",
       "        [ 3.91602418e-01,  6.22536010e-01, -5.82463390e-02,\n",
       "          1.05592326e-03]]),\n",
       " (16, 4))"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numh_tgt_len, embed_dim = num_heads * tgt_len, head_dim\n",
    "\n",
    "attn_output_dec_sa_permuted_reshaped = attn_output_dec_sa_permuted.reshape(numh_tgt_len, embed_dim)\n",
    "\n",
    "# attn_output_reshaped_np = attn_output_reshaped_np.T\n",
    "\n",
    "attn_output_dec_sa_permuted_reshaped, attn_output_dec_sa_permuted_reshaped.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[-1.02595520e+00,  9.66156820e-01, -2.29126388e+00,\n",
       "          1.80570101e+00, -2.39825494e-02, -1.33530222e+00,\n",
       "         -1.67510990e+00, -9.04894110e-01,  1.53036526e+00,\n",
       "         -1.85140924e+00, -1.51471974e+00,  9.21301429e-01,\n",
       "          5.28027889e-01,  7.11727333e-01, -2.57351917e-01,\n",
       "          1.51332413e+00],\n",
       "        [-4.94813659e-01,  3.63254786e-02, -1.18165435e+00,\n",
       "          1.23624897e+00, -8.35023637e-02, -1.15296018e+00,\n",
       "         -1.09796131e+00, -5.63298594e-01,  1.37871488e+00,\n",
       "         -1.49100274e+00, -1.79188216e+00,  7.92854025e-01,\n",
       "          4.19328104e-01,  7.33425081e-01,  5.74105293e-02,\n",
       "          1.38451307e-01],\n",
       "        [ 3.36763757e-01, -5.12013737e-01,  8.94194254e-02,\n",
       "          5.29699367e-01,  1.44866013e-01, -1.71497168e-01,\n",
       "          2.57711394e-01, -4.27455359e-01,  1.35127615e+00,\n",
       "         -8.67529220e-01, -1.43847060e+00, -6.94138398e-01,\n",
       "          1.55118275e-01,  2.82365411e-03,  1.78452262e-01,\n",
       "          6.81186312e-02],\n",
       "        [ 4.78021036e-01, -4.68885516e-02, -2.54583696e-01,\n",
       "          4.71600681e-01, -5.85121327e-02, -7.90012909e-01,\n",
       "         -3.01850517e-01, -4.11758949e-01,  1.29407121e+00,\n",
       "         -1.14007400e+00, -1.47584499e+00,  2.71347134e-01,\n",
       "          3.91602418e-01,  6.22536010e-01, -5.82463390e-02,\n",
       "          1.05592326e-03]]),\n",
       " (4, 16))"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_attn_dec_sa_op = np.zeros(attn_output_dec_sa_permuted_reshaped.shape)\n",
    "\n",
    "i = 0\n",
    "\n",
    "while i < attn_output_dec_sa_permuted_reshaped.shape[1]:\n",
    "    for j in range(attn_output_dec_sa_permuted_reshaped.shape[1]):\n",
    "\n",
    "        pos = i*attn_output_dec_sa_permuted_reshaped.shape[1] + j\n",
    "\n",
    "        blk = (j)*attn_output_dec_sa_permuted_reshaped.shape[1]\n",
    "        offset = i\n",
    "\n",
    "        final_attn_dec_sa_op[pos] = attn_output_dec_sa_permuted_reshaped[blk + offset]\n",
    "\n",
    "    i += 1\n",
    "        \n",
    "final_attn_dec_sa_op = final_attn_dec_sa_op.reshape(attn_output_dec_sa_permuted_reshaped.shape[1], -1)\n",
    "\n",
    "final_attn_dec_sa_op, final_attn_dec_sa_op.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Post self attention in the decoder self attention block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_num = 0\n",
    "\n",
    "weight_dec = state_dict[\"transformer.decoder.layers.{}.self_attn.out_proj.weight\".format(layer_num)].numpy()\n",
    "bias_dec = state_dict[\"transformer.decoder.layers.{}.self_attn.out_proj.bias\".format(layer_num)].numpy()\n",
    "\n",
    "# Output projection of the attention values\n",
    "op_dec_1 = np.matmul(final_attn_dec_sa_op, weight_dec.T) + bias_dec\n",
    "\n",
    "# Residual connection 1\n",
    "output_dec_1 = op_dec_1 + x_dec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Layer Norm 1\n",
    "\n",
    "norm_weight = state_dict[\"transformer.decoder.layers.{}.norm1.weight\".format(layer_num)].numpy()\n",
    "norm_bias = state_dict[\"transformer.decoder.layers.{}.norm1.bias\".format(layer_num)].numpy()\n",
    "\n",
    "linear_result_dec_1 = output_dec_1 * norm_weight + norm_bias\n",
    "\n",
    "\n",
    "mean = np.mean(linear_result_dec_1, axis=-1, keepdims=True)\n",
    "std = np.std(linear_result_dec_1, axis=-1, keepdims=True)\n",
    "epsilon = 1e-5 \n",
    "normalized_linear_result_dec_1 = (linear_result_dec_1 - mean) / (std + epsilon)\n",
    "\n",
    "\n",
    "layernorm_dec_1 = torch.nn.LayerNorm(normalized_linear_result_dec_1.shape[1:])\n",
    "\n",
    "linear_op_dec_1 = layernorm_dec_1(torch.tensor(normalized_linear_result_dec_1, dtype=torch.float32))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoder_0 norm1(x + sa(x)) = \n",
      "\n",
      "tensor([[ 0.7908,  2.6168,  0.1973,  0.1147, -0.8474, -0.2229,  0.2594,  0.6727,\n",
      "         -0.1353, -0.3840, -1.5653, -1.8525,  0.6594, -0.4768,  0.5674, -0.3942],\n",
      "        [-0.2944,  1.0971, -0.4035,  0.2262, -0.7558,  0.2582,  0.4699,  1.7153,\n",
      "         -0.6264,  0.8104, -1.2103, -2.6650,  0.3666,  0.4385, -0.2554,  0.8285],\n",
      "        [-2.0041,  0.1375,  0.0984, -1.5183, -0.8067, -0.8488,  0.9485,  0.4903,\n",
      "          1.1037, -0.7078, -0.4518,  0.9925, -0.5847,  1.0922,  0.4934,  1.5658],\n",
      "        [ 0.3119,  1.1703,  0.4139,  2.0400, -1.0685, -0.5466, -0.1719,  1.3237,\n",
      "         -0.9920, -0.0162, -0.9911, -1.6993,  0.1209,  1.0915, -0.9577, -0.0288]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(\"Decoder_{} norm1(x + sa(x)) = \\n\".format(layer_num))\n",
    "print(linear_op_dec_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "self_attn_dec = linear_op_dec_1.detach().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. CROSS ATTENTION (with masking)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = x_enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_num = 0\n",
    "\n",
    "query_dec_ca = self_attn_dec\n",
    "key_dec_ca, value_dec_ca = memory, memory\n",
    "\n",
    "\n",
    "tgt_len, embed_dim = query_dec_ca.shape\n",
    "\n",
    "\n",
    "W_dec_ca = state_dict[\"transformer.decoder.layers.{}.multihead_attn.in_proj_weight\".format(layer_num)].numpy()\n",
    "b_dec_ca = state_dict[\"transformer.decoder.layers.{}.multihead_attn.in_proj_bias\".format(layer_num)].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q_dec_ca_shape =  (4, 16)\n",
      "K_dec_ca_shape =  (4, 16)\n",
      "V_dec_ca_shape =  (4, 16)\n",
      "\n",
      "After reshaping... \n",
      "\n",
      "Q_dec_ca_shape =  (4, 4, 4)\n",
      "K_dec_ca_shape =  (4, 4, 4)\n",
      "V_dec_ca_shape =  (4, 4, 4)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "head_dim = embed_dim // num_heads\n",
    "\n",
    "# embed_dim\n",
    "E = query_dec_ca.shape[-1]\n",
    "\n",
    "# W_q, W_kv = W_dec_ca.split([E, E * 2])\n",
    "split_indices = [E, E * 2]\n",
    "\n",
    "\n",
    "# Split the array\n",
    "W_q = W_dec_ca[:split_indices[0], :]\n",
    "W_kv = W_dec_ca[split_indices[0]:, :]\n",
    "\n",
    "\n",
    "Q_dec_ca = np.matmul(query_dec_ca, W_q.T)\n",
    "\n",
    "tempop1 = np.matmul(key_dec_ca, W_kv.T)\n",
    "\n",
    "K_dec_ca, V_dec_ca = tempop1[:, 0:embed_dim], tempop1[:, embed_dim:2*embed_dim]\n",
    "\n",
    "print(\"Q_dec_ca_shape = \", Q_dec_ca.shape)\n",
    "print(\"K_dec_ca_shape = \", K_dec_ca.shape)\n",
    "print(\"V_dec_ca_shape = \", V_dec_ca.shape)\n",
    "print()\n",
    "\n",
    "print(\"After reshaping... \\n\")\n",
    "\n",
    "Q_dec_ca = np.transpose(np.reshape(Q_dec_ca, (tgt_len, num_heads, head_dim)), (1, 0, 2))\n",
    "K_dec_ca = np.transpose(np.reshape(K_dec_ca, (K_dec_ca.shape[0], num_heads, head_dim)), (1, 0, 2))\n",
    "V_dec_ca = np.transpose(np.reshape(V_dec_ca, (V_dec_ca.shape[0], num_heads, head_dim)), (1, 0, 2))\n",
    "\n",
    "print(\"Q_dec_ca_shape = \", Q_dec_ca.shape)\n",
    "print(\"K_dec_ca_shape = \", K_dec_ca.shape)\n",
    "print(\"V_dec_ca_shape = \", V_dec_ca.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q_dec_ca_0 = \n",
      "[[[ 1.1264023  -0.99372196  1.4196618   0.3692187 ]\n",
      "  [ 0.8368701  -0.39764592  1.7715282  -0.20809937]\n",
      "  [-0.80293286 -0.44238973 -0.44811496 -0.08087807]\n",
      "  [ 0.14036858  0.35611072  1.3061354   0.5647947 ]]\n",
      "\n",
      " [[ 0.34243727  0.6420679   1.0896097  -0.35935146]\n",
      "  [ 0.5932352  -0.02133346  0.17508404 -0.10132178]\n",
      "  [ 0.22525965 -0.34866813 -1.386473    0.5934426 ]\n",
      "  [ 1.0678444  -0.36406344  0.22718026 -0.70181704]]\n",
      "\n",
      " [[ 0.2477008   0.7129092  -1.5833832  -0.24581257]\n",
      "  [ 0.74955493 -0.14404999 -0.86124974  0.54051477]\n",
      "  [ 1.2827715  -0.9601834  -0.98511726 -0.02957391]\n",
      "  [ 0.630754    0.139621    0.13323686  0.7768027 ]]\n",
      "\n",
      " [[-0.5755281   0.7306101  -0.5634235  -1.4578571 ]\n",
      "  [-1.0339795   0.39026204 -0.66240704 -0.78707576]\n",
      "  [ 0.80315405 -0.98282826  0.02963641  0.14811504]\n",
      "  [-1.5224515   1.3331584  -0.5388493  -0.9295683 ]]]\n",
      "\n",
      "K_dec_ca_0 = \n",
      "[[[ 0.95142776 -0.33068627  0.6432074   0.57482743]\n",
      "  [ 0.74835044 -0.771075    0.7212278   0.51452005]\n",
      "  [-0.85368836 -0.758265    0.1419755  -1.1842299 ]\n",
      "  [ 0.09124306 -0.03390143  0.08835108 -0.17418046]]\n",
      "\n",
      " [[ 1.3066467   0.568587   -0.5934071   0.52526975]\n",
      "  [ 0.8624976   0.15071765 -0.75072086  0.19896287]\n",
      "  [-0.06874279  1.0256451  -0.48978415  1.0462722 ]\n",
      "  [ 1.2265112   0.86069226  0.328519    0.48125365]]\n",
      "\n",
      " [[-0.5275822  -0.55288136 -0.9860158  -0.1613704 ]\n",
      "  [-0.95025015 -0.26450378 -1.042937   -0.24228251]\n",
      "  [ 0.295321   -0.48780394 -0.27644557  0.8070261 ]\n",
      "  [ 0.5597233  -0.56254524 -0.18676668  0.4669406 ]]\n",
      "\n",
      " [[ 0.01470425 -0.06337745 -0.44124508  0.7674739 ]\n",
      "  [-0.42150283 -0.27298465  0.35315555  0.92618924]\n",
      "  [ 0.7237263   0.02458898 -0.90587515  0.15439534]\n",
      "  [-0.02122766 -0.45587838 -0.76865983  0.24242294]]]\n",
      "\n",
      "V_dec_ca_0 = \n",
      "[[[ 0.03884685 -0.6825258  -0.9350386   0.41813895]\n",
      "  [-0.17347123 -0.31428093 -0.5732488  -0.05033935]\n",
      "  [-0.7933146  -0.6954463  -0.02350719  0.8651541 ]\n",
      "  [-0.50799024 -1.1255643  -0.88036025  0.40825635]]\n",
      "\n",
      " [[-0.5621627  -0.22038198  0.14937153 -0.20583731]\n",
      "  [-0.09700433 -0.3715715  -0.28510317 -0.88427967]\n",
      "  [-0.16594435  0.75277156 -0.00840142 -0.37099436]\n",
      "  [-0.634578    0.3404766  -0.6516323   0.21701914]]\n",
      "\n",
      " [[ 0.94656277 -0.4971023   0.03386408 -0.02010286]\n",
      "  [ 0.6969339  -0.8405541  -0.08277085 -0.01530955]\n",
      "  [ 0.70044863 -0.6308099  -0.40443486  0.3145757 ]\n",
      "  [ 0.82350504 -0.77995455 -0.08896793 -0.14422797]]\n",
      "\n",
      " [[-0.05022697 -1.3696971  -0.36573842 -0.2704253 ]\n",
      "  [ 0.4938829  -0.90002847  0.45821935 -0.39851287]\n",
      "  [-0.4391116   0.37816218 -0.7839299   0.5517502 ]\n",
      "  [ 0.8322775  -1.3276825  -0.42189345 -0.94568866]]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Q_dec_ca_{} = \".format(layer_num))\n",
    "print(Q_dec_ca)\n",
    "print()\n",
    "\n",
    "print(\"K_dec_ca_{} = \".format(layer_num))\n",
    "print(K_dec_ca)\n",
    "print()\n",
    "\n",
    "print(\"V_dec_ca_{} = \".format(layer_num))\n",
    "print(V_dec_ca)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SoftMax (Scaled Dot Product of Q_dec_ca and K_dec_ca) = \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[0.37049463, 0.4298884 , 0.08394103, 0.115676  ],\n",
       "        [0.34362885, 0.37149283, 0.13540995, 0.14946844],\n",
       "        [0.1550957 , 0.18271734, 0.42273828, 0.23944868],\n",
       "        [0.3496295 , 0.32968232, 0.12520583, 0.19548234]],\n",
       "\n",
       "       [[0.22347003, 0.17626783, 0.19703059, 0.4032315 ],\n",
       "        [0.27847624, 0.245871  , 0.18112177, 0.29453105],\n",
       "        [0.29538497, 0.30592418, 0.2537745 , 0.14491634],\n",
       "        [0.2999228 , 0.281205  , 0.11160002, 0.30727214]],\n",
       "\n",
       "       [[0.3081194 , 0.34238714, 0.17676455, 0.17272897],\n",
       "        [0.22735739, 0.19056454, 0.29483008, 0.287248  ],\n",
       "        [0.23956823, 0.16378114, 0.27357313, 0.32307753],\n",
       "        [0.17402405, 0.15002577, 0.34607592, 0.32987425]],\n",
       "\n",
       "       [[0.21477   , 0.16061941, 0.32229054, 0.30232006],\n",
       "        [0.23761584, 0.20637609, 0.24874291, 0.30726513],\n",
       "        [0.23228645, 0.22125815, 0.28067073, 0.2657847 ],\n",
       "        [0.25358972, 0.2304999 , 0.23621926, 0.27969116]]], dtype=float32)"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_len = K_dec_ca.shape[1]\n",
    "\n",
    "Q_dec_ca1 = Q_dec_ca\n",
    "K_dec_ca1 = K_dec_ca\n",
    "V_dec_ca1 = V_dec_ca\n",
    "\n",
    "scale_factor = 1 / math.sqrt(Q_dec_ca1.shape[-1]) \n",
    "\n",
    "K_dec_ca1_T = np.transpose(K_dec_ca1, axes=(0, 2, 1))\n",
    "\n",
    "attn_weight_ca = Q_dec_ca1 @ K_dec_ca1_T * scale_factor\n",
    "# attn_weight_ca += attn_bias\n",
    "\n",
    "exp_attn_weight_ca = np.exp(attn_weight_ca)\n",
    "sum_exp_attn_weight_ca = np.sum(exp_attn_weight_ca, axis=-1, keepdims=True)\n",
    "sum_exp_attn_weight_ca = exp_attn_weight_ca / sum_exp_attn_weight_ca\n",
    "\n",
    "print(\"SoftMax (Scaled Dot Product of Q_dec_ca and K_dec_ca) = \")\n",
    "sum_exp_attn_weight_ca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_output_dec_ca = sum_exp_attn_weight_ca @ V_dec_ca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Attention Values (Cross attention decoder)= \n",
      "\n",
      "[[[-0.18553464 -0.57655513 -0.6966696   0.25312534]\n",
      "  [-0.23444562 -0.6136954  -0.6690333   0.3031558 ]\n",
      "  [-0.48267326 -0.72678804 -0.47050145  0.5191439 ]\n",
      "  [-0.24223913 -0.64934593 -0.69094515  0.31772697]]\n",
      "\n",
      " [[-0.4313032   0.17086504 -0.28128847 -0.18745682]\n",
      "  [-0.39735857  0.08389443 -0.2219498  -0.2780158 ]\n",
      "  [-0.32980356  0.06160462 -0.13966209 -0.39402306]\n",
      "  [-0.40939105  0.01804294 -0.23653856 -0.2851182 ]]\n",
      "\n",
      " [[ 0.79633325 -0.6871874  -0.10476258  0.01925761]\n",
      "  [ 0.79108244 -0.6832218  -0.15286937  0.0438292 ]\n",
      "  [ 0.7985909  -0.68131524 -0.1448296   0.03213922]\n",
      "  [ 0.78334427 -0.68820757 -0.175838    0.05549478]]\n",
      "\n",
      " [[ 0.1786326  -0.71823883 -0.38515076 -0.23016493]\n",
      "  [ 0.23649485 -0.8250915  -0.31696987 -0.29983407]\n",
      "  [ 0.19556943 -0.7640394  -0.3157303  -0.24747981]\n",
      "  [ 0.23015694 -0.8368094  -0.2903072  -0.294601  ]]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Final Attention Values (Cross attention decoder)= \\n\")\n",
    "print(attn_output_dec_ca)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reshaping the final attention output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 4, 4)"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_output_dec_ca_permuted = np.transpose(attn_output_dec_ca, axes=(0,1,2))\n",
    "\n",
    "attn_output_dec_ca_permuted.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[-0.18553464, -0.57655513, -0.6966696 ,  0.25312534],\n",
       "        [-0.23444562, -0.6136954 , -0.6690333 ,  0.3031558 ],\n",
       "        [-0.48267326, -0.72678804, -0.47050145,  0.5191439 ],\n",
       "        [-0.24223913, -0.64934593, -0.69094515,  0.31772697],\n",
       "        [-0.4313032 ,  0.17086504, -0.28128847, -0.18745682],\n",
       "        [-0.39735857,  0.08389443, -0.2219498 , -0.2780158 ],\n",
       "        [-0.32980356,  0.06160462, -0.13966209, -0.39402306],\n",
       "        [-0.40939105,  0.01804294, -0.23653856, -0.2851182 ],\n",
       "        [ 0.79633325, -0.6871874 , -0.10476258,  0.01925761],\n",
       "        [ 0.79108244, -0.6832218 , -0.15286937,  0.0438292 ],\n",
       "        [ 0.7985909 , -0.68131524, -0.1448296 ,  0.03213922],\n",
       "        [ 0.78334427, -0.68820757, -0.175838  ,  0.05549478],\n",
       "        [ 0.1786326 , -0.71823883, -0.38515076, -0.23016493],\n",
       "        [ 0.23649485, -0.8250915 , -0.31696987, -0.29983407],\n",
       "        [ 0.19556943, -0.7640394 , -0.3157303 , -0.24747981],\n",
       "        [ 0.23015694, -0.8368094 , -0.2903072 , -0.294601  ]],\n",
       "       dtype=float32),\n",
       " (16, 4))"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numh_tgt_len, embed_dim = num_heads * tgt_len, head_dim\n",
    "\n",
    "attn_output_dec_ca_permuted_reshaped = attn_output_dec_ca_permuted.reshape(numh_tgt_len, embed_dim)\n",
    "\n",
    "# attn_output_reshaped_np = attn_output_reshaped_np.T\n",
    "\n",
    "attn_output_dec_ca_permuted_reshaped, attn_output_dec_ca_permuted_reshaped.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[-0.18553464, -0.57655513, -0.69666958,  0.25312534, -0.4313032 ,\n",
       "          0.17086504, -0.28128847, -0.18745682,  0.79633325, -0.68718737,\n",
       "         -0.10476258,  0.01925761,  0.1786326 , -0.71823883, -0.38515076,\n",
       "         -0.23016493],\n",
       "        [-0.23444562, -0.61369538, -0.66903329,  0.30315581, -0.39735857,\n",
       "          0.08389443, -0.2219498 , -0.27801579,  0.79108244, -0.68322182,\n",
       "         -0.15286937,  0.0438292 ,  0.23649485, -0.82509148, -0.31696987,\n",
       "         -0.29983407],\n",
       "        [-0.48267326, -0.72678804, -0.47050145,  0.51914388, -0.32980356,\n",
       "          0.06160462, -0.13966209, -0.39402306,  0.7985909 , -0.68131524,\n",
       "         -0.1448296 ,  0.03213922,  0.19556943, -0.7640394 , -0.3157303 ,\n",
       "         -0.24747981],\n",
       "        [-0.24223913, -0.64934593, -0.69094515,  0.31772697, -0.40939105,\n",
       "          0.01804294, -0.23653856, -0.28511819,  0.78334427, -0.68820757,\n",
       "         -0.17583799,  0.05549478,  0.23015694, -0.8368094 , -0.29030719,\n",
       "         -0.29460099]]),\n",
       " (4, 16))"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_attn_dec_ca_op = np.zeros(attn_output_dec_ca_permuted_reshaped.shape)\n",
    "\n",
    "i = 0\n",
    "\n",
    "while i < attn_output_dec_ca_permuted_reshaped.shape[1]:\n",
    "    for j in range(attn_output_dec_ca_permuted_reshaped.shape[1]):\n",
    "\n",
    "        pos = i*attn_output_dec_ca_permuted_reshaped.shape[1] + j\n",
    "\n",
    "        blk = (j)*attn_output_dec_ca_permuted_reshaped.shape[1]\n",
    "        offset = i\n",
    "\n",
    "        final_attn_dec_ca_op[pos] = attn_output_dec_ca_permuted_reshaped[blk + offset]\n",
    "\n",
    "    i += 1\n",
    "        \n",
    "final_attn_dec_ca_op = final_attn_dec_ca_op.reshape(attn_output_dec_ca_permuted_reshaped.shape[1], -1)\n",
    "\n",
    "final_attn_dec_ca_op, final_attn_dec_ca_op.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11. Post cross attention in the decoder block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_num = 0\n",
    "\n",
    "weight_dec_ca = state_dict[\"transformer.decoder.layers.{}.multihead_attn.out_proj.weight\".format(layer_num)].numpy()\n",
    "bias_dec_ca = state_dict[\"transformer.decoder.layers.{}.multihead_attn.out_proj.bias\".format(layer_num)].numpy()\n",
    "\n",
    "# Output projection of the attention values\n",
    "op_dec_ca1 = np.matmul(final_attn_dec_ca_op, weight_dec_ca.T) + bias_dec_ca\n",
    "\n",
    "# Residual connection 2\n",
    "output_dec_2 = op_dec_ca1 + self_attn_dec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Layer Norm 2\n",
    "\n",
    "norm_weight = state_dict[\"transformer.decoder.layers.{}.norm2.weight\".format(layer_num)].numpy()\n",
    "norm_bias = state_dict[\"transformer.decoder.layers.{}.norm2.bias\".format(layer_num)].numpy()\n",
    "\n",
    "\n",
    "linear_result_dec_2 = output_dec_2*norm_weight + norm_bias\n",
    "\n",
    "\n",
    "mean = np.mean(linear_result_dec_2, axis=-1, keepdims=True)\n",
    "std = np.std(linear_result_dec_2, axis=-1, keepdims=True)\n",
    "epsilon = 1e-5 \n",
    "normalized_linear_result_dec_2 = (linear_result_dec_2 - mean) / (std + epsilon)\n",
    "\n",
    "\n",
    "layernorm_dec_2 = torch.nn.LayerNorm(normalized_linear_result_dec_2.shape[1:])\n",
    "\n",
    "linear_op_dec_2 = layernorm_dec_2(torch.tensor(normalized_linear_result_dec_2, dtype=torch.float32))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_op_dec_2 = linear_op_dec_2.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear projections \n",
    "\n",
    "linear1_weight = state_dict[\"transformer.decoder.layers.{}.linear1.weight\".format(layer_num)].numpy()\n",
    "linear1_bias = state_dict[\"transformer.decoder.layers.{}.linear1.bias\".format(layer_num)].numpy()\n",
    "linear2_weight = state_dict[\"transformer.decoder.layers.{}.linear2.weight\".format(layer_num)].numpy()\n",
    "linear2_bias = state_dict[\"transformer.decoder.layers.{}.linear2.bias\".format(layer_num)].numpy()\n",
    "\n",
    "\n",
    "op_dec_1 = np.matmul(linear_op_dec_2, linear1_weight.T) + linear1_bias\n",
    "\n",
    "# ReLU activation\n",
    "op_dec_1_relu = np.maximum(op_dec_1, 0)\n",
    "\n",
    "# Linear projection 2\n",
    "op_dec_2 = np.matmul(op_dec_1_relu, linear2_weight.T) + linear2_bias\n",
    "\n",
    "ff_dec = op_dec_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Residual conenction 2 \n",
    "\n",
    "output_enc_3 = linear_op_dec_2 + ff_dec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Layer Norm 3\n",
    "\n",
    "norm_weight = state_dict[\"transformer.decoder.layers.{}.norm2.weight\".format(layer_num)].numpy()\n",
    "norm_bias = state_dict[\"transformer.decoder.layers.{}.norm2.bias\".format(layer_num)].numpy()\n",
    "\n",
    "\n",
    "linear_result_dec_3 = output_enc_3*norm_weight + norm_bias\n",
    "\n",
    "\n",
    "mean = np.mean(linear_result_dec_3, axis=-1, keepdims=True)\n",
    "std = np.std(linear_result_dec_3, axis=-1, keepdims=True)\n",
    "epsilon = 1e-5 \n",
    "normalized_linear_result_dec_3 = (linear_result_dec_3 - mean) / (std + epsilon)\n",
    "\n",
    "\n",
    "layernorm_dec_3 = torch.nn.LayerNorm(normalized_linear_result_dec_3.shape[1:])\n",
    "\n",
    "linear_op_dec_3 = layernorm_dec_3(torch.tensor(normalized_linear_result_dec_3, dtype=torch.float32))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_op_dec_3 = linear_op_dec_3.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "norm3(x'' + ff(x'')) \n",
      " where, x'' = Decoder_curr_layer norm2(x' + mha(x'))\n",
      "\n",
      "[[-0.5356866   1.9318144  -0.51183444 -0.06886522 -0.35834506  0.00633696\n",
      "   0.74473906  0.38577303  0.36656466  0.24361272 -1.5169702  -1.1969402\n",
      "   1.2439213  -1.0069604   1.6122651  -1.3394252 ]\n",
      " [-0.95041865  0.7818231  -0.9022948  -0.16324307 -0.3592338   0.16954553\n",
      "   0.85607445  1.721027   -0.5615748   1.3511652  -1.3389051  -2.1498117\n",
      "   0.5818949   0.4841478   0.7635639  -0.28375974]\n",
      " [-1.7267284   0.1277106  -0.20488055 -1.9431874   0.58657104 -1.0944295\n",
      "   0.8495381   0.56639     0.32088426 -0.5575043  -1.0710053   0.7707297\n",
      "  -0.08021259  1.0479496   1.6733918   0.7347833 ]\n",
      " [-0.18048695  1.1450682  -0.14022484  1.8074682  -0.66768867 -0.591716\n",
      "   0.12012903  1.2484851  -1.2300383   0.6516065  -1.4823984  -1.1182386\n",
      "   0.0629568   1.5355982  -0.04201251 -1.1185073 ]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"norm3(x'' + ff(x'')) \\n where, x'' = Decoder_curr_layer norm2(x' + mha(x'))\\n\")\n",
    "print(linear_op_dec_3)\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12. Feed forward layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "dec_output_final = linear_op_dec_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_ff = state_dict[\"fc.weight\"].numpy()\n",
    "b_ff = state_dict[\"fc.bias\"].numpy()\n",
    "\n",
    "final_op = dec_output_final@W_ff.T + b_ff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 0.37746853,  0.07337449, -0.8095904 ,  0.14691624, -0.4905476 ,\n",
       "          0.37394795, -0.2704848 , -0.92419446,  0.6273823 , -0.6122049 ,\n",
       "          0.25933215, -0.16878009,  0.13227718,  1.3098803 ,  0.7643256 ,\n",
       "         -0.61062217, -0.13705996,  0.75523925, -0.5415498 ,  0.7422502 ],\n",
       "        [ 0.43439144,  0.35608122, -0.49823174, -0.43256435, -0.23772678,\n",
       "          0.8758416 , -1.064142  , -0.3422812 ,  0.3772208 , -0.445173  ,\n",
       "          0.65189123, -0.26663378, -0.06459779,  0.780676  ,  0.5272392 ,\n",
       "         -0.6572926 ,  0.09763362,  1.0803541 , -0.9153302 ,  1.1743307 ],\n",
       "        [-0.5873185 ,  0.21614808, -0.60998964, -0.07736345, -0.5957562 ,\n",
       "          0.68771374, -0.267695  , -0.44631606,  0.51722544,  1.1136649 ,\n",
       "          0.10397579, -0.30067962, -1.1872319 , -0.25477836,  0.6387973 ,\n",
       "          0.09141681,  0.02895935, -0.05193508,  0.38341376, -0.10680325],\n",
       "        [ 0.7888366 ,  0.7641175 , -0.12451119, -0.02487959,  0.18648735,\n",
       "         -0.01425064, -0.9725598 , -0.91765755,  0.38885283, -0.32677057,\n",
       "          0.17021924, -0.866742  ,  0.21357349,  0.38245967,  0.2635069 ,\n",
       "         -0.99247587, -0.4423721 ,  1.0394772 , -1.2948374 ,  0.77811533]],\n",
       "       dtype=float32),\n",
       " (4, 20))"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_op, final_op.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "my_project_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
