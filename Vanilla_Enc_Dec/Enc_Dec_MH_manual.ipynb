{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-head attention transformer\n",
    "### Encoder and Decoder\n",
    "### (With masking)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions to get the manual calulations for each component"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function to fetch word embeddings with help of token indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=512, dropout=0):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.encoding = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1).float()\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(torch.log(torch.tensor(10000.0)) / d_model))\n",
    "        self.encoding[:, 0::2] = torch.sin(position * div_term)\n",
    "        self.encoding[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.encoding = self.encoding.unsqueeze(0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.encoding[:, :x.shape[1]].detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def look_up_table(sentence, vocab_embeds, embedding):\n",
    "\n",
    "    for i in range(sentence.size(0)):\n",
    "        for j in range(sentence.size(1)):\n",
    "            \n",
    "            # Get the index for the current word token index in the sequence\n",
    "            word_index = sentence[i, j].item()\n",
    "\n",
    "            if word_index < 0 or word_index >= vocab_embeds.size(0):\n",
    "                raise ValueError(f\"Invalid word index: {word_index}\")\n",
    "\n",
    "            # Lookup the corresponding embedding vector for the word\n",
    "            embedding[i, j, :] = vocab_embeds[word_index, :]\n",
    "\n",
    "            print(f\"Word index: {word_index}, Embedding: {vocab_embeds[word_index, :]}\")\n",
    "    print()\n",
    "\n",
    "    return embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embeddings and Positional encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding_outputs(src_sentence,  tgt_sentence, max_seq_len, state_dict, d_model):\n",
    "\n",
    "    src_vocab_embeds = state_dict[\"src_embedding.weight\"]\n",
    "\n",
    "    src_embedding = torch.zeros(src_sentence.size(0), src_sentence.size(1), d_model)\n",
    "    print(\"Source sentence embedding\")\n",
    "    src_embedding =  look_up_table(src_sentence, src_vocab_embeds, src_embedding)\n",
    "    print(src_embedding.shape)\n",
    "\n",
    "    tgt_vocab_embeds = state_dict[\"tgt_embedding.weight\"]\n",
    "\n",
    "    tgt_embedding = torch.zeros(tgt_sentence.size(0), tgt_sentence.size(1), d_model)\n",
    "\n",
    "\n",
    "    print(\"Target sentence embedding\")\n",
    "    tgt_embedding =  look_up_table(tgt_sentence, tgt_vocab_embeds, tgt_embedding)\n",
    "\n",
    "    pe = PositionalEncoding(d_model = d_model, dropout=0, max_len=max_seq_len)\n",
    "\n",
    "    print(\"PE of src :\")\n",
    "    print(pe(src_sentence))\n",
    "    print()\n",
    "    print(\"PE of tgt :\")\n",
    "    print(pe(tgt_sentence))\n",
    "    print()\n",
    "\n",
    "    pe_src_embeds = src_embedding + pe(src_sentence)\n",
    "\n",
    "    pe_tgt_embeds = tgt_embedding + pe(tgt_sentence)\n",
    "\n",
    "    print(\"PE source embeddings : \\n\")\n",
    "    print(pe_src_embeds)\n",
    "    print()\n",
    "\n",
    "    print(\"PE target embeddings : \\n\")\n",
    "    print(pe_tgt_embeds)\n",
    "    print()\n",
    "\n",
    "    return pe_src_embeds, pe_tgt_embeds\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder function to display the intermediate outputs and get the final outputs from the encoder\n",
    "\n",
    "### Self attention \n",
    "\n",
    "#### Functions to perform the attention calculation with Q,K and V matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def atten_product_needs_wts_false(Q, V, K, bsz, head_dim, src_len, tgt_len, embed_dim, attn_mask, num_heads):\n",
    "\n",
    "\n",
    "    # *** For multi-head attention ***\n",
    "    #  (bsz*num_heads, src_len , head_dim) -> (bsz, num_heads, tgt_len, head_dim)\n",
    "    Q1 = Q.view(bsz, num_heads, tgt_len, head_dim)\n",
    "    K1 = K.view(bsz, num_heads, src_len, head_dim)\n",
    "    V1 = V.view(bsz, num_heads, src_len, head_dim)\n",
    "\n",
    "\n",
    "    L, S = Q1.size(-2), K1.size(-2)\n",
    "\n",
    "    scale_factor = 1 / math.sqrt(Q1.size(-1)) \n",
    "    # scale_factor = 1\n",
    "    attn_bias = torch.zeros(L, S, dtype=Q1.dtype)\n",
    "\n",
    "    if attn_mask is not None:\n",
    "        if attn_mask.dtype == torch.bool:\n",
    "            # attn_mask.masked_fill_(attn_mask.logical_not(), float(\"-inf\"))\n",
    "\n",
    "            masked_tensor = attn_mask.float().masked_fill(attn_mask, float('-inf'))\n",
    "            masked_tensor = masked_tensor.masked_fill(~attn_mask, 0)\n",
    "            attn_mask = masked_tensor\n",
    "\n",
    "            print(\"Attnetion mask infunction = \")\n",
    "            print(attn_mask)\n",
    "            print()\n",
    "            attn_bias = attn_bias.unsqueeze(0).unsqueeze(0)\n",
    "            attn_bias += attn_mask\n",
    "\n",
    "        else:\n",
    "            attn_bias += attn_mask\n",
    "            attn_bias = attn_bias.unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "\n",
    "    # print(\"Attnetion bias = \", attn_bias.shape)\n",
    "    # print(attn_bias)\n",
    "    # print()\n",
    "            \n",
    "\n",
    "    # (bsz, num_heads, tgt_len, head_dim) @ (bsz, num_heads, head_dim, tgt_len) -> (bsz, num_heads, tgt_len, tgt_len) \n",
    "    attn_weight = Q1 @ K1.transpose(-2, -1) * scale_factor\n",
    "    attn_weight += attn_bias\n",
    "\n",
    "    # (bsz, num_heads, tgt_len, tgt_len) \n",
    "    attn_weight = torch.softmax(attn_weight, dim=-1)\n",
    "\n",
    "\n",
    "    sum_last_dim = attn_weight.sum(dim=-1)\n",
    "    tolerance = 1e-6  \n",
    "    assert torch.allclose(sum_last_dim, torch.ones_like(sum_last_dim), atol=tolerance), \"Attention weights sum is not approximately equal to 1\"\n",
    "\n",
    "\n",
    "    print(attn_weight)\n",
    "    # (bsz, num_heads, tgt_len, tgt_len) @ (bsz, num_heads, tgt_len, head_dim) -> (bsz, num_heads, tgt_len, head_dim) \n",
    "    attn_output = attn_weight @ V1\n",
    "\n",
    "    print(\"ATT = \", attn_output)\n",
    "\n",
    "    # print(\"Dot product attention  = \")\n",
    "    # print(attn_weight.shape, attn_weight)\n",
    "\n",
    "    # print(attn_output.shape)\n",
    "    # print(bsz, tgt_len, embed_dim)\n",
    "    \n",
    "    # (bsz*tgt_len, embed_dim)\n",
    "    attn_output = attn_output.permute(2, 0, 1, 3).contiguous().view(bsz * tgt_len, embed_dim)\n",
    "\n",
    "    print(\"Attention output = \")\n",
    "    print(attn_weight.shape, attn_weight)\n",
    "\n",
    "    return attn_output\n",
    "\n",
    "\n",
    "\n",
    "def atten_product_needs_wts_true(Q, K, V, bsz, tgt_len, embed_dim, attn_mask):\n",
    "\n",
    "    # *** For multi-head attention ***\n",
    "    #  (bsz*num_heads, src_len , head_dim)\n",
    "    \n",
    "    B, Nt, E = Q.shape\n",
    "\n",
    "    Q_scaled = Q / math.sqrt(E)\n",
    "\n",
    "    if attn_mask is not None:\n",
    "        temp_pdt_matrix = torch.baddbmm(attn_mask, Q_scaled, K.transpose(-2, -1))\n",
    "    else:\n",
    "        temp_pdt_matrix = torch.bmm(Q_scaled, K.transpose(-2, -1))\n",
    "\n",
    "    attn_wt_matrix = torch.nn.functional.softmax(temp_pdt_matrix, dim=-1)\n",
    "\n",
    "    attn_output = torch.bmm(attn_wt_matrix, V)\n",
    "\n",
    "    attn_output = attn_output.transpose(0, 1).contiguous().view(tgt_len * bsz, embed_dim)\n",
    "\n",
    "\n",
    "    sum_last_dim = attn_wt_matrix.sum(dim=-1)\n",
    "\n",
    "    tolerance = 1e-6  \n",
    "    assert torch.allclose(sum_last_dim, torch.ones_like(sum_last_dim), atol=tolerance), \"Attention weights sum is not approximately equal to 1\"\n",
    "\n",
    "\n",
    "    print(\"Encoder Attention output = \")\n",
    "    print(attn_output)\n",
    "    print()\n",
    "\n",
    "    return attn_output, attn_wt_matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function to get the Q,K,V matrices from the model's intialised weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_qkv(query, key, value ,W, b):\n",
    "\n",
    "    # embed_dim\n",
    "    E = query.size(-1)\n",
    "\n",
    "    if key is value:\n",
    "        if query is key:\n",
    "            \n",
    "            # (src_len, bsz, embed_dim) @ (embed_dim*num_heads, embed_dim).T -> (src_len, bsz, embed_dim*num_heads)\n",
    "            tempop1 = query@W.T\n",
    "\n",
    "            # reshape to 3, E and not E, 3 is deliberate for better memory coalescing and keeping same order as chunk()\n",
    "            tempop1 = tempop1.unflatten(-1, (3, E)).unsqueeze(0).transpose(0, -2).squeeze(-2).contiguous()\n",
    "\n",
    "            # (src_len, bsz, embed_dim)\n",
    "            return tempop1[0], tempop1[1], tempop1[2]\n",
    "        \n",
    "\n",
    "        else:\n",
    "\n",
    "            # (embed_dim*1, embed_dim)\n",
    "            # (embed_dim*2, embed_dim)\n",
    "            W_q, W_kv = W.split([E, E * 2])\n",
    "\n",
    "\n",
    "            if b is None:\n",
    "                b_q = b_kv = None\n",
    "            else:\n",
    "                b_q, b_kv = b.split([E, E * 2])\n",
    "\n",
    "            # (src_len, bsz, embed_dim) @ (embed_dim*1, embed_dim).T -> (src_len, bsz, embed_dim)\n",
    "            q_matmul = query@W_q.T\n",
    "\n",
    "\n",
    "            # (src_len, bsz, embed_dim) @ (embed_dim*2, embed_dim).T -> (src_len, bsz, embed_dim*2)\n",
    "            kv_matmul = key@W_kv.T\n",
    "            print(kv_matmul)\n",
    "\n",
    "            kv_matmul = kv_matmul.unflatten(-1, (2, E)).unsqueeze(0).transpose(0, -2).squeeze(-2).contiguous()\n",
    "\n",
    "            # (src_len, bsz, embed_dim)\n",
    "            return q_matmul, kv_matmul[0], kv_matmul[1]\n",
    "\n",
    "    else:\n",
    "\n",
    "        W_q, W_k, W_v = W.chunk(3)\n",
    "        if b is None:\n",
    "            b_q = b_k = b_v = None\n",
    "        else:\n",
    "            b_q, b_k, b_v = b.chunk(3)\n",
    "\n",
    "\n",
    "        q_matmul = query@W_q.T\n",
    "        k_matmul = key@W_k.T\n",
    "        v_matmul = value@W_v.T\n",
    "\n",
    "        return q_matmul, k_matmul, v_matmul\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encoder block's self attention output function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder_block_attn_output(x, state_dict, layer_num, embed_dim, num_heads, need_weights = False, src_mask = None):\n",
    "\n",
    "    # (src_len, bsz, embed_dim)\n",
    "    query_enc = key_enc = value_enc = x\n",
    "\n",
    "    tgt_len, bsz, embed_dim = x.shape\n",
    "    \n",
    "    # (embed_dim*num_heads, embed_dim)\n",
    "    W_enc = state_dict[\"transformer.encoder.layers.{}.self_attn.in_proj_weight\".format(layer_num)]\n",
    "    b_enc = state_dict[\"transformer.encoder.layers.{}.self_attn.in_proj_bias\".format(layer_num)]\n",
    "\n",
    "\n",
    "    head_dim = embed_dim//num_heads\n",
    "    \n",
    "    # (src_len, bsz, embed_dim)\n",
    "    Q_enc,K_enc,V_enc = get_qkv(query_enc, key_enc, value_enc ,W_enc, b_enc)\n",
    "\n",
    "    print(\"Q_enc_shape = \", Q_enc.shape)\n",
    "    print(\"K_enc_shape = \", K_enc.shape)\n",
    "    print(\"V_enc_shape = \", V_enc.shape)\n",
    "    print()\n",
    "    \n",
    "    # (1, src_len, bsz, embed_dim)\n",
    "    # Q_enc = Q_enc.unsqueeze(0)\n",
    "    # K_enc = K_enc.unsqueeze(0)\n",
    "    # V_enc = V_enc.unsqueeze(0)\n",
    "\n",
    "    # (1, src_len, bsz, embed_dim) -> ( bsz*num_heads, src_len , head_dim)\n",
    "    Q_enc = Q_enc.reshape(tgt_len, bsz * num_heads, head_dim).transpose(0, 1)\n",
    "    K_enc = K_enc.reshape(K_enc.shape[0], bsz * num_heads, head_dim).transpose(0, 1)\n",
    "    V_enc = V_enc.reshape(V_enc.shape[0], bsz * num_heads, head_dim).transpose(0, 1)\n",
    "\n",
    "    print(\"Q_enc_shape = \", Q_enc.shape)\n",
    "    print(\"K_enc_shape = \", K_enc.shape)\n",
    "    print(\"V_enc_shape = \", V_enc.shape)\n",
    "    print()\n",
    "\n",
    "\n",
    "    print(\"Q_enc_{} = \".format(layer_num))\n",
    "    print(Q_enc)\n",
    "    print()\n",
    "\n",
    "    print(\"K_enc_{} = \".format(layer_num))\n",
    "    print(K_enc)\n",
    "    print()\n",
    "\n",
    "    print(\"V_enc_{} = \".format(layer_num))\n",
    "    print(V_enc)\n",
    "    print()\n",
    "\n",
    "\n",
    "    src_len = K_enc.size(1)\n",
    "\n",
    "    attn_mask = src_mask\n",
    "    if attn_mask is not None:\n",
    "\n",
    "        # Ensuring attn_mask's dim is 3\n",
    "        if attn_mask.dim() == 2:\n",
    "            correct_2d_size = (tgt_len, src_len)\n",
    "            if attn_mask.shape != correct_2d_size:\n",
    "                raise RuntimeError(f\"The shape of the 2D attn_mask is {attn_mask.shape}, but should be {correct_2d_size}.\")\n",
    "            attn_mask = attn_mask.unsqueeze(0)\n",
    "        elif attn_mask.dim() == 3:\n",
    "            correct_3d_size = (bsz * num_heads, tgt_len, src_len)\n",
    "            if attn_mask.shape != correct_3d_size:\n",
    "                raise RuntimeError(f\"The shape of the 3D attn_mask is {attn_mask.shape}, but should be {correct_3d_size}.\")\n",
    "        else:\n",
    "            raise RuntimeError(f\"attn_mask's dimension {attn_mask.dim()} is not supported\")\n",
    "\n",
    "    # attn_mask can be either (L,S) or (N*num_heads, L, S)\n",
    "    # if attn_mask's shape is (1, L, S) we need to unsqueeze to (1, 1, L, S)\n",
    "    # in order to match the input for SDPA of (N, num_heads, L, S)\n",
    "    if attn_mask is not None:\n",
    "        if attn_mask.size(0) == 1 and attn_mask.dim() == 3:\n",
    "            attn_mask = attn_mask.unsqueeze(0)\n",
    "        else:\n",
    "            attn_mask = attn_mask.view(bsz, num_heads, -1, src_len)\n",
    "\n",
    "\n",
    "    if need_weights is False:\n",
    "\n",
    "        attn_output = atten_product_needs_wts_false(Q_enc, V_enc, K_enc, bsz, head_dim, src_len, tgt_len, embed_dim, attn_mask, num_heads=num_heads)\n",
    "\n",
    "        return attn_output, src_len, head_dim, None\n",
    "    \n",
    "    else:\n",
    "\n",
    "        attn_enc_output,attn_wt_matrix_enc = atten_product_needs_wts_true(Q_enc, K_enc, V_enc, bsz, tgt_len, embed_dim,  attn_mask)\n",
    "\n",
    "        return attn_enc_output, src_len, head_dim, attn_wt_matrix_enc\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Post self attention in encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function to perform the linear layer calculations after encoder's self attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder_block_post_attn_output(x, attn_enc_output, state_dict, layer_num, bsz, tgt_len):\n",
    "\n",
    "    # (bsz*src_len , embed_dim) @ (embed_dim , embed_dim).T -> (bsz*src_len , embed_dim)\n",
    "\n",
    "    op_enc_1 = torch.matmul(attn_enc_output, state_dict[\"transformer.encoder.layers.{}.self_attn.out_proj.weight\".format(layer_num)].t()) + state_dict[\"transformer.encoder.layers.{}.self_attn.out_proj.bias\".format(layer_num)]\n",
    "\n",
    "    # (bsz*src_len , embed_dim) -> (src_len, bsz, embed_dim)\n",
    "    attn_enc_output = op_enc_1.view(tgt_len, bsz, attn_enc_output.size(1))\n",
    "\n",
    "    # Here src is the original passed inputs to the 1st transformer encoder layer which \n",
    "    # are pe_src_embeds \n",
    "    # (src_len, bsz, embed_dim)\n",
    "    output_enc_1 = attn_enc_output + x\n",
    "\n",
    "    #  (src_len, bsz, embed_dim) @ (embed_dim) -> (src_len, bsz, embed_dim) \n",
    "    linear_result_enc_1 = output_enc_1*state_dict[\"transformer.encoder.layers.{}.norm1.weight\".format(layer_num)] + state_dict[\"transformer.encoder.layers.{}.norm1.bias\".format(layer_num)]\n",
    "    # Layer normalization from Torch's implementation\n",
    "    layernorm_enc_1 = torch.nn.LayerNorm(normalized_shape=linear_result_enc_1.shape[2:])\n",
    "    linear_op_enc_1 = layernorm_enc_1(linear_result_enc_1)\n",
    "\n",
    "    # Manual Layer Normalization\n",
    "    x = linear_result_enc_1\n",
    "\n",
    "    # Obtained layer norm weights and biases (learnable)\n",
    "    w = layernorm_enc_1.weight\n",
    "    b = layernorm_enc_1.bias\n",
    "\n",
    "    linear_result_enc_1f = w*x + b\n",
    "\n",
    "    epsilon = 1e-05  \n",
    "    mean = linear_result_enc_1f.mean(dim=-1, keepdim=True)\n",
    "    # print(\"mean = \", mean)\n",
    "    std = linear_result_enc_1f.std(dim=-1, unbiased=False, keepdim=True)\n",
    "    # print(\"std =\", std)\n",
    "\n",
    "    # print((linear_result_enc_1f - mean) / (std + epsilon))\n",
    "\n",
    "    normalized_result_enc_1 = (linear_result_enc_1f - mean) / (std + epsilon) * w + b\n",
    "\n",
    "    # print(normalized_result_enc_1)\n",
    "\n",
    "    op_enc_1 = torch.matmul(normalized_result_enc_1, state_dict[\"transformer.encoder.layers.{}.linear1.weight\".format(layer_num)].t()) + state_dict[\"transformer.encoder.layers.{}.linear1.bias\".format(layer_num)]\n",
    "\n",
    "    op_enc_1_relu = torch.nn.functional.relu(op_enc_1)\n",
    "\n",
    "    op_enc_2 = torch.matmul(op_enc_1_relu, state_dict[\"transformer.encoder.layers.{}.linear2.weight\".format(layer_num)].t()) + state_dict[\"transformer.encoder.layers.{}.linear2.bias\".format(layer_num)]\n",
    "    \n",
    "\n",
    "    output_enc_2 = op_enc_2 + linear_op_enc_1\n",
    "    output_enc_2_norm = output_enc_2*state_dict[\"transformer.encoder.layers.{}.norm2.weight\".format(layer_num)] + state_dict[\"transformer.encoder.layers.{}.norm2.bias\".format(layer_num)]\n",
    "\n",
    "    # Layer normalization from Torch's implementation\n",
    "    layernorm_enc_final = torch.nn.LayerNorm(normalized_shape=output_enc_2_norm.shape[2:])\n",
    "    output_enc_final = layernorm_enc_final(output_enc_2_norm)\n",
    "\n",
    "\n",
    "    # Manual Layer Normalization \n",
    "    x = output_enc_2_norm\n",
    "\n",
    "    # Obtained layer norm weights and biases (learnable)\n",
    "    w = layernorm_enc_final.weight\n",
    "    b = layernorm_enc_final.bias\n",
    "\n",
    "    linear_result_enc_2 = w*x + b\n",
    "\n",
    "    print(linear_result_enc_2)\n",
    "    epsilon = 1e-05  \n",
    "    mean = linear_result_enc_2.mean(dim=-1, keepdim=True)\n",
    "    # print(\"mean = \", mean)\n",
    "    std = linear_result_enc_2.std(dim=-1, unbiased=False, keepdim=True)\n",
    "    # print(\"std = \", std)\n",
    "\n",
    "    # print((linear_result_enc_2 - mean))\n",
    "    output_enc_final = (linear_result_enc_2 - mean) / (std + epsilon) * w + b\n",
    "    \n",
    "\n",
    "    print(\"Final Encoder {} Output :\".format(layer_num))\n",
    "    print(\"norm2(norm1(x + self_atten(x)) + feed_fwd_op)\\n\")\n",
    "    print(output_enc_final)\n",
    "    print()\n",
    "\n",
    "    # (src_len, bsz, embed_dim) \n",
    "    return output_enc_final\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder functions to display the intermediate outputs and get the final outputs from the decoder\n",
    "\n",
    "\n",
    "### Self attention outputs from a decoder block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoder_block_self_attn_output(x, state_dict, layer_num, num_heads, tgt_mask = None,need_weights = False):\n",
    "\n",
    "    # (tgt_len, bsz, embed_dim)\n",
    "    query_dec = key_dec = value_dec = x\n",
    "\n",
    "    tgt_len, bsz, embed_dim = x.shape\n",
    " \n",
    "    # (embed_dim*num_heads, embed_dim)\n",
    "    W_dec = state_dict[\"transformer.decoder.layers.{}.self_attn.in_proj_weight\".format(layer_num)]\n",
    "    b_dec = state_dict[\"transformer.decoder.layers.{}.self_attn.in_proj_bias\".format(layer_num)]\n",
    "\n",
    "\n",
    "    head_dim = embed_dim//num_heads\n",
    "\n",
    "    # (tgt_len, bsz, embed_dim)\n",
    "    Q_dec,K_dec,V_dec = get_qkv(query_dec, key_dec, value_dec ,W_dec, b_dec)\n",
    "    \n",
    "    # Q_dec = Q_dec.unsqueeze(0)\n",
    "    # K_dec = K_dec.unsqueeze(0)\n",
    "    # V_dec = V_dec.unsqueeze(0)\n",
    "\n",
    "    # (1, tgt_len, bsz, embed_dim)\n",
    "    # print(Q_dec.shape, K_dec.shape , V_dec.shape)\n",
    "    # print(tgt_len, bsz * num_heads, head_dim)\n",
    "\n",
    "    # (1, tgt_len, bsz, embed_dim) -> ( bsz*num_heads, tgt_len , head_dim )\n",
    "    Q_dec = Q_dec.reshape(tgt_len, bsz * num_heads, head_dim).transpose(0, 1)\n",
    "    K_dec = K_dec.reshape(K_dec.shape[0], bsz * num_heads, head_dim).transpose(0, 1)\n",
    "    V_dec = V_dec.reshape(V_dec.shape[0], bsz * num_heads, head_dim).transpose(0, 1)\n",
    "\n",
    "    print(\"Q_dec_{} = \".format(layer_num))\n",
    "    print(Q_dec)\n",
    "    print()\n",
    "\n",
    "    print(\"K_dec_{} = \".format(layer_num))\n",
    "    print(K_dec)\n",
    "    print()\n",
    "\n",
    "    print(\"V_dec_{} = \".format(layer_num))\n",
    "    print(V_dec)\n",
    "    print()\n",
    "\n",
    "    src_len = K_dec.size(1)\n",
    "\n",
    "\n",
    "    attn_mask = tgt_mask\n",
    "    if attn_mask is not None:\n",
    "\n",
    "        # Ensuring attn_mask's dim is 3\n",
    "        if attn_mask.dim() == 2:\n",
    "            correct_2d_size = (tgt_len, src_len)\n",
    "            if attn_mask.shape != correct_2d_size:\n",
    "                raise RuntimeError(f\"The shape of the 2D attn_mask is {attn_mask.shape}, but should be {correct_2d_size}.\")\n",
    "            attn_mask = attn_mask.unsqueeze(0)\n",
    "        elif attn_mask.dim() == 3:\n",
    "            correct_3d_size = (bsz * num_heads, tgt_len, src_len)\n",
    "            if attn_mask.shape != correct_3d_size:\n",
    "                raise RuntimeError(f\"The shape of the 3D attn_mask is {attn_mask.shape}, but should be {correct_3d_size}.\")\n",
    "        else:\n",
    "            raise RuntimeError(f\"attn_mask's dimension {attn_mask.dim()} is not supported\")\n",
    "\n",
    "    # attn_mask can be either (L,S) or (N*num_heads, L, S)\n",
    "    # if attn_mask's shape is (1, L, S) we need to unsqueeze to (1, 1, L, S)\n",
    "    # in order to match the input for SDPA of (N, num_heads, L, S)\n",
    "    if attn_mask is not None:\n",
    "        if attn_mask.size(0) == 1 and attn_mask.dim() == 3:\n",
    "            attn_mask = attn_mask.unsqueeze(0)\n",
    "        else:\n",
    "            attn_mask = attn_mask.view(bsz, num_heads, -1, src_len)\n",
    "\n",
    "    \n",
    "    \n",
    "    if need_weights is False:\n",
    "        attn_output = atten_product_needs_wts_false(Q = Q_dec, V = V_dec, K = K_dec, bsz = bsz, head_dim=head_dim, src_len=src_len, tgt_len=tgt_len, attn_mask = attn_mask, embed_dim=embed_dim, num_heads=num_heads\n",
    "        )\n",
    "\n",
    "        print(\"Decoder Self Attention = \")\n",
    "        print(attn_output)\n",
    "        print()\n",
    "\n",
    "        op_dec_1 = torch.matmul(attn_output, state_dict[\"transformer.decoder.layers.{}.self_attn.out_proj.weight\".format(layer_num)].t()) + state_dict[\"transformer.decoder.layers.{}.self_attn.out_proj.bias\".format(layer_num)]\n",
    "        attn_dec_output = op_dec_1.view(tgt_len, bsz, attn_output.size(1))\n",
    "\n",
    "        return attn_dec_output, None\n",
    "    \n",
    "    else:\n",
    "\n",
    "        attn_dec_output,attn_wt_matrix_dec = atten_product_needs_wts_true(Q=Q_dec, K=K_dec, V=V_dec, bsz=bsz, tgt_len=tgt_len, attn_mask = attn_mask, embed_dim=embed_dim)\n",
    "\n",
    "        print(\"Decoder Attention output = \")\n",
    "        print(attn_wt_matrix_dec)\n",
    "        print()\n",
    "\n",
    "        op_dec_1 = torch.matmul(attn_dec_output, state_dict[\"transformer.decoder.layers.{}.self_attn.out_proj.weight\".format(layer_num)].t()) + state_dict[\"transformer.decoder.layers.{}.self_attn.out_proj.bias\".format(layer_num)]\n",
    "        attn_dec_output = op_dec_1.view(tgt_len, bsz, attn_dec_output.size(1))\n",
    "    \n",
    "\n",
    "        return attn_dec_output, attn_wt_matrix_dec\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function to perform the linear layer calculations after decoder's self attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dec_post_self_attn(self_attn_dec, x, state_dict, layer_num):\n",
    "\n",
    "    # (bsz*tgt_len , embed_dim) @ (embed_dim , embed_dim).T -> (bsz*tgt_len , embed_dim)\n",
    "    output_dec_1 = self_attn_dec + x\n",
    "\n",
    "    print(output_dec_1)\n",
    "\n",
    "\n",
    "    # (bsz*tgt_len , embed_dim) @ (embed_dim , embed_dim).T -> (bsz*tgt_len , embed_dim)\n",
    "    linear_result_dec_1 = output_dec_1*state_dict[\"transformer.decoder.layers.{}.norm1.weight\".format(layer_num)] + state_dict[\"transformer.decoder.layers.{}.norm1.bias\".format(layer_num)]\n",
    "\n",
    "    layernorm_dec_1 = torch.nn.LayerNorm(normalized_shape=linear_result_dec_1.shape[2:])\n",
    "    linear_op_dec_1 = layernorm_dec_1(linear_result_dec_1)\n",
    "\n",
    "    # From torch's implementation\n",
    "    # print(linear_op_dec_1)\n",
    "\n",
    "    x = linear_result_dec_1\n",
    "\n",
    "    # Obtained layer norm weights and biases (learnable)\n",
    "    w = layernorm_dec_1.weight\n",
    "    b = layernorm_dec_1.bias\n",
    "\n",
    "    linear_result_dec_1f = w*x + b\n",
    "\n",
    "    epsilon = 1e-05  \n",
    "    mean = linear_result_dec_1f.mean(dim=-1, keepdim=True)\n",
    "    std = linear_result_dec_1f.std(dim=-1, unbiased=False, keepdim=True)\n",
    "\n",
    "    normalized_result_dec_1 = (linear_result_dec_1f - mean) / (std + epsilon) * w + b\n",
    "\n",
    "\n",
    "    print(\"Decoder_{} norm1(x + sa(x))\".format(layer_num))\n",
    "    print(normalized_result_dec_1)\n",
    "    print()\n",
    "\n",
    "    # (tgt_len, bsz, embed_dim)\n",
    "    return normalized_result_dec_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross attention in decoder \n",
    "### query, key, value =  x, mem, mem \n",
    "\n",
    "### Cross attention between the encoder's final output and the decoder layer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoder_block_cross_attn_output(x_dec, memory, state_dict, layer_num, tgt_len, src_len, head_dim, num_heads, memory_mask = None,need_weights = False):\n",
    "\n",
    "    # (tgt_len, bsz, embed_dim)\n",
    "    query_dec_mha = x_dec\n",
    "\n",
    "    # (src_len, bsz, embed_dim)\n",
    "    key_dec_mha, value_dec_mha = memory, memory\n",
    "\n",
    "    tgt_len, bsz, embed_dim = query_dec_mha.shape\n",
    "\n",
    "    W_dec_mha = state_dict[\"transformer.decoder.layers.{}.multihead_attn.in_proj_weight\".format(layer_num)]\n",
    "    b_dec_mha = state_dict[\"transformer.decoder.layers.{}.multihead_attn.in_proj_bias\".format(layer_num)]\n",
    "\n",
    "\n",
    "    ########## CHANGE FROM SELF ATTENTION ############\n",
    "\n",
    "    Q_dec_mha,K_dec_mha,V_dec_mha = get_qkv(query_dec_mha, key_dec_mha, value_dec_mha ,W_dec_mha, b_dec_mha)\n",
    "\n",
    "\n",
    "    #################################################\n",
    "\n",
    "    # K_dec_mha = K_dec_mha.unsqueeze(0)\n",
    "    # V_dec_mha = V_dec_mha.unsqueeze(0)\n",
    "    # Q_dec_mha = Q_dec_mha.unsqueeze(0)\n",
    "\n",
    "\n",
    "    Q_dec_mha = Q_dec_mha.reshape(Q_dec_mha.shape[0], bsz * num_heads, head_dim).transpose(0, 1)\n",
    "    K_dec_mha = K_dec_mha.reshape(K_dec_mha.shape[0], bsz * num_heads, head_dim).transpose(0, 1)\n",
    "    V_dec_mha = V_dec_mha.reshape(V_dec_mha.shape[0], bsz * num_heads, head_dim).transpose(0, 1)\n",
    "\n",
    "    print(\"Q_dec_{} = \".format(layer_num))\n",
    "    print(Q_dec_mha)\n",
    "    print()\n",
    "\n",
    "    print(\"K_dec_{} = \".format(layer_num))\n",
    "    print(K_dec_mha)\n",
    "    print()\n",
    "\n",
    "    print(\"V_dec_{} = \".format(layer_num))\n",
    "    print(V_dec_mha)\n",
    "    print()\n",
    "\n",
    "\n",
    "    attn_mask = memory_mask\n",
    "    if attn_mask is not None:\n",
    "\n",
    "        # Ensuring attn_mask's dim is 3\n",
    "        if attn_mask.dim() == 2:\n",
    "            correct_2d_size = (tgt_len, src_len)\n",
    "            if attn_mask.shape != correct_2d_size:\n",
    "                raise RuntimeError(f\"The shape of the 2D attn_mask is {attn_mask.shape}, but should be {correct_2d_size}.\")\n",
    "            attn_mask = attn_mask.unsqueeze(0)\n",
    "        elif attn_mask.dim() == 3:\n",
    "            correct_3d_size = (bsz * num_heads, tgt_len, src_len)\n",
    "            if attn_mask.shape != correct_3d_size:\n",
    "                raise RuntimeError(f\"The shape of the 3D attn_mask is {attn_mask.shape}, but should be {correct_3d_size}.\")\n",
    "        else:\n",
    "            raise RuntimeError(f\"attn_mask's dimension {attn_mask.dim()} is not supported\")\n",
    "\n",
    "    # attn_mask can be either (L,S) or (N*num_heads, L, S)\n",
    "    # if attn_mask's shape is (1, L, S) we need to unsqueeze to (1, 1, L, S)\n",
    "    # in order to match the input for SDPA of (N, num_heads, L, S)\n",
    "    if attn_mask is not None:\n",
    "        if attn_mask.size(0) == 1 and attn_mask.dim() == 3:\n",
    "            attn_mask = attn_mask.unsqueeze(0)\n",
    "        else:\n",
    "            attn_mask = attn_mask.view(bsz, num_heads, -1, src_len)\n",
    "\n",
    "\n",
    "\n",
    "    if need_weights is False:\n",
    "\n",
    "        attn_output_dec_mha = atten_product_needs_wts_false(Q =Q_dec_mha, V=V_dec_mha, K=K_dec_mha, bsz=bsz, head_dim=head_dim, src_len=src_len, tgt_len=tgt_len, attn_mask=attn_mask, embed_dim=embed_dim, num_heads=num_heads)\n",
    "        \n",
    "        print(\"Cross attention in decoder_{}\".format(layer_num))\n",
    "        print(attn_output_dec_mha)\n",
    "        print()\n",
    "\n",
    "        op_dec_mha_1 = torch.matmul(attn_output_dec_mha, state_dict[\"transformer.decoder.layers.{}.multihead_attn.out_proj.weight\".format(layer_num)].t()) + state_dict[\"transformer.decoder.layers.{}.multihead_attn.out_proj.bias\".format(layer_num)]\n",
    "\n",
    "        attn_dec_mha_output = op_dec_mha_1.view(tgt_len, bsz, attn_output_dec_mha.size(1))\n",
    "\n",
    "        return attn_dec_mha_output, None\n",
    "    \n",
    "    else: \n",
    "\n",
    "    \n",
    "        attn_dec_mha_output ,attn_wt_matrix_dec_mha = atten_product_needs_wts_true(Q=Q_dec_mha, K=K_dec_mha, V=V_dec_mha, bsz=bsz, tgt_len=tgt_len, attn_mask=attn_mask, embed_dim=embed_dim)\n",
    "\n",
    "        print(\"Decoder mha Attention output = \")\n",
    "        print(attn_dec_mha_output)\n",
    "        print()\n",
    "\n",
    "\n",
    "        op_dec_mha = torch.matmul(attn_dec_mha_output, state_dict[\"transformer.decoder.layers.{}.multihead_attn.out_proj.weight\".format(layer_num)].t()) + state_dict[\"transformer.decoder.layers.{}.multihead_attn.out_proj.bias\".format(layer_num)]\n",
    "        attn_dec_output_mha = op_dec_mha.view(tgt_len, bsz, attn_dec_mha_output.size(1))\n",
    "\n",
    "\n",
    "        return attn_dec_output_mha , attn_wt_matrix_dec_mha\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function to perform the linear layer calculations after Decoder's cross attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoder_block_post_attn_output(x_dec, attn_dec_mha_output, state_dict, layer_num):\n",
    "\n",
    "    # (tgt_len, bsz, embed_dim)\n",
    "    output_dec_2 = attn_dec_mha_output + x_dec\n",
    "\n",
    "    print(output_dec_2)\n",
    "\n",
    "    linear_result_dec_2 = output_dec_2*state_dict[\"transformer.decoder.layers.{}.norm2.weight\".format(layer_num)] + state_dict[\"transformer.decoder.layers.{}.norm2.bias\".format(layer_num)]\n",
    "\n",
    "    # Layer normalization from Torch's implementation \n",
    "    layernorm_dec_2 = torch.nn.LayerNorm(normalized_shape=linear_result_dec_2.shape[2:])\n",
    "    linear_op_dec_2 = layernorm_dec_2(linear_result_dec_2)\n",
    "\n",
    "    x = linear_result_dec_2\n",
    "    w = layernorm_dec_2.weight\n",
    "    b = layernorm_dec_2.bias\n",
    "\n",
    "    linear_result_dec_2f = w*x + b\n",
    "\n",
    "    epsilon = 1e-05  \n",
    "    mean = linear_result_dec_2f.mean(dim=-1, keepdim=True)\n",
    "    # print(\"mean = \", mean)\n",
    "    std = linear_result_dec_2f.std(dim=-1, unbiased=False, keepdim=True)\n",
    "    # print(\"std = \", std)\n",
    "    normalized_result_dec_2 = (linear_result_dec_2f - mean) / (std + epsilon) * w + b\n",
    "    print((linear_result_dec_2f - mean))\n",
    "    print(\"norm2(x' + mha(x', mem)), \\n where x' = Decoder_curr_layer norm1(x + sa(x))\")\n",
    "    print(normalized_result_dec_2)\n",
    "    print(\"\\n\\n\")\n",
    "\n",
    "    x_dec2_norm = normalized_result_dec_2\n",
    "\n",
    "    op_dec_1 = torch.matmul(x_dec2_norm, state_dict[\"transformer.decoder.layers.{}.linear1.weight\".format(layer_num)].t()) + state_dict[\"transformer.decoder.layers.{}.linear1.bias\".format(layer_num)]\n",
    "\n",
    "    op_dec_1_relu = torch.nn.functional.relu(op_dec_1)\n",
    "    op_dec_2 = torch.matmul(op_dec_1_relu, state_dict[\"transformer.decoder.layers.{}.linear2.weight\".format(layer_num)].t()) + state_dict[\"transformer.decoder.layers.{}.linear2.bias\".format(layer_num)]\n",
    "\n",
    "\n",
    "    ff_dec = op_dec_2\n",
    "\n",
    "    x_dec3_unorm = x_dec2_norm + ff_dec\n",
    "\n",
    "    linear_result_dec_3 = x_dec3_unorm*state_dict[\"transformer.decoder.layers.0.norm3.weight\"] + state_dict[\"transformer.decoder.layers.0.norm3.bias\"]\n",
    "\n",
    "    # Layer normalization from Torch's implementation \n",
    "    layernorm_dec_3 = torch.nn.LayerNorm(normalized_shape=linear_result_dec_3.shape[2:])\n",
    "    linear_op_dec_3 = layernorm_dec_3(linear_result_dec_3)\n",
    "\n",
    "    # From torch's implementation\n",
    "    x = linear_result_dec_3\n",
    "\n",
    "    # Obtained layer norm weights and biases (learnable)\n",
    "    w = layernorm_dec_3.weight\n",
    "    b = layernorm_dec_3.bias\n",
    "\n",
    "    linear_result_dec_3f = w*x + b\n",
    "\n",
    "    epsilon = 1e-5  \n",
    "    mean = linear_result_dec_3f.mean(dim=-1, keepdim=True)\n",
    "    std = linear_result_dec_3f.std(dim=-1, unbiased=False, keepdim=True)\n",
    "    print(\"mean = \", mean)\n",
    "    print(\"std = \", std)\n",
    "\n",
    "    print( (linear_result_dec_3f - mean))\n",
    "    normalized_result_dec_3 = (linear_result_dec_3f - mean) / (std + epsilon) * w + b\n",
    "\n",
    "    print(\"norm3(x'' + ff(x'')) \\n where, x'' = Decoder_curr_layer norm2(x' + mha(x'))\")\n",
    "    print(normalized_result_dec_3)\n",
    "    print()\n",
    "\n",
    "    return normalized_result_dec_3\n",
    "\n",
    "def feef_fwd_transformer(dec_output_final, state_dict):\n",
    "\n",
    "    # (tgt_len, bsz, embed_dim) @ (vocab_size, embed_dim).T -> (tgt_len, bsz, vocab_size)\n",
    "    final_op = dec_output_final@state_dict[\"fc.weight\"].T + state_dict[\"fc.bias\"]\n",
    "\n",
    "    return final_op"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialising the same transformer as in the other notebook\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import math\n",
    "import copy\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=512, dropout=0):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.encoding = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1).float()\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(torch.log(torch.tensor(10000.0)) / d_model))\n",
    "        self.encoding[:, 0::2] = torch.sin(position * div_term)\n",
    "        self.encoding[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.encoding = self.encoding.unsqueeze(0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        print(x.shape)\n",
    "        return self.encoding[:, :x.size(1)].detach()\n",
    "\n",
    "\n",
    "\n",
    "class TransformerModel1(nn.Module):\n",
    "    def __init__(self, src_vocab_size, tgt_vocab_size, d_model, num_heads, num_encoder_layers, num_decoder_layers, max_seq_len, d_ff, dropout = 0):\n",
    "\n",
    "        super(TransformerModel1, self).__init__()\n",
    "\n",
    "        self.src_embedding = nn.Embedding(src_vocab_size, d_model)\n",
    "        self.tgt_embedding = nn.Embedding(tgt_vocab_size, d_model)\n",
    "\n",
    "        self.positional_encoding = PositionalEncoding(d_model, dropout=0, max_len=max_seq_len)\n",
    "\n",
    "        self.transformer = nn.Transformer(\n",
    "            d_model=d_model,\n",
    "            nhead=num_heads,\n",
    "            num_encoder_layers=num_encoder_layers,\n",
    "            num_decoder_layers=num_decoder_layers,\n",
    "            dropout=dropout,\n",
    "            dim_feedforward=d_ff,\n",
    "        )\n",
    "\n",
    "        self.fc = nn.Linear(d_model, tgt_vocab_size)\n",
    "\n",
    "\n",
    "\n",
    "    def generate_mask(self, src, tgt):\n",
    "\n",
    "        src_mask = None\n",
    "        seq_length = tgt.size(0)\n",
    "        \n",
    "        nopeak_mask = (torch.triu(torch.ones(seq_length, seq_length), diagonal=1)).bool()\n",
    "\n",
    "        return src_mask, nopeak_mask\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "\n",
    "        src_mask, tgt_mask = self.generate_mask(src, tgt)\n",
    "\n",
    "        print(\"Tgt mask shape = \", tgt_mask.shape)\n",
    "\n",
    "        src = self.src_embedding(src) + self.positional_encoding(src)\n",
    "        tgt = self.tgt_embedding(tgt) + self.positional_encoding(tgt)\n",
    "\n",
    "\n",
    "        output = self.transformer(src, tgt, src_mask = src_mask, tgt_mask = tgt_mask, tgt_is_causal = False)\n",
    "        output = self.fc(output)\n",
    "        \n",
    "        return output\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sreevaatsav/.pyenv/versions/project_env/lib/python3.10/site-packages/torch/nn/modules/transformer.py:288: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "src_vocab_size = 20\n",
    "tgt_vocab_size = 20\n",
    "d_model = 16\n",
    "num_heads = 4\n",
    "num_encoder_layers = 1\n",
    "num_decoder_layers = 1\n",
    "d_ff = 20\n",
    "max_seq_len = 5\n",
    "dropout = 0\n",
    "\n",
    "# src_vocab_size = 20\n",
    "# tgt_vocab_size = 20\n",
    "# d_model = 2\n",
    "# num_heads = 1\n",
    "# num_encoder_layers = 1\n",
    "# num_decoder_layers = 1\n",
    "# d_ff = 3\n",
    "# max_seq_len = 4\n",
    "# dropout = 0\n",
    "\n",
    "\n",
    "transformer = TransformerModel1(src_vocab_size, tgt_vocab_size, d_model, num_heads, num_encoder_layers, num_decoder_layers, max_seq_len, d_ff)\n",
    "\n",
    "# Generate random sample data\n",
    "# src_data = torch.randint(1, src_vocab_size, (max_seq_len , 3))  # (seq_length, batch_size,)\n",
    "# tgt_data = torch.randint(1, tgt_vocab_size, ( max_seq_len, 3))  # (seq_length, batch_size)\n",
    "\n",
    "src_data = torch.tensor([[0, 2, 4], [1, 0, 7], [2, 2, 0], [3, 5, 6], [6, 1, 9]])\n",
    "tgt_data = torch.tensor([[1, 7, 9], [3, 4, 1], [5, 2, 8], [8, 0, 3], [4, 5, 9]]) \n",
    "\n",
    "\n",
    "# src_data = torch.tensor([[2], [1], [5], [4]])\n",
    "# tgt_data = torch.tensor([[1], [16], [5], [3], [9]]) \n",
    "\n",
    "\n",
    "state_dict = transformer.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class PositionalEncoding(nn.Module):\n",
    "#     def __init__(self, d_model, max_len=512, dropout=0):\n",
    "#         super(PositionalEncoding, self).__init__()\n",
    "#         self.encoding = torch.zeros(max_len, d_model)\n",
    "#         position = torch.arange(0, max_len).unsqueeze(1).float()\n",
    "#         div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(torch.log(torch.tensor(10000.0)) / d_model))\n",
    "#         self.encoding[:, 0::2] = torch.sin(position * div_term)\n",
    "#         self.encoding[:, 1::2] = torch.cos(position * div_term)\n",
    "#         self.encoding = self.encoding.unsqueeze(0)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         return self.encoding[:, :x.size(1)].detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "state_dict1 = copy.deepcopy(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5, 3]), torch.Size([5, 3]))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_data.shape, tgt_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 3])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tgt_data.view(-1)\n",
    "tgt_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5, 3]), torch.Size([5, 3]))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_data.shape, tgt_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross verifying the intermediate outputs for the 1st forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_mask(src, tgt):\n",
    "    src_mask = None\n",
    "    seq_length = tgt.size(0)\n",
    "    nopeak_mask = (torch.triu(torch.ones(seq_length, seq_length), diagonal=1)).bool()\n",
    "    # tgt_mask = tgt_mask & nopeak_mask\n",
    "    return src_mask, nopeak_mask\n",
    "\n",
    "src_mask, tgt_mask = generate_mask(src_data, tgt_data[:-1, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(None,\n",
       " tensor([[False,  True,  True,  True],\n",
       "         [False, False,  True,  True],\n",
       "         [False, False, False,  True],\n",
       "         [False, False, False, False]]))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_mask, tgt_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_intermediate_outputs_mask(src_sentence, tgt_sentence,d_model, num_heads ,state_dict, num_encoder_layers , num_decoder_layers, tgt_mask, d_ff):\n",
    "\n",
    "    pe_src_embeds, pe_tgt_embeds = get_embedding_outputs(src_sentence=src_sentence,  tgt_sentence=tgt_sentence, state_dict=state_dict, max_seq_len=max_seq_len, d_model = d_model)\n",
    "    \n",
    "    print(\"###\"*25)\n",
    "    print(\"### Encoder Start ###\")\n",
    "    print()\n",
    "\n",
    "    x_enc = pe_src_embeds\n",
    "\n",
    "    for lno in range(num_encoder_layers):\n",
    "        attn_enc_output, src_len, head_dim, attn_weights = encoder_block_attn_output(x_enc, state_dict, layer_num = lno, need_weights = False, embed_dim = d_model, num_heads= num_heads, src_mask=None)\n",
    "\n",
    "        if lno == 0:\n",
    "            tgt_len, bsz, embed_dim = x_enc.shape\n",
    "\n",
    "        output_enc_final = encoder_block_post_attn_output(x_enc, attn_enc_output, state_dict, layer_num = lno , bsz = bsz, tgt_len = tgt_len)\n",
    "\n",
    "        x_enc = output_enc_final\n",
    "\n",
    "    \n",
    "    print(\"### Encoder Done ###\")\n",
    "    print(\"\\n\\n\\n\")\n",
    "    print(\"### Decoder Start ###\")\n",
    "\n",
    "    x_dec = pe_tgt_embeds\n",
    "    memory = x_enc\n",
    "\n",
    "    for lno in range(num_decoder_layers):\n",
    "\n",
    "        self_attn_dec, dec_sa_wts = decoder_block_self_attn_output(x_dec, state_dict, layer_num = lno, need_weights = False, tgt_mask=tgt_mask, num_heads = num_heads)\n",
    "        x_dec = dec_post_self_attn(self_attn_dec, x_dec, state_dict, layer_num = lno)\n",
    "\n",
    "        attn_dec_mha_output, attn_dec_mha_wts = decoder_block_cross_attn_output(x_dec, memory, state_dict, num_heads = num_heads, layer_num = lno, tgt_len = tgt_len, src_len = src_len, head_dim = head_dim, need_weights = False, memory_mask=None)\n",
    "        final_op = decoder_block_post_attn_output(x_dec, attn_dec_mha_output, state_dict, layer_num = lno)\n",
    "\n",
    "        print(pe_tgt_embeds.shape, final_op.shape)\n",
    "        x_dec = final_op\n",
    "\n",
    "    print(\"### Decoder Done ###\")\n",
    "\n",
    "    final_op = feef_fwd_transformer(final_op, state_dict)\n",
    "\n",
    "    return final_op\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# state_dict1 = transformer.state_dict\n",
    "\n",
    "src_mask, tgt_mask = generate_mask(src_data, tgt_data[:-1, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(None,\n",
       " tensor([[False,  True,  True,  True],\n",
       "         [False, False,  True,  True],\n",
       "         [False, False, False,  True],\n",
       "         [False, False, False, False]]))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_mask, tgt_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0, 2, 4],\n",
       "         [1, 0, 7],\n",
       "         [2, 2, 0],\n",
       "         [3, 5, 6],\n",
       "         [6, 1, 9]]),\n",
       " tensor([[1, 7, 9],\n",
       "         [3, 4, 1],\n",
       "         [5, 2, 8],\n",
       "         [8, 0, 3]]))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_data, tgt_data[:-1, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source sentence embedding\n",
      "Word index: 0, Embedding: tensor([-1.1258, -1.1524, -0.2506, -0.4339,  0.8487,  0.6920, -0.3160, -2.1152,\n",
      "         0.3223, -1.2633,  0.3500,  0.3081,  0.1198,  1.2377,  1.1168, -0.2473])\n",
      "Word index: 2, Embedding: tensor([-0.6136,  0.0316, -0.4927,  0.2484,  0.4397,  0.1124,  0.6408,  0.4412,\n",
      "        -0.1023,  0.7924, -0.2897,  0.0525,  0.5229,  2.3022, -1.4689, -1.5867])\n",
      "Word index: 4, Embedding: tensor([-0.5692,  0.9200,  1.1108,  1.2899, -1.4782,  2.5672, -0.4731,  0.3356,\n",
      "        -1.6293, -0.5497, -0.4798, -0.4997, -1.0670,  1.1149, -0.1407,  0.8058])\n",
      "Word index: 1, Embedding: tensor([-1.3527, -1.6959,  0.5667,  0.7935,  0.5988, -1.5551, -0.3414,  1.8530,\n",
      "         0.7502, -0.5855, -0.1734,  0.1835,  1.3894,  1.5863,  0.9463, -0.8437])\n",
      "Word index: 0, Embedding: tensor([-1.1258, -1.1524, -0.2506, -0.4339,  0.8487,  0.6920, -0.3160, -2.1152,\n",
      "         0.3223, -1.2633,  0.3500,  0.3081,  0.1198,  1.2377,  1.1168, -0.2473])\n",
      "Word index: 7, Embedding: tensor([-0.2188, -2.4351, -0.0729, -0.0340,  0.9625,  0.3492, -0.9215, -0.0562,\n",
      "        -0.6227, -0.4637,  1.9218, -0.4025,  0.1239,  1.1648,  0.9234,  1.3873])\n",
      "Word index: 2, Embedding: tensor([-0.6136,  0.0316, -0.4927,  0.2484,  0.4397,  0.1124,  0.6408,  0.4412,\n",
      "        -0.1023,  0.7924, -0.2897,  0.0525,  0.5229,  2.3022, -1.4689, -1.5867])\n",
      "Word index: 2, Embedding: tensor([-0.6136,  0.0316, -0.4927,  0.2484,  0.4397,  0.1124,  0.6408,  0.4412,\n",
      "        -0.1023,  0.7924, -0.2897,  0.0525,  0.5229,  2.3022, -1.4689, -1.5867])\n",
      "Word index: 0, Embedding: tensor([-1.1258, -1.1524, -0.2506, -0.4339,  0.8487,  0.6920, -0.3160, -2.1152,\n",
      "         0.3223, -1.2633,  0.3500,  0.3081,  0.1198,  1.2377,  1.1168, -0.2473])\n",
      "Word index: 3, Embedding: tensor([-0.6731,  0.8728,  1.0554,  0.1778, -0.2303, -0.3918,  0.5433, -0.3952,\n",
      "        -0.4462,  0.7440,  1.5210,  3.4105, -1.5312, -1.2341,  1.8197, -0.5515])\n",
      "Word index: 5, Embedding: tensor([-9.3348e-02,  6.8705e-01, -8.3832e-01,  8.9182e-04,  8.4189e-01,\n",
      "        -4.0003e-01,  1.0395e+00,  3.5815e-01, -2.4600e-01,  2.3025e+00,\n",
      "        -1.8817e+00, -4.9727e-02, -1.0450e+00, -9.5650e-01,  3.3532e-02,\n",
      "         7.1009e-01])\n",
      "Word index: 6, Embedding: tensor([ 1.6459, -1.3602,  0.3446,  0.5199, -2.6133, -1.6965, -0.2282,  0.2800,\n",
      "         0.2469,  0.0769,  0.3380,  0.4544,  0.4569, -0.8654,  0.7813, -0.9268])\n",
      "Word index: 6, Embedding: tensor([ 1.6459, -1.3602,  0.3446,  0.5199, -2.6133, -1.6965, -0.2282,  0.2800,\n",
      "         0.2469,  0.0769,  0.3380,  0.4544,  0.4569, -0.8654,  0.7813, -0.9268])\n",
      "Word index: 1, Embedding: tensor([-1.3527, -1.6959,  0.5667,  0.7935,  0.5988, -1.5551, -0.3414,  1.8530,\n",
      "         0.7502, -0.5855, -0.1734,  0.1835,  1.3894,  1.5863,  0.9463, -0.8437])\n",
      "Word index: 9, Embedding: tensor([-0.2596,  0.1183,  0.2440,  1.1646,  0.2886,  0.3866, -0.2011, -0.1179,\n",
      "         0.1922, -0.7722, -1.9003,  0.1307, -0.7043,  0.3147,  0.1574,  0.3854])\n",
      "\n",
      "torch.Size([5, 3, 16])\n",
      "Target sentence embedding\n",
      "Word index: 1, Embedding: tensor([ 0.6442,  3.9300, -0.1244,  0.2953,  0.3827, -0.5497, -0.9940,  1.3459,\n",
      "         1.9457, -1.2904, -2.3495, -2.0689,  0.9094, -0.6946,  1.9595, -1.1038])\n",
      "Word index: 7, Embedding: tensor([ 2.2557,  1.2288, -0.4855,  0.4536,  1.3514,  0.4339, -0.5133, -0.1860,\n",
      "         0.2757,  0.1097,  0.3594, -0.7537,  0.2294, -0.2544,  1.5800, -0.2444])\n",
      "Word index: 9, Embedding: tensor([ 0.3427, -1.6045, -0.5873,  0.6004,  0.4378, -0.0965,  0.3303, -0.1875,\n",
      "        -1.4271,  0.5926, -1.1582,  0.0358,  0.2160, -0.9161,  1.5599, -3.1537])\n",
      "Word index: 3, Embedding: tensor([ 0.4990,  0.8780,  0.3894,  1.4625,  0.4795, -0.5334, -0.0347,  0.6573,\n",
      "        -0.3112, -0.5620, -0.4835, -1.2721, -0.1740,  0.5541, -0.1817, -0.2345])\n",
      "Word index: 4, Embedding: tensor([ 0.2942,  0.7973,  1.2642,  0.9355,  0.5455, -1.5374,  0.3124,  0.7401,\n",
      "         1.4502,  4.1015,  1.1182, -1.5668, -0.6990,  0.5744,  1.2381, -0.6405])\n",
      "Word index: 1, Embedding: tensor([ 0.6442,  3.9300, -0.1244,  0.2953,  0.3827, -0.5497, -0.9940,  1.3459,\n",
      "         1.9457, -1.2904, -2.3495, -2.0689,  0.9094, -0.6946,  1.9595, -1.1038])\n",
      "Word index: 5, Embedding: tensor([-0.7645,  0.2408,  0.1664, -2.2318,  1.3892, -0.5023,  1.6797, -1.0240,\n",
      "         1.6859, -1.2177,  0.7650,  1.1971, -0.7128, -0.0656,  2.2050,  1.7852])\n",
      "Word index: 2, Embedding: tensor([ 0.5411,  1.5390,  1.0860,  1.2464,  0.1151,  1.6193,  0.4637,  1.3007,\n",
      "         0.8732,  0.0651,  0.7732, -0.9701, -0.8877, -0.3183, -0.3344,  0.4543])\n",
      "Word index: 8, Embedding: tensor([-1.1991, -0.0257,  1.8024, -1.0597,  3.4028, -0.5687, -0.4755,  1.7432,\n",
      "        -0.2044, -0.3164,  1.2937,  1.3453,  0.1939,  1.5717, -0.3827,  1.3951])\n",
      "Word index: 8, Embedding: tensor([-1.1991, -0.0257,  1.8024, -1.0597,  3.4028, -0.5687, -0.4755,  1.7432,\n",
      "        -0.2044, -0.3164,  1.2937,  1.3453,  0.1939,  1.5717, -0.3827,  1.3951])\n",
      "Word index: 0, Embedding: tensor([-0.1292, -0.0546,  0.4083,  1.1264,  1.9351,  1.0077,  1.0046, -0.4335,\n",
      "        -1.2426,  1.2846,  0.2438,  0.5304, -0.0145, -2.2357,  1.4660, -1.2191])\n",
      "Word index: 3, Embedding: tensor([ 0.4990,  0.8780,  0.3894,  1.4625,  0.4795, -0.5334, -0.0347,  0.6573,\n",
      "        -0.3112, -0.5620, -0.4835, -1.2721, -0.1740,  0.5541, -0.1817, -0.2345])\n",
      "\n",
      "PE of src :\n",
      "torch.Size([5, 3])\n",
      "tensor([[[ 0.0000e+00,  1.0000e+00,  0.0000e+00,  1.0000e+00,  0.0000e+00,\n",
      "           1.0000e+00,  0.0000e+00,  1.0000e+00,  0.0000e+00,  1.0000e+00,\n",
      "           0.0000e+00,  1.0000e+00,  0.0000e+00,  1.0000e+00,  0.0000e+00,\n",
      "           1.0000e+00],\n",
      "         [ 8.4147e-01,  5.4030e-01,  3.1098e-01,  9.5042e-01,  9.9833e-02,\n",
      "           9.9500e-01,  3.1618e-02,  9.9950e-01,  9.9998e-03,  9.9995e-01,\n",
      "           3.1623e-03,  9.9999e-01,  1.0000e-03,  1.0000e+00,  3.1623e-04,\n",
      "           1.0000e+00],\n",
      "         [ 9.0930e-01, -4.1615e-01,  5.9113e-01,  8.0658e-01,  1.9867e-01,\n",
      "           9.8007e-01,  6.3203e-02,  9.9800e-01,  1.9999e-02,  9.9980e-01,\n",
      "           6.3245e-03,  9.9998e-01,  2.0000e-03,  1.0000e+00,  6.3246e-04,\n",
      "           1.0000e+00]]])\n",
      "\n",
      "PE of tgt :\n",
      "torch.Size([4, 3])\n",
      "tensor([[[ 0.0000e+00,  1.0000e+00,  0.0000e+00,  1.0000e+00,  0.0000e+00,\n",
      "           1.0000e+00,  0.0000e+00,  1.0000e+00,  0.0000e+00,  1.0000e+00,\n",
      "           0.0000e+00,  1.0000e+00,  0.0000e+00,  1.0000e+00,  0.0000e+00,\n",
      "           1.0000e+00],\n",
      "         [ 8.4147e-01,  5.4030e-01,  3.1098e-01,  9.5042e-01,  9.9833e-02,\n",
      "           9.9500e-01,  3.1618e-02,  9.9950e-01,  9.9998e-03,  9.9995e-01,\n",
      "           3.1623e-03,  9.9999e-01,  1.0000e-03,  1.0000e+00,  3.1623e-04,\n",
      "           1.0000e+00],\n",
      "         [ 9.0930e-01, -4.1615e-01,  5.9113e-01,  8.0658e-01,  1.9867e-01,\n",
      "           9.8007e-01,  6.3203e-02,  9.9800e-01,  1.9999e-02,  9.9980e-01,\n",
      "           6.3245e-03,  9.9998e-01,  2.0000e-03,  1.0000e+00,  6.3246e-04,\n",
      "           1.0000e+00]]])\n",
      "\n",
      "torch.Size([5, 3])\n",
      "torch.Size([4, 3])\n",
      "PE source embeddings : \n",
      "\n",
      "tensor([[[-1.1258, -0.1524, -0.2506,  0.5661,  0.8487,  1.6920, -0.3160,\n",
      "          -1.1152,  0.3223, -0.2633,  0.3500,  1.3081,  0.1198,  2.2377,\n",
      "           1.1168,  0.7527],\n",
      "         [ 0.2279,  0.5719, -0.1817,  1.1988,  0.5395,  1.1074,  0.6724,\n",
      "           1.4407, -0.0923,  1.7924, -0.2865,  1.0525,  0.5239,  3.3022,\n",
      "          -1.4686, -0.5867],\n",
      "         [ 0.3400,  0.5038,  1.7019,  2.0965, -1.2795,  3.5473, -0.4099,\n",
      "           1.3336, -1.6093,  0.4501, -0.4735,  0.5003, -1.0650,  2.1149,\n",
      "          -0.1400,  1.8058]],\n",
      "\n",
      "        [[-1.3527, -0.6959,  0.5667,  1.7935,  0.5988, -0.5551, -0.3414,\n",
      "           2.8530,  0.7502,  0.4145, -0.1734,  1.1835,  1.3894,  2.5863,\n",
      "           0.9463,  0.1563],\n",
      "         [-0.2844, -0.6121,  0.0604,  0.5165,  0.9485,  1.6870, -0.2844,\n",
      "          -1.1157,  0.3323, -0.2634,  0.3531,  1.3081,  0.1208,  2.2377,\n",
      "           1.1171,  0.7527],\n",
      "         [ 0.6905, -2.8512,  0.5182,  0.7726,  1.1612,  1.3292, -0.8583,\n",
      "           0.9418, -0.6027,  0.5361,  1.9281,  0.5974,  0.1259,  2.1648,\n",
      "           0.9240,  2.3873]],\n",
      "\n",
      "        [[-0.6136,  1.0316, -0.4927,  1.2484,  0.4397,  1.1124,  0.6408,\n",
      "           1.4412, -0.1023,  1.7924, -0.2897,  1.0525,  0.5229,  3.3022,\n",
      "          -1.4689, -0.5867],\n",
      "         [ 0.2279,  0.5719, -0.1817,  1.1988,  0.5395,  1.1074,  0.6724,\n",
      "           1.4407, -0.0923,  1.7924, -0.2865,  1.0525,  0.5239,  3.3022,\n",
      "          -1.4686, -0.5867],\n",
      "         [-0.2165, -1.5685,  0.3405,  0.3727,  1.0474,  1.6721, -0.2528,\n",
      "          -1.1172,  0.3423, -0.2635,  0.3563,  1.3081,  0.1218,  2.2377,\n",
      "           1.1174,  0.7527]],\n",
      "\n",
      "        [[-0.6731,  1.8728,  1.0554,  1.1778, -0.2303,  0.6082,  0.5433,\n",
      "           0.6048, -0.4462,  1.7440,  1.5210,  4.4105, -1.5312, -0.2341,\n",
      "           1.8197,  0.4485],\n",
      "         [ 0.7481,  1.2274, -0.5273,  0.9513,  0.9417,  0.5950,  1.0711,\n",
      "           1.3577, -0.2360,  3.3025, -1.8785,  0.9503, -1.0440,  0.0435,\n",
      "           0.0338,  1.7101],\n",
      "         [ 2.5552, -1.7763,  0.9357,  1.3264, -2.4147, -0.7164, -0.1650,\n",
      "           1.2780,  0.2669,  1.0767,  0.3443,  1.4544,  0.4589,  0.1346,\n",
      "           0.7819,  0.0732]],\n",
      "\n",
      "        [[ 1.6459, -0.3602,  0.3446,  1.5199, -2.6133, -0.6965, -0.2282,\n",
      "           1.2800,  0.2469,  1.0769,  0.3380,  1.4544,  0.4569,  0.1346,\n",
      "           0.7813,  0.0732],\n",
      "         [-0.5112, -1.1556,  0.8776,  1.7439,  0.6987, -0.5601, -0.3097,\n",
      "           2.8525,  0.7602,  0.4145, -0.1702,  1.1835,  1.3904,  2.5863,\n",
      "           0.9466,  0.1563],\n",
      "         [ 0.6497, -0.2978,  0.8351,  1.9712,  0.4872,  1.3667, -0.1379,\n",
      "           0.8801,  0.2122,  0.2276, -1.8940,  1.1307, -0.7023,  1.3147,\n",
      "           0.1580,  1.3854]]])\n",
      "\n",
      "PE target embeddings : \n",
      "\n",
      "tensor([[[ 6.4423e-01,  4.9300e+00, -1.2442e-01,  1.2953e+00,  3.8265e-01,\n",
      "           4.5028e-01, -9.9404e-01,  2.3459e+00,  1.9457e+00, -2.9036e-01,\n",
      "          -2.3495e+00, -1.0689e+00,  9.0942e-01,  3.0538e-01,  1.9595e+00,\n",
      "          -1.0383e-01],\n",
      "         [ 3.0972e+00,  1.7691e+00, -1.7448e-01,  1.4040e+00,  1.4513e+00,\n",
      "           1.4289e+00, -4.8163e-01,  8.1347e-01,  2.8566e-01,  1.1096e+00,\n",
      "           3.6259e-01,  2.4626e-01,  2.3040e-01,  7.4556e-01,  1.5803e+00,\n",
      "           7.5564e-01],\n",
      "         [ 1.2520e+00, -2.0206e+00,  3.8218e-03,  1.4070e+00,  6.3647e-01,\n",
      "           8.8361e-01,  3.9347e-01,  8.1048e-01, -1.4071e+00,  1.5924e+00,\n",
      "          -1.1519e+00,  1.0357e+00,  2.1801e-01,  8.3918e-02,  1.5605e+00,\n",
      "          -2.1537e+00]],\n",
      "\n",
      "        [[ 4.9895e-01,  1.8780e+00,  3.8944e-01,  2.4625e+00,  4.7951e-01,\n",
      "           4.6660e-01, -3.4651e-02,  1.6573e+00, -3.1122e-01,  4.3800e-01,\n",
      "          -4.8349e-01, -2.7211e-01, -1.7402e-01,  1.5541e+00, -1.8166e-01,\n",
      "           7.6553e-01],\n",
      "         [ 1.1357e+00,  1.3376e+00,  1.5752e+00,  1.8859e+00,  6.4530e-01,\n",
      "          -5.4238e-01,  3.4406e-01,  1.7396e+00,  1.4602e+00,  5.1014e+00,\n",
      "           1.1214e+00, -5.6685e-01, -6.9798e-01,  1.5744e+00,  1.2384e+00,\n",
      "           3.5946e-01],\n",
      "         [ 1.5535e+00,  3.5139e+00,  4.6670e-01,  1.1019e+00,  5.8132e-01,\n",
      "           4.3035e-01, -9.3083e-01,  2.3439e+00,  1.9657e+00, -2.9056e-01,\n",
      "          -2.3432e+00, -1.0689e+00,  9.1142e-01,  3.0538e-01,  1.9601e+00,\n",
      "          -1.0383e-01]],\n",
      "\n",
      "        [[-7.6447e-01,  1.2408e+00,  1.6643e-01, -1.2318e+00,  1.3892e+00,\n",
      "           4.9767e-01,  1.6797e+00, -2.3953e-02,  1.6859e+00, -2.1769e-01,\n",
      "           7.6496e-01,  2.1971e+00, -7.1279e-01,  9.3442e-01,  2.2050e+00,\n",
      "           2.7852e+00],\n",
      "         [ 1.3826e+00,  2.0793e+00,  1.3970e+00,  2.1968e+00,  2.1491e-01,\n",
      "           2.6143e+00,  4.9531e-01,  2.3002e+00,  8.8323e-01,  1.0651e+00,\n",
      "           7.7640e-01,  2.9857e-02, -8.8668e-01,  6.8168e-01, -3.3409e-01,\n",
      "           1.4543e+00],\n",
      "         [-2.8981e-01, -4.4183e-01,  2.3935e+00, -2.5307e-01,  3.6015e+00,\n",
      "           4.1140e-01, -4.1229e-01,  2.7412e+00, -1.8441e-01,  6.8339e-01,\n",
      "           1.3001e+00,  2.3452e+00,  1.9594e-01,  2.5717e+00, -3.8210e-01,\n",
      "           2.3951e+00]],\n",
      "\n",
      "        [[-1.1991e+00,  9.7431e-01,  1.8024e+00, -5.9653e-02,  3.4028e+00,\n",
      "           4.3133e-01, -4.7549e-01,  2.7432e+00, -2.0441e-01,  6.8359e-01,\n",
      "           1.2937e+00,  2.3453e+00,  1.9394e-01,  2.5717e+00, -3.8274e-01,\n",
      "           2.3951e+00],\n",
      "         [ 7.1224e-01,  4.8571e-01,  7.1933e-01,  2.0768e+00,  2.0349e+00,\n",
      "           2.0027e+00,  1.0363e+00,  5.6598e-01, -1.2326e+00,  2.2845e+00,\n",
      "           2.4693e-01,  1.5304e+00, -1.3531e-02, -1.2357e+00,  1.4663e+00,\n",
      "          -2.1906e-01],\n",
      "         [ 1.4083e+00,  4.6185e-01,  9.8057e-01,  2.2691e+00,  6.7818e-01,\n",
      "           4.4667e-01,  2.8552e-02,  1.6553e+00, -2.9123e-01,  4.3780e-01,\n",
      "          -4.7717e-01, -2.7213e-01, -1.7202e-01,  1.5541e+00, -1.8102e-01,\n",
      "           7.6553e-01]]])\n",
      "\n",
      "###########################################################################\n",
      "### Encoder Start ###\n",
      "\n",
      "Q_enc_shape =  torch.Size([5, 3, 16])\n",
      "K_enc_shape =  torch.Size([5, 3, 16])\n",
      "V_enc_shape =  torch.Size([5, 3, 16])\n",
      "\n",
      "Q_enc_shape =  torch.Size([12, 5, 4])\n",
      "K_enc_shape =  torch.Size([12, 5, 4])\n",
      "V_enc_shape =  torch.Size([12, 5, 4])\n",
      "\n",
      "Q_enc_0 = \n",
      "tensor([[[-3.2546e-01,  1.3022e+00,  5.0607e-01, -5.3552e-01],\n",
      "         [-1.1174e+00,  7.5657e-01, -9.7703e-01,  1.4006e+00],\n",
      "         [-1.3674e+00,  3.8452e-01, -6.2080e-01,  3.1212e-01],\n",
      "         [ 1.3097e+00,  1.7662e+00,  5.1150e-02, -1.0957e+00],\n",
      "         [-3.3950e-01, -6.6494e-01, -7.4291e-01,  1.3138e+00]],\n",
      "\n",
      "        [[ 1.0498e+00,  4.5765e-01,  5.6608e-01,  1.2285e+00],\n",
      "         [ 1.0749e+00,  9.3711e-01, -4.5114e-01,  6.8148e-01],\n",
      "         [ 1.4387e+00, -2.1246e-02,  6.7543e-01, -1.5367e-01],\n",
      "         [-6.7305e-01,  7.0575e-01,  4.4276e-01,  7.6966e-01],\n",
      "         [ 1.9275e-01,  9.5411e-01,  6.1045e-02, -3.2638e-01]],\n",
      "\n",
      "        [[-2.3524e-01,  3.0658e-02, -1.5798e-01,  3.6842e-04],\n",
      "         [-2.9684e-01,  1.2599e+00,  7.9889e-01,  9.0739e-01],\n",
      "         [-4.8075e-01,  1.1659e+00,  1.0318e+00,  8.8630e-01],\n",
      "         [-9.1547e-01,  4.1168e-01,  4.6660e-01, -3.5256e-01],\n",
      "         [-5.5510e-01,  4.2123e-03, -7.8951e-01, -4.8569e-03]],\n",
      "\n",
      "        [[ 7.3641e-01, -2.5511e-01, -6.2547e-01,  3.9190e-01],\n",
      "         [-4.3740e-01, -4.5592e-01, -2.2021e+00,  1.0927e+00],\n",
      "         [ 1.8292e-01, -6.6728e-01, -1.2892e+00,  1.0165e+00],\n",
      "         [ 6.5179e-02,  1.1636e+00,  6.8790e-01, -1.7247e-02],\n",
      "         [-1.0638e-01, -1.5200e-01,  5.0028e-01, -4.5269e-01]],\n",
      "\n",
      "        [[-1.3020e+00,  2.0654e-01, -3.0614e-01,  3.8642e-01],\n",
      "         [-2.6009e-01,  1.1243e+00,  8.2072e-01, -4.6122e-01],\n",
      "         [-1.3020e+00,  2.0654e-01, -3.0614e-01,  3.8642e-01],\n",
      "         [-1.3187e+00,  3.1598e-01, -3.0355e-01, -1.4622e+00],\n",
      "         [-1.0521e+00,  5.7860e-01, -6.6238e-01,  1.4749e+00]],\n",
      "\n",
      "        [[ 1.4473e+00,  3.2989e-01,  7.5273e-01, -2.7997e-01],\n",
      "         [ 1.0584e+00,  8.0879e-01,  6.4338e-01,  1.1022e+00],\n",
      "         [ 1.4473e+00,  3.2989e-01,  7.5273e-01, -2.7997e-01],\n",
      "         [-6.5663e-02, -1.7166e-01,  7.9662e-01, -1.3151e+00],\n",
      "         [ 1.0835e+00,  1.2883e+00, -3.7384e-01,  5.5518e-01]],\n",
      "\n",
      "        [[-2.7498e-01,  1.0637e+00,  7.3980e-01,  1.0341e+00],\n",
      "         [-2.9477e-02, -7.1476e-02, -4.5002e-01,  1.4820e-01],\n",
      "         [-2.7498e-01,  1.0637e+00,  7.3980e-01,  1.0341e+00],\n",
      "         [ 8.7448e-02,  4.4208e-01,  1.7863e+00,  6.8859e-02],\n",
      "         [-9.1074e-02,  1.1578e+00,  5.0685e-01,  1.0552e+00]],\n",
      "\n",
      "        [[ 1.5365e-01, -9.1222e-01, -1.2271e+00,  7.8964e-01],\n",
      "         [ 7.0714e-01, -5.0005e-01, -5.6333e-01,  1.6503e-01],\n",
      "         [ 1.5365e-01, -9.1222e-01, -1.2271e+00,  7.8964e-01],\n",
      "         [ 2.2593e-01,  7.9762e-02,  1.1165e+00, -1.4267e+00],\n",
      "         [-4.6667e-01, -7.0086e-01, -2.1400e+00,  8.6579e-01]],\n",
      "\n",
      "        [[-6.7676e-01, -7.9889e-01, -1.1476e+00, -2.8914e-01],\n",
      "         [-6.8435e-01,  7.4800e-01,  1.3373e+00, -6.8016e-01],\n",
      "         [-1.7236e-01,  1.0628e+00,  1.1541e+00, -4.7275e-01],\n",
      "         [-1.8640e-01, -9.0439e-01, -9.4907e-02,  1.3766e+00],\n",
      "         [-9.1920e-01, -2.2247e-01, -5.4976e-01,  3.2762e-02]],\n",
      "\n",
      "        [[ 1.0344e+00, -3.6369e-02,  1.3085e+00, -4.6883e-01],\n",
      "         [ 1.6470e+00,  2.5760e+00, -1.7980e-01,  1.7043e-01],\n",
      "         [ 1.0997e+00,  1.1216e+00,  6.9461e-01,  1.2989e+00],\n",
      "         [ 2.4263e-01,  1.6181e+00,  1.8957e-01, -2.5599e-01],\n",
      "         [ 6.9858e-01, -3.5224e-01,  6.3148e-01, -1.8636e-03]],\n",
      "\n",
      "        [[-5.6970e-01, -5.2710e-01,  1.9680e-01,  1.9160e+00],\n",
      "         [ 7.7860e-02, -5.1337e-04, -9.9718e-01,  8.7974e-01],\n",
      "         [ 1.8956e-01, -8.8437e-02, -6.9935e-01,  2.8267e-01],\n",
      "         [-1.3029e-01, -1.1488e-01, -1.3309e+00,  2.7744e-01],\n",
      "         [ 2.6931e-01, -1.2521e-01,  1.6623e-01,  7.4329e-01]],\n",
      "\n",
      "        [[ 1.6952e+00,  1.2042e-01,  3.6132e-01,  2.6084e-01],\n",
      "         [ 2.7152e-01, -2.0412e+00, -1.1783e+00, -3.4333e-02],\n",
      "         [ 5.0506e-01, -6.6746e-01, -8.6463e-01, -9.0957e-03],\n",
      "         [-3.3773e-01, -5.6435e-01,  2.6112e-01, -8.5368e-01],\n",
      "         [ 2.2799e-01,  2.7533e-01,  2.0173e-01, -8.7058e-01]]])\n",
      "\n",
      "K_enc_0 = \n",
      "tensor([[[-3.6940e-01,  3.8444e-01,  7.7726e-01,  6.7787e-01],\n",
      "         [ 8.5510e-01, -3.2077e-01,  1.2798e-01,  1.4454e+00],\n",
      "         [ 1.3886e+00, -9.5822e-02, -1.8016e-01,  1.1143e+00],\n",
      "         [ 3.4734e-01,  3.6453e-01,  2.6624e-01,  6.2801e-01],\n",
      "         [ 4.5841e-01, -9.5854e-02,  1.2334e+00, -8.7633e-02]],\n",
      "\n",
      "        [[-3.1459e-01,  9.6074e-02, -2.0220e-02, -1.5848e+00],\n",
      "         [-4.4948e-01, -4.2407e-01,  4.5235e-01, -2.1921e-02],\n",
      "         [-2.2233e+00, -4.0850e-01,  5.6345e-01, -1.0690e+00],\n",
      "         [-3.7130e-01, -4.4807e-01,  7.2592e-01, -9.3092e-01],\n",
      "         [ 6.1529e-01, -5.2513e-01,  1.2355e+00,  5.1628e-01]],\n",
      "\n",
      "        [[ 1.3581e-01, -2.6657e-01,  1.1215e+00, -4.7084e-01],\n",
      "         [-1.4552e+00,  5.6021e-01,  1.9505e-01, -2.1863e-01],\n",
      "         [-2.0022e-01, -1.3379e-01,  8.6809e-01, -5.0485e-01],\n",
      "         [ 1.4136e+00, -7.6261e-01,  2.5550e+00, -7.0812e-01],\n",
      "         [ 8.3264e-01, -2.3651e-02,  1.2615e+00,  9.8742e-01]],\n",
      "\n",
      "        [[ 7.6088e-01,  3.5409e-01,  1.5975e-01, -3.0504e-01],\n",
      "         [ 4.0640e-01,  8.8627e-01, -1.2833e-01, -1.3372e+00],\n",
      "         [ 9.0715e-01,  3.8808e-01, -2.9932e-01, -3.8840e-01],\n",
      "         [ 7.1420e-01,  1.0147e+00, -4.1493e-02, -3.0298e-01],\n",
      "         [-7.0781e-01, -5.4659e-01, -7.0713e-01,  3.4588e-01]],\n",
      "\n",
      "        [[ 1.0669e+00, -1.0126e-01, -1.5605e-01,  8.8763e-01],\n",
      "         [-6.9109e-01,  3.7900e-01,  8.0136e-01,  4.5117e-01],\n",
      "         [ 1.0669e+00, -1.0126e-01, -1.5605e-01,  8.8763e-01],\n",
      "         [ 1.1760e+00, -1.3653e+00, -6.3528e-01, -3.6947e-01],\n",
      "         [ 5.3341e-01, -3.2621e-01,  1.5209e-01,  1.2187e+00]],\n",
      "\n",
      "        [[-1.9210e+00, -5.3149e-01,  7.6326e-01, -1.2478e+00],\n",
      "         [-1.2262e-02, -2.6918e-02,  1.7960e-01, -1.7636e+00],\n",
      "         [-1.9210e+00, -5.3149e-01,  7.6326e-01, -1.2478e+00],\n",
      "         [-1.1927e+00, -9.9550e-01, -8.1319e-02,  7.5868e-02],\n",
      "         [-1.4715e-01, -5.4706e-01,  6.5217e-01, -2.0079e-01]],\n",
      "\n",
      "        [[ 1.4529e-01, -1.3388e-01,  6.6323e-01, -1.9883e-01],\n",
      "         [ 4.8132e-01, -2.6666e-01,  9.1660e-01, -1.6483e-01],\n",
      "         [ 1.4529e-01, -1.3388e-01,  6.6323e-01, -1.9883e-01],\n",
      "         [ 1.6220e+00, -1.5911e-01,  1.0106e+00, -5.5737e-01],\n",
      "         [-1.1097e+00,  5.6013e-01, -9.8062e-03,  8.7385e-02]],\n",
      "\n",
      "        [[ 8.2285e-01,  2.9747e-01, -2.6385e-01, -2.1446e-01],\n",
      "         [ 6.7659e-01,  2.6348e-01,  1.9523e-01, -1.3110e-01],\n",
      "         [ 8.2285e-01,  2.9747e-01, -2.6385e-01, -2.1446e-01],\n",
      "         [ 6.4185e-01,  5.4550e-01,  1.0770e-03,  2.7126e-01],\n",
      "         [ 3.2210e-01,  7.9566e-01, -9.2859e-02, -1.1632e+00]],\n",
      "\n",
      "        [[ 8.3721e-01,  6.9811e-01,  4.2317e-02, -1.7469e-01],\n",
      "         [-1.0917e+00, -3.2708e-03,  1.1678e+00,  5.9581e-01],\n",
      "         [-8.7103e-01,  2.1802e-01,  9.6128e-01,  4.1781e-01],\n",
      "         [-4.3227e-02, -2.6227e-01,  1.4174e+00, -3.4770e-01],\n",
      "         [ 3.5376e-01, -3.0646e-01, -2.4600e-01, -2.2979e-01]],\n",
      "\n",
      "        [[-2.1028e-01, -5.6786e-01, -4.4119e-01, -1.4743e+00],\n",
      "         [ 1.6441e+00, -1.4273e+00,  1.1289e+00, -1.5604e+00],\n",
      "         [ 2.4218e-01,  8.3325e-02,  4.3134e-01, -1.7524e+00],\n",
      "         [ 1.1721e+00, -5.3788e-01,  1.6871e+00,  3.4863e-01],\n",
      "         [-2.2770e-01, -2.6715e-01, -8.8350e-01, -8.1879e-01]],\n",
      "\n",
      "        [[ 2.5200e+00, -1.1564e+00,  1.1951e+00,  1.8357e-01],\n",
      "         [ 6.1488e-01, -7.6819e-01,  6.1949e-01, -1.6884e-01],\n",
      "         [ 6.3870e-01, -1.4063e-01,  7.2596e-01,  7.4177e-02],\n",
      "         [ 1.3355e+00,  1.0229e-01,  8.6604e-01,  1.5324e+00],\n",
      "         [ 1.4991e+00, -3.4851e-01,  2.7341e-01,  2.4904e-01]],\n",
      "\n",
      "        [[-2.9672e-01,  1.0659e-01,  1.8363e+00, -1.1087e+00],\n",
      "         [ 1.0870e+00,  1.7162e-01,  2.9044e-01, -6.8089e-01],\n",
      "         [ 8.7204e-01,  3.1448e-01,  4.3926e-02,  9.3961e-03],\n",
      "         [-5.9666e-01, -5.8619e-01, -8.2296e-01,  6.6032e-01],\n",
      "         [-2.4118e-02,  3.6473e-01,  6.8654e-01, -6.0414e-01]]])\n",
      "\n",
      "V_enc_0 = \n",
      "tensor([[[-0.5466,  0.3032, -0.1280, -1.1986],\n",
      "         [ 0.9067,  0.1102, -0.5327, -0.2930],\n",
      "         [ 0.5386,  0.3840,  0.2656,  0.4803],\n",
      "         [ 1.0739, -0.2026, -0.3439,  0.3880],\n",
      "         [ 1.7295,  0.3236, -0.1392,  1.1382]],\n",
      "\n",
      "        [[-0.5474,  0.0611, -0.3015,  0.5204],\n",
      "         [ 0.1719,  0.2989, -1.0815, -0.0698],\n",
      "         [-0.0261,  0.3445, -0.9454,  1.0343],\n",
      "         [ 2.0800, -1.0441, -0.3011, -0.0696],\n",
      "         [ 1.2115, -0.5317,  0.4774, -0.7802]],\n",
      "\n",
      "        [[-0.4887,  0.5826, -0.3057,  0.3452],\n",
      "         [-0.5933, -1.0738,  0.1526,  0.0993],\n",
      "         [-0.3913, -1.5088, -0.2439,  0.8571],\n",
      "         [ 1.2306,  0.5393,  1.2133, -0.3330],\n",
      "         [ 0.6134, -0.0646,  0.4135,  0.1567]],\n",
      "\n",
      "        [[ 0.6816,  0.9360,  0.8121, -0.8151],\n",
      "         [-0.6215,  1.3152,  0.4951, -2.2198],\n",
      "         [ 0.3693,  0.4548,  1.0756, -0.4015],\n",
      "         [ 1.0646,  1.2502,  1.3851, -0.6608],\n",
      "         [-0.9381,  0.8090,  0.4921,  0.0046]],\n",
      "\n",
      "        [[ 0.7291,  0.3335,  0.2746,  0.5240],\n",
      "         [-0.3562,  0.2528, -0.1190, -1.1549],\n",
      "         [ 0.7291,  0.3335,  0.2746,  0.5240],\n",
      "         [ 0.4918, -1.2115, -0.4908,  0.5729],\n",
      "         [ 1.0971,  0.0597, -0.5237, -0.2493]],\n",
      "\n",
      "        [[-0.0583,  0.3595, -0.8283,  0.9298],\n",
      "         [-0.5795,  0.0761, -0.1845,  0.4159],\n",
      "         [-0.0583,  0.3595, -0.8283,  0.9298],\n",
      "         [ 1.0636,  0.3899,  0.3448,  0.0795],\n",
      "         [ 0.1398,  0.3139, -0.9644, -0.1743]],\n",
      "\n",
      "        [[-0.4469, -1.5965, -0.2927,  0.9740],\n",
      "         [-0.5443,  0.4949, -0.3545,  0.4622],\n",
      "         [-0.4469, -1.5965, -0.2927,  0.9740],\n",
      "         [ 0.5775, -1.4067,  1.0781, -0.0048],\n",
      "         [-0.6489, -1.1615,  0.1037,  0.2162]],\n",
      "\n",
      "        [[ 0.2382,  0.3669,  1.1916, -0.4420],\n",
      "         [ 0.5505,  0.8481,  0.9280, -0.8557],\n",
      "         [ 0.2382,  0.3669,  1.1916, -0.4420],\n",
      "         [ 0.7249,  1.3243,  0.8216,  1.0775],\n",
      "         [-0.7526,  1.2273,  0.6111, -2.2604]],\n",
      "\n",
      "        [[ 1.2822,  1.6565,  1.3015, -1.0550],\n",
      "         [ 0.1254,  0.8267, -0.1362, -0.9523],\n",
      "         [-0.1997,  0.2424, -0.3472, -1.4379],\n",
      "         [ 2.0764,  0.2628, -0.3585,  0.8989],\n",
      "         [ 0.9481, -0.3056, -0.1666, -1.2705]],\n",
      "\n",
      "        [[ 1.7196, -0.2056,  0.1951,  0.7704],\n",
      "         [-0.0139,  0.4115,  0.8601,  0.6931],\n",
      "         [-0.7178,  0.3487,  0.0407,  0.2202],\n",
      "         [ 1.0411, -0.2441,  0.8197, -1.0804],\n",
      "         [ 0.8065,  0.2468, -0.1807, -0.2926]],\n",
      "\n",
      "        [[ 0.2972,  0.9438, -0.0073,  1.0273],\n",
      "         [-1.1770, -0.0383, -0.0563,  0.3457],\n",
      "         [-0.6865,  0.4517, -0.3555,  0.6008],\n",
      "         [ 0.4156, -0.1954,  0.3637,  0.4122],\n",
      "         [-0.4300, -0.0483, -0.2714,  0.4073]],\n",
      "\n",
      "        [[ 1.4412,  0.4255,  2.0812, -0.3714],\n",
      "         [ 0.9682,  0.8145,  1.6699, -2.2493],\n",
      "         [ 0.3748,  0.7679,  0.8283, -1.1857],\n",
      "         [-1.2449,  0.6410,  0.5083, -0.3660],\n",
      "         [ 0.0924,  0.9744,  0.8392, -0.1375]]])\n",
      "\n",
      "tensor([[[[0.2937, 0.1051, 0.1127, 0.2298, 0.2587],\n",
      "          [0.2736, 0.2485, 0.1851, 0.2255, 0.0673],\n",
      "          [0.3306, 0.1723, 0.1306, 0.2346, 0.1318],\n",
      "          [0.1495, 0.1155, 0.2377, 0.2383, 0.2589],\n",
      "          [0.1567, 0.3391, 0.2593, 0.1634, 0.0815]],\n",
      "\n",
      "         [[0.0775, 0.1913, 0.0410, 0.1225, 0.5677],\n",
      "          [0.1959, 0.2186, 0.0579, 0.1555, 0.3720],\n",
      "          [0.1671, 0.1586, 0.0498, 0.1974, 0.4270],\n",
      "          [0.1252, 0.2209, 0.2763, 0.1597, 0.2179],\n",
      "          [0.2839, 0.1719, 0.1737, 0.2003, 0.1703]],\n",
      "\n",
      "         [[0.1978, 0.2599, 0.2104, 0.1508, 0.1810],\n",
      "          [0.1474, 0.2433, 0.1499, 0.1420, 0.3174],\n",
      "          [0.1493, 0.2458, 0.1512, 0.1551, 0.2985],\n",
      "          [0.1861, 0.3522, 0.2116, 0.1364, 0.1136],\n",
      "          [0.1775, 0.3984, 0.2154, 0.0707, 0.1380]],\n",
      "\n",
      "         [[0.2057, 0.1508, 0.2454, 0.1980, 0.2001],\n",
      "          [0.0941, 0.0703, 0.1432, 0.1021, 0.5903],\n",
      "          [0.1503, 0.0868, 0.1941, 0.1368, 0.4320],\n",
      "          [0.2044, 0.2517, 0.1790, 0.2797, 0.0852],\n",
      "          [0.2101, 0.2416, 0.1889, 0.1904, 0.1690]]],\n",
      "\n",
      "\n",
      "        [[[0.1489, 0.3901, 0.1489, 0.1027, 0.2094],\n",
      "          [0.1607, 0.4335, 0.1607, 0.0855, 0.1596],\n",
      "          [0.1489, 0.3901, 0.1489, 0.1027, 0.2094],\n",
      "          [0.1090, 0.4460, 0.1090, 0.2240, 0.1120],\n",
      "          [0.1900, 0.2906, 0.1900, 0.0577, 0.2717]],\n",
      "\n",
      "         [[0.1035, 0.3862, 0.1035, 0.0982, 0.3086],\n",
      "          [0.0962, 0.2021, 0.0962, 0.1854, 0.4201],\n",
      "          [0.1035, 0.3862, 0.1035, 0.0982, 0.3086],\n",
      "          [0.2661, 0.2662, 0.2661, 0.0809, 0.1208],\n",
      "          [0.0896, 0.3372, 0.0896, 0.1668, 0.3167]],\n",
      "\n",
      "         [[0.1884, 0.1873, 0.1884, 0.1433, 0.2927],\n",
      "          [0.1996, 0.1890, 0.1996, 0.1761, 0.2357],\n",
      "          [0.1884, 0.1873, 0.1884, 0.1433, 0.2927],\n",
      "          [0.1895, 0.2344, 0.1895, 0.2708, 0.1158],\n",
      "          [0.1861, 0.1842, 0.1861, 0.1549, 0.2887]],\n",
      "\n",
      "         [[0.2401, 0.1880, 0.2401, 0.2177, 0.1140],\n",
      "          [0.2298, 0.1948, 0.2298, 0.1958, 0.1498],\n",
      "          [0.2401, 0.1880, 0.2401, 0.2177, 0.1140],\n",
      "          [0.1641, 0.1963, 0.1641, 0.1331, 0.3424],\n",
      "          [0.2454, 0.1630, 0.2454, 0.2181, 0.1279]]],\n",
      "\n",
      "\n",
      "        [[[0.1569, 0.1870, 0.1835, 0.1444, 0.3282],\n",
      "          [0.1125, 0.2735, 0.2549, 0.2824, 0.0766],\n",
      "          [0.1773, 0.2301, 0.2351, 0.2645, 0.0930],\n",
      "          [0.1143, 0.3027, 0.2397, 0.1593, 0.1838],\n",
      "          [0.1349, 0.2630, 0.2447, 0.1537, 0.2037]],\n",
      "\n",
      "         [[0.0591, 0.4468, 0.1395, 0.3172, 0.0374],\n",
      "          [0.0993, 0.1303, 0.3010, 0.3108, 0.1586],\n",
      "          [0.0448, 0.1252, 0.0937, 0.6672, 0.0690],\n",
      "          [0.1655, 0.1213, 0.3332, 0.1943, 0.1857],\n",
      "          [0.1000, 0.3652, 0.1376, 0.3152, 0.0820]],\n",
      "\n",
      "         [[0.1287, 0.1348, 0.1447, 0.4564, 0.1353],\n",
      "          [0.1475, 0.1563, 0.1650, 0.3003, 0.2309],\n",
      "          [0.1904, 0.1818, 0.1767, 0.2186, 0.2324],\n",
      "          [0.1353, 0.2092, 0.1942, 0.2041, 0.2572],\n",
      "          [0.2296, 0.1450, 0.1544, 0.2906, 0.1805]],\n",
      "\n",
      "         [[0.1322, 0.3429, 0.3016, 0.0766, 0.1467],\n",
      "          [0.0586, 0.1633, 0.1566, 0.5303, 0.0912],\n",
      "          [0.0856, 0.2314, 0.2317, 0.3133, 0.1380],\n",
      "          [0.3390, 0.1794, 0.1288, 0.1440, 0.2088],\n",
      "          [0.2843, 0.2386, 0.1715, 0.0884, 0.2172]]]])\n",
      "ATT =  tensor([[[[ 0.6896,  0.1811, -0.1787,  0.0549],\n",
      "          [ 0.5340,  0.1575, -0.2051, -0.1477],\n",
      "          [ 0.5259,  0.1645, -0.1985, -0.1429],\n",
      "          [ 0.8548,  0.1849, -0.1355,  0.2883],\n",
      "          [ 0.6780,  0.1777, -0.1994, -0.0064]],\n",
      "\n",
      "         [[ 0.9321, -0.3537, -0.0348, -0.3821],\n",
      "          [ 0.7030, -0.2629, -0.2195, -0.1545],\n",
      "          [ 0.8625, -0.3584, -0.1246, -0.2195],\n",
      "          [ 0.5585, -0.1138, -0.4819,  0.1544],\n",
      "          [ 0.4924, -0.1710, -0.4147,  0.1686]],\n",
      "\n",
      "         [[-0.0366, -0.4117,  0.1857,  0.2526],\n",
      "          [ 0.0944, -0.3455,  0.2590,  0.2060],\n",
      "          [ 0.0961, -0.3407,  0.2666,  0.2007],\n",
      "          [-0.1451, -0.5228,  0.1577,  0.2530],\n",
      "          [-0.2358, -0.6202,  0.0968,  0.2836]],\n",
      "\n",
      "         [[ 0.1601,  0.9118,  0.8784, -0.7308],\n",
      "          [-0.3717,  0.8509,  0.6972, -0.3551],\n",
      "          [-0.1394,  0.8636,  0.7759, -0.4815],\n",
      "          [ 0.2670,  1.0223,  0.9125, -0.9816],\n",
      "          [ 0.1069,  0.9751,  0.8403, -0.9085]]],\n",
      "\n",
      "\n",
      "        [[[ 0.3584,  0.0860, -0.1247, -0.2878],\n",
      "          [ 0.2971,  0.1228, -0.0888, -0.3230],\n",
      "          [ 0.3584,  0.0860, -0.1247, -0.2878],\n",
      "          [ 0.2332, -0.0792, -0.1618, -0.3004],\n",
      "          [ 0.5000,  0.1465, -0.1008, -0.1711]],\n",
      "\n",
      "         [[-0.0883,  0.2390, -0.5065,  0.3071],\n",
      "          [ 0.1275,  0.2887, -0.5379,  0.2045],\n",
      "          [-0.0883,  0.2390, -0.5065,  0.3071],\n",
      "          [-0.0824,  0.2810, -0.5785,  0.5908],\n",
      "          [ 0.0158,  0.2546, -0.4587,  0.2650]],\n",
      "\n",
      "         [[-0.3774, -1.0502,  0.0082,  0.5161],\n",
      "          [-0.3326, -1.0653,  0.0304,  0.5263],\n",
      "          [-0.3774, -1.0502,  0.0082,  0.5161],\n",
      "          [-0.2157, -1.0044,  0.1099,  0.5012],\n",
      "          [-0.3644, -1.0562,  0.0228,  0.5093]],\n",
      "\n",
      "         [[ 0.2900,  0.7639,  0.9953, -0.3962],\n",
      "          [ 0.2459,  0.7769,  0.9809, -0.4975],\n",
      "          [ 0.2900,  0.7639,  0.9953, -0.3962],\n",
      "          [ 0.0251,  0.8834,  0.8918, -0.9435],\n",
      "          [ 0.2685,  0.7643,  0.9936, -0.4106]]],\n",
      "\n",
      "\n",
      "        [[[ 0.7990,  0.3966,  0.0085, -0.8946],\n",
      "          [ 0.7867,  0.5251, -0.0933, -0.5892],\n",
      "          [ 0.8465,  0.5820,  0.0075, -0.6247],\n",
      "          [ 0.6418,  0.4835, -0.0634, -0.8440],\n",
      "          [ 0.6694,  0.4783, -0.0343, -0.8652]],\n",
      "\n",
      "         [[ 0.3557,  0.1522,  0.6547,  0.0323],\n",
      "          [ 0.4043,  0.1015,  0.3697, -0.1491],\n",
      "          [ 0.7585, -0.0709,  0.6547, -0.5991],\n",
      "          [ 0.3957,  0.1305,  0.2758,  0.0207],\n",
      "          [ 0.4624,  0.1210,  0.5827, -0.0041]],\n",
      "\n",
      "         [[-0.0883,  0.0860,  0.0693,  0.5091],\n",
      "          [-0.2279,  0.1379, -0.0220,  0.5225],\n",
      "          [-0.2878,  0.1987, -0.0580,  0.5494],\n",
      "          [-0.3651,  0.1551, -0.0774,  0.5169],\n",
      "          [-0.1652,  0.2154, -0.0081,  0.5720]],\n",
      "\n",
      "         [[ 0.5537,  0.7592,  1.2596, -1.2262],\n",
      "          [-0.3505,  0.7070,  0.8705, -0.7814],\n",
      "          [ 0.0570,  0.7381,  1.0316, -0.9606],\n",
      "          [ 0.5505,  0.6850,  1.3601, -0.7635],\n",
      "          [ 0.6150,  0.7153,  1.3593, -0.9077]]]])\n",
      "Attention output = \n",
      "torch.Size([3, 4, 5, 5]) tensor([[[[0.2937, 0.1051, 0.1127, 0.2298, 0.2587],\n",
      "          [0.2736, 0.2485, 0.1851, 0.2255, 0.0673],\n",
      "          [0.3306, 0.1723, 0.1306, 0.2346, 0.1318],\n",
      "          [0.1495, 0.1155, 0.2377, 0.2383, 0.2589],\n",
      "          [0.1567, 0.3391, 0.2593, 0.1634, 0.0815]],\n",
      "\n",
      "         [[0.0775, 0.1913, 0.0410, 0.1225, 0.5677],\n",
      "          [0.1959, 0.2186, 0.0579, 0.1555, 0.3720],\n",
      "          [0.1671, 0.1586, 0.0498, 0.1974, 0.4270],\n",
      "          [0.1252, 0.2209, 0.2763, 0.1597, 0.2179],\n",
      "          [0.2839, 0.1719, 0.1737, 0.2003, 0.1703]],\n",
      "\n",
      "         [[0.1978, 0.2599, 0.2104, 0.1508, 0.1810],\n",
      "          [0.1474, 0.2433, 0.1499, 0.1420, 0.3174],\n",
      "          [0.1493, 0.2458, 0.1512, 0.1551, 0.2985],\n",
      "          [0.1861, 0.3522, 0.2116, 0.1364, 0.1136],\n",
      "          [0.1775, 0.3984, 0.2154, 0.0707, 0.1380]],\n",
      "\n",
      "         [[0.2057, 0.1508, 0.2454, 0.1980, 0.2001],\n",
      "          [0.0941, 0.0703, 0.1432, 0.1021, 0.5903],\n",
      "          [0.1503, 0.0868, 0.1941, 0.1368, 0.4320],\n",
      "          [0.2044, 0.2517, 0.1790, 0.2797, 0.0852],\n",
      "          [0.2101, 0.2416, 0.1889, 0.1904, 0.1690]]],\n",
      "\n",
      "\n",
      "        [[[0.1489, 0.3901, 0.1489, 0.1027, 0.2094],\n",
      "          [0.1607, 0.4335, 0.1607, 0.0855, 0.1596],\n",
      "          [0.1489, 0.3901, 0.1489, 0.1027, 0.2094],\n",
      "          [0.1090, 0.4460, 0.1090, 0.2240, 0.1120],\n",
      "          [0.1900, 0.2906, 0.1900, 0.0577, 0.2717]],\n",
      "\n",
      "         [[0.1035, 0.3862, 0.1035, 0.0982, 0.3086],\n",
      "          [0.0962, 0.2021, 0.0962, 0.1854, 0.4201],\n",
      "          [0.1035, 0.3862, 0.1035, 0.0982, 0.3086],\n",
      "          [0.2661, 0.2662, 0.2661, 0.0809, 0.1208],\n",
      "          [0.0896, 0.3372, 0.0896, 0.1668, 0.3167]],\n",
      "\n",
      "         [[0.1884, 0.1873, 0.1884, 0.1433, 0.2927],\n",
      "          [0.1996, 0.1890, 0.1996, 0.1761, 0.2357],\n",
      "          [0.1884, 0.1873, 0.1884, 0.1433, 0.2927],\n",
      "          [0.1895, 0.2344, 0.1895, 0.2708, 0.1158],\n",
      "          [0.1861, 0.1842, 0.1861, 0.1549, 0.2887]],\n",
      "\n",
      "         [[0.2401, 0.1880, 0.2401, 0.2177, 0.1140],\n",
      "          [0.2298, 0.1948, 0.2298, 0.1958, 0.1498],\n",
      "          [0.2401, 0.1880, 0.2401, 0.2177, 0.1140],\n",
      "          [0.1641, 0.1963, 0.1641, 0.1331, 0.3424],\n",
      "          [0.2454, 0.1630, 0.2454, 0.2181, 0.1279]]],\n",
      "\n",
      "\n",
      "        [[[0.1569, 0.1870, 0.1835, 0.1444, 0.3282],\n",
      "          [0.1125, 0.2735, 0.2549, 0.2824, 0.0766],\n",
      "          [0.1773, 0.2301, 0.2351, 0.2645, 0.0930],\n",
      "          [0.1143, 0.3027, 0.2397, 0.1593, 0.1838],\n",
      "          [0.1349, 0.2630, 0.2447, 0.1537, 0.2037]],\n",
      "\n",
      "         [[0.0591, 0.4468, 0.1395, 0.3172, 0.0374],\n",
      "          [0.0993, 0.1303, 0.3010, 0.3108, 0.1586],\n",
      "          [0.0448, 0.1252, 0.0937, 0.6672, 0.0690],\n",
      "          [0.1655, 0.1213, 0.3332, 0.1943, 0.1857],\n",
      "          [0.1000, 0.3652, 0.1376, 0.3152, 0.0820]],\n",
      "\n",
      "         [[0.1287, 0.1348, 0.1447, 0.4564, 0.1353],\n",
      "          [0.1475, 0.1563, 0.1650, 0.3003, 0.2309],\n",
      "          [0.1904, 0.1818, 0.1767, 0.2186, 0.2324],\n",
      "          [0.1353, 0.2092, 0.1942, 0.2041, 0.2572],\n",
      "          [0.2296, 0.1450, 0.1544, 0.2906, 0.1805]],\n",
      "\n",
      "         [[0.1322, 0.3429, 0.3016, 0.0766, 0.1467],\n",
      "          [0.0586, 0.1633, 0.1566, 0.5303, 0.0912],\n",
      "          [0.0856, 0.2314, 0.2317, 0.3133, 0.1380],\n",
      "          [0.3390, 0.1794, 0.1288, 0.1440, 0.2088],\n",
      "          [0.2843, 0.2386, 0.1715, 0.0884, 0.2172]]]])\n",
      "tensor([[[-7.0855e-01, -2.0022e+00, -1.1072e+00,  4.1574e-01,  1.4185e-02,\n",
      "           5.7522e-01, -3.0359e+00, -1.2718e+00,  3.1716e-01,  1.3002e+00,\n",
      "          -4.9508e-01,  4.4006e-01, -2.1009e+00,  3.8047e+00,  3.9936e-01,\n",
      "           1.6945e+00],\n",
      "         [ 1.0289e+00, -9.4738e-01,  6.4586e-02, -3.3351e-03, -1.0578e-02,\n",
      "           5.9400e-01, -1.0631e+00,  2.3456e+00, -2.6882e-01,  2.0373e+00,\n",
      "           7.9845e-01,  5.2101e-01, -1.0384e+00,  2.2264e+00, -2.3832e+00,\n",
      "           1.0151e-02],\n",
      "         [ 7.7322e-01, -2.6280e-01,  1.9945e+00,  1.8999e+00, -1.6541e+00,\n",
      "           1.4577e+00, -2.4265e+00,  1.0777e+00, -1.3361e+00,  2.8181e+00,\n",
      "           4.8615e-01, -7.7322e-01, -2.3671e+00,  1.9208e+00,  1.0352e-01,\n",
      "           2.6210e+00]],\n",
      "\n",
      "        [[-9.5234e-01, -2.3328e+00, -1.6301e-01,  1.8438e+00, -2.0144e-01,\n",
      "          -2.3209e-03, -3.4777e+00,  2.5282e+00,  2.6920e-01,  7.9352e-01,\n",
      "          -5.2704e-01,  8.8657e-01,  9.6102e-02,  2.8568e+00, -5.0028e-01,\n",
      "          -4.3246e-01],\n",
      "         [-9.6491e-02, -3.3283e+00, -2.2395e-01, -6.2554e-01, -1.2524e-01,\n",
      "           1.4171e-01, -2.4165e+00, -1.0711e+00,  9.2389e-01,  9.8408e-01,\n",
      "           1.7662e-01,  6.6819e-01, -1.8738e+00,  3.5459e+00, -4.3316e-01,\n",
      "           1.6882e+00],\n",
      "         [-7.0247e-01, -4.2828e+00, -5.1078e-01,  6.4020e-01, -6.5742e-01,\n",
      "          -2.3008e-01, -2.0111e+00,  1.0228e+00, -4.1144e-01,  2.1126e+00,\n",
      "           1.6627e+00, -9.1296e-02, -1.6701e+00,  2.3772e+00,  6.0037e-01,\n",
      "           2.6201e+00]],\n",
      "\n",
      "        [[ 6.6871e-02, -3.9651e-01, -4.5673e-01,  6.1654e-01,  5.5910e-02,\n",
      "           8.4948e-01, -1.5377e+00,  1.9038e+00, -6.1341e-01,  2.2274e+00,\n",
      "           8.1373e-02,  3.6132e-01, -1.0566e+00,  2.8128e+00, -1.6733e+00,\n",
      "           2.9009e-02],\n",
      "         [ 1.0289e+00, -9.4738e-01,  6.4586e-02, -3.3351e-03, -1.0578e-02,\n",
      "           5.9400e-01, -1.0631e+00,  2.3456e+00, -2.6882e-01,  2.0373e+00,\n",
      "           7.9845e-01,  5.2101e-01, -1.0384e+00,  2.2264e+00, -2.3832e+00,\n",
      "           1.0151e-02],\n",
      "         [-6.6849e-01, -3.6408e+00, -6.6230e-01,  3.7873e-01,  4.4965e-01,\n",
      "          -2.4266e-01, -2.4863e+00, -7.5140e-01,  1.7676e-01,  2.4228e+00,\n",
      "           3.8770e-02,  3.2903e-01, -2.5750e+00,  3.2522e+00,  1.0827e+00,\n",
      "           1.6912e+00]],\n",
      "\n",
      "        [[-3.8449e-01, -2.4668e-01, -1.5886e-02, -4.3078e-01,  3.2160e-01,\n",
      "          -1.2100e+00, -1.1466e+00, -1.5776e-01, -1.4644e+00,  1.7293e+00,\n",
      "           1.2118e+00,  3.5551e+00, -1.6432e+00, -2.7455e-01,  6.4347e-01,\n",
      "           1.1171e+00],\n",
      "         [ 1.4605e+00, -3.4662e-01,  4.9265e-02, -1.1387e+00,  1.9410e+00,\n",
      "          -8.1850e-01,  6.1729e-02,  1.9006e+00, -9.1922e-01,  3.1569e+00,\n",
      "          -1.3356e+00,  1.5894e+00, -1.5102e+00, -1.5223e+00, -2.0833e-01,\n",
      "           2.2689e+00],\n",
      "         [ 2.3628e+00, -2.7771e+00,  1.9280e-01,  5.9137e-01, -1.3717e+00,\n",
      "          -6.5128e-01, -1.1094e+00,  2.0776e+00, -1.2903e+00,  1.7932e+00,\n",
      "           1.8870e+00,  1.4007e+00, -3.5165e-01, -7.7844e-01, -2.9887e-01,\n",
      "           5.2186e-01]],\n",
      "\n",
      "        [[ 2.0348e+00, -1.4455e+00, -2.5966e-01,  9.8624e-01, -2.0086e+00,\n",
      "          -6.5385e-01, -1.8741e+00,  1.7346e+00, -3.6093e-01,  1.0499e+00,\n",
      "           1.0959e+00,  1.4179e+00, -9.2338e-01, -5.8600e-01, -3.1269e-01,\n",
      "           5.5939e-01],\n",
      "         [-2.4260e-01, -2.7883e+00,  1.7775e-01,  1.8167e+00, -1.6044e-01,\n",
      "          -2.2583e-01, -3.1035e+00,  2.4981e+00,  6.6914e-01,  5.9776e-01,\n",
      "          -2.0325e-02,  1.0974e+00,  7.8711e-02,  2.2695e+00, -1.1073e+00,\n",
      "          -6.0535e-01],\n",
      "         [ 1.5614e+00, -1.1661e+00,  2.0983e+00,  1.2493e+00,  5.4227e-01,\n",
      "           1.0073e-01, -2.5515e+00,  1.6432e+00, -7.8802e-01,  2.9944e+00,\n",
      "          -9.4831e-01,  2.8871e-01, -2.2601e+00,  1.1375e+00,  2.4147e-01,\n",
      "           2.6622e+00]]], grad_fn=<AddBackward0>)\n",
      "Final Encoder 0 Output :\n",
      "norm2(norm1(x + self_atten(x)) + feed_fwd_op)\n",
      "\n",
      "tensor([[[-3.7375e-01, -1.1816e+00, -6.2271e-01,  3.2832e-01,  7.7567e-02,\n",
      "           4.2790e-01, -1.8270e+00, -7.2545e-01,  2.6676e-01,  8.8063e-01,\n",
      "          -2.4044e-01,  3.4351e-01, -1.2432e+00,  2.4446e+00,  3.1809e-01,\n",
      "           1.1268e+00],\n",
      "         [ 6.2961e-01, -9.5662e-01, -1.4439e-01, -1.9890e-01, -2.0471e-01,\n",
      "           2.8054e-01, -1.0495e+00,  1.6865e+00, -4.1198e-01,  1.4390e+00,\n",
      "           4.4464e-01,  2.2196e-01, -1.0297e+00,  1.5907e+00, -2.1091e+00,\n",
      "          -1.8808e-01],\n",
      "         [ 2.2659e-01, -3.9538e-01,  9.5977e-01,  9.0300e-01, -1.2306e+00,\n",
      "           6.3752e-01, -1.6944e+00,  4.0936e-01, -1.0398e+00,  1.4542e+00,\n",
      "           5.4246e-02, -7.0181e-01, -1.6587e+00,  9.1554e-01, -1.7546e-01,\n",
      "           1.3359e+00]],\n",
      "\n",
      "        [[-6.4031e-01, -1.5285e+00, -1.3243e-01,  1.1588e+00, -1.5716e-01,\n",
      "          -2.9037e-02, -2.2652e+00,  1.5992e+00,  1.4567e-01,  4.8304e-01,\n",
      "          -3.6666e-01,  5.4290e-01,  3.4292e-02,  1.8106e+00, -3.4944e-01,\n",
      "          -3.0581e-01],\n",
      "         [ 2.0698e-02, -2.0308e+00, -6.0208e-02, -3.1513e-01,  2.4471e-03,\n",
      "           1.7190e-01, -1.4520e+00, -5.9800e-01,  6.6842e-01,  7.0662e-01,\n",
      "           1.9406e-01,  5.0610e-01, -1.1075e+00,  2.3328e+00, -1.9301e-01,\n",
      "           1.1536e+00],\n",
      "         [-4.2186e-01, -2.4860e+00, -3.1135e-01,  3.5220e-01, -3.9589e-01,\n",
      "          -1.4952e-01, -1.1763e+00,  5.7275e-01, -2.5408e-01,  1.2011e+00,\n",
      "           9.4165e-01, -6.9513e-02, -9.7968e-01,  1.3536e+00,  3.2923e-01,\n",
      "           1.4936e+00]],\n",
      "\n",
      "        [[-1.1205e-01, -4.8962e-01, -5.3869e-01,  3.3582e-01, -1.2098e-01,\n",
      "           5.2562e-01, -1.4195e+00,  1.3847e+00, -6.6635e-01,  1.6484e+00,\n",
      "          -1.0024e-01,  1.2787e-01, -1.0275e+00,  2.1253e+00, -1.5299e+00,\n",
      "          -1.4290e-01],\n",
      "         [ 6.2961e-01, -9.5662e-01, -1.4439e-01, -1.9890e-01, -2.0471e-01,\n",
      "           2.8054e-01, -1.0495e+00,  1.6865e+00, -4.1198e-01,  1.4390e+00,\n",
      "           4.4464e-01,  2.2196e-01, -1.0297e+00,  1.5907e+00, -2.1091e+00,\n",
      "          -1.8808e-01],\n",
      "         [-3.4086e-01, -2.0489e+00, -3.3730e-01,  2.6092e-01,  3.0168e-01,\n",
      "          -9.6161e-02, -1.3855e+00, -3.8851e-01,  1.4486e-01,  1.4355e+00,\n",
      "           6.5565e-02,  2.3236e-01, -1.4364e+00,  1.9122e+00,  6.6546e-01,\n",
      "           1.0151e+00]],\n",
      "\n",
      "        [[-3.7440e-01, -2.6796e-01, -8.9702e-02, -4.1015e-01,  1.7096e-01,\n",
      "          -1.0120e+00, -9.6303e-01, -1.9928e-01, -1.2085e+00,  1.2582e+00,\n",
      "           8.5853e-01,  2.6684e+00, -1.3466e+00, -2.8949e-01,  4.1956e-01,\n",
      "           7.8540e-01],\n",
      "         [ 7.8984e-01, -4.2887e-01, -1.6188e-01, -9.6306e-01,  1.1139e+00,\n",
      "          -7.4711e-01, -1.5348e-01,  1.0867e+00, -8.1504e-01,  1.9339e+00,\n",
      "          -1.0959e+00,  8.7682e-01, -1.2136e+00, -1.2218e+00, -3.3561e-01,\n",
      "           1.3351e+00],\n",
      "         [ 1.5560e+00, -2.0378e+00,  3.8728e-02,  3.1740e-01, -1.0552e+00,\n",
      "          -5.5145e-01, -8.7177e-01,  1.3566e+00, -9.9827e-01,  1.1578e+00,\n",
      "           1.2233e+00,  8.8327e-01, -3.4195e-01, -6.4036e-01, -3.0505e-01,\n",
      "           2.6880e-01]],\n",
      "\n",
      "        [[ 1.6284e+00, -1.1962e+00, -2.3377e-01,  7.7739e-01, -1.6532e+00,\n",
      "          -5.5369e-01, -1.5440e+00,  1.3848e+00, -3.1596e-01,  8.2905e-01,\n",
      "           8.6639e-01,  1.1277e+00, -7.7245e-01, -4.9863e-01, -2.7681e-01,\n",
      "           4.3096e-01],\n",
      "         [-2.0189e-01, -1.9033e+00,  7.9052e-02,  1.1745e+00, -1.4698e-01,\n",
      "          -1.9068e-01, -2.1140e+00,  1.6299e+00,  4.0748e-01,  3.5977e-01,\n",
      "          -5.3331e-02,  6.9374e-01,  1.2861e-02,  1.4771e+00, -7.7985e-01,\n",
      "          -4.4434e-01],\n",
      "         [ 7.1405e-01, -1.0003e+00,  1.0515e+00,  5.1789e-01,  7.3503e-02,\n",
      "          -2.0403e-01, -1.8711e+00,  7.6548e-01, -7.6264e-01,  1.6148e+00,\n",
      "          -8.6339e-01, -8.5874e-02, -1.6879e+00,  4.4762e-01, -1.1556e-01,\n",
      "           1.4060e+00]]], grad_fn=<AddBackward0>)\n",
      "\n",
      "### Encoder Done ###\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "### Decoder Start ###\n",
      "Q_dec_0 = \n",
      "tensor([[[-4.7772e-01,  6.9994e-01, -2.4317e-01,  1.3753e-01],\n",
      "         [-2.0450e-01,  6.6614e-01,  3.5755e-01, -3.1147e-01],\n",
      "         [-9.6026e-01,  2.4673e+00, -2.9600e-01,  2.9330e+00],\n",
      "         [-8.9694e-01,  2.3048e+00,  9.7365e-01,  2.5556e+00]],\n",
      "\n",
      "        [[-9.8551e-01, -7.5628e-01,  2.1984e+00, -1.0317e+00],\n",
      "         [-1.5500e+00,  3.5095e-01, -6.8803e-02, -5.2188e-01],\n",
      "         [-1.8930e+00,  5.8479e-01,  1.4320e+00,  1.2516e-01],\n",
      "         [-3.0953e+00,  2.3745e+00,  9.2446e-01,  4.7702e-02]],\n",
      "\n",
      "        [[-8.3401e-01,  1.4440e+00,  4.0272e-01, -4.4414e-01],\n",
      "         [-5.0454e-01,  3.9886e-01, -2.7185e-02, -2.6923e-01],\n",
      "         [-3.8632e-01, -1.6106e+00, -6.3674e-01,  1.9070e-01],\n",
      "         [-6.7301e-01, -1.8924e+00, -1.4568e+00,  1.3190e-01]],\n",
      "\n",
      "        [[ 5.0697e-01,  1.3615e+00, -8.3323e-01, -1.5126e-01],\n",
      "         [ 2.2424e-01,  1.6721e+00, -1.2046e+00,  3.8664e-02],\n",
      "         [ 2.7266e+00,  1.4596e+00,  1.5861e+00,  1.1367e+00],\n",
      "         [ 3.3616e+00,  1.9529e+00,  7.4700e-01,  1.5359e+00]],\n",
      "\n",
      "        [[-8.6815e-01,  1.2308e+00, -3.3001e-02, -3.7308e-01],\n",
      "         [ 4.9939e-01,  1.5917e+00,  2.1246e-01,  7.2611e-02],\n",
      "         [ 1.1958e-01,  1.7782e+00, -8.0772e-01, -3.1812e-01],\n",
      "         [-8.7687e-01,  3.1814e-01,  2.2518e-01, -2.2660e-01]],\n",
      "\n",
      "        [[-1.9160e+00,  3.4028e-01,  1.2422e+00,  5.2760e-01],\n",
      "         [-6.4376e-01,  5.2752e-01, -7.8839e-01, -1.0246e+00],\n",
      "         [-2.1223e+00,  1.9347e-01,  2.1142e-01, -1.4601e+00],\n",
      "         [-1.2929e+00, -5.0758e-01,  2.0995e-01, -1.9992e-02]],\n",
      "\n",
      "        [[-1.9333e+00,  2.1132e-01, -1.0623e-01, -2.4567e-01],\n",
      "         [-1.4550e+00, -5.4271e-02, -6.6537e-01,  1.1971e+00],\n",
      "         [-1.6575e+00, -3.0775e-01,  1.6449e-02,  7.6033e-01],\n",
      "         [-1.0600e+00, -1.3049e+00,  6.8640e-01, -1.7993e-03]],\n",
      "\n",
      "        [[ 1.8422e-02,  1.3085e+00, -1.0644e+00, -2.7683e-01],\n",
      "         [ 2.3151e-01,  7.6883e-01, -1.3854e+00, -5.2912e-01],\n",
      "         [ 8.1380e-01,  2.1532e+00, -1.1853e+00,  3.6409e-01],\n",
      "         [-3.3871e-01,  1.6210e+00,  2.8922e-01,  1.8015e+00]],\n",
      "\n",
      "        [[-6.7476e-01, -1.2707e+00,  1.1343e+00, -1.9244e+00],\n",
      "         [-6.3113e-01,  6.2221e-01,  2.5213e-02, -1.3570e-01],\n",
      "         [-1.0503e+00,  2.2270e+00,  1.2420e+00,  2.2824e+00],\n",
      "         [-3.5791e-01,  5.8840e-01,  6.2594e-01, -5.8470e-01]],\n",
      "\n",
      "        [[ 4.6403e-01, -2.9052e-01,  3.5396e-02,  1.2491e+00],\n",
      "         [-9.0462e-01, -8.1335e-01,  2.3499e+00, -5.4459e-01],\n",
      "         [-3.0144e+00,  2.3174e+00,  1.0760e+00,  5.3479e-01],\n",
      "         [-1.4691e+00,  2.9387e-01,  8.2767e-02, -3.4799e-02]],\n",
      "\n",
      "        [[-8.6633e-01,  8.0823e-02,  2.3088e-01, -1.6974e+00],\n",
      "         [-9.7112e-01,  1.4730e+00,  1.0912e-01, -7.1013e-01],\n",
      "         [-8.1012e-01, -1.8634e+00, -1.7504e+00, -1.3410e-01],\n",
      "         [-6.4165e-01,  4.2784e-01, -3.2078e-01, -5.3522e-01]],\n",
      "\n",
      "        [[-1.9658e+00,  7.5742e-01, -1.6647e-01,  2.2219e-01],\n",
      "         [ 1.6952e-01,  1.2077e+00, -1.1462e+00, -3.7947e-01],\n",
      "         [ 3.0241e+00,  1.7991e+00,  4.3402e-01,  1.3077e+00],\n",
      "         [-1.1321e-01,  1.5183e+00, -1.5175e+00, -1.8955e-01]]])\n",
      "\n",
      "K_dec_0 = \n",
      "tensor([[[ 7.8398e-01, -2.8978e+00, -1.9969e+00, -8.1523e-02],\n",
      "         [-2.4054e-01, -1.1789e+00, -3.2179e-01,  3.0028e-01],\n",
      "         [ 2.2459e+00,  1.1807e-01, -8.4272e-01, -5.2088e-01],\n",
      "         [ 3.5401e-01,  8.6656e-01,  4.8169e-01,  1.7585e-01]],\n",
      "\n",
      "        [[-2.2458e-01, -1.4127e-01, -1.5425e-02, -4.3561e-01],\n",
      "         [-4.5286e-01, -1.7143e-01,  7.3920e-01, -2.2124e-01],\n",
      "         [-8.1991e-01,  9.2554e-01,  3.2352e-02,  7.1836e-01],\n",
      "         [-1.3430e+00,  1.0354e+00,  7.8981e-01, -3.3099e-01]],\n",
      "\n",
      "        [[ 4.3926e-01,  1.0542e+00,  2.5666e-03, -2.5535e+00],\n",
      "         [ 8.3438e-01,  4.8703e-01,  3.7392e-01, -3.1019e-01],\n",
      "         [ 5.9615e-01, -2.5148e-01,  2.7867e-01, -1.7343e+00],\n",
      "         [ 1.0606e+00, -8.2105e-02, -9.2336e-01, -1.8193e-01]],\n",
      "\n",
      "        [[ 2.1036e-01,  1.3534e+00, -4.9534e-01, -2.2155e+00],\n",
      "         [-2.2480e-01,  1.1705e+00, -5.1477e-01, -4.6465e-01],\n",
      "         [ 8.3970e-01,  8.2966e-01,  1.5627e-01, -1.7664e-01],\n",
      "         [ 1.2857e+00,  1.8845e+00, -7.1693e-02, -4.5686e-01]],\n",
      "\n",
      "        [[ 1.5785e+00, -8.0012e-01, -1.2165e+00,  1.1637e+00],\n",
      "         [ 4.7456e-01, -1.2277e+00, -1.7932e+00,  6.6939e-01],\n",
      "         [ 2.9108e-01, -1.0741e+00, -4.3732e-01, -3.2007e-02],\n",
      "         [ 1.0022e+00,  3.9241e-02, -3.9030e-01,  8.0718e-01]],\n",
      "\n",
      "        [[-1.1497e+00,  1.8438e-01,  4.2984e-01,  2.9018e-01],\n",
      "         [-1.5817e+00, -4.8684e-01, -5.8341e-01, -5.8716e-01],\n",
      "         [-1.0270e+00, -4.7455e-01,  1.2170e-02,  3.4087e-01],\n",
      "         [-8.9543e-01,  4.3494e-02,  1.0376e+00,  7.5883e-01]],\n",
      "\n",
      "        [[-5.1195e-03, -4.5586e-02,  3.8176e-01, -7.6378e-01],\n",
      "         [ 1.7891e-01, -8.7227e-01,  2.0302e-01, -1.4165e+00],\n",
      "         [ 6.7352e-01,  5.4276e-02,  1.2021e+00, -4.5426e-01],\n",
      "         [ 4.1585e-01, -1.5690e+00,  5.2294e-01,  6.9571e-01]],\n",
      "\n",
      "        [[ 1.0573e+00,  9.9475e-01, -2.7674e-01, -1.8682e+00],\n",
      "         [ 2.2913e+00, -3.4779e-02,  1.0709e+00, -2.4700e+00],\n",
      "         [ 6.2046e-01,  1.0307e+00, -2.8338e-01, -1.4275e+00],\n",
      "         [ 6.6790e-01, -3.8380e-01, -1.8104e-01, -8.4886e-01]],\n",
      "\n",
      "        [[-4.2460e-01, -4.5856e-01,  7.9080e-01,  8.4442e-01],\n",
      "         [ 4.9698e-01, -2.2869e+00, -1.9085e+00,  1.1746e-02],\n",
      "         [ 6.7006e-02,  1.4774e+00,  5.7012e-01,  2.6911e-01],\n",
      "         [-5.2754e-01, -5.6802e-01, -2.3336e-01,  3.9355e-01]],\n",
      "\n",
      "        [[-2.2514e-01, -8.0630e-01,  1.2966e+00,  8.4586e-01],\n",
      "         [-5.8476e-01,  7.6748e-02, -1.4493e-01,  7.0403e-02],\n",
      "         [-1.7032e+00,  1.2534e+00,  6.6030e-01,  1.7503e-01],\n",
      "         [-8.1305e-01,  4.6589e-02,  6.0970e-01,  2.8478e-01]],\n",
      "\n",
      "        [[-6.5667e-01, -7.2372e-01,  3.2590e-01,  7.6517e-01],\n",
      "         [ 1.1380e-01,  6.9370e-01,  2.3814e-01, -2.5993e+00],\n",
      "         [ 7.3511e-01, -4.4257e-01, -6.8778e-01, -2.2767e-01],\n",
      "         [ 5.0892e-01,  1.2657e-01,  6.0949e-01, -3.5594e-01]],\n",
      "\n",
      "        [[-7.2759e-02, -1.0743e+00, -6.6493e-02, -6.7504e-01],\n",
      "         [ 7.4326e-01,  1.2273e+00, -6.6120e-01, -2.2911e+00],\n",
      "         [ 1.8186e+00,  1.7584e+00, -2.3756e-01, -5.3247e-01],\n",
      "         [ 3.0809e-01,  1.0444e+00, -6.8064e-01, -5.4025e-01]]])\n",
      "\n",
      "V_dec_0 = \n",
      "tensor([[[-1.0260,  0.9662, -2.2913,  1.8057],\n",
      "         [ 0.5957, -0.4363, -0.6107,  0.1757],\n",
      "         [ 1.1529,  0.6390,  0.4691,  0.1807],\n",
      "         [ 2.0451, -0.5014,  1.6166, -0.3022]],\n",
      "\n",
      "        [[-0.0240, -1.3353, -1.6751, -0.9049],\n",
      "         [-0.4670, -1.8406, -0.9363, -0.2806],\n",
      "         [ 0.2908,  0.4321,  1.0739, -0.3611],\n",
      "         [-0.6475, -2.4038,  1.9287,  0.6706]],\n",
      "\n",
      "        [[ 1.5304, -1.8514, -1.5147,  0.9213],\n",
      "         [ 0.9723, -0.8812, -0.8274,  1.0724],\n",
      "         [ 1.3649, -0.4078, -1.0577, -1.9731],\n",
      "         [ 1.7294, -1.8886, -2.2192, -0.2645]],\n",
      "\n",
      "        [[ 0.5280,  0.7117, -0.2574,  1.5133],\n",
      "         [ 0.4975,  0.9409, -0.3728, -0.8591],\n",
      "         [ 0.0418, -0.3742,  0.1779,  0.3042],\n",
      "         [-0.5309, -0.6696, -0.5338, -0.4551]],\n",
      "\n",
      "        [[-0.2025, -0.9508, -1.4185, -0.6180],\n",
      "         [-0.5360,  0.3189,  0.1011,  0.4221],\n",
      "         [ 0.9998, -0.8083, -0.9948,  0.1297],\n",
      "         [ 0.0029,  0.2251, -1.0812, -1.1702]],\n",
      "\n",
      "        [[-0.2939, -0.2418, -0.6054,  1.2000],\n",
      "         [-0.2660, -1.9972, -1.2478,  1.2041],\n",
      "         [-1.2195, -0.9846, -0.4558,  0.8347],\n",
      "         [-0.4921, -0.1789,  1.0497,  1.6185]],\n",
      "\n",
      "        [[ 0.0906, -1.0008, -0.3126,  0.5912],\n",
      "         [ 1.2918,  0.2884, -0.8203,  0.6532],\n",
      "         [ 1.3875, -1.1471, -1.8779,  0.5379],\n",
      "         [ 0.0668,  0.4564,  0.1974,  0.0822]],\n",
      "\n",
      "        [[ 0.0505, -0.4071,  0.3731,  0.2199],\n",
      "         [ 2.5341,  0.5179,  0.5687, -1.4736],\n",
      "         [ 0.9913,  0.6232, -1.3847, -1.2899],\n",
      "         [-0.1291,  0.0954, -0.4044,  0.5938]],\n",
      "\n",
      "        [[-0.8220,  0.0050, -0.6552, -0.7446],\n",
      "         [-1.1864,  0.6515, -2.0828,  1.4967],\n",
      "         [ 1.8847, -0.8161,  1.8250, -0.6112],\n",
      "         [ 0.4353, -0.7510, -0.4023, -0.1333]],\n",
      "\n",
      "        [[ 0.2215, -0.0855,  0.7154,  1.1723],\n",
      "         [-0.1140, -1.0901, -1.3602, -0.3474],\n",
      "         [-0.7375, -2.1587,  2.2436,  1.2281],\n",
      "         [-0.5570, -1.5954, -0.6214,  0.2769]],\n",
      "\n",
      "        [[-1.2167,  0.5764,  1.2362,  0.0251],\n",
      "         [ 1.1200, -1.7065, -1.3507,  1.0216],\n",
      "         [ 1.3190, -1.7436, -2.0552, -0.1642],\n",
      "         [ 0.5620, -0.7362, -0.6634,  1.1727]],\n",
      "\n",
      "        [[-0.0334,  0.3178,  0.5097,  0.4712],\n",
      "         [ 0.5073,  0.2432,  0.0210,  1.4853],\n",
      "         [-0.5516, -1.1381, -0.2555, -0.4832],\n",
      "         [ 0.4768,  0.4724, -0.0945, -0.8872]]])\n",
      "\n",
      "Attnetion mask infunction = \n",
      "tensor([[[[0., -inf, -inf, -inf],\n",
      "          [0., 0., -inf, -inf],\n",
      "          [0., 0., 0., -inf],\n",
      "          [0., 0., 0., 0.]]]])\n",
      "\n",
      "tensor([[[[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.2855, 0.7145, 0.0000, 0.0000],\n",
      "          [0.0349, 0.6494, 0.3158, 0.0000],\n",
      "          [0.0020, 0.0860, 0.0342, 0.8778]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.4776, 0.5224, 0.0000, 0.0000],\n",
      "          [0.1721, 0.3684, 0.4595, 0.0000],\n",
      "          [0.0220, 0.0430, 0.2058, 0.7292]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.6271, 0.3729, 0.0000, 0.0000],\n",
      "          [0.1866, 0.3004, 0.5130, 0.0000],\n",
      "          [0.0944, 0.1250, 0.2658, 0.5149]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.5390, 0.4610, 0.0000, 0.0000],\n",
      "          [0.0919, 0.1183, 0.7898, 0.0000],\n",
      "          [0.0168, 0.0258, 0.1776, 0.7797]]],\n",
      "\n",
      "\n",
      "        [[[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.6671, 0.3329, 0.0000, 0.0000],\n",
      "          [0.3976, 0.3476, 0.2548, 0.0000],\n",
      "          [0.1590, 0.2389, 0.3347, 0.2674]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.3077, 0.6923, 0.0000, 0.0000],\n",
      "          [0.2333, 0.5895, 0.1772, 0.0000],\n",
      "          [0.2273, 0.3232, 0.2374, 0.2120]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.6089, 0.3911, 0.0000, 0.0000],\n",
      "          [0.4175, 0.3172, 0.2653, 0.0000],\n",
      "          [0.1787, 0.2616, 0.1548, 0.4049]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.7364, 0.2636, 0.0000, 0.0000],\n",
      "          [0.4616, 0.1015, 0.4369, 0.0000],\n",
      "          [0.2643, 0.0658, 0.4354, 0.2346]]],\n",
      "\n",
      "\n",
      "        [[[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.6979, 0.3021, 0.0000, 0.0000],\n",
      "          [0.2486, 0.0014, 0.7499, 0.0000],\n",
      "          [0.2580, 0.0700, 0.4612, 0.2107]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.8428, 0.1572, 0.0000, 0.0000],\n",
      "          [0.0160, 0.0286, 0.9555, 0.0000],\n",
      "          [0.1238, 0.1752, 0.4889, 0.2121]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.1347, 0.8653, 0.0000, 0.0000],\n",
      "          [0.4165, 0.1101, 0.4734, 0.0000],\n",
      "          [0.1740, 0.4593, 0.1815, 0.1852]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.1085, 0.8915, 0.0000, 0.0000],\n",
      "          [0.0041, 0.0339, 0.9620, 0.0000],\n",
      "          [0.0365, 0.3662, 0.3165, 0.2808]]]])\n",
      "ATT =  tensor([[[[-1.0260,  0.9662, -2.2913,  1.8057],\n",
      "          [ 0.1327, -0.0359, -1.0906,  0.6411],\n",
      "          [ 0.7151, -0.0479, -0.3284,  0.2341],\n",
      "          [ 1.8836, -0.4538,  1.3778, -0.2403]],\n",
      "\n",
      "         [[-0.0240, -1.3353, -1.6751, -0.9049],\n",
      "          [-0.2554, -1.5993, -1.2891, -0.5787],\n",
      "          [-0.0425, -0.7094, -0.1399, -0.4250],\n",
      "          [-0.4329, -1.7725,  1.5505,  0.3828]],\n",
      "\n",
      "         [[ 1.5304, -1.8514, -1.5147,  0.9213],\n",
      "          [ 1.3223, -1.4896, -1.2584,  0.9776],\n",
      "          [ 1.2779, -0.8194, -1.0738, -0.5182],\n",
      "          [ 1.5191, -1.3656, -1.6701, -0.4396]],\n",
      "\n",
      "         [[ 0.5280,  0.7117, -0.2574,  1.5133],\n",
      "          [ 0.5139,  0.8174, -0.3106,  0.4195],\n",
      "          [ 0.1404, -0.1188,  0.0728,  0.2777],\n",
      "          [-0.3847, -0.5523, -0.3986, -0.2975]]],\n",
      "\n",
      "\n",
      "        [[[-0.2025, -0.9508, -1.4185, -0.6180],\n",
      "          [-0.3135, -0.5281, -0.9127, -0.2717],\n",
      "          [-0.0121, -0.4732, -0.7824, -0.0660],\n",
      "          [ 0.1751, -0.2853, -0.8235, -0.2669]],\n",
      "\n",
      "         [[-0.2939, -0.2418, -0.6054,  1.2000],\n",
      "          [-0.2746, -1.4570, -1.0501,  1.2028],\n",
      "          [-0.4415, -1.4082, -0.9576,  1.1377],\n",
      "          [-0.5467, -0.9722, -0.4265,  1.2033]],\n",
      "\n",
      "         [[ 0.0906, -1.0008, -0.3126,  0.5912],\n",
      "          [ 0.5604, -0.4966, -0.5112,  0.6154],\n",
      "          [ 0.8157, -0.6307, -0.8889,  0.5967],\n",
      "          [ 0.5959, -0.0962, -0.4812,  0.3931]],\n",
      "\n",
      "         [[ 0.0505, -0.4071,  0.3731,  0.2199],\n",
      "          [ 0.7052, -0.1633,  0.4247, -0.2266],\n",
      "          [ 0.7137,  0.1369, -0.3751, -0.6117],\n",
      "          [ 0.5814,  0.2202, -0.5617, -0.4612]]],\n",
      "\n",
      "\n",
      "        [[[-0.8220,  0.0050, -0.6552, -0.7446],\n",
      "          [-0.9321,  0.2003, -1.0866, -0.0674],\n",
      "          [ 1.2073, -0.6098,  1.2027, -0.6413],\n",
      "          [ 0.6657, -0.4877,  0.4420, -0.3973]],\n",
      "\n",
      "         [[ 0.2215, -0.0855,  0.7154,  1.1723],\n",
      "          [ 0.1687, -0.2435,  0.3891,  0.9334],\n",
      "          [-0.7044, -2.0951,  2.1163,  1.1823],\n",
      "          [-0.4712, -1.5954,  0.8153,  0.7434]],\n",
      "\n",
      "         [[-1.2167,  0.5764,  1.2362,  0.0251],\n",
      "          [ 0.8051, -1.3988, -1.0021,  0.8873],\n",
      "          [ 0.2410, -0.7733, -0.6068,  0.0452],\n",
      "          [ 0.6461, -1.1362, -0.9011,  0.6610]],\n",
      "\n",
      "         [[-0.0334,  0.3178,  0.5097,  0.4712],\n",
      "          [ 0.4487,  0.2513,  0.0740,  1.3753],\n",
      "          [-0.5136, -1.0854, -0.2430, -0.4125],\n",
      "          [ 0.1439, -0.1269, -0.0811,  0.1590]]]])\n",
      "Attention output = \n",
      "torch.Size([3, 4, 4, 4]) tensor([[[[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.2855, 0.7145, 0.0000, 0.0000],\n",
      "          [0.0349, 0.6494, 0.3158, 0.0000],\n",
      "          [0.0020, 0.0860, 0.0342, 0.8778]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.4776, 0.5224, 0.0000, 0.0000],\n",
      "          [0.1721, 0.3684, 0.4595, 0.0000],\n",
      "          [0.0220, 0.0430, 0.2058, 0.7292]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.6271, 0.3729, 0.0000, 0.0000],\n",
      "          [0.1866, 0.3004, 0.5130, 0.0000],\n",
      "          [0.0944, 0.1250, 0.2658, 0.5149]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.5390, 0.4610, 0.0000, 0.0000],\n",
      "          [0.0919, 0.1183, 0.7898, 0.0000],\n",
      "          [0.0168, 0.0258, 0.1776, 0.7797]]],\n",
      "\n",
      "\n",
      "        [[[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.6671, 0.3329, 0.0000, 0.0000],\n",
      "          [0.3976, 0.3476, 0.2548, 0.0000],\n",
      "          [0.1590, 0.2389, 0.3347, 0.2674]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.3077, 0.6923, 0.0000, 0.0000],\n",
      "          [0.2333, 0.5895, 0.1772, 0.0000],\n",
      "          [0.2273, 0.3232, 0.2374, 0.2120]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.6089, 0.3911, 0.0000, 0.0000],\n",
      "          [0.4175, 0.3172, 0.2653, 0.0000],\n",
      "          [0.1787, 0.2616, 0.1548, 0.4049]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.7364, 0.2636, 0.0000, 0.0000],\n",
      "          [0.4616, 0.1015, 0.4369, 0.0000],\n",
      "          [0.2643, 0.0658, 0.4354, 0.2346]]],\n",
      "\n",
      "\n",
      "        [[[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.6979, 0.3021, 0.0000, 0.0000],\n",
      "          [0.2486, 0.0014, 0.7499, 0.0000],\n",
      "          [0.2580, 0.0700, 0.4612, 0.2107]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.8428, 0.1572, 0.0000, 0.0000],\n",
      "          [0.0160, 0.0286, 0.9555, 0.0000],\n",
      "          [0.1238, 0.1752, 0.4889, 0.2121]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.1347, 0.8653, 0.0000, 0.0000],\n",
      "          [0.4165, 0.1101, 0.4734, 0.0000],\n",
      "          [0.1740, 0.4593, 0.1815, 0.1852]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.1085, 0.8915, 0.0000, 0.0000],\n",
      "          [0.0041, 0.0339, 0.9620, 0.0000],\n",
      "          [0.0365, 0.3662, 0.3165, 0.2808]]]])\n",
      "Decoder Self Attention = \n",
      "tensor([[-1.0260,  0.9662, -2.2913,  1.8057, -0.0240, -1.3353, -1.6751, -0.9049,\n",
      "          1.5304, -1.8514, -1.5147,  0.9213,  0.5280,  0.7117, -0.2574,  1.5133],\n",
      "        [-0.2025, -0.9508, -1.4185, -0.6180, -0.2939, -0.2418, -0.6054,  1.2000,\n",
      "          0.0906, -1.0008, -0.3126,  0.5912,  0.0505, -0.4071,  0.3731,  0.2199],\n",
      "        [-0.8220,  0.0050, -0.6552, -0.7446,  0.2215, -0.0855,  0.7154,  1.1723,\n",
      "         -1.2167,  0.5764,  1.2362,  0.0251, -0.0334,  0.3178,  0.5097,  0.4712],\n",
      "        [ 0.1327, -0.0359, -1.0906,  0.6411, -0.2554, -1.5993, -1.2891, -0.5787,\n",
      "          1.3223, -1.4896, -1.2584,  0.9776,  0.5139,  0.8174, -0.3106,  0.4195],\n",
      "        [-0.3135, -0.5281, -0.9127, -0.2717, -0.2746, -1.4570, -1.0501,  1.2028,\n",
      "          0.5604, -0.4966, -0.5112,  0.6154,  0.7052, -0.1633,  0.4247, -0.2266],\n",
      "        [-0.9321,  0.2003, -1.0866, -0.0674,  0.1687, -0.2435,  0.3891,  0.9334,\n",
      "          0.8051, -1.3988, -1.0021,  0.8873,  0.4487,  0.2513,  0.0740,  1.3753],\n",
      "        [ 0.7151, -0.0479, -0.3284,  0.2341, -0.0425, -0.7094, -0.1399, -0.4250,\n",
      "          1.2779, -0.8194, -1.0738, -0.5182,  0.1404, -0.1188,  0.0728,  0.2777],\n",
      "        [-0.0121, -0.4732, -0.7824, -0.0660, -0.4415, -1.4082, -0.9576,  1.1377,\n",
      "          0.8157, -0.6307, -0.8889,  0.5967,  0.7137,  0.1369, -0.3751, -0.6117],\n",
      "        [ 1.2073, -0.6098,  1.2027, -0.6413, -0.7044, -2.0951,  2.1163,  1.1823,\n",
      "          0.2410, -0.7733, -0.6068,  0.0452, -0.5136, -1.0854, -0.2430, -0.4125],\n",
      "        [ 1.8836, -0.4538,  1.3778, -0.2403, -0.4329, -1.7725,  1.5505,  0.3828,\n",
      "          1.5191, -1.3656, -1.6701, -0.4396, -0.3847, -0.5523, -0.3986, -0.2975],\n",
      "        [ 0.1751, -0.2853, -0.8235, -0.2669, -0.5467, -0.9722, -0.4265,  1.2033,\n",
      "          0.5959, -0.0962, -0.4812,  0.3931,  0.5814,  0.2202, -0.5617, -0.4612],\n",
      "        [ 0.6657, -0.4877,  0.4420, -0.3973, -0.4712, -1.5954,  0.8153,  0.7434,\n",
      "          0.6461, -1.1362, -0.9011,  0.6610,  0.1439, -0.1269, -0.0811,  0.1590]])\n",
      "\n",
      "tensor([[[ 2.4178,  6.4858,  1.0955,  0.9115, -1.2320,  0.1593,  1.2339,\n",
      "           2.1548,  0.3546, -0.1995, -2.8314, -3.4712,  2.1251, -0.4063,\n",
      "           1.9201, -0.2223],\n",
      "         [ 2.3868,  1.3617, -0.0797,  1.8172,  1.1180,  1.2900, -0.7405,\n",
      "           1.6991, -0.7419,  0.4510,  0.1050, -0.3909,  0.9082,  0.7243,\n",
      "           1.4996,  0.0213],\n",
      "         [ 1.3108, -1.8060, -0.1405,  1.0082,  1.2915,  1.5410, -0.0666,\n",
      "           0.8730, -0.9167,  0.6163, -1.1891,  2.0229, -0.0515, -0.5373,\n",
      "           1.7190, -1.8931]],\n",
      "\n",
      "        [[ 1.6150,  2.7634,  1.1097,  3.2483, -0.9860, -0.3194,  0.9438,\n",
      "           1.8039, -1.6929,  0.1895, -0.8390, -2.7705,  0.6489,  1.2766,\n",
      "          -0.5733,  0.5343],\n",
      "         [ 1.1537,  1.5793,  1.9269,  2.9180,  0.2915, -0.7941,  0.5764,\n",
      "           2.2866,  1.3300,  4.6918,  0.3362, -2.1118,  0.0835,  1.4781,\n",
      "           0.8991, -0.0645],\n",
      "         [ 1.5976,  3.5038,  1.3581,  1.6350, -0.6656,  0.5190, -0.9641,\n",
      "           3.4398,  1.2020, -0.9855, -2.7428, -2.0063,  2.1380,  0.8433,\n",
      "           1.6515, -0.9249]],\n",
      "\n",
      "        [[-0.4710,  1.1109,  0.9821, -0.8715,  0.1576, -0.0602,  1.9657,\n",
      "           0.6512,  1.5133, -0.0538,  0.4943,  1.1943, -0.0443,  1.3206,\n",
      "           1.3300,  2.0200],\n",
      "         [ 1.4878,  1.9744,  1.6295,  3.8244, -0.0701,  2.3265,  0.6656,\n",
      "           2.7279,  0.4213,  0.9395, -0.0145, -1.7779, -0.3029,  0.7004,\n",
      "          -0.5933,  0.8197],\n",
      "         [-1.8217, -1.8890,  3.6883,  2.4278,  2.2276, -0.6216, -1.1938,\n",
      "           3.9296,  1.4547,  0.1001,  1.5089,  1.5059,  1.5856,  5.1414,\n",
      "          -1.5917,  0.4781]],\n",
      "\n",
      "        [[-2.2695, -0.4747,  3.2027,  2.5367,  1.1320, -0.9084, -1.1498,\n",
      "           4.4856,  0.6743,  0.6511,  1.1926,  0.7372,  1.4956,  5.1360,\n",
      "          -2.0018,  0.4714],\n",
      "         [ 0.9656,  0.2214,  0.9326,  3.4090,  2.1834,  1.9654,  0.9749,\n",
      "           0.6215, -1.6716,  2.1337, -0.2736,  0.2823,  0.1785, -1.3913,\n",
      "           1.1017, -1.0079],\n",
      "         [ 0.8427,  0.0236,  1.8906,  4.1938, -0.7392, -0.3632, -0.3807,\n",
      "           2.6903, -0.0402, -0.2593, -0.4936, -1.8411,  0.9877,  2.9653,\n",
      "          -1.1321, -0.3621]]])\n",
      "Decoder_0 norm1(x + sa(x))\n",
      "tensor([[[ 0.7908,  2.6168,  0.1973,  0.1147, -0.8474, -0.2229,  0.2594,\n",
      "           0.6727, -0.1353, -0.3840, -1.5653, -1.8525,  0.6594, -0.4768,\n",
      "           0.5674, -0.3942],\n",
      "         [ 1.8166,  0.7032, -0.8625,  1.1979,  0.4385,  0.6253, -1.5803,\n",
      "           1.0697, -1.5817, -0.2860, -0.6618, -1.2005,  0.2106,  0.0108,\n",
      "           0.8529, -0.7528],\n",
      "         [ 0.8868, -1.6856, -0.3111,  0.6370,  0.8709,  1.0768, -0.2500,\n",
      "           0.5254, -0.9517,  0.3136, -1.1765,  1.4745, -0.2376, -0.6386,\n",
      "           1.2237, -1.7576]],\n",
      "\n",
      "        [[ 0.7643,  1.5078,  0.4371,  1.8217, -0.9197, -0.4881,  0.3297,\n",
      "           0.8866, -1.3773, -0.1586, -0.8245, -2.0750,  0.1388,  0.5452,\n",
      "          -0.6525,  0.0646],\n",
      "         [ 0.0777,  0.3595,  0.5896,  1.2457, -0.4931, -1.2117, -0.3045,\n",
      "           0.8278,  0.1944,  2.4201, -0.4635, -2.0841, -0.6308,  0.2925,\n",
      "          -0.0909, -0.7288],\n",
      "         [ 0.5650,  1.6445,  0.4293,  0.5861, -0.7167, -0.0458, -0.8857,\n",
      "           1.6082,  0.3409, -0.8978, -1.8930, -1.4759,  0.8710,  0.1378,\n",
      "           0.5955, -0.8635]],\n",
      "\n",
      "        [[-1.4051,  0.4890,  0.3348, -1.8845, -0.6523, -0.9132,  1.5125,\n",
      "          -0.0613,  0.9709, -0.9054, -0.2492,  0.5889, -0.8941,  0.7401,\n",
      "           0.7514,  1.5776],\n",
      "         [ 0.4206,  0.7826,  0.5261,  2.1589, -0.7384,  1.0446, -0.1910,\n",
      "           1.3432, -0.3728,  0.0127, -0.6970, -2.0089, -0.9116, -0.1651,\n",
      "          -1.1276, -0.0764],\n",
      "         [-1.3861, -1.4185,  1.2659,  0.6592,  0.5629, -0.8085, -1.0839,\n",
      "           1.3821,  0.1909, -0.4611,  0.2169,  0.2155,  0.2539,  1.9653,\n",
      "          -1.2754, -0.2792]],\n",
      "\n",
      "        [[-1.5597, -0.6853,  1.1063,  0.7818,  0.0975, -0.8966, -1.0142,\n",
      "           1.7313, -0.1255, -0.1368,  0.1270, -0.0949,  0.2746,  2.0481,\n",
      "          -1.4293, -0.2244],\n",
      "         [ 0.2276, -0.3342,  0.2027,  2.0720,  1.1468,  0.9822,  0.2346,\n",
      "          -0.0321, -1.7630,  1.1093, -0.7078, -0.2882, -0.3665, -1.5515,\n",
      "           0.3303, -1.2621],\n",
      "         [ 0.2152, -0.2975,  0.8711,  2.3128, -0.7750, -0.5396, -0.5506,\n",
      "           1.3717, -0.3374, -0.4746, -0.6212, -1.4648,  0.3060,  1.5439,\n",
      "          -1.0210, -0.5389]]], grad_fn=<AddBackward0>)\n",
      "\n",
      "tensor([[[ 2.1871e-01, -4.9100e-01, -5.4095e-01, -1.2013e-01,  6.1055e-01,\n",
      "           4.1499e-01, -5.5717e-01,  5.8387e-01, -4.8621e-01, -9.0443e-01,\n",
      "          -1.1204e+00,  1.4398e-01,  5.4772e-02, -5.3158e-01,  1.1172e-01,\n",
      "           9.1729e-01,  7.8404e-02, -4.2073e-01, -6.8839e-01,  7.6318e-02,\n",
      "           8.1048e-01, -3.1396e-01,  3.5708e-01, -3.2522e-01,  3.9409e-01,\n",
      "          -5.9085e-01,  2.5554e-01, -2.8741e-01,  7.2147e-01, -5.5758e-01,\n",
      "          -4.1145e-01, -1.7487e+00],\n",
      "         [ 1.1422e+00, -5.5634e-01,  9.5442e-01,  6.6854e-01,  1.5455e+00,\n",
      "           2.4915e-01, -4.1408e-01,  7.3157e-01, -6.1866e-01, -6.1640e-01,\n",
      "          -8.7306e-01, -3.5587e-01, -2.1273e-02, -2.5283e-01, -8.2939e-01,\n",
      "           6.3720e-01, -1.2648e-01, -7.1513e-01, -1.0224e+00,  4.7903e-01,\n",
      "          -7.5905e-01, -2.4574e-01,  2.2063e-01, -2.7187e-01,  1.2170e+00,\n",
      "          -5.7510e-01,  5.1257e-02,  3.6739e-01, -3.0251e-01, -1.4431e+00,\n",
      "          -2.5644e-01, -4.0864e-01],\n",
      "         [-1.9290e-02, -2.3710e-01,  3.4712e-01, -4.5448e-01,  1.1406e+00,\n",
      "           8.4033e-01,  3.2221e-01,  8.3303e-01,  6.8344e-01, -8.5867e-01,\n",
      "          -1.6477e-02,  4.1307e-01,  1.4483e-01, -1.8358e-01, -7.5614e-01,\n",
      "           2.2426e-01, -4.8474e-01, -9.3054e-01, -7.9374e-01,  2.4299e-01,\n",
      "          -6.9764e-01,  4.6378e-01, -4.7720e-01,  1.1181e-01,  6.4937e-01,\n",
      "          -8.3878e-01, -1.3301e-01, -1.5321e-01,  6.0463e-01, -9.5588e-01,\n",
      "          -4.5790e-01, -7.1773e-01]],\n",
      "\n",
      "        [[ 5.3807e-01, -6.9575e-01,  5.2078e-01,  2.5054e-01,  7.9420e-01,\n",
      "           9.3927e-02, -6.0433e-01,  4.6509e-02, -9.7246e-01, -3.4358e-01,\n",
      "          -1.0273e+00, -1.3950e-01, -5.6731e-01, -1.6047e-01,  4.1702e-01,\n",
      "           9.7180e-01, -3.4953e-01, -3.5750e-01, -6.0977e-01, -6.9964e-02,\n",
      "          -4.6194e-02, -2.5575e-01, -3.7027e-01, -7.4143e-01,  5.9139e-01,\n",
      "          -9.2014e-01, -1.0295e-01,  1.0413e-02,  5.6755e-01, -9.0578e-01,\n",
      "           5.2839e-01, -5.1253e-01],\n",
      "         [ 5.9962e-01, -1.1144e+00, -1.8328e-01,  3.2491e-01,  1.0677e+00,\n",
      "          -2.5316e-02, -5.1732e-01,  9.2715e-01, -8.8597e-01, -9.4497e-01,\n",
      "          -9.6905e-01, -2.0325e-01,  2.5950e-01, -9.7842e-01, -2.8173e-01,\n",
      "           7.7701e-01, -1.2517e-01, -3.7821e-01, -6.7487e-01,  5.8025e-02,\n",
      "           8.4514e-01, -6.4097e-01,  4.8270e-01, -5.5854e-01,  7.8354e-01,\n",
      "          -7.4232e-01,  5.0651e-01,  3.3030e-01,  4.6277e-01, -4.7540e-01,\n",
      "          -2.9457e-01, -2.0932e+00],\n",
      "         [ 3.0636e-01, -1.2157e+00,  2.2442e-01,  2.0059e-01,  1.2200e+00,\n",
      "           5.7382e-01, -6.7735e-01,  1.2414e+00, -3.4908e-01, -7.1295e-01,\n",
      "          -2.7655e-01,  2.2943e-01,  8.7165e-01, -8.3922e-01, -1.4664e-01,\n",
      "           5.7783e-01, -1.8874e-01, -6.0897e-01, -3.6983e-01,  1.3857e-01,\n",
      "           5.1121e-01, -4.9277e-01, -2.0057e-02, -6.1947e-01,  4.1729e-01,\n",
      "          -6.5379e-01,  3.1304e-01,  1.1915e-01,  4.6239e-01, -1.8720e-01,\n",
      "          -3.4319e-01, -8.4701e-01]],\n",
      "\n",
      "        [[ 7.5158e-01, -8.8047e-02,  4.6001e-01,  2.8919e-01,  1.1618e+00,\n",
      "           6.1508e-01, -5.2624e-01,  3.3103e-01, -4.1790e-01, -7.1148e-01,\n",
      "          -1.0633e+00, -2.3792e-02, -1.9047e-01,  2.3624e-01, -3.8913e-01,\n",
      "           8.3474e-01,  8.3032e-03, -7.1618e-01, -1.0481e+00,  4.0279e-01,\n",
      "          -5.4690e-01, -6.9307e-04,  1.8807e-01,  1.9723e-02,  7.7020e-01,\n",
      "          -5.4455e-01,  1.4835e-02, -1.2159e-01, -1.2087e-03, -1.4097e+00,\n",
      "          -4.6851e-01, -3.0023e-01],\n",
      "         [ 1.1422e+00, -5.5634e-01,  9.5442e-01,  6.6854e-01,  1.5455e+00,\n",
      "           2.4915e-01, -4.1408e-01,  7.3157e-01, -6.1866e-01, -6.1640e-01,\n",
      "          -8.7306e-01, -3.5587e-01, -2.1273e-02, -2.5283e-01, -8.2939e-01,\n",
      "           6.3720e-01, -1.2648e-01, -7.1513e-01, -1.0224e+00,  4.7903e-01,\n",
      "          -7.5905e-01, -2.4574e-01,  2.2063e-01, -2.7187e-01,  1.2170e+00,\n",
      "          -5.7510e-01,  5.1257e-02,  3.6739e-01, -3.0251e-01, -1.4431e+00,\n",
      "          -2.5644e-01, -4.0864e-01],\n",
      "         [ 2.7505e-01, -9.9303e-01, -2.1539e-01, -2.3105e-01,  6.4218e-01,\n",
      "           4.4383e-01, -6.9543e-01,  1.1004e+00, -5.4765e-01, -1.0770e+00,\n",
      "          -9.3775e-01,  6.4301e-02,  4.8086e-01, -4.0105e-01,  8.0588e-02,\n",
      "           8.5539e-01,  1.2871e-01, -2.1720e-01, -5.2077e-01, -2.8270e-03,\n",
      "           7.4007e-01, -4.4294e-01,  6.1962e-01, -6.5201e-01,  4.0665e-01,\n",
      "          -7.2963e-01,  1.0714e-01, -7.4978e-02,  2.7114e-01, -8.6110e-02,\n",
      "          -3.5201e-01, -1.3318e+00]],\n",
      "\n",
      "        [[-6.1762e-01, -8.6327e-01, -2.3163e-01, -2.5709e-01, -7.9528e-01,\n",
      "           1.2163e+00,  4.3142e-01,  3.5160e-01, -3.3851e-01,  2.1241e-02,\n",
      "           2.1588e-01, -2.3144e-02,  1.1663e+00, -4.5809e-01, -1.7600e-01,\n",
      "          -3.5731e-01, -9.0978e-01, -1.6508e-01,  1.0920e+00, -5.5378e-01,\n",
      "          -3.3840e-01,  4.3178e-01, -1.2390e+00, -9.2217e-01,  6.4636e-01,\n",
      "          -4.0012e-01, -1.6105e-01,  3.7984e-01,  6.1693e-01,  3.5141e-01,\n",
      "          -1.9845e-01, -4.7704e-01],\n",
      "         [-8.9176e-01, -1.0693e+00,  7.7563e-02, -1.5424e+00, -4.2243e-02,\n",
      "           5.6941e-01, -1.7243e-01,  1.2022e+00,  1.5271e-01, -7.0415e-01,\n",
      "          -8.6230e-02,  6.2292e-01,  4.4206e-01, -1.3558e-01, -1.4211e+00,\n",
      "          -9.5073e-02, -1.1919e+00, -8.3059e-01, -8.0435e-02,  9.7018e-01,\n",
      "          -2.9500e-01,  9.8530e-01,  6.0197e-02, -3.9448e-01,  7.9200e-01,\n",
      "          -7.8035e-01, -4.2928e-01,  1.1287e+00, -8.3955e-01,  3.5702e-01,\n",
      "          -5.3820e-01, -6.3667e-03],\n",
      "         [ 3.5754e-01, -1.1429e+00,  9.1141e-01,  6.4604e-01,  1.4104e+00,\n",
      "           3.4218e-01,  4.4494e-01,  1.2385e+00, -4.4160e-01, -2.7458e-02,\n",
      "           2.7152e-01, -2.8896e-01,  9.9017e-01, -1.0494e+00, -8.0665e-01,\n",
      "           2.6337e-01, -8.2439e-01, -1.6375e-01, -6.3407e-02,  4.2858e-01,\n",
      "          -6.4962e-01, -7.4426e-01, -5.8557e-01, -9.0216e-01,  1.6834e+00,\n",
      "          -8.5023e-01, -2.2094e-01,  2.6278e-02,  2.5750e-01, -1.9072e-01,\n",
      "           1.6641e-01, -1.5484e-01]],\n",
      "\n",
      "        [[ 3.3894e-01, -8.2906e-01,  3.8138e-01,  5.0344e-01,  1.1239e+00,\n",
      "           6.3743e-01,  4.6097e-01,  8.4234e-01, -2.4070e-01,  4.0081e-01,\n",
      "           3.3310e-02, -2.8389e-03,  5.0000e-01, -1.3991e+00, -8.8225e-01,\n",
      "           1.3270e-01, -7.5226e-01, -5.4969e-01, -1.8288e-01,  8.1507e-01,\n",
      "          -7.5439e-01, -6.4959e-01, -8.2858e-01, -1.1202e+00,  1.8448e+00,\n",
      "          -1.0070e+00, -7.6016e-01,  1.6484e-01,  4.9596e-01, -6.5866e-01,\n",
      "           4.7006e-01, -4.3714e-01],\n",
      "         [ 8.5796e-01, -1.0154e+00,  8.2697e-01,  5.9639e-01,  9.4076e-01,\n",
      "          -1.3196e-01, -6.4591e-01,  2.6395e-01, -1.1268e+00, -1.9986e-01,\n",
      "          -1.0071e+00, -4.6337e-01, -5.6698e-01, -4.2487e-01,  2.0717e-01,\n",
      "           8.6224e-01, -3.1824e-01, -1.8251e-01, -5.5133e-01, -1.1791e-02,\n",
      "          -1.5918e-01, -4.8062e-01, -2.5620e-01, -1.0743e+00,  9.5180e-01,\n",
      "          -1.0034e+00, -1.2534e-01,  2.3580e-01,  3.7030e-01, -8.2987e-01,\n",
      "           7.0625e-01, -5.7749e-01],\n",
      "         [-2.4828e-01, -1.0266e+00,  5.2077e-01, -1.1936e+00,  8.0867e-01,\n",
      "           4.6980e-01,  5.1360e-02,  1.3096e+00,  1.4599e-01, -1.2290e+00,\n",
      "          -3.2174e-01,  1.8978e-01, -1.2543e-01, -1.4641e-01, -9.1471e-01,\n",
      "           4.1685e-01, -9.6049e-01, -8.2091e-01, -6.8357e-01,  3.5349e-01,\n",
      "          -6.0618e-01,  7.6800e-01, -2.6245e-01, -5.0678e-01,  7.9575e-01,\n",
      "          -1.1708e+00, -3.3417e-01,  5.0545e-01,  3.8351e-02, -5.5487e-01,\n",
      "          -8.6104e-02, -9.3943e-01]]], grad_fn=<UnsafeViewBackward0>)\n",
      "Q_dec_0 = \n",
      "tensor([[[ 1.1264, -0.9937,  1.4197,  0.3692],\n",
      "         [ 0.5315, -0.1477,  1.5717,  0.3135],\n",
      "         [-0.4461, -0.9850, -0.2231, -0.2376],\n",
      "         [-1.5032,  1.0439, -0.5421,  0.6831]],\n",
      "\n",
      "        [[ 0.3424,  0.6421,  1.0896, -0.3594],\n",
      "         [ 0.7930, -0.1733,  0.5092, -0.7895],\n",
      "         [-0.1030, -0.2075, -1.1655,  0.3057],\n",
      "         [ 1.4190, -0.7153, -1.2435, -0.7504]],\n",
      "\n",
      "        [[ 0.2477,  0.7129, -1.5834, -0.2458],\n",
      "         [ 0.4674,  0.2677, -0.1703,  0.6343],\n",
      "         [ 1.2016, -0.8431, -1.2045, -0.2515],\n",
      "         [ 0.2781, -0.4196,  1.2837,  1.2424]],\n",
      "\n",
      "        [[-0.5755,  0.7306, -0.5634, -1.4579],\n",
      "         [-1.3932,  1.4242, -0.5126, -1.1859],\n",
      "         [ 1.0859, -1.0135,  0.0910, -0.3297],\n",
      "         [-1.0083,  0.1004, -0.3474,  0.3922]],\n",
      "\n",
      "        [[ 0.7831,  0.4742,  1.4391,  0.4960],\n",
      "         [ 0.6462,  0.1021,  1.8678, -0.0025],\n",
      "         [-0.1364,  0.0882,  1.3650,  0.0339],\n",
      "         [ 0.6614, -0.0038,  1.1531, -0.8338]],\n",
      "\n",
      "        [[ 0.3197,  0.3848,  0.6179, -0.6321],\n",
      "         [ 0.7613,  0.0658,  0.2844, -0.7146],\n",
      "         [ 0.0966, -0.2962,  0.7528, -0.8452],\n",
      "         [ 0.5340,  0.4131,  0.6603, -0.0586]],\n",
      "\n",
      "        [[-0.5872,  0.3414, -0.0255, -0.0209],\n",
      "         [ 1.0331,  1.1131,  0.9313,  0.8801],\n",
      "         [ 0.0267,  0.0143,  0.0644,  0.1883],\n",
      "         [-0.4241,  0.6617,  1.0566,  0.8480]],\n",
      "\n",
      "        [[-1.5897,  0.5409, -0.1393,  0.4806],\n",
      "         [-0.2379,  0.1150,  0.0037, -1.7682],\n",
      "         [-1.2230,  0.6391, -0.0303, -0.3960],\n",
      "         [ 0.0791,  0.2972,  0.2179,  0.1961]],\n",
      "\n",
      "        [[ 0.3904,  1.1918,  0.1367,  0.1941],\n",
      "         [ 0.4169, -0.2131,  0.9732,  0.7036],\n",
      "         [-1.6220,  1.0338, -0.9424,  0.4563],\n",
      "         [-0.4023,  0.8388,  0.5567,  0.5134]],\n",
      "\n",
      "        [[ 0.4708, -0.0276,  0.2574, -0.1544],\n",
      "         [ 0.6340,  0.3117,  0.9641, -0.7536],\n",
      "         [ 1.2045, -0.8541, -1.1923, -0.8818],\n",
      "         [ 0.9327, -0.7568,  0.1666, -1.2516]],\n",
      "\n",
      "        [[-0.1381,  0.0493,  1.0622,  0.4993],\n",
      "         [ 0.0181,  0.5128, -1.1357, -0.1677],\n",
      "         [-0.0279, -0.5661,  1.4773,  1.0398],\n",
      "         [ 0.2868,  0.0731,  0.8344,  0.7971]],\n",
      "\n",
      "        [[ 0.4640, -0.5308,  0.0771,  0.9066],\n",
      "         [-0.9338,  0.3317, -0.6056, -0.7232],\n",
      "         [-0.6184, -0.2663, -0.2339,  0.6299],\n",
      "         [-1.3747,  1.0081, -0.5460, -0.5643]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "K_dec_0 = \n",
      "tensor([[[ 0.2187, -0.4910, -0.5410, -0.1201],\n",
      "         [ 0.5381, -0.6958,  0.5208,  0.2505],\n",
      "         [ 0.7516, -0.0880,  0.4600,  0.2892],\n",
      "         [-0.6176, -0.8633, -0.2316, -0.2571],\n",
      "         [ 0.3389, -0.8291,  0.3814,  0.5034]],\n",
      "\n",
      "        [[ 0.6105,  0.4150, -0.5572,  0.5839],\n",
      "         [ 0.7942,  0.0939, -0.6043,  0.0465],\n",
      "         [ 1.1618,  0.6151, -0.5262,  0.3310],\n",
      "         [-0.7953,  1.2163,  0.4314,  0.3516],\n",
      "         [ 1.1239,  0.6374,  0.4610,  0.8423]],\n",
      "\n",
      "        [[-0.4862, -0.9044, -1.1204,  0.1440],\n",
      "         [-0.9725, -0.3436, -1.0273, -0.1395],\n",
      "         [-0.4179, -0.7115, -1.0633, -0.0238],\n",
      "         [-0.3385,  0.0212,  0.2159, -0.0231],\n",
      "         [-0.2407,  0.4008,  0.0333, -0.0028]],\n",
      "\n",
      "        [[ 0.0548, -0.5316,  0.1117,  0.9173],\n",
      "         [-0.5673, -0.1605,  0.4170,  0.9718],\n",
      "         [-0.1905,  0.2362, -0.3891,  0.8347],\n",
      "         [ 1.1663, -0.4581, -0.1760, -0.3573],\n",
      "         [ 0.5000, -1.3991, -0.8822,  0.1327]],\n",
      "\n",
      "        [[ 1.1422, -0.5563,  0.9544,  0.6685],\n",
      "         [ 0.5996, -1.1144, -0.1833,  0.3249],\n",
      "         [ 1.1422, -0.5563,  0.9544,  0.6685],\n",
      "         [-0.8918, -1.0693,  0.0776, -1.5424],\n",
      "         [ 0.8580, -1.0154,  0.8270,  0.5964]],\n",
      "\n",
      "        [[ 1.5455,  0.2492, -0.4141,  0.7316],\n",
      "         [ 1.0677, -0.0253, -0.5173,  0.9271],\n",
      "         [ 1.5455,  0.2492, -0.4141,  0.7316],\n",
      "         [-0.0422,  0.5694, -0.1724,  1.2022],\n",
      "         [ 0.9408, -0.1320, -0.6459,  0.2640]],\n",
      "\n",
      "        [[-0.6187, -0.6164, -0.8731, -0.3559],\n",
      "         [-0.8860, -0.9450, -0.9690, -0.2033],\n",
      "         [-0.6187, -0.6164, -0.8731, -0.3559],\n",
      "         [ 0.1527, -0.7041, -0.0862,  0.6229],\n",
      "         [-1.1268, -0.1999, -1.0071, -0.4634]],\n",
      "\n",
      "        [[-0.0213, -0.2528, -0.8294,  0.6372],\n",
      "         [ 0.2595, -0.9784, -0.2817,  0.7770],\n",
      "         [-0.0213, -0.2528, -0.8294,  0.6372],\n",
      "         [ 0.4421, -0.1356, -1.4211, -0.0951],\n",
      "         [-0.5670, -0.4249,  0.2072,  0.8622]],\n",
      "\n",
      "        [[-0.0193, -0.2371,  0.3471, -0.4545],\n",
      "         [ 0.3064, -1.2157,  0.2244,  0.2006],\n",
      "         [ 0.2751, -0.9930, -0.2154, -0.2311],\n",
      "         [ 0.3575, -1.1429,  0.9114,  0.6460],\n",
      "         [-0.2483, -1.0266,  0.5208, -1.1936]],\n",
      "\n",
      "        [[ 1.1406,  0.8403,  0.3222,  0.8330],\n",
      "         [ 1.2200,  0.5738, -0.6773,  1.2414],\n",
      "         [ 0.6422,  0.4438, -0.6954,  1.1004],\n",
      "         [ 1.4104,  0.3422,  0.4449,  1.2385],\n",
      "         [ 0.8087,  0.4698,  0.0514,  1.3096]],\n",
      "\n",
      "        [[ 0.6834, -0.8587, -0.0165,  0.4131],\n",
      "         [-0.3491, -0.7130, -0.2765,  0.2294],\n",
      "         [-0.5477, -1.0770, -0.9377,  0.0643],\n",
      "         [-0.4416, -0.0275,  0.2715, -0.2890],\n",
      "         [ 0.1460, -1.2290, -0.3217,  0.1898]],\n",
      "\n",
      "        [[ 0.1448, -0.1836, -0.7561,  0.2243],\n",
      "         [ 0.8716, -0.8392, -0.1466,  0.5778],\n",
      "         [ 0.4809, -0.4011,  0.0806,  0.8554],\n",
      "         [ 0.9902, -1.0494, -0.8067,  0.2634],\n",
      "         [-0.1254, -0.1464, -0.9147,  0.4168]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "V_dec_0 = \n",
      "tensor([[[ 7.8404e-02, -4.2073e-01, -6.8839e-01,  7.6318e-02],\n",
      "         [-3.4953e-01, -3.5750e-01, -6.0977e-01, -6.9964e-02],\n",
      "         [ 8.3032e-03, -7.1618e-01, -1.0481e+00,  4.0279e-01],\n",
      "         [-9.0978e-01, -1.6508e-01,  1.0920e+00, -5.5378e-01],\n",
      "         [-7.5226e-01, -5.4969e-01, -1.8288e-01,  8.1507e-01]],\n",
      "\n",
      "        [[ 8.1048e-01, -3.1396e-01,  3.5708e-01, -3.2522e-01],\n",
      "         [-4.6194e-02, -2.5575e-01, -3.7027e-01, -7.4143e-01],\n",
      "         [-5.4690e-01, -6.9307e-04,  1.8807e-01,  1.9723e-02],\n",
      "         [-3.3840e-01,  4.3178e-01, -1.2390e+00, -9.2217e-01],\n",
      "         [-7.5439e-01, -6.4959e-01, -8.2858e-01, -1.1202e+00]],\n",
      "\n",
      "        [[ 3.9409e-01, -5.9085e-01,  2.5554e-01, -2.8741e-01],\n",
      "         [ 5.9139e-01, -9.2014e-01, -1.0295e-01,  1.0413e-02],\n",
      "         [ 7.7020e-01, -5.4455e-01,  1.4835e-02, -1.2159e-01],\n",
      "         [ 6.4636e-01, -4.0012e-01, -1.6105e-01,  3.7984e-01],\n",
      "         [ 1.8448e+00, -1.0070e+00, -7.6016e-01,  1.6484e-01]],\n",
      "\n",
      "        [[ 7.2147e-01, -5.5758e-01, -4.1145e-01, -1.7487e+00],\n",
      "         [ 5.6755e-01, -9.0578e-01,  5.2839e-01, -5.1253e-01],\n",
      "         [-1.2087e-03, -1.4097e+00, -4.6851e-01, -3.0023e-01],\n",
      "         [ 6.1693e-01,  3.5141e-01, -1.9845e-01, -4.7704e-01],\n",
      "         [ 4.9596e-01, -6.5866e-01,  4.7006e-01, -4.3714e-01]],\n",
      "\n",
      "        [[-1.2648e-01, -7.1513e-01, -1.0224e+00,  4.7903e-01],\n",
      "         [-1.2517e-01, -3.7821e-01, -6.7487e-01,  5.8025e-02],\n",
      "         [-1.2648e-01, -7.1513e-01, -1.0224e+00,  4.7903e-01],\n",
      "         [-1.1919e+00, -8.3059e-01, -8.0435e-02,  9.7018e-01],\n",
      "         [-3.1824e-01, -1.8251e-01, -5.5133e-01, -1.1791e-02]],\n",
      "\n",
      "        [[-7.5905e-01, -2.4574e-01,  2.2063e-01, -2.7187e-01],\n",
      "         [ 8.4514e-01, -6.4097e-01,  4.8270e-01, -5.5854e-01],\n",
      "         [-7.5905e-01, -2.4574e-01,  2.2063e-01, -2.7187e-01],\n",
      "         [-2.9500e-01,  9.8530e-01,  6.0197e-02, -3.9448e-01],\n",
      "         [-1.5918e-01, -4.8062e-01, -2.5620e-01, -1.0743e+00]],\n",
      "\n",
      "        [[ 1.2170e+00, -5.7510e-01,  5.1257e-02,  3.6739e-01],\n",
      "         [ 7.8354e-01, -7.4232e-01,  5.0651e-01,  3.3030e-01],\n",
      "         [ 1.2170e+00, -5.7510e-01,  5.1257e-02,  3.6739e-01],\n",
      "         [ 7.9200e-01, -7.8035e-01, -4.2928e-01,  1.1287e+00],\n",
      "         [ 9.5180e-01, -1.0034e+00, -1.2534e-01,  2.3580e-01]],\n",
      "\n",
      "        [[-3.0251e-01, -1.4431e+00, -2.5644e-01, -4.0864e-01],\n",
      "         [ 4.6277e-01, -4.7540e-01, -2.9457e-01, -2.0932e+00],\n",
      "         [-3.0251e-01, -1.4431e+00, -2.5644e-01, -4.0864e-01],\n",
      "         [-8.3955e-01,  3.5702e-01, -5.3820e-01, -6.3667e-03],\n",
      "         [ 3.7030e-01, -8.2987e-01,  7.0625e-01, -5.7749e-01]],\n",
      "\n",
      "        [[-4.8474e-01, -9.3054e-01, -7.9374e-01,  2.4299e-01],\n",
      "         [-1.8874e-01, -6.0897e-01, -3.6983e-01,  1.3857e-01],\n",
      "         [ 1.2871e-01, -2.1720e-01, -5.2077e-01, -2.8270e-03],\n",
      "         [-8.2439e-01, -1.6375e-01, -6.3407e-02,  4.2858e-01],\n",
      "         [-9.6049e-01, -8.2091e-01, -6.8357e-01,  3.5349e-01]],\n",
      "\n",
      "        [[-6.9764e-01,  4.6378e-01, -4.7720e-01,  1.1181e-01],\n",
      "         [ 5.1121e-01, -4.9277e-01, -2.0057e-02, -6.1947e-01],\n",
      "         [ 7.4007e-01, -4.4294e-01,  6.1962e-01, -6.5201e-01],\n",
      "         [-6.4962e-01, -7.4426e-01, -5.8557e-01, -9.0216e-01],\n",
      "         [-6.0618e-01,  7.6800e-01, -2.6245e-01, -5.0678e-01]],\n",
      "\n",
      "        [[ 6.4937e-01, -8.3878e-01, -1.3301e-01, -1.5321e-01],\n",
      "         [ 4.1729e-01, -6.5379e-01,  3.1304e-01,  1.1915e-01],\n",
      "         [ 4.0665e-01, -7.2963e-01,  1.0714e-01, -7.4978e-02],\n",
      "         [ 1.6834e+00, -8.5023e-01, -2.2094e-01,  2.6278e-02],\n",
      "         [ 7.9575e-01, -1.1708e+00, -3.3417e-01,  5.0545e-01]],\n",
      "\n",
      "        [[ 6.0463e-01, -9.5588e-01, -4.5790e-01, -7.1773e-01],\n",
      "         [ 4.6239e-01, -1.8720e-01, -3.4319e-01, -8.4701e-01],\n",
      "         [ 2.7114e-01, -8.6110e-02, -3.5201e-01, -1.3318e+00],\n",
      "         [ 2.5750e-01, -1.9072e-01,  1.6641e-01, -1.5484e-01],\n",
      "         [ 3.8351e-02, -5.5487e-01, -8.6104e-02, -9.3943e-01]]],\n",
      "       grad_fn=<TransposeBackward0>)\n",
      "\n",
      "tensor([[[[0.0991, 0.2989, 0.2405, 0.0905, 0.2710],\n",
      "          [0.1025, 0.2766, 0.2685, 0.1053, 0.2471],\n",
      "          [0.2049, 0.1794, 0.1271, 0.2912, 0.1974],\n",
      "          [0.2263, 0.1361, 0.1640, 0.3066, 0.1670]],\n",
      "\n",
      "         [[0.1426, 0.1425, 0.1779, 0.2590, 0.2780],\n",
      "          [0.1703, 0.2300, 0.2319, 0.1282, 0.2396],\n",
      "          [0.2530, 0.2453, 0.2276, 0.1358, 0.1384],\n",
      "          [0.2041, 0.3286, 0.3031, 0.0334, 0.1308]],\n",
      "\n",
      "         [[0.2308, 0.2553, 0.2433, 0.1159, 0.1548],\n",
      "          [0.2025, 0.1766, 0.1992, 0.2008, 0.2209],\n",
      "          [0.3122, 0.1803, 0.2959, 0.1055, 0.1061],\n",
      "          [0.1684, 0.1245, 0.1526, 0.3008, 0.2536]],\n",
      "\n",
      "         [[0.1334, 0.1611, 0.2318, 0.2733, 0.2004],\n",
      "          [0.1394, 0.2508, 0.3412, 0.1553, 0.1133],\n",
      "          [0.1544, 0.0917, 0.0907, 0.3312, 0.3319],\n",
      "          [0.2063, 0.2757, 0.2604, 0.0968, 0.1608]]],\n",
      "\n",
      "\n",
      "        [[[0.3196, 0.0917, 0.3196, 0.0392, 0.2298],\n",
      "          [0.3034, 0.0855, 0.3034, 0.0677, 0.2400],\n",
      "          [0.2517, 0.1166, 0.2517, 0.1497, 0.2303],\n",
      "          [0.2411, 0.1208, 0.2411, 0.1867, 0.2103]],\n",
      "\n",
      "         [[0.2234, 0.1787, 0.2234, 0.1712, 0.2034],\n",
      "          [0.2416, 0.1834, 0.2416, 0.1167, 0.2167],\n",
      "          [0.2065, 0.1861, 0.2065, 0.1638, 0.2370],\n",
      "          [0.2334, 0.1865, 0.2334, 0.1743, 0.1724]],\n",
      "\n",
      "         [[0.1983, 0.2028, 0.1983, 0.1527, 0.2479],\n",
      "          [0.1480, 0.1098, 0.1480, 0.4657, 0.1286],\n",
      "          [0.1952, 0.1963, 0.1952, 0.2217, 0.1917],\n",
      "          [0.1668, 0.1606, 0.1668, 0.3158, 0.1899]],\n",
      "\n",
      "         [[0.2116, 0.1385, 0.2116, 0.1321, 0.3061],\n",
      "          [0.1816, 0.1490, 0.1816, 0.3302, 0.1576],\n",
      "          [0.2083, 0.1342, 0.2083, 0.1900, 0.2592],\n",
      "          [0.2011, 0.1964, 0.2011, 0.1819, 0.2195]]],\n",
      "\n",
      "\n",
      "        [[[0.2793, 0.1755, 0.1854, 0.2026, 0.1572],\n",
      "          [0.1530, 0.2156, 0.1451, 0.3533, 0.1331],\n",
      "          [0.2960, 0.1686, 0.2164, 0.1345, 0.1845],\n",
      "          [0.2520, 0.1790, 0.1566, 0.2479, 0.1645]],\n",
      "\n",
      "         [[0.2192, 0.1910, 0.1685, 0.2316, 0.1897],\n",
      "          [0.2852, 0.1486, 0.1267, 0.2618, 0.1777],\n",
      "          [0.1598, 0.2847, 0.2286, 0.1808, 0.1460],\n",
      "          [0.2314, 0.1893, 0.1656, 0.2483, 0.1654]],\n",
      "\n",
      "         [[0.2267, 0.2033, 0.1380, 0.2446, 0.1875],\n",
      "          [0.1657, 0.2006, 0.2692, 0.1828, 0.1817],\n",
      "          [0.2723, 0.1989, 0.1245, 0.1878, 0.2165],\n",
      "          [0.2703, 0.1954, 0.1332, 0.2022, 0.1989]],\n",
      "\n",
      "         [[0.1464, 0.2478, 0.2305, 0.2277, 0.1476],\n",
      "          [0.2719, 0.1271, 0.1385, 0.1589, 0.3037],\n",
      "          [0.2006, 0.1820, 0.2059, 0.1765, 0.2349],\n",
      "          [0.2955, 0.0987, 0.1400, 0.1071, 0.3586]]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "ATT =  tensor([[[[-3.8087e-01, -4.8469e-01, -4.5335e-01,  2.5429e-01],\n",
      "          [-3.6809e-01, -4.8750e-01, -4.5082e-01,  2.3966e-01],\n",
      "          [-4.5904e-01, -3.9794e-01, -1.0171e-01,  5.3882e-02],\n",
      "          [-4.3301e-01, -4.0375e-01, -1.0648e-01,  4.0151e-02]],\n",
      "\n",
      "         [[-2.8565e-01, -1.5008e-01, -5.1969e-01, -6.9882e-01],\n",
      "          [-2.2357e-01, -2.1269e-01, -3.3815e-01, -6.0797e-01],\n",
      "          [-8.1091e-02, -1.7358e-01, -2.4057e-01, -5.3987e-01],\n",
      "          [-1.2543e-01, -2.1890e-01, -1.4148e-01, -4.8133e-01]],\n",
      "\n",
      "         [[ 7.8970e-01, -7.0596e-01, -9.9999e-02, -2.3735e-02],\n",
      "          [ 8.7500e-01, -6.9345e-01, -1.6377e-01,  3.2111e-02],\n",
      "          [ 7.2151e-01, -6.6057e-01, -3.2046e-02, -6.6277e-02],\n",
      "          [ 9.1990e-01, -6.7298e-01, -2.0879e-01,  9.0423e-02]],\n",
      "\n",
      "         [[ 4.5541e-01, -5.8301e-01, -3.8370e-02, -6.0344e-01],\n",
      "          [ 3.9450e-01, -8.0597e-01, -6.2294e-02, -5.9841e-01],\n",
      "          [ 5.3229e-01, -3.9931e-01,  3.2718e-02, -6.4734e-01],\n",
      "          [ 4.4445e-01, -8.0375e-01, -4.8565e-03, -6.9671e-01]]],\n",
      "\n",
      "\n",
      "        [[[-2.1225e-01, -5.6635e-01, -8.4531e-01,  3.4689e-01],\n",
      "          [-2.4455e-01, -5.6629e-01, -8.1583e-01,  3.5847e-01],\n",
      "          [-3.2998e-01, -5.7049e-01, -7.3242e-01,  3.9045e-01],\n",
      "          [-3.6561e-01, -5.8398e-01, -7.0546e-01,  4.1666e-01]],\n",
      "\n",
      "         [[-2.7089e-01, -1.5345e-01,  1.4304e-01, -5.0729e-01],\n",
      "          [-2.8066e-01, -2.2547e-01,  1.4664e-01, -5.1267e-01],\n",
      "          [-2.4226e-01, -1.7338e-01,  1.3013e-01, -5.3550e-01],\n",
      "          [-2.7552e-01, -1.4536e-01,  1.5936e-01, -4.8504e-01]],\n",
      "\n",
      "         [[ 9.9849e-01, -7.4653e-01,  2.6417e-02,  4.4348e-01],\n",
      "          [ 9.3738e-01, -7.4412e-01, -1.4528e-01,  7.0097e-01],\n",
      "          [ 9.8689e-01, -7.3552e-01,  2.3461e-04,  5.0365e-01],\n",
      "          [ 9.6280e-01, -7.4811e-01, -6.0913e-02,  5.7689e-01]],\n",
      "\n",
      "         [[-6.1494e-02, -8.8354e-01, -4.2378e-03, -6.4052e-01],\n",
      "          [-2.5980e-01, -6.0783e-01, -2.0349e-01, -5.5345e-01],\n",
      "          [-1.2748e-01, -8.1223e-01, -6.5599e-02, -6.0206e-01],\n",
      "          [-1.0213e-01, -7.9100e-01, -1.0381e-01, -7.0348e-01]]],\n",
      "\n",
      "\n",
      "        [[[-4.6264e-01, -5.6924e-01, -5.0342e-01,  2.3406e-01],\n",
      "          [-5.1526e-01, -4.7227e-01, -3.9010e-01,  2.6509e-01],\n",
      "          [-4.3556e-01, -5.9860e-01, -5.4462e-01,  2.1755e-01],\n",
      "          [-4.9815e-01, -5.5312e-01, -4.7592e-01,  2.4999e-01]],\n",
      "\n",
      "         [[-1.9607e-01, -9.3744e-02, -1.8947e-01, -5.0872e-01],\n",
      "          [-3.0699e-01, -5.5437e-02, -2.6048e-01, -4.6900e-01],\n",
      "          [-2.7179e-03, -1.8986e-01, -8.4521e-02, -5.4468e-01],\n",
      "          [-2.0372e-01, -1.1708e-01, -2.0044e-01, -5.0722e-01]],\n",
      "\n",
      "         [[ 8.4905e-01, -8.5118e-01, -6.8434e-02,  8.0340e-02],\n",
      "          [ 7.5310e-01, -8.3472e-01, -3.1517e-02,  7.4980e-02],\n",
      "          [ 7.9886e-01, -8.6246e-01, -7.4484e-02,  8.7022e-02],\n",
      "          [ 8.0985e-01, -8.5644e-01, -7.1636e-02,  7.7718e-02]],\n",
      "\n",
      "         [[ 3.2987e-01, -3.3150e-01, -2.0803e-01, -7.9589e-01],\n",
      "          [ 3.1324e-01, -4.9439e-01, -2.1655e-01, -7.9709e-01],\n",
      "          [ 3.1576e-01, -4.0759e-01, -2.1766e-01, -8.2041e-01],\n",
      "          [ 3.0364e-01, -5.3243e-01, -2.3154e-01, -8.3567e-01]]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n",
      "Attention output = \n",
      "torch.Size([3, 4, 4, 5]) tensor([[[[0.0991, 0.2989, 0.2405, 0.0905, 0.2710],\n",
      "          [0.1025, 0.2766, 0.2685, 0.1053, 0.2471],\n",
      "          [0.2049, 0.1794, 0.1271, 0.2912, 0.1974],\n",
      "          [0.2263, 0.1361, 0.1640, 0.3066, 0.1670]],\n",
      "\n",
      "         [[0.1426, 0.1425, 0.1779, 0.2590, 0.2780],\n",
      "          [0.1703, 0.2300, 0.2319, 0.1282, 0.2396],\n",
      "          [0.2530, 0.2453, 0.2276, 0.1358, 0.1384],\n",
      "          [0.2041, 0.3286, 0.3031, 0.0334, 0.1308]],\n",
      "\n",
      "         [[0.2308, 0.2553, 0.2433, 0.1159, 0.1548],\n",
      "          [0.2025, 0.1766, 0.1992, 0.2008, 0.2209],\n",
      "          [0.3122, 0.1803, 0.2959, 0.1055, 0.1061],\n",
      "          [0.1684, 0.1245, 0.1526, 0.3008, 0.2536]],\n",
      "\n",
      "         [[0.1334, 0.1611, 0.2318, 0.2733, 0.2004],\n",
      "          [0.1394, 0.2508, 0.3412, 0.1553, 0.1133],\n",
      "          [0.1544, 0.0917, 0.0907, 0.3312, 0.3319],\n",
      "          [0.2063, 0.2757, 0.2604, 0.0968, 0.1608]]],\n",
      "\n",
      "\n",
      "        [[[0.3196, 0.0917, 0.3196, 0.0392, 0.2298],\n",
      "          [0.3034, 0.0855, 0.3034, 0.0677, 0.2400],\n",
      "          [0.2517, 0.1166, 0.2517, 0.1497, 0.2303],\n",
      "          [0.2411, 0.1208, 0.2411, 0.1867, 0.2103]],\n",
      "\n",
      "         [[0.2234, 0.1787, 0.2234, 0.1712, 0.2034],\n",
      "          [0.2416, 0.1834, 0.2416, 0.1167, 0.2167],\n",
      "          [0.2065, 0.1861, 0.2065, 0.1638, 0.2370],\n",
      "          [0.2334, 0.1865, 0.2334, 0.1743, 0.1724]],\n",
      "\n",
      "         [[0.1983, 0.2028, 0.1983, 0.1527, 0.2479],\n",
      "          [0.1480, 0.1098, 0.1480, 0.4657, 0.1286],\n",
      "          [0.1952, 0.1963, 0.1952, 0.2217, 0.1917],\n",
      "          [0.1668, 0.1606, 0.1668, 0.3158, 0.1899]],\n",
      "\n",
      "         [[0.2116, 0.1385, 0.2116, 0.1321, 0.3061],\n",
      "          [0.1816, 0.1490, 0.1816, 0.3302, 0.1576],\n",
      "          [0.2083, 0.1342, 0.2083, 0.1900, 0.2592],\n",
      "          [0.2011, 0.1964, 0.2011, 0.1819, 0.2195]]],\n",
      "\n",
      "\n",
      "        [[[0.2793, 0.1755, 0.1854, 0.2026, 0.1572],\n",
      "          [0.1530, 0.2156, 0.1451, 0.3533, 0.1331],\n",
      "          [0.2960, 0.1686, 0.2164, 0.1345, 0.1845],\n",
      "          [0.2520, 0.1790, 0.1566, 0.2479, 0.1645]],\n",
      "\n",
      "         [[0.2192, 0.1910, 0.1685, 0.2316, 0.1897],\n",
      "          [0.2852, 0.1486, 0.1267, 0.2618, 0.1777],\n",
      "          [0.1598, 0.2847, 0.2286, 0.1808, 0.1460],\n",
      "          [0.2314, 0.1893, 0.1656, 0.2483, 0.1654]],\n",
      "\n",
      "         [[0.2267, 0.2033, 0.1380, 0.2446, 0.1875],\n",
      "          [0.1657, 0.2006, 0.2692, 0.1828, 0.1817],\n",
      "          [0.2723, 0.1989, 0.1245, 0.1878, 0.2165],\n",
      "          [0.2703, 0.1954, 0.1332, 0.2022, 0.1989]],\n",
      "\n",
      "         [[0.1464, 0.2478, 0.2305, 0.2277, 0.1476],\n",
      "          [0.2719, 0.1271, 0.1385, 0.1589, 0.3037],\n",
      "          [0.2006, 0.1820, 0.2059, 0.1765, 0.2349],\n",
      "          [0.2955, 0.0987, 0.1400, 0.1071, 0.3586]]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Cross attention in decoder_0\n",
      "tensor([[-3.8087e-01, -4.8469e-01, -4.5335e-01,  2.5429e-01, -2.8565e-01,\n",
      "         -1.5008e-01, -5.1969e-01, -6.9882e-01,  7.8970e-01, -7.0596e-01,\n",
      "         -9.9999e-02, -2.3735e-02,  4.5541e-01, -5.8301e-01, -3.8370e-02,\n",
      "         -6.0344e-01],\n",
      "        [-2.1225e-01, -5.6635e-01, -8.4531e-01,  3.4689e-01, -2.7089e-01,\n",
      "         -1.5345e-01,  1.4304e-01, -5.0729e-01,  9.9849e-01, -7.4653e-01,\n",
      "          2.6417e-02,  4.4348e-01, -6.1494e-02, -8.8354e-01, -4.2378e-03,\n",
      "         -6.4052e-01],\n",
      "        [-4.6264e-01, -5.6924e-01, -5.0342e-01,  2.3406e-01, -1.9607e-01,\n",
      "         -9.3744e-02, -1.8947e-01, -5.0872e-01,  8.4905e-01, -8.5118e-01,\n",
      "         -6.8434e-02,  8.0340e-02,  3.2987e-01, -3.3150e-01, -2.0803e-01,\n",
      "         -7.9589e-01],\n",
      "        [-3.6809e-01, -4.8750e-01, -4.5082e-01,  2.3966e-01, -2.2357e-01,\n",
      "         -2.1269e-01, -3.3815e-01, -6.0797e-01,  8.7500e-01, -6.9345e-01,\n",
      "         -1.6377e-01,  3.2111e-02,  3.9450e-01, -8.0597e-01, -6.2294e-02,\n",
      "         -5.9841e-01],\n",
      "        [-2.4455e-01, -5.6629e-01, -8.1583e-01,  3.5847e-01, -2.8066e-01,\n",
      "         -2.2547e-01,  1.4664e-01, -5.1267e-01,  9.3738e-01, -7.4412e-01,\n",
      "         -1.4528e-01,  7.0097e-01, -2.5980e-01, -6.0783e-01, -2.0349e-01,\n",
      "         -5.5345e-01],\n",
      "        [-5.1526e-01, -4.7227e-01, -3.9010e-01,  2.6509e-01, -3.0699e-01,\n",
      "         -5.5437e-02, -2.6048e-01, -4.6900e-01,  7.5310e-01, -8.3472e-01,\n",
      "         -3.1517e-02,  7.4980e-02,  3.1324e-01, -4.9439e-01, -2.1655e-01,\n",
      "         -7.9709e-01],\n",
      "        [-4.5904e-01, -3.9794e-01, -1.0171e-01,  5.3882e-02, -8.1091e-02,\n",
      "         -1.7358e-01, -2.4057e-01, -5.3987e-01,  7.2151e-01, -6.6057e-01,\n",
      "         -3.2046e-02, -6.6277e-02,  5.3229e-01, -3.9931e-01,  3.2718e-02,\n",
      "         -6.4734e-01],\n",
      "        [-3.2998e-01, -5.7049e-01, -7.3242e-01,  3.9045e-01, -2.4226e-01,\n",
      "         -1.7338e-01,  1.3013e-01, -5.3550e-01,  9.8689e-01, -7.3552e-01,\n",
      "          2.3461e-04,  5.0365e-01, -1.2748e-01, -8.1223e-01, -6.5599e-02,\n",
      "         -6.0206e-01],\n",
      "        [-4.3556e-01, -5.9860e-01, -5.4462e-01,  2.1755e-01, -2.7179e-03,\n",
      "         -1.8986e-01, -8.4521e-02, -5.4468e-01,  7.9886e-01, -8.6246e-01,\n",
      "         -7.4484e-02,  8.7022e-02,  3.1576e-01, -4.0759e-01, -2.1766e-01,\n",
      "         -8.2041e-01],\n",
      "        [-4.3301e-01, -4.0375e-01, -1.0648e-01,  4.0151e-02, -1.2543e-01,\n",
      "         -2.1890e-01, -1.4148e-01, -4.8133e-01,  9.1990e-01, -6.7298e-01,\n",
      "         -2.0879e-01,  9.0423e-02,  4.4445e-01, -8.0375e-01, -4.8565e-03,\n",
      "         -6.9671e-01],\n",
      "        [-3.6561e-01, -5.8398e-01, -7.0546e-01,  4.1666e-01, -2.7552e-01,\n",
      "         -1.4536e-01,  1.5936e-01, -4.8504e-01,  9.6280e-01, -7.4811e-01,\n",
      "         -6.0913e-02,  5.7689e-01, -1.0213e-01, -7.9100e-01, -1.0381e-01,\n",
      "         -7.0348e-01],\n",
      "        [-4.9815e-01, -5.5312e-01, -4.7592e-01,  2.4999e-01, -2.0372e-01,\n",
      "         -1.1708e-01, -2.0044e-01, -5.0722e-01,  8.0985e-01, -8.5644e-01,\n",
      "         -7.1636e-02,  7.7718e-02,  3.0364e-01, -5.3243e-01, -2.3154e-01,\n",
      "         -8.3567e-01]], grad_fn=<ViewBackward0>)\n",
      "\n",
      "tensor([[[-2.5833e-01,  3.2212e+00,  5.1332e-01, -2.3962e-01, -1.0613e+00,\n",
      "          -8.4086e-01,  7.0142e-01,  9.8474e-01, -5.1217e-01,  4.2041e-01,\n",
      "          -1.8794e+00, -1.2066e+00,  5.0933e-01, -9.9594e-01,  1.3189e+00,\n",
      "          -1.1162e+00],\n",
      "         [ 4.3665e-01,  9.0133e-01, -9.6815e-01,  1.0834e+00,  2.9870e-01,\n",
      "          -4.7124e-02, -1.4949e+00,  1.0231e+00, -1.9521e+00,  1.7808e-01,\n",
      "          -1.1310e+00, -4.0128e-01, -6.5593e-02, -6.9477e-01,  1.3386e+00,\n",
      "          -1.9410e+00],\n",
      "         [-1.4478e-01, -1.0916e+00, -8.4945e-02,  5.8056e-01,  6.8298e-01,\n",
      "           4.2983e-01, -4.7158e-03,  9.2044e-01, -1.1629e+00,  1.1905e+00,\n",
      "          -1.3356e+00,  2.1059e+00, -5.4268e-01, -1.3373e+00,  1.8583e+00,\n",
      "          -2.8027e+00]],\n",
      "\n",
      "        [[-3.5090e-01,  1.9832e+00,  6.9428e-01,  1.4975e+00, -1.1216e+00,\n",
      "          -1.1013e+00,  7.2208e-01,  1.0724e+00, -1.6593e+00,  6.9954e-01,\n",
      "          -1.2631e+00, -1.2783e+00, -2.1037e-02,  1.6641e-02,  9.9432e-04,\n",
      "          -8.3525e-01],\n",
      "         [-1.3040e+00,  6.0048e-01,  3.6489e-01,  1.0915e+00, -7.6539e-01,\n",
      "          -1.9044e+00, -3.5429e-02,  7.4244e-01, -2.7537e-01,  2.8864e+00,\n",
      "          -8.6273e-01, -1.2626e+00, -1.2302e+00, -4.7428e-01,  4.5568e-01,\n",
      "          -1.9226e+00],\n",
      "         [-4.5947e-01,  2.1156e+00,  6.2181e-01,  4.9067e-01, -8.4300e-01,\n",
      "          -6.3612e-01, -6.8401e-01,  1.9457e+00,  1.0192e-01, -8.5265e-02,\n",
      "          -2.0549e+00, -8.3388e-01,  7.0570e-01, -5.5334e-01,  1.1879e+00,\n",
      "          -1.8031e+00]],\n",
      "\n",
      "        [[-2.1997e+00,  1.0397e+00,  7.9738e-01, -2.0726e+00, -8.3756e-01,\n",
      "          -1.3103e+00,  1.8189e+00,  5.0057e-01,  8.3913e-01, -8.2471e-02,\n",
      "          -5.6722e-01,  1.2051e+00, -9.3613e-01,  3.0523e-01,  1.3150e+00,\n",
      "           7.4685e-01],\n",
      "         [-9.7942e-01,  9.9068e-01,  4.4449e-01,  2.0566e+00, -9.0621e-01,\n",
      "           3.8901e-01, -1.3045e-02,  1.3739e+00, -7.5564e-01,  5.1658e-01,\n",
      "          -1.1333e+00, -1.1669e+00, -1.2713e+00, -8.8247e-01, -6.4150e-01,\n",
      "          -1.2471e+00],\n",
      "         [-2.5064e+00, -8.1540e-01,  1.5253e+00,  6.1439e-01,  3.4637e-01,\n",
      "          -1.4219e+00, -8.4125e-01,  1.7544e+00,  6.3365e-02,  4.6309e-01,\n",
      "           4.9131e-02,  8.7985e-01, -1.0610e-01,  1.3862e+00, -7.4928e-01,\n",
      "          -1.4161e+00]],\n",
      "\n",
      "        [[-2.5127e+00, -3.1298e-01,  1.4567e+00,  5.2880e-01, -7.2896e-02,\n",
      "          -1.3713e+00, -6.9096e-01,  2.0578e+00, -2.5296e-01,  8.0366e-01,\n",
      "          -4.8165e-01,  8.0399e-01,  2.5870e-01,  1.5427e+00, -8.8096e-01,\n",
      "          -1.2967e+00],\n",
      "         [-1.1411e+00, -1.5864e-01,  7.4255e-02,  1.9718e+00,  9.7832e-01,\n",
      "           3.0855e-01,  3.6069e-01, -9.9292e-03, -2.1660e+00,  1.6647e+00,\n",
      "          -1.1506e+00,  5.9994e-01, -7.5785e-01, -2.3304e+00,  8.3408e-01,\n",
      "          -2.5099e+00],\n",
      "         [-8.9681e-01,  2.1653e-01,  1.0776e+00,  2.2151e+00, -9.3214e-01,\n",
      "          -1.1795e+00, -3.3349e-01,  1.6943e+00, -5.3266e-01,  4.1743e-01,\n",
      "          -8.2560e-01, -7.7776e-01,  6.3484e-02,  8.9149e-01, -4.5054e-01,\n",
      "          -1.6007e+00]]], grad_fn=<AddBackward0>)\n",
      "tensor([[[-0.2308,  3.2488,  0.5409, -0.2121, -1.0337, -0.8133,  0.7290,\n",
      "           1.0123, -0.4846,  0.4480, -1.8519, -1.1790,  0.5369, -0.9684,\n",
      "           1.3465, -1.0886],\n",
      "         [ 0.6514,  1.1161, -0.7534,  1.2982,  0.5135,  0.1676, -1.2802,\n",
      "           1.2379, -1.7373,  0.3928, -0.9163, -0.1865,  0.1492, -0.4800,\n",
      "           1.5534, -1.7262],\n",
      "         [-0.0986, -1.0455, -0.0388,  0.6267,  0.7292,  0.4760,  0.0415,\n",
      "           0.9666, -1.1168,  1.2367, -1.2894,  2.1521, -0.4965, -1.2911,\n",
      "           1.9044, -2.7565]],\n",
      "\n",
      "        [[-0.2919,  2.0422,  0.7533,  1.5565, -1.0626, -1.0423,  0.7811,\n",
      "           1.1314, -1.6002,  0.7585, -1.2041, -1.2193,  0.0380,  0.0757,\n",
      "           0.0600, -0.7762],\n",
      "         [-1.0605,  0.8440,  0.6084,  1.3350, -0.5219, -1.6609,  0.2080,\n",
      "           0.9859, -0.0319,  3.1298, -0.6193, -1.0191, -0.9867, -0.2308,\n",
      "           0.6992, -1.6791],\n",
      "         [-0.4105,  2.1646,  0.6708,  0.5397, -0.7940, -0.5871, -0.6350,\n",
      "           1.9947,  0.1509, -0.0363, -2.0059, -0.7849,  0.7547, -0.5044,\n",
      "           1.2369, -1.7541]],\n",
      "\n",
      "        [[-2.2348,  1.0046,  0.7622, -2.1077, -0.8727, -1.3454,  1.7838,\n",
      "           0.4654,  0.8040, -0.1176, -0.6024,  1.1700, -0.9713,  0.2701,\n",
      "           1.2799,  0.7117],\n",
      "         [-0.7778,  1.1923,  0.6461,  2.2582, -0.7046,  0.5906,  0.1886,\n",
      "           1.5755, -0.5540,  0.7182, -0.9317, -0.9653, -1.0697, -0.6809,\n",
      "          -0.4399, -1.0455],\n",
      "         [-2.4580, -0.7670,  1.5737,  0.6628,  0.3948, -1.3735, -0.7928,\n",
      "           1.8028,  0.1118,  0.5115,  0.0975,  0.9283, -0.0577,  1.4346,\n",
      "          -0.7009, -1.3677]],\n",
      "\n",
      "        [[-2.4864, -0.2867,  1.4830,  0.5551, -0.0466, -1.3450, -0.6647,\n",
      "           2.0841, -0.2267,  0.8300, -0.4553,  0.8303,  0.2850,  1.5690,\n",
      "          -0.8547, -1.2704],\n",
      "         [-0.9266,  0.0559,  0.2888,  2.1864,  1.1928,  0.5231,  0.5752,\n",
      "           0.2046, -1.9515,  1.8792, -0.9361,  0.8145, -0.5433, -2.1159,\n",
      "           1.0486, -2.2954],\n",
      "         [-0.8372,  0.2761,  1.1372,  2.2747, -0.8726, -1.1199, -0.2739,\n",
      "           1.7539, -0.4731,  0.4770, -0.7660, -0.7182,  0.1231,  0.9511,\n",
      "          -0.3909, -1.5411]]], grad_fn=<SubBackward0>)\n",
      "norm2(x' + mha(x', mem)), \n",
      " where x' = Decoder_curr_layer norm1(x + sa(x))\n",
      "tensor([[[-0.1896,  2.6687,  0.4443, -0.1742, -0.8491, -0.6681,  0.5988,\n",
      "           0.8316, -0.3981,  0.3680, -1.5212, -0.9685,  0.4410, -0.7955,\n",
      "           1.1061, -0.8943],\n",
      "         [ 0.6305,  1.0803, -0.7292,  1.2565,  0.4970,  0.1622, -1.2391,\n",
      "           1.1981, -1.6816,  0.3802, -0.8869, -0.1805,  0.1444, -0.4646,\n",
      "           1.5035, -1.6708],\n",
      "         [-0.0783, -0.8297, -0.0308,  0.4974,  0.5787,  0.3778,  0.0329,\n",
      "           0.7672, -0.8863,  0.9815, -1.0234,  1.7080, -0.3941, -1.0247,\n",
      "           1.5115, -2.1877]],\n",
      "\n",
      "        [[-0.2749,  1.9234,  0.7095,  1.4659, -1.0008, -0.9817,  0.7356,\n",
      "           1.0656, -1.5071,  0.7144, -1.1341, -1.1483,  0.0358,  0.0713,\n",
      "           0.0565, -0.7311],\n",
      "         [-0.8734,  0.6951,  0.5010,  1.0994, -0.4298, -1.3679,  0.1713,\n",
      "           0.8120, -0.0263,  2.5776, -0.5100, -0.8393, -0.8126, -0.1901,\n",
      "           0.5758, -1.3829],\n",
      "         [-0.3579,  1.8873,  0.5849,  0.4705, -0.6923, -0.5119, -0.5537,\n",
      "           1.7391,  0.1316, -0.0316, -1.7489, -0.6843,  0.6580, -0.4397,\n",
      "           1.0784, -1.5294]],\n",
      "\n",
      "        [[-1.8824,  0.8462,  0.6421, -1.7754, -0.7351, -1.1333,  1.5026,\n",
      "           0.3921,  0.6772, -0.0991, -0.5074,  0.9855, -0.8181,  0.2275,\n",
      "           1.0781,  0.5995],\n",
      "         [-0.7677,  1.1768,  0.6377,  2.2289, -0.6955,  0.5829,  0.1861,\n",
      "           1.5550, -0.5469,  0.7088, -0.9196, -0.9527, -1.0558, -0.6720,\n",
      "          -0.4342, -1.0319],\n",
      "         [-2.1437, -0.6689,  1.3725,  0.5780,  0.3443, -1.1979, -0.6915,\n",
      "           1.5723,  0.0975,  0.4461,  0.0851,  0.8095, -0.0503,  1.2511,\n",
      "          -0.6113, -1.1928]],\n",
      "\n",
      "        [[-2.1256, -0.2451,  1.2678,  0.4746, -0.0398, -1.1498, -0.5682,\n",
      "           1.7816, -0.1938,  0.7095, -0.3893,  0.7098,  0.2436,  1.3413,\n",
      "          -0.7306, -1.0860],\n",
      "         [-0.7028,  0.0424,  0.2190,  1.6582,  0.9047,  0.3967,  0.4363,\n",
      "           0.1552, -1.4801,  1.4253, -0.7100,  0.6177, -0.4121, -1.6048,\n",
      "           0.7953, -1.7409],\n",
      "         [-0.8025,  0.2647,  1.0901,  2.1804, -0.8364, -1.0735, -0.2626,\n",
      "           1.6812, -0.4535,  0.4573, -0.7343, -0.6884,  0.1180,  0.9117,\n",
      "          -0.3748, -1.4773]]], grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "\n",
      "mean =  tensor([[[-0.1680],\n",
      "         [-0.2093],\n",
      "         [-0.4187]],\n",
      "\n",
      "        [[-0.2555],\n",
      "         [-0.2919],\n",
      "         [-0.1679]],\n",
      "\n",
      "        [[-0.1004],\n",
      "         [-0.1838],\n",
      "         [-0.2336]],\n",
      "\n",
      "        [[-0.2572],\n",
      "         [-0.5717],\n",
      "         [-0.3214]]], grad_fn=<MeanBackward1>)\n",
      "std =  tensor([[[1.2019],\n",
      "         [1.1933],\n",
      "         [1.3001]],\n",
      "\n",
      "        [[1.0881],\n",
      "         [1.0077],\n",
      "         [1.2268]],\n",
      "\n",
      "        [[1.0707],\n",
      "         [1.0349],\n",
      "         [1.2610]],\n",
      "\n",
      "        [[1.2807],\n",
      "         [1.2564],\n",
      "         [1.2417]]], grad_fn=<StdBackward0>)\n",
      "tensor([[[-0.6389,  2.3677, -0.2469, -0.2830, -0.4513, -0.1870,  0.9664,\n",
      "           0.5793,  0.1116,  0.1752, -1.9849, -1.3458,  1.4495, -0.9397,\n",
      "           2.0279, -1.6004],\n",
      "         [-0.1889,  0.9722, -2.2346,  1.7350, -0.3887,  0.9548, -0.1664,\n",
      "           0.8758, -0.5687,  0.3908, -1.0443, -0.0619,  0.4761, -0.1947,\n",
      "           1.9527, -2.5090],\n",
      "         [-0.8452, -1.8343, -0.8386,  1.3082, -0.4643,  0.7655,  1.0499,\n",
      "           0.3371, -0.0445,  0.6443, -0.8189,  1.5836, -0.2716,  0.0707,\n",
      "           2.3857, -3.0275]],\n",
      "\n",
      "        [[-0.2572,  1.6794,  0.1795,  1.5257, -0.6673, -0.4704,  0.8902,\n",
      "           1.0075, -1.5950,  0.6580, -1.5816, -1.5856,  0.3042,  0.8250,\n",
      "           0.5540, -1.4666],\n",
      "         [-0.1447, -0.7195,  0.0931,  0.5377, -0.0194, -0.9815,  0.4051,\n",
      "           0.4223,  0.0551,  2.3089, -0.2896, -1.1204, -0.3527,  1.0504,\n",
      "           1.0254, -2.2701],\n",
      "         [-0.6196,  1.8374, -0.6570,  0.7332, -0.2281, -0.3928,  0.0953,\n",
      "           1.3347,  0.2659, -0.7079, -2.2943, -0.4644,  1.7486, -0.2192,\n",
      "           1.7970, -2.2288]],\n",
      "\n",
      "        [[-1.3403,  0.3987,  0.3479, -2.6046,  0.3868, -0.9330,  1.3171,\n",
      "           0.2103,  0.8271, -0.4820, -1.2758, -0.0130,  0.1258,  0.8474,\n",
      "           1.8049,  0.3828],\n",
      "         [-0.8183,  0.8178,  0.3155,  2.2650, -0.8871,  0.6916,  0.6969,\n",
      "           1.4846, -0.6231,  0.7733, -0.7520, -1.5349, -0.7991,  0.0447,\n",
      "          -0.2616, -1.4133],\n",
      "         [-0.8708, -1.1103,  1.0322,  1.0568,  1.6372, -1.0808, -0.8502,\n",
      "           0.9688, -1.2964, -0.5711, -0.7945,  0.7840,  0.0636,  3.0848,\n",
      "          -0.3646, -1.6885]],\n",
      "\n",
      "        [[-1.0091, -0.5163,  0.9428,  0.9953,  1.1982, -1.0392, -0.6397,\n",
      "           1.2434, -1.5660, -0.2821, -1.4814,  0.6665,  0.3186,  3.2561,\n",
      "          -0.5157, -1.5713],\n",
      "         [-1.5182, -1.3527, -0.0723,  2.2802, -0.3207,  1.5616,  1.3052,\n",
      "          -0.2022, -0.8648,  1.3542, -0.2271, -0.1627, -0.5121, -0.0476,\n",
      "           1.3849, -2.6059],\n",
      "         [ 0.1997, -0.3723,  0.5034,  2.5518, -0.1611, -0.7214, -0.3991,\n",
      "           1.1570, -1.1681,  0.1298, -1.2850, -0.7257,  0.3357,  2.6669,\n",
      "          -0.5747, -2.1369]]], grad_fn=<SubBackward0>)\n",
      "norm3(x'' + ff(x'')) \n",
      " where, x'' = Decoder_curr_layer norm2(x' + mha(x'))\n",
      "tensor([[[-0.5315,  1.9698, -0.2054, -0.2354, -0.3754, -0.1556,  0.8040,\n",
      "           0.4820,  0.0929,  0.1458, -1.6513, -1.1196,  1.2059, -0.7818,\n",
      "           1.6872, -1.3315],\n",
      "         [-0.1583,  0.8147, -1.8726,  1.4539, -0.3257,  0.8001, -0.1394,\n",
      "           0.7339, -0.4766,  0.3275, -0.8751, -0.0518,  0.3990, -0.1632,\n",
      "           1.6363, -2.1025],\n",
      "         [-0.6501, -1.4109, -0.6451,  1.0062, -0.3571,  0.5888,  0.8075,\n",
      "           0.2593, -0.0342,  0.4956, -0.6299,  1.2181, -0.2089,  0.0544,\n",
      "           1.8351, -2.3287]],\n",
      "\n",
      "        [[-0.2364,  1.5434,  0.1650,  1.4022, -0.6132, -0.4323,  0.8181,\n",
      "           0.9259, -1.4658,  0.6047, -1.4535, -1.4571,  0.2796,  0.7582,\n",
      "           0.5091, -1.3478],\n",
      "         [-0.1436, -0.7140,  0.0924,  0.5335, -0.0192, -0.9739,  0.4019,\n",
      "           0.4191,  0.0547,  2.2912, -0.2874, -1.1118, -0.3500,  1.0423,\n",
      "           1.0175, -2.2527],\n",
      "         [-0.5051,  1.4977, -0.5356,  0.5977, -0.1859, -0.3202,  0.0777,\n",
      "           1.0880,  0.2167, -0.5771, -1.8702, -0.3785,  1.4254, -0.1787,\n",
      "           1.4648, -1.8168]],\n",
      "\n",
      "        [[-1.2518,  0.3724,  0.3250, -2.4326,  0.3612, -0.8714,  1.2301,\n",
      "           0.1964,  0.7725, -0.4502, -1.1916, -0.0122,  0.1175,  0.7914,\n",
      "           1.6857,  0.3576],\n",
      "         [-0.7907,  0.7902,  0.3049,  2.1885, -0.8571,  0.6682,  0.6734,\n",
      "           1.4345, -0.6021,  0.7472, -0.7266, -1.4830, -0.7721,  0.0432,\n",
      "          -0.2528, -1.3656],\n",
      "         [-0.6906, -0.8806,  0.8186,  0.8381,  1.2984, -0.8572, -0.6743,\n",
      "           0.7683, -1.0281, -0.4529, -0.6301,  0.6217,  0.0505,  2.4464,\n",
      "          -0.2892, -1.3391]],\n",
      "\n",
      "        [[-0.7879, -0.4032,  0.7361,  0.7771,  0.9356, -0.8114, -0.4995,\n",
      "           0.9708, -1.2227, -0.2202, -1.1567,  0.5204,  0.2488,  2.5423,\n",
      "          -0.4027, -1.2268],\n",
      "         [-1.2083, -1.0766, -0.0576,  1.8148, -0.2552,  1.2429,  1.0388,\n",
      "          -0.1609, -0.6883,  1.0778, -0.1807, -0.1295, -0.4075, -0.0379,\n",
      "           1.1023, -2.0740],\n",
      "         [ 0.1608, -0.2998,  0.4054,  2.0551, -0.1297, -0.5810, -0.3214,\n",
      "           0.9318, -0.9407,  0.1045, -1.0349, -0.5844,  0.2703,  2.1478,\n",
      "          -0.4628, -1.7210]]], grad_fn=<AddBackward0>)\n",
      "\n",
      "torch.Size([4, 3, 16]) torch.Size([4, 3, 16])\n",
      "### Decoder Done ###\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "final_op = get_all_intermediate_outputs_mask(src_data, tgt_data[:-1, :], state_dict = state_dict1, num_heads = num_heads, num_encoder_layers = num_encoder_layers , num_decoder_layers = num_encoder_layers, d_model=d_model,  d_ff = d_ff, tgt_mask = tgt_mask)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.3537,  0.1155, -0.8124,  0.1555, -0.4748,  0.4457, -0.2639,\n",
       "          -0.9541,  0.6162, -0.5125,  0.1428, -0.2489,  0.0589,  1.2046,\n",
       "           0.8338, -0.5819, -0.0806,  0.8763, -0.4904,  0.7461],\n",
       "         [-0.0444,  0.0119, -0.6471,  0.3500, -0.9460, -0.2212, -0.6706,\n",
       "          -1.3310,  0.6262, -0.2693,  0.4500, -0.6924,  0.2379,  0.9995,\n",
       "           0.2602, -1.3019, -0.3170,  0.3038, -1.3511,  0.9221],\n",
       "         [-0.0196,  0.4328, -0.3184,  0.6117, -0.9117, -0.7116, -0.5982,\n",
       "          -0.4323,  0.8859,  0.9107,  1.1187, -0.2624, -0.2117,  0.5869,\n",
       "           0.0296, -0.7270, -0.5524,  0.0364, -1.0120,  0.4388]],\n",
       "\n",
       "        [[ 0.8691,  0.4473, -0.2262,  0.0048,  0.1786,  0.2042, -0.8962,\n",
       "          -0.8976,  0.3013, -0.4675,  0.1693, -0.7494,  0.1840,  0.6553,\n",
       "           0.5174, -0.9502, -0.2718,  1.3030, -1.2017,  0.8147],\n",
       "         [ 0.4970,  1.0755, -0.7045,  0.0839, -0.4611, -0.3945, -1.5203,\n",
       "          -0.1266,  0.5559,  0.3850,  1.2145, -0.3492,  0.0050,  0.7232,\n",
       "          -0.1804, -0.4222, -0.2748,  1.4072, -0.5783,  0.3465],\n",
       "         [ 0.1546,  0.5492, -0.7755,  0.2941, -0.5924,  0.0691, -0.2382,\n",
       "          -1.3224,  0.8436, -0.3913,  0.2699, -0.2313,  0.2467,  1.2080,\n",
       "           0.7139, -1.1096, -0.5625,  0.3462, -0.8604,  0.8659]],\n",
       "\n",
       "        [[-0.3538,  0.1999, -0.8290, -0.1853, -0.3900,  0.5394, -0.1235,\n",
       "          -0.0689,  0.5040,  1.0271, -0.0966,  0.1310, -1.1464,  0.0936,\n",
       "           0.5067,  0.3603,  0.2634,  0.4279,  0.6210, -0.0603],\n",
       "         [ 1.1585,  0.6900,  0.0755,  0.1040,  0.5914, -0.0102, -0.3975,\n",
       "           0.1138,  0.4404, -0.4944,  0.6497, -0.2051,  0.7330,  0.5275,\n",
       "           0.1297, -0.9539, -0.6907,  0.7425, -1.1543,  0.5814],\n",
       "         [-0.0804,  0.9181, -0.0079,  0.2537,  0.0390, -0.1308, -0.5177,\n",
       "          -0.7836, -0.0202,  0.9574,  0.2747,  0.0477,  0.3572, -0.7775,\n",
       "           0.0879, -1.0388, -0.7028,  0.3280, -0.8375,  0.3006]],\n",
       "\n",
       "        [[ 0.1131,  0.8539,  0.1072,  0.1939,  0.0062,  0.0726, -0.5509,\n",
       "          -0.8903,  0.1061,  0.8460,  0.1894, -0.1975,  0.3598, -0.7184,\n",
       "           0.1977, -1.0012, -0.6234,  0.5140, -0.9988,  0.5827],\n",
       "         [ 0.6809,  0.4067,  0.1350,  0.5242, -0.3161, -0.3942, -0.8301,\n",
       "          -0.0556,  0.2618,  0.4302,  1.1739,  0.0170,  0.4065,  0.2276,\n",
       "           0.0710, -0.7892, -0.5031,  0.3589, -1.3255,  0.4691],\n",
       "         [ 0.5947,  1.1280, -0.0572,  0.1176,  0.2648, -0.6308, -0.8504,\n",
       "          -0.7117,  0.2512,  0.1747,  0.3201, -0.1505,  0.3922,  0.0720,\n",
       "          -0.0701, -1.1456, -0.6929,  0.7546, -1.3188,  0.6159]]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# state_dict[\"transformer.encoder.layers.0.self_attn.out_proj.weight\"]\n",
    "\n",
    "final_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['src_embedding.weight', 'tgt_embedding.weight', 'transformer.encoder.layers.0.self_attn.in_proj_weight', 'transformer.encoder.layers.0.self_attn.in_proj_bias', 'transformer.encoder.layers.0.self_attn.out_proj.weight', 'transformer.encoder.layers.0.self_attn.out_proj.bias', 'transformer.encoder.layers.0.linear1.weight', 'transformer.encoder.layers.0.linear1.bias', 'transformer.encoder.layers.0.linear2.weight', 'transformer.encoder.layers.0.linear2.bias', 'transformer.encoder.layers.0.norm1.weight', 'transformer.encoder.layers.0.norm1.bias', 'transformer.encoder.layers.0.norm2.weight', 'transformer.encoder.layers.0.norm2.bias', 'transformer.encoder.norm.weight', 'transformer.encoder.norm.bias', 'transformer.decoder.layers.0.self_attn.in_proj_weight', 'transformer.decoder.layers.0.self_attn.in_proj_bias', 'transformer.decoder.layers.0.self_attn.out_proj.weight', 'transformer.decoder.layers.0.self_attn.out_proj.bias', 'transformer.decoder.layers.0.multihead_attn.in_proj_weight', 'transformer.decoder.layers.0.multihead_attn.in_proj_bias', 'transformer.decoder.layers.0.multihead_attn.out_proj.weight', 'transformer.decoder.layers.0.multihead_attn.out_proj.bias', 'transformer.decoder.layers.0.linear1.weight', 'transformer.decoder.layers.0.linear1.bias', 'transformer.decoder.layers.0.linear2.weight', 'transformer.decoder.layers.0.linear2.bias', 'transformer.decoder.layers.0.norm1.weight', 'transformer.decoder.layers.0.norm1.bias', 'transformer.decoder.layers.0.norm2.weight', 'transformer.decoder.layers.0.norm2.bias', 'transformer.decoder.layers.0.norm3.weight', 'transformer.decoder.layers.0.norm3.bias', 'transformer.decoder.norm.weight', 'transformer.decoder.norm.bias', 'fc.weight', 'fc.bias'])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "my_project_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
